2023-03-01 13:45:09,437 -                train: [    INFO] - Device: cuda:0
2023-03-01 13:45:11,797 -                train: [    INFO] - 
Epoch: 1/20
2023-03-01 13:45:12,029 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.6765 (0.6781) Acc D Real: 100.000% 
Loss D Fake: 0.7037 (0.7025) Acc D Fake: 0.000% 
Loss D: 1.380 
Loss G: 0.6817 (0.6829) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,037 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.6729 (0.6764) Acc D Real: 100.000% 
Loss D Fake: 0.7062 (0.7037) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.6793 (0.6817) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,045 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.6700 (0.6748) Acc D Real: 100.000% 
Loss D Fake: 0.7086 (0.7049) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.6769 (0.6805) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,061 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.6671 (0.6733) Acc D Real: 100.000% 
Loss D Fake: 0.7111 (0.7062) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6745 (0.6793) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,068 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.6662 (0.6721) Acc D Real: 100.000% 
Loss D Fake: 0.7136 (0.7074) Acc D Fake: 0.000% 
Loss D: 1.380 
Loss G: 0.6721 (0.6781) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,075 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.6615 (0.6706) Acc D Real: 100.000% 
Loss D Fake: 0.7161 (0.7086) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6697 (0.6769) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,082 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.6592 (0.6691) Acc D Real: 100.000% 
Loss D Fake: 0.7187 (0.7099) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6672 (0.6757) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,089 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.6570 (0.6678) Acc D Real: 100.000% 
Loss D Fake: 0.7214 (0.7112) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6646 (0.6744) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,096 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.6538 (0.6664) Acc D Real: 100.000% 
Loss D Fake: 0.7241 (0.7125) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6620 (0.6732) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,103 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.6512 (0.6650) Acc D Real: 100.000% 
Loss D Fake: 0.7269 (0.7138) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6593 (0.6719) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,110 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.6475 (0.6635) Acc D Real: 100.000% 
Loss D Fake: 0.7298 (0.7151) Acc D Fake: 0.000% 
Loss D: 1.377 
Loss G: 0.6566 (0.6707) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,116 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.6436 (0.6620) Acc D Real: 100.000% 
Loss D Fake: 0.7329 (0.7165) Acc D Fake: 0.000% 
Loss D: 1.376 
Loss G: 0.6538 (0.6694) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,123 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.6420 (0.6606) Acc D Real: 100.000% 
Loss D Fake: 0.7360 (0.7179) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6508 (0.6680) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,131 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.6386 (0.6591) Acc D Real: 100.000% 
Loss D Fake: 0.7393 (0.7193) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6478 (0.6667) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,138 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.6359 (0.6577) Acc D Real: 100.000% 
Loss D Fake: 0.7427 (0.7208) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.6446 (0.6653) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,146 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.6324 (0.6562) Acc D Real: 100.000% 
Loss D Fake: 0.7463 (0.7223) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.6413 (0.6639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,152 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.6280 (0.6546) Acc D Real: 100.000% 
Loss D Fake: 0.7501 (0.7238) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6379 (0.6624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,159 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.6253 (0.6531) Acc D Real: 100.000% 
Loss D Fake: 0.7541 (0.7254) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.6342 (0.6610) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,166 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.6198 (0.6514) Acc D Real: 100.000% 
Loss D Fake: 0.7584 (0.7271) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.6304 (0.6594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,173 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.6179 (0.6498) Acc D Real: 100.000% 
Loss D Fake: 0.7629 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.381 
Loss G: 0.6264 (0.6579) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,180 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.6136 (0.6482) Acc D Real: 100.000% 
Loss D Fake: 0.7677 (0.7305) Acc D Fake: 0.000% 
Loss D: 1.381 
Loss G: 0.6221 (0.6562) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,188 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.6109 (0.6465) Acc D Real: 100.000% 
Loss D Fake: 0.7729 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.6176 (0.6546) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,195 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.6035 (0.6448) Acc D Real: 100.000% 
Loss D Fake: 0.7784 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.382 
Loss G: 0.6127 (0.6528) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,202 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.5980 (0.6429) Acc D Real: 100.000% 
Loss D Fake: 0.7845 (0.7363) Acc D Fake: 0.000% 
Loss D: 1.382 
Loss G: 0.6076 (0.6510) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,209 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.5956 (0.6411) Acc D Real: 100.000% 
Loss D Fake: 0.7910 (0.7384) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6020 (0.6491) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,217 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.5883 (0.6391) Acc D Real: 100.000% 
Loss D Fake: 0.7982 (0.7406) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.5960 (0.6471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,224 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.5831 (0.6371) Acc D Real: 100.000% 
Loss D Fake: 0.8060 (0.7430) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.5895 (0.6451) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,231 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.5761 (0.6350) Acc D Real: 100.000% 
Loss D Fake: 0.8147 (0.7454) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.5824 (0.6429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,239 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.5680 (0.6328) Acc D Real: 100.000% 
Loss D Fake: 0.8243 (0.7481) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5746 (0.6406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,247 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.5624 (0.6305) Acc D Real: 100.000% 
Loss D Fake: 0.8352 (0.7509) Acc D Fake: 0.000% 
Loss D: 1.398 
Loss G: 0.5661 (0.6382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,254 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.5557 (0.6282) Acc D Real: 100.000% 
Loss D Fake: 0.8473 (0.7539) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.5568 (0.6357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,262 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.5431 (0.6256) Acc D Real: 100.000% 
Loss D Fake: 0.8608 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.404 
Loss G: 0.5466 (0.6330) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,270 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.5385 (0.6230) Acc D Real: 100.000% 
Loss D Fake: 0.8762 (0.7606) Acc D Fake: 0.000% 
Loss D: 1.415 
Loss G: 0.5355 (0.6301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,277 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.5230 (0.6202) Acc D Real: 100.000% 
Loss D Fake: 0.8935 (0.7644) Acc D Fake: 0.000% 
Loss D: 1.417 
Loss G: 0.5234 (0.6271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,285 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.5182 (0.6173) Acc D Real: 100.000% 
Loss D Fake: 0.9131 (0.7686) Acc D Fake: 0.000% 
Loss D: 1.431 
Loss G: 0.5105 (0.6238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,292 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.5097 (0.6144) Acc D Real: 100.000% 
Loss D Fake: 0.9344 (0.7730) Acc D Fake: 0.000% 
Loss D: 1.444 
Loss G: 0.4972 (0.6204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,299 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4966 (0.6113) Acc D Real: 100.000% 
Loss D Fake: 0.9570 (0.7779) Acc D Fake: 0.000% 
Loss D: 1.454 
Loss G: 0.4838 (0.6168) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,307 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4885 (0.6082) Acc D Real: 100.000% 
Loss D Fake: 0.9802 (0.7831) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4710 (0.6131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,314 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4779 (0.6049) Acc D Real: 100.000% 
Loss D Fake: 1.0030 (0.7886) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4592 (0.6092) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,322 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4785 (0.6018) Acc D Real: 100.000% 
Loss D Fake: 1.0240 (0.7943) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4492 (0.6053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,330 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4750 (0.5988) Acc D Real: 100.000% 
Loss D Fake: 1.0412 (0.8002) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4414 (0.6014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,337 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4691 (0.5958) Acc D Real: 100.000% 
Loss D Fake: 1.0543 (0.8061) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4357 (0.5976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,345 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4685 (0.5929) Acc D Real: 100.000% 
Loss D Fake: 1.0634 (0.8119) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.4318 (0.5938) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,352 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4699 (0.5902) Acc D Real: 100.000% 
Loss D Fake: 1.0687 (0.8176) Acc D Fake: 0.000% 
Loss D: 1.539 
Loss G: 0.4297 (0.5902) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,359 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4683 (0.5875) Acc D Real: 100.000% 
Loss D Fake: 1.0704 (0.8231) Acc D Fake: 0.000% 
Loss D: 1.539 
Loss G: 0.4290 (0.5867) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,367 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4793 (0.5852) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (0.8284) Acc D Fake: 0.000% 
Loss D: 1.548 
Loss G: 0.4298 (0.5833) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,376 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4775 (0.5830) Acc D Real: 100.000% 
Loss D Fake: 1.0646 (0.8333) Acc D Fake: 0.000% 
Loss D: 1.542 
Loss G: 0.4319 (0.5802) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,384 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4823 (0.5809) Acc D Real: 100.000% 
Loss D Fake: 1.0581 (0.8379) Acc D Fake: 0.000% 
Loss D: 1.540 
Loss G: 0.4349 (0.5772) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,393 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4826 (0.5790) Acc D Real: 100.000% 
Loss D Fake: 1.0504 (0.8421) Acc D Fake: 0.000% 
Loss D: 1.533 
Loss G: 0.4384 (0.5744) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,400 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4868 (0.5771) Acc D Real: 100.000% 
Loss D Fake: 1.0424 (0.8461) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4421 (0.5718) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,407 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4870 (0.5754) Acc D Real: 100.000% 
Loss D Fake: 1.0347 (0.8497) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4458 (0.5694) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,415 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4962 (0.5739) Acc D Real: 100.000% 
Loss D Fake: 1.0274 (0.8530) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4494 (0.5671) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,422 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4958 (0.5725) Acc D Real: 100.000% 
Loss D Fake: 1.0206 (0.8561) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4527 (0.5650) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,430 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4972 (0.5711) Acc D Real: 100.000% 
Loss D Fake: 1.0145 (0.8590) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4557 (0.5630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,437 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.5011 (0.5699) Acc D Real: 100.000% 
Loss D Fake: 1.0091 (0.8617) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4584 (0.5612) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,445 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.5001 (0.5686) Acc D Real: 100.000% 
Loss D Fake: 1.0043 (0.8642) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4608 (0.5594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,452 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.5027 (0.5675) Acc D Real: 100.000% 
Loss D Fake: 1.0000 (0.8665) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4630 (0.5577) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,460 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.5038 (0.5664) Acc D Real: 100.000% 
Loss D Fake: 0.9963 (0.8687) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4649 (0.5562) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,467 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.5031 (0.5654) Acc D Real: 100.000% 
Loss D Fake: 0.9931 (0.8708) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4666 (0.5547) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,474 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.5025 (0.5643) Acc D Real: 100.000% 
Loss D Fake: 0.9903 (0.8728) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4680 (0.5533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,481 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.5008 (0.5633) Acc D Real: 100.000% 
Loss D Fake: 0.9880 (0.8746) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4691 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,488 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.5041 (0.5624) Acc D Real: 100.000% 
Loss D Fake: 0.9861 (0.8764) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4701 (0.5506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,496 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.5073 (0.5615) Acc D Real: 100.000% 
Loss D Fake: 0.9844 (0.8781) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4709 (0.5494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,503 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.5065 (0.5607) Acc D Real: 100.000% 
Loss D Fake: 0.9830 (0.8797) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4717 (0.5482) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,510 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.5082 (0.5599) Acc D Real: 100.000% 
Loss D Fake: 0.9818 (0.8813) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4723 (0.5470) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,517 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.5031 (0.5590) Acc D Real: 100.000% 
Loss D Fake: 0.9807 (0.8827) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4728 (0.5459) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,525 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.5047 (0.5582) Acc D Real: 100.000% 
Loss D Fake: 0.9799 (0.8842) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4732 (0.5448) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,532 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.5051 (0.5574) Acc D Real: 100.000% 
Loss D Fake: 0.9793 (0.8855) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4735 (0.5438) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,539 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.5089 (0.5568) Acc D Real: 100.000% 
Loss D Fake: 0.9788 (0.8869) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4738 (0.5428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,546 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.5007 (0.5560) Acc D Real: 100.000% 
Loss D Fake: 0.9784 (0.8882) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4739 (0.5418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,553 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.5048 (0.5553) Acc D Real: 100.000% 
Loss D Fake: 0.9781 (0.8894) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4740 (0.5409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,560 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.5056 (0.5546) Acc D Real: 100.000% 
Loss D Fake: 0.9780 (0.8906) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4741 (0.5400) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,568 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.5033 (0.5539) Acc D Real: 100.000% 
Loss D Fake: 0.9779 (0.8918) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4741 (0.5391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,575 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.5013 (0.5532) Acc D Real: 100.000% 
Loss D Fake: 0.9780 (0.8930) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4740 (0.5382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,582 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.5002 (0.5525) Acc D Real: 100.000% 
Loss D Fake: 0.9781 (0.8941) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4738 (0.5374) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,589 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4990 (0.5518) Acc D Real: 100.000% 
Loss D Fake: 0.9784 (0.8952) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4736 (0.5365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,596 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4997 (0.5511) Acc D Real: 100.000% 
Loss D Fake: 0.9788 (0.8962) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4734 (0.5357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,603 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4974 (0.5504) Acc D Real: 100.000% 
Loss D Fake: 0.9793 (0.8973) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4730 (0.5349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,611 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.5001 (0.5498) Acc D Real: 100.000% 
Loss D Fake: 0.9799 (0.8983) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4727 (0.5342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,618 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4971 (0.5492) Acc D Real: 100.000% 
Loss D Fake: 0.9805 (0.8993) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4723 (0.5334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,625 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4940 (0.5485) Acc D Real: 100.000% 
Loss D Fake: 0.9812 (0.9003) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4719 (0.5326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,632 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4944 (0.5478) Acc D Real: 100.000% 
Loss D Fake: 0.9819 (0.9013) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4714 (0.5319) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,640 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4914 (0.5472) Acc D Real: 100.000% 
Loss D Fake: 0.9828 (0.9023) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4708 (0.5312) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,648 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4904 (0.5465) Acc D Real: 100.000% 
Loss D Fake: 0.9838 (0.9033) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4702 (0.5305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,656 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4913 (0.5459) Acc D Real: 100.000% 
Loss D Fake: 0.9849 (0.9042) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4696 (0.5298) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,665 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4907 (0.5452) Acc D Real: 100.000% 
Loss D Fake: 0.9860 (0.9051) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4689 (0.5291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,673 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4872 (0.5446) Acc D Real: 100.000% 
Loss D Fake: 0.9872 (0.9061) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4682 (0.5284) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,681 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4871 (0.5439) Acc D Real: 100.000% 
Loss D Fake: 0.9884 (0.9070) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4674 (0.5277) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,688 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4834 (0.5432) Acc D Real: 100.000% 
Loss D Fake: 0.9898 (0.9079) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4666 (0.5270) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,696 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4828 (0.5426) Acc D Real: 100.000% 
Loss D Fake: 0.9912 (0.9088) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4658 (0.5263) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,704 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4814 (0.5419) Acc D Real: 100.000% 
Loss D Fake: 0.9928 (0.9097) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4649 (0.5257) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,712 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4792 (0.5412) Acc D Real: 100.000% 
Loss D Fake: 0.9944 (0.9107) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4639 (0.5250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,720 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4768 (0.5406) Acc D Real: 100.000% 
Loss D Fake: 0.9962 (0.9116) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4629 (0.5243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,728 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4764 (0.5399) Acc D Real: 100.000% 
Loss D Fake: 0.9981 (0.9125) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4618 (0.5237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,736 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4737 (0.5392) Acc D Real: 100.000% 
Loss D Fake: 1.0001 (0.9134) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4606 (0.5230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,744 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4717 (0.5385) Acc D Real: 100.000% 
Loss D Fake: 1.0022 (0.9143) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4594 (0.5224) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,751 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4681 (0.5378) Acc D Real: 100.000% 
Loss D Fake: 1.0045 (0.9152) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4580 (0.5217) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,759 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4667 (0.5371) Acc D Real: 100.000% 
Loss D Fake: 1.0070 (0.9162) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4566 (0.5210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,766 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4634 (0.5363) Acc D Real: 100.000% 
Loss D Fake: 1.0096 (0.9171) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4551 (0.5204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,774 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4595 (0.5356) Acc D Real: 100.000% 
Loss D Fake: 1.0126 (0.9180) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4534 (0.5197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,782 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4561 (0.5348) Acc D Real: 100.000% 
Loss D Fake: 1.0159 (0.9190) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4515 (0.5191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,789 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4523 (0.5340) Acc D Real: 100.000% 
Loss D Fake: 1.0195 (0.9200) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4494 (0.5184) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,796 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4481 (0.5332) Acc D Real: 100.000% 
Loss D Fake: 1.0237 (0.9210) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4471 (0.5177) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,804 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4434 (0.5323) Acc D Real: 100.000% 
Loss D Fake: 1.0284 (0.9220) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4444 (0.5170) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,811 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4380 (0.5314) Acc D Real: 100.000% 
Loss D Fake: 1.0339 (0.9230) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4413 (0.5163) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,819 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4315 (0.5305) Acc D Real: 100.000% 
Loss D Fake: 1.0404 (0.9241) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4377 (0.5155) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,826 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4240 (0.5295) Acc D Real: 100.000% 
Loss D Fake: 1.0483 (0.9253) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4333 (0.5148) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,833 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4132 (0.5284) Acc D Real: 100.000% 
Loss D Fake: 1.0581 (0.9265) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4278 (0.5140) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,840 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4012 (0.5273) Acc D Real: 100.000% 
Loss D Fake: 1.0712 (0.9278) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4205 (0.5131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,848 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3886 (0.5260) Acc D Real: 100.000% 
Loss D Fake: 1.0894 (0.9293) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4108 (0.5122) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,855 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3592 (0.5245) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (0.9309) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.3970 (0.5112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,863 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3281 (0.5228) Acc D Real: 100.000% 
Loss D Fake: 1.1579 (0.9330) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.3794 (0.5100) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,870 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2830 (0.5207) Acc D Real: 100.000% 
Loss D Fake: 1.2039 (0.9353) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.3751 (0.5088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,877 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.2713 (0.5185) Acc D Real: 100.000% 
Loss D Fake: 1.1779 (0.9374) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.3916 (0.5078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,885 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.2659 (0.5163) Acc D Real: 100.000% 
Loss D Fake: 1.1221 (0.9390) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.4119 (0.5070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,892 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2657 (0.5142) Acc D Real: 100.000% 
Loss D Fake: 1.0823 (0.9403) Acc D Fake: 0.000% 
Loss D: 1.348 
Loss G: 0.4252 (0.5063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,900 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.2669 (0.5121) Acc D Real: 100.000% 
Loss D Fake: 1.0631 (0.9413) Acc D Fake: 0.000% 
Loss D: 1.330 
Loss G: 0.4302 (0.5056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,907 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.2426 (0.5098) Acc D Real: 100.000% 
Loss D Fake: 1.0594 (0.9423) Acc D Fake: 0.000% 
Loss D: 1.302 
Loss G: 0.4308 (0.5050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,914 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2247 (0.5075) Acc D Real: 100.000% 
Loss D Fake: 1.0596 (0.9433) Acc D Fake: 0.000% 
Loss D: 1.284 
Loss G: 0.4327 (0.5044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,921 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.1999 (0.5049) Acc D Real: 100.000% 
Loss D Fake: 1.0494 (0.9441) Acc D Fake: 0.000% 
Loss D: 1.249 
Loss G: 0.4423 (0.5039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,929 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.2101 (0.5025) Acc D Real: 100.000% 
Loss D Fake: 1.0258 (0.9448) Acc D Fake: 0.000% 
Loss D: 1.236 
Loss G: 0.4540 (0.5035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,936 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.1978 (0.5000) Acc D Real: 100.000% 
Loss D Fake: 1.0045 (0.9453) Acc D Fake: 0.000% 
Loss D: 1.202 
Loss G: 0.4649 (0.5032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,944 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.1950 (0.4976) Acc D Real: 100.000% 
Loss D Fake: 0.9882 (0.9456) Acc D Fake: 0.000% 
Loss D: 1.183 
Loss G: 0.4714 (0.5029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,951 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.1844 (0.4951) Acc D Real: 100.000% 
Loss D Fake: 0.9801 (0.9459) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.4755 (0.5027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,958 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.1730 (0.4925) Acc D Real: 100.000% 
Loss D Fake: 0.9732 (0.9461) Acc D Fake: 0.000% 
Loss D: 1.146 
Loss G: 0.4806 (0.5025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,965 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.1649 (0.4899) Acc D Real: 100.000% 
Loss D Fake: 0.9637 (0.9463) Acc D Fake: 0.000% 
Loss D: 1.129 
Loss G: 0.4873 (0.5024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,973 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.1592 (0.4873) Acc D Real: 100.000% 
Loss D Fake: 0.9522 (0.9463) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.4945 (0.5023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,980 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.1529 (0.4848) Acc D Real: 100.000% 
Loss D Fake: 0.9405 (0.9463) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.5023 (0.5023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,988 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.1390 (0.4821) Acc D Real: 100.000% 
Loss D Fake: 0.9274 (0.9461) Acc D Fake: 0.000% 
Loss D: 1.066 
Loss G: 0.5119 (0.5024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:12,995 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1435 (0.4795) Acc D Real: 100.000% 
Loss D Fake: 0.9122 (0.9459) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.5217 (0.5026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,003 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1453 (0.4770) Acc D Real: 100.000% 
Loss D Fake: 0.8985 (0.9455) Acc D Fake: 0.000% 
Loss D: 1.044 
Loss G: 0.5304 (0.5028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,011 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.1673 (0.4746) Acc D Real: 100.000% 
Loss D Fake: 0.8879 (0.9451) Acc D Fake: 0.000% 
Loss D: 1.055 
Loss G: 0.5362 (0.5030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,018 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.1654 (0.4723) Acc D Real: 100.000% 
Loss D Fake: 0.8824 (0.9446) Acc D Fake: 0.000% 
Loss D: 1.048 
Loss G: 0.5392 (0.5033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,025 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.1489 (0.4699) Acc D Real: 100.000% 
Loss D Fake: 0.8821 (0.9442) Acc D Fake: 0.000% 
Loss D: 1.031 
Loss G: 0.5375 (0.5036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,032 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.1411 (0.4675) Acc D Real: 100.000% 
Loss D Fake: 0.8890 (0.9437) Acc D Fake: 0.000% 
Loss D: 1.030 
Loss G: 0.5343 (0.5038) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,040 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1232 (0.4650) Acc D Real: 100.000% 
Loss D Fake: 0.8948 (0.9434) Acc D Fake: 0.000% 
Loss D: 1.018 
Loss G: 0.5355 (0.5040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,048 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1303 (0.4626) Acc D Real: 100.000% 
Loss D Fake: 0.8895 (0.9430) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.5437 (0.5043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,055 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.1151 (0.4601) Acc D Real: 100.000% 
Loss D Fake: 0.8733 (0.9425) Acc D Fake: 0.000% 
Loss D: 0.988 
Loss G: 0.5560 (0.5047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,063 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.1204 (0.4577) Acc D Real: 100.000% 
Loss D Fake: 0.8549 (0.9419) Acc D Fake: 0.000% 
Loss D: 0.975 
Loss G: 0.5680 (0.5051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,070 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.1327 (0.4554) Acc D Real: 100.000% 
Loss D Fake: 0.8421 (0.9412) Acc D Fake: 0.000% 
Loss D: 0.975 
Loss G: 0.5735 (0.5056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,078 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1204 (0.4530) Acc D Real: 100.000% 
Loss D Fake: 0.8384 (0.9404) Acc D Fake: 0.000% 
Loss D: 0.959 
Loss G: 0.5775 (0.5061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,085 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.1300 (0.4507) Acc D Real: 100.000% 
Loss D Fake: 0.8353 (0.9397) Acc D Fake: 0.000% 
Loss D: 0.965 
Loss G: 0.5800 (0.5066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,092 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1439 (0.4486) Acc D Real: 100.000% 
Loss D Fake: 0.8352 (0.9390) Acc D Fake: 0.000% 
Loss D: 0.979 
Loss G: 0.5818 (0.5072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:13,099 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.1196 (0.4463) Acc D Real: 100.000% 
Loss D Fake: 0.8354 (0.9383) Acc D Fake: 0.011% 
Loss D: 0.955 
Loss G: 0.5846 (0.5077) Acc G: 99.977% 
LR: 2.000e-04 

2023-03-01 13:45:13,107 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.1920 (0.4446) Acc D Real: 100.000% 
Loss D Fake: 2.6019 (0.9497) Acc D Fake: 0.011% 
Loss D: 2.794 
Loss G: 0.5854 (0.5082) Acc G: 99.943% 
LR: 2.000e-04 

2023-03-01 13:45:13,114 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1199 (0.4424) Acc D Real: 100.000% 
Loss D Fake: 0.8083 (0.9487) Acc D Fake: 0.045% 
Loss D: 0.928 
Loss G: 0.6092 (0.5089) Acc G: 99.929% 
LR: 2.000e-04 

2023-03-01 13:45:13,121 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.2744 (0.4413) Acc D Real: 100.000% 
Loss D Fake: 0.7851 (0.9476) Acc D Fake: 0.068% 
Loss D: 1.060 
Loss G: 0.6212 (0.5097) Acc G: 99.896% 
LR: 2.000e-04 

2023-03-01 13:45:13,128 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3250 (0.4405) Acc D Real: 100.000% 
Loss D Fake: 0.7725 (0.9464) Acc D Fake: 0.101% 
Loss D: 1.097 
Loss G: 0.6288 (0.5105) Acc G: 99.863% 
LR: 2.000e-04 

2023-03-01 13:45:13,135 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4457 (0.4405) Acc D Real: 100.000% 
Loss D Fake: 0.7645 (0.9452) Acc D Fake: 0.133% 
Loss D: 1.210 
Loss G: 0.6341 (0.5113) Acc G: 99.831% 
LR: 2.000e-04 

2023-03-01 13:45:13,143 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.5235 (0.4411) Acc D Real: 100.000% 
Loss D Fake: 0.7591 (0.9440) Acc D Fake: 0.173% 
Loss D: 1.283 
Loss G: 0.6376 (0.5121) Acc G: 99.788% 
LR: 2.000e-04 

2023-03-01 13:45:13,150 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.5660 (0.4419) Acc D Real: 100.000% 
Loss D Fake: 0.7555 (0.9427) Acc D Fake: 0.216% 
Loss D: 1.322 
Loss G: 0.6400 (0.5130) Acc G: 99.745% 
LR: 2.000e-04 

2023-03-01 13:45:13,157 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.5659 (0.4427) Acc D Real: 100.000% 
Loss D Fake: 0.7533 (0.9415) Acc D Fake: 0.258% 
Loss D: 1.319 
Loss G: 0.6415 (0.5138) Acc G: 99.703% 
LR: 2.000e-04 

2023-03-01 13:45:13,165 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.5823 (0.4436) Acc D Real: 100.000% 
Loss D Fake: 0.7520 (0.9403) Acc D Fake: 0.300% 
Loss D: 1.334 
Loss G: 0.6423 (0.5146) Acc G: 99.662% 
LR: 2.000e-04 

2023-03-01 13:45:13,174 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.5624 (0.4444) Acc D Real: 100.000% 
Loss D Fake: 0.7515 (0.9390) Acc D Fake: 0.341% 
Loss D: 1.314 
Loss G: 0.6425 (0.5155) Acc G: 99.621% 
LR: 2.000e-04 

2023-03-01 13:45:13,183 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.5848 (0.4453) Acc D Real: 100.000% 
Loss D Fake: 0.7516 (0.9378) Acc D Fake: 0.382% 
Loss D: 1.336 
Loss G: 0.6421 (0.5163) Acc G: 99.581% 
LR: 2.000e-04 

2023-03-01 13:45:13,192 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.5860 (0.4462) Acc D Real: 100.000% 
Loss D Fake: 0.7525 (0.9367) Acc D Fake: 0.411% 
Loss D: 1.339 
Loss G: 0.6412 (0.5171) Acc G: 99.552% 
LR: 2.000e-04 

2023-03-01 13:45:13,201 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.5742 (0.4470) Acc D Real: 99.997% 
Loss D Fake: 0.7540 (0.9355) Acc D Fake: 0.440% 
Loss D: 1.328 
Loss G: 0.6398 (0.5178) Acc G: 99.534% 
LR: 2.000e-04 

2023-03-01 13:45:13,208 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.5795 (0.4478) Acc D Real: 99.997% 
Loss D Fake: 0.7563 (0.9344) Acc D Fake: 0.448% 
Loss D: 1.336 
Loss G: 0.6377 (0.5186) Acc G: 99.536% 
LR: 2.000e-04 

2023-03-01 13:45:13,215 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.5898 (0.4487) Acc D Real: 99.997% 
Loss D Fake: 0.7595 (0.9333) Acc D Fake: 0.445% 
Loss D: 1.349 
Loss G: 0.6347 (0.5193) Acc G: 99.539% 
LR: 2.000e-04 

2023-03-01 13:45:13,223 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.5862 (0.4495) Acc D Real: 99.995% 
Loss D Fake: 0.7639 (0.9322) Acc D Fake: 0.442% 
Loss D: 1.350 
Loss G: 0.6308 (0.5200) Acc G: 99.542% 
LR: 2.000e-04 

2023-03-01 13:45:13,231 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.5902 (0.4504) Acc D Real: 99.995% 
Loss D Fake: 0.7697 (0.9312) Acc D Fake: 0.439% 
Loss D: 1.360 
Loss G: 0.6257 (0.5207) Acc G: 99.545% 
LR: 2.000e-04 

2023-03-01 13:45:13,239 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.5896 (0.4513) Acc D Real: 99.995% 
Loss D Fake: 0.7771 (0.9303) Acc D Fake: 0.437% 
Loss D: 1.367 
Loss G: 0.6193 (0.5213) Acc G: 99.548% 
LR: 2.000e-04 

2023-03-01 13:45:13,248 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.5886 (0.4521) Acc D Real: 99.993% 
Loss D Fake: 0.7860 (0.9294) Acc D Fake: 0.434% 
Loss D: 1.375 
Loss G: 0.6116 (0.5218) Acc G: 99.551% 
LR: 2.000e-04 

2023-03-01 13:45:13,256 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.5894 (0.4529) Acc D Real: 99.993% 
Loss D Fake: 0.7963 (0.9286) Acc D Fake: 0.432% 
Loss D: 1.386 
Loss G: 0.6030 (0.5223) Acc G: 99.553% 
LR: 2.000e-04 

2023-03-01 13:45:13,264 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.5791 (0.4537) Acc D Real: 99.993% 
Loss D Fake: 0.8076 (0.9279) Acc D Fake: 0.429% 
Loss D: 1.387 
Loss G: 0.5937 (0.5228) Acc G: 99.556% 
LR: 2.000e-04 

2023-03-01 13:45:13,273 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.5878 (0.4545) Acc D Real: 99.993% 
Loss D Fake: 0.8196 (0.9272) Acc D Fake: 0.426% 
Loss D: 1.407 
Loss G: 0.5841 (0.5231) Acc G: 99.559% 
LR: 2.000e-04 

2023-03-01 13:45:13,281 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.5875 (0.4553) Acc D Real: 99.993% 
Loss D Fake: 0.8322 (0.9267) Acc D Fake: 0.424% 
Loss D: 1.420 
Loss G: 0.5743 (0.5234) Acc G: 99.561% 
LR: 2.000e-04 

2023-03-01 13:45:13,289 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.5849 (0.4561) Acc D Real: 99.993% 
Loss D Fake: 0.8451 (0.9262) Acc D Fake: 0.421% 
Loss D: 1.430 
Loss G: 0.5646 (0.5237) Acc G: 99.564% 
LR: 2.000e-04 

2023-03-01 13:45:13,297 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.5909 (0.4568) Acc D Real: 99.993% 
Loss D Fake: 0.8583 (0.9258) Acc D Fake: 0.419% 
Loss D: 1.449 
Loss G: 0.5549 (0.5239) Acc G: 99.566% 
LR: 2.000e-04 

2023-03-01 13:45:13,305 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.5839 (0.4576) Acc D Real: 99.993% 
Loss D Fake: 0.8718 (0.9255) Acc D Fake: 0.416% 
Loss D: 1.456 
Loss G: 0.5454 (0.5240) Acc G: 99.569% 
LR: 2.000e-04 

2023-03-01 13:45:13,313 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.5751 (0.4583) Acc D Real: 99.993% 
Loss D Fake: 0.8857 (0.9252) Acc D Fake: 0.414% 
Loss D: 1.461 
Loss G: 0.5360 (0.5240) Acc G: 99.572% 
LR: 2.000e-04 

2023-03-01 13:45:13,321 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.5818 (0.4590) Acc D Real: 99.993% 
Loss D Fake: 0.9002 (0.9251) Acc D Fake: 0.412% 
Loss D: 1.482 
Loss G: 0.5265 (0.5241) Acc G: 99.574% 
LR: 2.000e-04 

2023-03-01 13:45:13,328 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.5907 (0.4597) Acc D Real: 99.993% 
Loss D Fake: 0.9155 (0.9250) Acc D Fake: 0.409% 
Loss D: 1.506 
Loss G: 0.5170 (0.5240) Acc G: 99.576% 
LR: 2.000e-04 

2023-03-01 13:45:13,336 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.5867 (0.4605) Acc D Real: 99.993% 
Loss D Fake: 0.9323 (0.9251) Acc D Fake: 0.407% 
Loss D: 1.519 
Loss G: 0.5070 (0.5239) Acc G: 99.579% 
LR: 2.000e-04 

2023-03-01 13:45:13,343 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.5856 (0.4612) Acc D Real: 99.993% 
Loss D Fake: 0.9514 (0.9252) Acc D Fake: 0.405% 
Loss D: 1.537 
Loss G: 0.4964 (0.5238) Acc G: 99.581% 
LR: 2.000e-04 

2023-03-01 13:45:13,351 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.5769 (0.4618) Acc D Real: 99.994% 
Loss D Fake: 0.9742 (0.9255) Acc D Fake: 0.402% 
Loss D: 1.551 
Loss G: 0.4846 (0.5235) Acc G: 99.584% 
LR: 2.000e-04 

2023-03-01 13:45:13,358 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.5837 (0.4625) Acc D Real: 99.994% 
Loss D Fake: 1.0030 (0.9259) Acc D Fake: 0.400% 
Loss D: 1.587 
Loss G: 0.4710 (0.5232) Acc G: 99.586% 
LR: 2.000e-04 

2023-03-01 13:45:13,366 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.5854 (0.4632) Acc D Real: 99.994% 
Loss D Fake: 1.0416 (0.9266) Acc D Fake: 0.398% 
Loss D: 1.627 
Loss G: 0.4552 (0.5229) Acc G: 99.588% 
LR: 2.000e-04 

2023-03-01 13:45:13,373 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.5740 (0.4638) Acc D Real: 99.994% 
Loss D Fake: 1.0941 (0.9275) Acc D Fake: 0.396% 
Loss D: 1.668 
Loss G: 0.4384 (0.5224) Acc G: 99.591% 
LR: 2.000e-04 

2023-03-01 13:45:13,380 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.5738 (0.4644) Acc D Real: 99.993% 
Loss D Fake: 1.1671 (0.9288) Acc D Fake: 0.393% 
Loss D: 1.741 
Loss G: 0.4290 (0.5219) Acc G: 99.593% 
LR: 2.000e-04 

2023-03-01 13:45:13,387 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.5883 (0.4651) Acc D Real: 99.993% 
Loss D Fake: 1.1610 (0.9301) Acc D Fake: 0.391% 
Loss D: 1.749 
Loss G: 0.4294 (0.5214) Acc G: 99.595% 
LR: 2.000e-04 

2023-03-01 13:45:13,394 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.5812 (0.4657) Acc D Real: 99.993% 
Loss D Fake: 1.1389 (0.9313) Acc D Fake: 0.389% 
Loss D: 1.720 
Loss G: 0.4328 (0.5209) Acc G: 99.597% 
LR: 2.000e-04 

2023-03-01 13:45:13,402 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.5758 (0.4663) Acc D Real: 99.993% 
Loss D Fake: 1.1237 (0.9323) Acc D Fake: 0.387% 
Loss D: 1.700 
Loss G: 0.4369 (0.5204) Acc G: 99.599% 
LR: 2.000e-04 

2023-03-01 13:45:13,409 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.5904 (0.4670) Acc D Real: 99.993% 
Loss D Fake: 1.1111 (0.9333) Acc D Fake: 0.385% 
Loss D: 1.701 
Loss G: 0.4411 (0.5200) Acc G: 99.602% 
LR: 2.000e-04 

2023-03-01 13:45:13,416 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.5875 (0.4677) Acc D Real: 99.993% 
Loss D Fake: 1.1007 (0.9342) Acc D Fake: 0.383% 
Loss D: 1.688 
Loss G: 0.4448 (0.5196) Acc G: 99.604% 
LR: 2.000e-04 

2023-03-01 13:45:13,423 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.5910 (0.4683) Acc D Real: 99.993% 
Loss D Fake: 1.0930 (0.9350) Acc D Fake: 0.381% 
Loss D: 1.684 
Loss G: 0.4480 (0.5192) Acc G: 99.606% 
LR: 2.000e-04 

2023-03-01 13:45:13,431 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.5940 (0.4690) Acc D Real: 99.993% 
Loss D Fake: 1.0879 (0.9358) Acc D Fake: 0.379% 
Loss D: 1.682 
Loss G: 0.4503 (0.5189) Acc G: 99.608% 
LR: 2.000e-04 

2023-03-01 13:45:13,438 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.5869 (0.4696) Acc D Real: 99.993% 
Loss D Fake: 1.0855 (0.9366) Acc D Fake: 0.377% 
Loss D: 1.672 
Loss G: 0.4518 (0.5185) Acc G: 99.610% 
LR: 2.000e-04 

2023-03-01 13:45:13,445 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.5901 (0.4702) Acc D Real: 99.993% 
Loss D Fake: 1.0860 (0.9374) Acc D Fake: 0.375% 
Loss D: 1.676 
Loss G: 0.4525 (0.5182) Acc G: 99.612% 
LR: 2.000e-04 

2023-03-01 13:45:13,452 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.5891 (0.4709) Acc D Real: 99.993% 
Loss D Fake: 1.0890 (0.9382) Acc D Fake: 0.381% 
Loss D: 1.678 
Loss G: 0.4523 (0.5178) Acc G: 99.614% 
LR: 2.000e-04 

2023-03-01 13:45:13,459 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.5954 (0.4715) Acc D Real: 99.993% 
Loss D Fake: 1.0948 (0.9390) Acc D Fake: 0.397% 
Loss D: 1.690 
Loss G: 0.4513 (0.5175) Acc G: 99.607% 
LR: 2.000e-04 

2023-03-01 13:45:13,467 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.5961 (0.4722) Acc D Real: 99.993% 
Loss D Fake: 1.1037 (0.9399) Acc D Fake: 0.421% 
Loss D: 1.700 
Loss G: 0.4494 (0.5171) Acc G: 99.592% 
LR: 2.000e-04 

2023-03-01 13:45:13,474 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.5891 (0.4728) Acc D Real: 99.993% 
Loss D Fake: 1.1159 (0.9408) Acc D Fake: 0.453% 
Loss D: 1.705 
Loss G: 0.4465 (0.5167) Acc G: 99.569% 
LR: 2.000e-04 

2023-03-01 13:45:13,481 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.5966 (0.4734) Acc D Real: 99.993% 
Loss D Fake: 1.1316 (0.9418) Acc D Fake: 0.493% 
Loss D: 1.728 
Loss G: 0.4426 (0.5164) Acc G: 99.519% 
LR: 2.000e-04 

2023-03-01 13:45:13,488 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.5925 (0.4740) Acc D Real: 99.993% 
Loss D Fake: 1.1518 (0.9428) Acc D Fake: 0.525% 
Loss D: 1.744 
Loss G: 0.4377 (0.5160) Acc G: 99.471% 
LR: 2.000e-04 

2023-03-01 13:45:13,496 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.5961 (0.4746) Acc D Real: 99.993% 
Loss D Fake: 1.1764 (0.9440) Acc D Fake: 0.556% 
Loss D: 1.772 
Loss G: 0.4316 (0.5155) Acc G: 99.423% 
LR: 2.000e-04 

2023-03-01 13:45:13,503 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.5971 (0.4752) Acc D Real: 99.993% 
Loss D Fake: 1.2050 (0.9453) Acc D Fake: 0.587% 
Loss D: 1.802 
Loss G: 0.4247 (0.5151) Acc G: 99.384% 
LR: 2.000e-04 

2023-03-01 13:45:13,510 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.6017 (0.4759) Acc D Real: 99.993% 
Loss D Fake: 1.2363 (0.9468) Acc D Fake: 0.615% 
Loss D: 1.838 
Loss G: 0.4171 (0.5146) Acc G: 99.337% 
LR: 2.000e-04 

2023-03-01 13:45:13,517 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.5984 (0.4765) Acc D Real: 99.993% 
Loss D Fake: 1.2679 (0.9484) Acc D Fake: 0.645% 
Loss D: 1.866 
Loss G: 0.4094 (0.5141) Acc G: 99.290% 
LR: 2.000e-04 

2023-03-01 13:45:13,524 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.5996 (0.4771) Acc D Real: 99.993% 
Loss D Fake: 1.2968 (0.9501) Acc D Fake: 0.675% 
Loss D: 1.896 
Loss G: 0.4021 (0.5135) Acc G: 99.252% 
LR: 2.000e-04 

2023-03-01 13:45:13,532 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.6007 (0.4777) Acc D Real: 99.993% 
Loss D Fake: 1.3197 (0.9520) Acc D Fake: 0.704% 
Loss D: 1.920 
Loss G: 0.3960 (0.5129) Acc G: 99.214% 
LR: 2.000e-04 

2023-03-01 13:45:13,539 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.6020 (0.4783) Acc D Real: 99.993% 
Loss D Fake: 1.3331 (0.9538) Acc D Fake: 0.726% 
Loss D: 1.935 
Loss G: 0.3922 (0.5123) Acc G: 99.185% 
LR: 2.000e-04 

2023-03-01 13:45:13,546 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.6026 (0.4789) Acc D Real: 99.993% 
Loss D Fake: 1.3329 (0.9557) Acc D Fake: 0.747% 
Loss D: 1.935 
Loss G: 0.3923 (0.5117) Acc G: 99.157% 
LR: 2.000e-04 

2023-03-01 13:45:13,553 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.6016 (0.4795) Acc D Real: 99.993% 
Loss D Fake: 1.3150 (0.9574) Acc D Fake: 0.767% 
Loss D: 1.917 
Loss G: 0.3975 (0.5112) Acc G: 99.128% 
LR: 2.000e-04 

2023-03-01 13:45:13,560 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.6086 (0.4802) Acc D Real: 99.993% 
Loss D Fake: 1.2792 (0.9590) Acc D Fake: 0.795% 
Loss D: 1.888 
Loss G: 0.4082 (0.5107) Acc G: 99.084% 
LR: 2.000e-04 

2023-03-01 13:45:13,567 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.6087 (0.4808) Acc D Real: 99.993% 
Loss D Fake: 1.2317 (0.9603) Acc D Fake: 0.832% 
Loss D: 1.840 
Loss G: 0.4222 (0.5103) Acc G: 99.032% 
LR: 2.000e-04 

2023-03-01 13:45:13,575 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.6085 (0.4814) Acc D Real: 99.993% 
Loss D Fake: 1.1827 (0.9614) Acc D Fake: 0.884% 
Loss D: 1.791 
Loss G: 0.4365 (0.5099) Acc G: 98.981% 
LR: 2.000e-04 

2023-03-01 13:45:13,582 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.6104 (0.4820) Acc D Real: 99.994% 
Loss D Fake: 1.1395 (0.9622) Acc D Fake: 0.935% 
Loss D: 1.750 
Loss G: 0.4492 (0.5096) Acc G: 98.938% 
LR: 2.000e-04 

2023-03-01 13:45:13,589 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.6102 (0.4826) Acc D Real: 99.994% 
Loss D Fake: 1.1046 (0.9629) Acc D Fake: 0.986% 
Loss D: 1.715 
Loss G: 0.4594 (0.5094) Acc G: 98.887% 
LR: 2.000e-04 

2023-03-01 13:45:13,596 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.6131 (0.4832) Acc D Real: 99.994% 
Loss D Fake: 1.0773 (0.9635) Acc D Fake: 1.037% 
Loss D: 1.690 
Loss G: 0.4674 (0.5092) Acc G: 98.837% 
LR: 2.000e-04 

2023-03-01 13:45:13,603 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.6127 (0.4839) Acc D Real: 99.994% 
Loss D Fake: 1.0562 (0.9639) Acc D Fake: 1.079% 
Loss D: 1.669 
Loss G: 0.4736 (0.5090) Acc G: 98.788% 
LR: 2.000e-04 

2023-03-01 13:45:13,610 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.6131 (0.4845) Acc D Real: 99.994% 
Loss D Fake: 1.0398 (0.9643) Acc D Fake: 1.129% 
Loss D: 1.653 
Loss G: 0.4786 (0.5089) Acc G: 98.746% 
LR: 2.000e-04 

2023-03-01 13:45:13,617 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.6161 (0.4851) Acc D Real: 99.994% 
Loss D Fake: 1.0268 (0.9646) Acc D Fake: 1.170% 
Loss D: 1.643 
Loss G: 0.4825 (0.5087) Acc G: 98.698% 
LR: 2.000e-04 

2023-03-01 13:45:13,625 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.6130 (0.4857) Acc D Real: 99.994% 
Loss D Fake: 1.0164 (0.9648) Acc D Fake: 1.211% 
Loss D: 1.629 
Loss G: 0.4858 (0.5086) Acc G: 98.665% 
LR: 2.000e-04 

2023-03-01 13:45:13,632 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.6128 (0.4863) Acc D Real: 99.994% 
Loss D Fake: 1.0079 (0.9650) Acc D Fake: 1.244% 
Loss D: 1.621 
Loss G: 0.4886 (0.5085) Acc G: 98.625% 
LR: 2.000e-04 

2023-03-01 13:45:13,639 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.6178 (0.4869) Acc D Real: 99.994% 
Loss D Fake: 1.0007 (0.9652) Acc D Fake: 1.285% 
Loss D: 1.619 
Loss G: 0.4910 (0.5085) Acc G: 98.585% 
LR: 2.000e-04 

2023-03-01 13:45:13,646 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.6147 (0.4875) Acc D Real: 99.994% 
Loss D Fake: 0.9946 (0.9653) Acc D Fake: 1.325% 
Loss D: 1.609 
Loss G: 0.4932 (0.5084) Acc G: 98.546% 
LR: 2.000e-04 

2023-03-01 13:45:13,653 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.6151 (0.4880) Acc D Real: 99.994% 
Loss D Fake: 0.9892 (0.9654) Acc D Fake: 1.364% 
Loss D: 1.604 
Loss G: 0.4952 (0.5083) Acc G: 98.507% 
LR: 2.000e-04 

2023-03-01 13:45:13,660 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.6163 (0.4886) Acc D Real: 99.994% 
Loss D Fake: 0.9844 (0.9655) Acc D Fake: 1.404% 
Loss D: 1.601 
Loss G: 0.4970 (0.5083) Acc G: 98.468% 
LR: 2.000e-04 

2023-03-01 13:45:13,667 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.6156 (0.4892) Acc D Real: 99.994% 
Loss D Fake: 0.9801 (0.9656) Acc D Fake: 1.443% 
Loss D: 1.596 
Loss G: 0.4987 (0.5082) Acc G: 98.430% 
LR: 2.000e-04 

2023-03-01 13:45:13,675 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.6129 (0.4898) Acc D Real: 99.994% 
Loss D Fake: 0.9761 (0.9656) Acc D Fake: 1.481% 
Loss D: 1.589 
Loss G: 0.5003 (0.5082) Acc G: 98.392% 
LR: 2.000e-04 

2023-03-01 13:45:13,682 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.6143 (0.4903) Acc D Real: 99.994% 
Loss D Fake: 0.9724 (0.9656) Acc D Fake: 1.519% 
Loss D: 1.587 
Loss G: 0.5018 (0.5082) Acc G: 98.354% 
LR: 2.000e-04 

2023-03-01 13:45:13,689 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.6154 (0.4909) Acc D Real: 99.994% 
Loss D Fake: 0.9689 (0.9656) Acc D Fake: 1.557% 
Loss D: 1.584 
Loss G: 0.5033 (0.5081) Acc G: 98.324% 
LR: 2.000e-04 

2023-03-01 13:45:13,697 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.6167 (0.4914) Acc D Real: 99.994% 
Loss D Fake: 0.9656 (0.9656) Acc D Fake: 1.587% 
Loss D: 1.582 
Loss G: 0.5047 (0.5081) Acc G: 98.295% 
LR: 2.000e-04 

2023-03-01 13:45:13,705 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.6147 (0.4920) Acc D Real: 99.994% 
Loss D Fake: 0.9624 (0.9656) Acc D Fake: 1.595% 
Loss D: 1.577 
Loss G: 0.5061 (0.5081) Acc G: 98.287% 
LR: 2.000e-04 

2023-03-01 13:45:13,717 -                train: [    INFO] - Best Loss 100000000000.000 to 0.983
2023-03-01 13:45:13,717 -                train: [    INFO] - 
Epoch: 2/20
2023-03-01 13:45:13,929 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.6175 (0.6167) Acc D Real: 100.000% 
Loss D Fake: 0.9565 (0.9579) Acc D Fake: 8.333% 
Loss D: 1.574 
Loss G: 0.5087 (0.5081) Acc G: 91.667% 
LR: 2.000e-04 

2023-03-01 13:45:13,936 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.6154 (0.6163) Acc D Real: 100.000% 
Loss D Fake: 0.9536 (0.9565) Acc D Fake: 7.778% 
Loss D: 1.569 
Loss G: 0.5100 (0.5087) Acc G: 92.222% 
LR: 2.000e-04 

2023-03-01 13:45:13,943 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.6172 (0.6165) Acc D Real: 100.000% 
Loss D Fake: 0.9509 (0.9551) Acc D Fake: 7.500% 
Loss D: 1.568 
Loss G: 0.5113 (0.5094) Acc G: 92.500% 
LR: 2.000e-04 

2023-03-01 13:45:13,960 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.6176 (0.6167) Acc D Real: 100.000% 
Loss D Fake: 0.9482 (0.9537) Acc D Fake: 7.333% 
Loss D: 1.566 
Loss G: 0.5126 (0.5100) Acc G: 92.667% 
LR: 2.000e-04 

2023-03-01 13:45:13,967 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.6179 (0.6169) Acc D Real: 100.000% 
Loss D Fake: 0.9456 (0.9523) Acc D Fake: 7.222% 
Loss D: 1.563 
Loss G: 0.5138 (0.5106) Acc G: 92.778% 
LR: 2.000e-04 

2023-03-01 13:45:13,974 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.6182 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9430 (0.9510) Acc D Fake: 7.143% 
Loss D: 1.561 
Loss G: 0.5150 (0.5113) Acc G: 92.857% 
LR: 2.000e-04 

2023-03-01 13:45:13,981 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.6167 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9404 (0.9497) Acc D Fake: 7.083% 
Loss D: 1.557 
Loss G: 0.5162 (0.5119) Acc G: 92.917% 
LR: 2.000e-04 

2023-03-01 13:45:13,988 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.6170 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9380 (0.9484) Acc D Fake: 7.037% 
Loss D: 1.555 
Loss G: 0.5174 (0.5125) Acc G: 92.963% 
LR: 2.000e-04 

2023-03-01 13:45:13,995 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.6178 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9355 (0.9471) Acc D Fake: 7.000% 
Loss D: 1.553 
Loss G: 0.5186 (0.5131) Acc G: 93.000% 
LR: 2.000e-04 

2023-03-01 13:45:14,002 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.6168 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9331 (0.9458) Acc D Fake: 6.970% 
Loss D: 1.550 
Loss G: 0.5198 (0.5137) Acc G: 93.030% 
LR: 2.000e-04 

2023-03-01 13:45:14,009 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.6188 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9308 (0.9446) Acc D Fake: 6.944% 
Loss D: 1.550 
Loss G: 0.5210 (0.5143) Acc G: 93.056% 
LR: 2.000e-04 

2023-03-01 13:45:14,016 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.6176 (0.6173) Acc D Real: 100.000% 
Loss D Fake: 0.9284 (0.9433) Acc D Fake: 6.923% 
Loss D: 1.546 
Loss G: 0.5221 (0.5149) Acc G: 93.077% 
LR: 2.000e-04 

2023-03-01 13:45:14,023 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.6172 (0.6173) Acc D Real: 100.000% 
Loss D Fake: 0.9261 (0.9421) Acc D Fake: 6.905% 
Loss D: 1.543 
Loss G: 0.5233 (0.5155) Acc G: 93.095% 
LR: 2.000e-04 

2023-03-01 13:45:14,029 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.6166 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9239 (0.9409) Acc D Fake: 6.889% 
Loss D: 1.540 
Loss G: 0.5244 (0.5161) Acc G: 93.111% 
LR: 2.000e-04 

2023-03-01 13:45:14,036 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.6173 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9216 (0.9397) Acc D Fake: 6.875% 
Loss D: 1.539 
Loss G: 0.5256 (0.5167) Acc G: 93.125% 
LR: 2.000e-04 

2023-03-01 13:45:14,043 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.6165 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9194 (0.9385) Acc D Fake: 6.863% 
Loss D: 1.536 
Loss G: 0.5267 (0.5173) Acc G: 93.137% 
LR: 2.000e-04 

2023-03-01 13:45:14,050 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.6164 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9173 (0.9373) Acc D Fake: 6.852% 
Loss D: 1.534 
Loss G: 0.5278 (0.5179) Acc G: 93.148% 
LR: 2.000e-04 

2023-03-01 13:45:14,056 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.6181 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9151 (0.9361) Acc D Fake: 6.842% 
Loss D: 1.533 
Loss G: 0.5289 (0.5185) Acc G: 93.158% 
LR: 2.000e-04 

2023-03-01 13:45:14,063 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.6182 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9130 (0.9350) Acc D Fake: 6.833% 
Loss D: 1.531 
Loss G: 0.5300 (0.5190) Acc G: 93.167% 
LR: 2.000e-04 

2023-03-01 13:45:14,070 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.6156 (0.6172) Acc D Real: 100.000% 
Loss D Fake: 0.9109 (0.9338) Acc D Fake: 6.825% 
Loss D: 1.527 
Loss G: 0.5311 (0.5196) Acc G: 93.175% 
LR: 2.000e-04 

2023-03-01 13:45:14,077 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.6168 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9089 (0.9327) Acc D Fake: 6.742% 
Loss D: 1.526 
Loss G: 0.5322 (0.5202) Acc G: 93.258% 
LR: 2.000e-04 

2023-03-01 13:45:14,084 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.6155 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9068 (0.9316) Acc D Fake: 6.667% 
Loss D: 1.522 
Loss G: 0.5333 (0.5208) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-01 13:45:14,091 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.6168 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.9048 (0.9305) Acc D Fake: 6.597% 
Loss D: 1.522 
Loss G: 0.5344 (0.5213) Acc G: 93.403% 
LR: 2.000e-04 

2023-03-01 13:45:14,097 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.6164 (0.6170) Acc D Real: 100.000% 
Loss D Fake: 0.9028 (0.9294) Acc D Fake: 6.533% 
Loss D: 1.519 
Loss G: 0.5355 (0.5219) Acc G: 93.467% 
LR: 2.000e-04 

2023-03-01 13:45:14,104 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.6172 (0.6170) Acc D Real: 100.000% 
Loss D Fake: 0.9009 (0.9283) Acc D Fake: 6.474% 
Loss D: 1.518 
Loss G: 0.5365 (0.5225) Acc G: 93.526% 
LR: 2.000e-04 

2023-03-01 13:45:14,111 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.6181 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.8989 (0.9272) Acc D Fake: 6.420% 
Loss D: 1.517 
Loss G: 0.5376 (0.5230) Acc G: 93.580% 
LR: 2.000e-04 

2023-03-01 13:45:14,118 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.6165 (0.6171) Acc D Real: 100.000% 
Loss D Fake: 0.8970 (0.9261) Acc D Fake: 6.310% 
Loss D: 1.514 
Loss G: 0.5386 (0.5236) Acc G: 93.690% 
LR: 2.000e-04 

2023-03-01 13:45:14,126 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.6152 (0.6170) Acc D Real: 100.000% 
Loss D Fake: 0.8951 (0.9250) Acc D Fake: 6.149% 
Loss D: 1.510 
Loss G: 0.5397 (0.5241) Acc G: 93.851% 
LR: 2.000e-04 

2023-03-01 13:45:14,133 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.6146 (0.6169) Acc D Real: 100.000% 
Loss D Fake: 0.8932 (0.9240) Acc D Fake: 6.000% 
Loss D: 1.508 
Loss G: 0.5407 (0.5247) Acc G: 94.000% 
LR: 2.000e-04 

2023-03-01 13:45:14,140 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.6165 (0.6169) Acc D Real: 100.000% 
Loss D Fake: 0.8914 (0.9229) Acc D Fake: 5.860% 
Loss D: 1.508 
Loss G: 0.5418 (0.5252) Acc G: 94.140% 
LR: 2.000e-04 

2023-03-01 13:45:14,148 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.6159 (0.6169) Acc D Real: 100.000% 
Loss D Fake: 0.8895 (0.9219) Acc D Fake: 5.729% 
Loss D: 1.505 
Loss G: 0.5428 (0.5258) Acc G: 94.271% 
LR: 2.000e-04 

2023-03-01 13:45:14,156 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.6162 (0.6168) Acc D Real: 100.000% 
Loss D Fake: 0.8877 (0.9208) Acc D Fake: 5.606% 
Loss D: 1.504 
Loss G: 0.5438 (0.5263) Acc G: 94.394% 
LR: 2.000e-04 

2023-03-01 13:45:14,163 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.6145 (0.6168) Acc D Real: 100.000% 
Loss D Fake: 0.8859 (0.9198) Acc D Fake: 5.490% 
Loss D: 1.500 
Loss G: 0.5448 (0.5269) Acc G: 94.510% 
LR: 2.000e-04 

2023-03-01 13:45:14,171 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.6138 (0.6167) Acc D Real: 100.000% 
Loss D Fake: 0.8842 (0.9188) Acc D Fake: 5.333% 
Loss D: 1.498 
Loss G: 0.5458 (0.5274) Acc G: 94.667% 
LR: 2.000e-04 

2023-03-01 13:45:14,178 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.6157 (0.6167) Acc D Real: 100.000% 
Loss D Fake: 0.8824 (0.9178) Acc D Fake: 5.185% 
Loss D: 1.498 
Loss G: 0.5468 (0.5280) Acc G: 94.815% 
LR: 2.000e-04 

2023-03-01 13:45:14,185 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.6162 (0.6167) Acc D Real: 100.000% 
Loss D Fake: 0.8807 (0.9168) Acc D Fake: 5.045% 
Loss D: 1.497 
Loss G: 0.5478 (0.5285) Acc G: 94.955% 
LR: 2.000e-04 

2023-03-01 13:45:14,193 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.6148 (0.6166) Acc D Real: 100.000% 
Loss D Fake: 0.8790 (0.9158) Acc D Fake: 4.912% 
Loss D: 1.494 
Loss G: 0.5488 (0.5290) Acc G: 95.088% 
LR: 2.000e-04 

2023-03-01 13:45:14,201 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.6163 (0.6166) Acc D Real: 100.000% 
Loss D Fake: 0.8773 (0.9148) Acc D Fake: 4.786% 
Loss D: 1.494 
Loss G: 0.5498 (0.5296) Acc G: 95.214% 
LR: 2.000e-04 

2023-03-01 13:45:14,210 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.6154 (0.6166) Acc D Real: 100.000% 
Loss D Fake: 0.8756 (0.9138) Acc D Fake: 4.667% 
Loss D: 1.491 
Loss G: 0.5508 (0.5301) Acc G: 95.333% 
LR: 2.000e-04 

2023-03-01 13:45:14,218 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.6134 (0.6165) Acc D Real: 100.000% 
Loss D Fake: 0.8739 (0.9128) Acc D Fake: 4.553% 
Loss D: 1.487 
Loss G: 0.5518 (0.5306) Acc G: 95.447% 
LR: 2.000e-04 

2023-03-01 13:45:14,226 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.6152 (0.6165) Acc D Real: 100.000% 
Loss D Fake: 0.8722 (0.9119) Acc D Fake: 4.444% 
Loss D: 1.487 
Loss G: 0.5528 (0.5311) Acc G: 95.556% 
LR: 2.000e-04 

2023-03-01 13:45:14,234 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.6145 (0.6164) Acc D Real: 100.000% 
Loss D Fake: 0.8706 (0.9109) Acc D Fake: 4.341% 
Loss D: 1.485 
Loss G: 0.5537 (0.5317) Acc G: 95.659% 
LR: 2.000e-04 

2023-03-01 13:45:14,242 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.6131 (0.6163) Acc D Real: 100.000% 
Loss D Fake: 0.8690 (0.9100) Acc D Fake: 4.242% 
Loss D: 1.482 
Loss G: 0.5547 (0.5322) Acc G: 95.758% 
LR: 2.000e-04 

2023-03-01 13:45:14,251 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.6141 (0.6163) Acc D Real: 100.000% 
Loss D Fake: 0.8674 (0.9090) Acc D Fake: 4.148% 
Loss D: 1.482 
Loss G: 0.5557 (0.5327) Acc G: 95.852% 
LR: 2.000e-04 

2023-03-01 13:45:14,259 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.6133 (0.6162) Acc D Real: 100.000% 
Loss D Fake: 0.8658 (0.9081) Acc D Fake: 4.058% 
Loss D: 1.479 
Loss G: 0.5566 (0.5332) Acc G: 95.942% 
LR: 2.000e-04 

2023-03-01 13:45:14,266 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.6134 (0.6162) Acc D Real: 100.000% 
Loss D Fake: 0.8642 (0.9071) Acc D Fake: 3.972% 
Loss D: 1.478 
Loss G: 0.5576 (0.5338) Acc G: 96.028% 
LR: 2.000e-04 

2023-03-01 13:45:14,274 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.6136 (0.6161) Acc D Real: 100.000% 
Loss D Fake: 0.8627 (0.9062) Acc D Fake: 3.889% 
Loss D: 1.476 
Loss G: 0.5585 (0.5343) Acc G: 96.111% 
LR: 2.000e-04 

2023-03-01 13:45:14,281 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.6137 (0.6161) Acc D Real: 100.000% 
Loss D Fake: 0.8611 (0.9053) Acc D Fake: 3.810% 
Loss D: 1.475 
Loss G: 0.5595 (0.5348) Acc G: 96.190% 
LR: 2.000e-04 

2023-03-01 13:45:14,289 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.6126 (0.6160) Acc D Real: 100.000% 
Loss D Fake: 0.8596 (0.9044) Acc D Fake: 3.733% 
Loss D: 1.472 
Loss G: 0.5604 (0.5353) Acc G: 96.267% 
LR: 2.000e-04 

2023-03-01 13:45:14,296 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.6135 (0.6159) Acc D Real: 100.000% 
Loss D Fake: 0.8581 (0.9035) Acc D Fake: 3.660% 
Loss D: 1.472 
Loss G: 0.5613 (0.5358) Acc G: 96.340% 
LR: 2.000e-04 

2023-03-01 13:45:14,303 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.6126 (0.6159) Acc D Real: 100.000% 
Loss D Fake: 0.8566 (0.9026) Acc D Fake: 3.590% 
Loss D: 1.469 
Loss G: 0.5623 (0.5363) Acc G: 96.410% 
LR: 2.000e-04 

2023-03-01 13:45:14,312 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.6118 (0.6158) Acc D Real: 100.000% 
Loss D Fake: 0.8551 (0.9017) Acc D Fake: 3.522% 
Loss D: 1.467 
Loss G: 0.5632 (0.5368) Acc G: 96.478% 
LR: 2.000e-04 

2023-03-01 13:45:14,319 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.6123 (0.6157) Acc D Real: 100.000% 
Loss D Fake: 0.8536 (0.9008) Acc D Fake: 3.457% 
Loss D: 1.466 
Loss G: 0.5641 (0.5373) Acc G: 96.543% 
LR: 2.000e-04 

2023-03-01 13:45:14,327 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.6126 (0.6157) Acc D Real: 100.000% 
Loss D Fake: 0.8521 (0.8999) Acc D Fake: 3.394% 
Loss D: 1.465 
Loss G: 0.5651 (0.5378) Acc G: 96.606% 
LR: 2.000e-04 

2023-03-01 13:45:14,334 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.6120 (0.6156) Acc D Real: 100.000% 
Loss D Fake: 0.8507 (0.8990) Acc D Fake: 3.333% 
Loss D: 1.463 
Loss G: 0.5660 (0.5383) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:14,341 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.6120 (0.6156) Acc D Real: 100.000% 
Loss D Fake: 0.8492 (0.8981) Acc D Fake: 3.275% 
Loss D: 1.461 
Loss G: 0.5669 (0.5388) Acc G: 96.725% 
LR: 2.000e-04 

2023-03-01 13:45:14,349 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.6122 (0.6155) Acc D Real: 100.000% 
Loss D Fake: 0.8478 (0.8973) Acc D Fake: 3.218% 
Loss D: 1.460 
Loss G: 0.5678 (0.5393) Acc G: 96.782% 
LR: 2.000e-04 

2023-03-01 13:45:14,356 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.6111 (0.6154) Acc D Real: 100.000% 
Loss D Fake: 0.8463 (0.8964) Acc D Fake: 3.164% 
Loss D: 1.457 
Loss G: 0.5687 (0.5398) Acc G: 96.836% 
LR: 2.000e-04 

2023-03-01 13:45:14,363 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.6104 (0.6153) Acc D Real: 100.000% 
Loss D Fake: 0.8449 (0.8956) Acc D Fake: 3.111% 
Loss D: 1.455 
Loss G: 0.5696 (0.5403) Acc G: 96.889% 
LR: 2.000e-04 

2023-03-01 13:45:14,371 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.6106 (0.6153) Acc D Real: 100.000% 
Loss D Fake: 0.8435 (0.8947) Acc D Fake: 3.060% 
Loss D: 1.454 
Loss G: 0.5705 (0.5408) Acc G: 96.940% 
LR: 2.000e-04 

2023-03-01 13:45:14,378 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.6112 (0.6152) Acc D Real: 100.000% 
Loss D Fake: 0.8421 (0.8939) Acc D Fake: 3.011% 
Loss D: 1.453 
Loss G: 0.5714 (0.5413) Acc G: 96.989% 
LR: 2.000e-04 

2023-03-01 13:45:14,385 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.6094 (0.6151) Acc D Real: 100.000% 
Loss D Fake: 0.8407 (0.8930) Acc D Fake: 2.963% 
Loss D: 1.450 
Loss G: 0.5723 (0.5418) Acc G: 97.037% 
LR: 2.000e-04 

2023-03-01 13:45:14,392 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.6105 (0.6150) Acc D Real: 100.000% 
Loss D Fake: 0.8394 (0.8922) Acc D Fake: 2.917% 
Loss D: 1.450 
Loss G: 0.5732 (0.5423) Acc G: 97.083% 
LR: 2.000e-04 

2023-03-01 13:45:14,400 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.6091 (0.6149) Acc D Real: 100.000% 
Loss D Fake: 0.8380 (0.8913) Acc D Fake: 2.872% 
Loss D: 1.447 
Loss G: 0.5741 (0.5428) Acc G: 97.128% 
LR: 2.000e-04 

2023-03-01 13:45:14,407 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.6101 (0.6149) Acc D Real: 100.000% 
Loss D Fake: 0.8367 (0.8905) Acc D Fake: 2.828% 
Loss D: 1.447 
Loss G: 0.5750 (0.5433) Acc G: 97.172% 
LR: 2.000e-04 

2023-03-01 13:45:14,415 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.6095 (0.6148) Acc D Real: 100.000% 
Loss D Fake: 0.8354 (0.8897) Acc D Fake: 2.786% 
Loss D: 1.445 
Loss G: 0.5759 (0.5438) Acc G: 97.214% 
LR: 2.000e-04 

2023-03-01 13:45:14,422 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.6086 (0.6147) Acc D Real: 100.000% 
Loss D Fake: 0.8340 (0.8889) Acc D Fake: 2.745% 
Loss D: 1.443 
Loss G: 0.5768 (0.5443) Acc G: 97.255% 
LR: 2.000e-04 

2023-03-01 13:45:14,431 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.6092 (0.6146) Acc D Real: 100.000% 
Loss D Fake: 0.8327 (0.8881) Acc D Fake: 2.705% 
Loss D: 1.442 
Loss G: 0.5776 (0.5447) Acc G: 97.295% 
LR: 2.000e-04 

2023-03-01 13:45:14,438 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.6092 (0.6145) Acc D Real: 100.000% 
Loss D Fake: 0.8314 (0.8873) Acc D Fake: 2.667% 
Loss D: 1.441 
Loss G: 0.5785 (0.5452) Acc G: 97.333% 
LR: 2.000e-04 

2023-03-01 13:45:14,446 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.6095 (0.6145) Acc D Real: 100.000% 
Loss D Fake: 0.8301 (0.8864) Acc D Fake: 2.629% 
Loss D: 1.440 
Loss G: 0.5794 (0.5457) Acc G: 97.371% 
LR: 2.000e-04 

2023-03-01 13:45:14,453 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.6089 (0.6144) Acc D Real: 100.000% 
Loss D Fake: 0.8288 (0.8856) Acc D Fake: 2.593% 
Loss D: 1.438 
Loss G: 0.5803 (0.5462) Acc G: 97.407% 
LR: 2.000e-04 

2023-03-01 13:45:14,460 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.6084 (0.6143) Acc D Real: 100.000% 
Loss D Fake: 0.8275 (0.8848) Acc D Fake: 2.557% 
Loss D: 1.436 
Loss G: 0.5812 (0.5467) Acc G: 97.443% 
LR: 2.000e-04 

2023-03-01 13:45:14,468 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.6084 (0.6142) Acc D Real: 100.000% 
Loss D Fake: 0.8262 (0.8841) Acc D Fake: 2.523% 
Loss D: 1.435 
Loss G: 0.5821 (0.5471) Acc G: 97.477% 
LR: 2.000e-04 

2023-03-01 13:45:14,475 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.6070 (0.6141) Acc D Real: 100.000% 
Loss D Fake: 0.8249 (0.8833) Acc D Fake: 2.489% 
Loss D: 1.432 
Loss G: 0.5830 (0.5476) Acc G: 97.511% 
LR: 2.000e-04 

2023-03-01 13:45:14,482 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.6073 (0.6140) Acc D Real: 100.000% 
Loss D Fake: 0.8236 (0.8825) Acc D Fake: 2.456% 
Loss D: 1.431 
Loss G: 0.5838 (0.5481) Acc G: 97.544% 
LR: 2.000e-04 

2023-03-01 13:45:14,490 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.6070 (0.6139) Acc D Real: 100.000% 
Loss D Fake: 0.8223 (0.8817) Acc D Fake: 2.424% 
Loss D: 1.429 
Loss G: 0.5847 (0.5486) Acc G: 97.576% 
LR: 2.000e-04 

2023-03-01 13:45:14,497 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.6069 (0.6139) Acc D Real: 100.000% 
Loss D Fake: 0.8211 (0.8809) Acc D Fake: 2.393% 
Loss D: 1.428 
Loss G: 0.5856 (0.5490) Acc G: 97.607% 
LR: 2.000e-04 

2023-03-01 13:45:14,504 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.6064 (0.6138) Acc D Real: 100.000% 
Loss D Fake: 0.8198 (0.8801) Acc D Fake: 2.363% 
Loss D: 1.426 
Loss G: 0.5865 (0.5495) Acc G: 97.637% 
LR: 2.000e-04 

2023-03-01 13:45:14,512 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.6059 (0.6137) Acc D Real: 100.000% 
Loss D Fake: 0.8186 (0.8794) Acc D Fake: 2.333% 
Loss D: 1.424 
Loss G: 0.5873 (0.5500) Acc G: 97.667% 
LR: 2.000e-04 

2023-03-01 13:45:14,519 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.6059 (0.6136) Acc D Real: 100.000% 
Loss D Fake: 0.8173 (0.8786) Acc D Fake: 2.305% 
Loss D: 1.423 
Loss G: 0.5882 (0.5505) Acc G: 97.695% 
LR: 2.000e-04 

2023-03-01 13:45:14,527 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.6060 (0.6135) Acc D Real: 100.000% 
Loss D Fake: 0.8161 (0.8779) Acc D Fake: 2.276% 
Loss D: 1.422 
Loss G: 0.5891 (0.5509) Acc G: 97.724% 
LR: 2.000e-04 

2023-03-01 13:45:14,534 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.6057 (0.6134) Acc D Real: 100.000% 
Loss D Fake: 0.8149 (0.8771) Acc D Fake: 2.249% 
Loss D: 1.421 
Loss G: 0.5899 (0.5514) Acc G: 97.751% 
LR: 2.000e-04 

2023-03-01 13:45:14,541 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.6055 (0.6133) Acc D Real: 100.000% 
Loss D Fake: 0.8137 (0.8763) Acc D Fake: 2.222% 
Loss D: 1.419 
Loss G: 0.5908 (0.5519) Acc G: 97.778% 
LR: 2.000e-04 

2023-03-01 13:45:14,548 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.6052 (0.6132) Acc D Real: 100.000% 
Loss D Fake: 0.8125 (0.8756) Acc D Fake: 2.196% 
Loss D: 1.418 
Loss G: 0.5917 (0.5523) Acc G: 97.804% 
LR: 2.000e-04 

2023-03-01 13:45:14,556 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.6050 (0.6131) Acc D Real: 100.000% 
Loss D Fake: 0.8112 (0.8748) Acc D Fake: 2.171% 
Loss D: 1.416 
Loss G: 0.5925 (0.5528) Acc G: 97.829% 
LR: 2.000e-04 

2023-03-01 13:45:14,563 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.6045 (0.6130) Acc D Real: 100.000% 
Loss D Fake: 0.8100 (0.8741) Acc D Fake: 2.146% 
Loss D: 1.414 
Loss G: 0.5934 (0.5533) Acc G: 97.854% 
LR: 2.000e-04 

2023-03-01 13:45:14,570 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.6044 (0.6129) Acc D Real: 100.000% 
Loss D Fake: 0.8088 (0.8734) Acc D Fake: 2.121% 
Loss D: 1.413 
Loss G: 0.5943 (0.5537) Acc G: 97.879% 
LR: 2.000e-04 

2023-03-01 13:45:14,578 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.6038 (0.6128) Acc D Real: 100.000% 
Loss D Fake: 0.8076 (0.8726) Acc D Fake: 2.097% 
Loss D: 1.411 
Loss G: 0.5952 (0.5542) Acc G: 97.903% 
LR: 2.000e-04 

2023-03-01 13:45:14,585 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.6035 (0.6127) Acc D Real: 100.000% 
Loss D Fake: 0.8064 (0.8719) Acc D Fake: 2.074% 
Loss D: 1.410 
Loss G: 0.5960 (0.5547) Acc G: 97.926% 
LR: 2.000e-04 

2023-03-01 13:45:14,592 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.6032 (0.6126) Acc D Real: 100.000% 
Loss D Fake: 0.8052 (0.8711) Acc D Fake: 2.051% 
Loss D: 1.408 
Loss G: 0.5969 (0.5551) Acc G: 97.949% 
LR: 2.000e-04 

2023-03-01 13:45:14,599 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.6029 (0.6125) Acc D Real: 100.000% 
Loss D Fake: 0.8040 (0.8704) Acc D Fake: 2.029% 
Loss D: 1.407 
Loss G: 0.5978 (0.5556) Acc G: 97.971% 
LR: 2.000e-04 

2023-03-01 13:45:14,607 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.6027 (0.6124) Acc D Real: 100.000% 
Loss D Fake: 0.8028 (0.8697) Acc D Fake: 2.007% 
Loss D: 1.406 
Loss G: 0.5986 (0.5561) Acc G: 97.993% 
LR: 2.000e-04 

2023-03-01 13:45:14,614 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.6025 (0.6123) Acc D Real: 100.000% 
Loss D Fake: 0.8017 (0.8690) Acc D Fake: 1.986% 
Loss D: 1.404 
Loss G: 0.5995 (0.5565) Acc G: 98.014% 
LR: 2.000e-04 

2023-03-01 13:45:14,621 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.6021 (0.6122) Acc D Real: 100.000% 
Loss D Fake: 0.8005 (0.8682) Acc D Fake: 1.965% 
Loss D: 1.403 
Loss G: 0.6004 (0.5570) Acc G: 98.035% 
LR: 2.000e-04 

2023-03-01 13:45:14,628 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.6020 (0.6121) Acc D Real: 100.000% 
Loss D Fake: 0.7993 (0.8675) Acc D Fake: 1.944% 
Loss D: 1.401 
Loss G: 0.6012 (0.5574) Acc G: 98.056% 
LR: 2.000e-04 

2023-03-01 13:45:14,636 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.6016 (0.6120) Acc D Real: 100.000% 
Loss D Fake: 0.7981 (0.8668) Acc D Fake: 1.924% 
Loss D: 1.400 
Loss G: 0.6021 (0.5579) Acc G: 98.076% 
LR: 2.000e-04 

2023-03-01 13:45:14,643 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.6013 (0.6118) Acc D Real: 100.000% 
Loss D Fake: 0.7970 (0.8661) Acc D Fake: 1.905% 
Loss D: 1.398 
Loss G: 0.6030 (0.5584) Acc G: 98.095% 
LR: 2.000e-04 

2023-03-01 13:45:14,651 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.6010 (0.6117) Acc D Real: 100.000% 
Loss D Fake: 0.7958 (0.8654) Acc D Fake: 1.886% 
Loss D: 1.397 
Loss G: 0.6039 (0.5588) Acc G: 98.114% 
LR: 2.000e-04 

2023-03-01 13:45:14,658 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.6006 (0.6116) Acc D Real: 100.000% 
Loss D Fake: 0.7946 (0.8647) Acc D Fake: 1.867% 
Loss D: 1.395 
Loss G: 0.6048 (0.5593) Acc G: 98.133% 
LR: 2.000e-04 

2023-03-01 13:45:14,665 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.6003 (0.6115) Acc D Real: 100.000% 
Loss D Fake: 0.7934 (0.8640) Acc D Fake: 1.848% 
Loss D: 1.394 
Loss G: 0.6057 (0.5597) Acc G: 98.152% 
LR: 2.000e-04 

2023-03-01 13:45:14,672 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.6000 (0.6114) Acc D Real: 100.000% 
Loss D Fake: 0.7922 (0.8633) Acc D Fake: 1.830% 
Loss D: 1.392 
Loss G: 0.6066 (0.5602) Acc G: 98.170% 
LR: 2.000e-04 

2023-03-01 13:45:14,679 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.5997 (0.6113) Acc D Real: 100.000% 
Loss D Fake: 0.7910 (0.8626) Acc D Fake: 1.812% 
Loss D: 1.391 
Loss G: 0.6075 (0.5607) Acc G: 98.188% 
LR: 2.000e-04 

2023-03-01 13:45:14,686 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.5993 (0.6112) Acc D Real: 100.000% 
Loss D Fake: 0.7898 (0.8619) Acc D Fake: 1.795% 
Loss D: 1.389 
Loss G: 0.6084 (0.5611) Acc G: 98.205% 
LR: 2.000e-04 

2023-03-01 13:45:14,693 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.5990 (0.6111) Acc D Real: 100.000% 
Loss D Fake: 0.7887 (0.8612) Acc D Fake: 1.778% 
Loss D: 1.388 
Loss G: 0.6093 (0.5616) Acc G: 98.222% 
LR: 2.000e-04 

2023-03-01 13:45:14,701 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.5988 (0.6109) Acc D Real: 100.000% 
Loss D Fake: 0.7875 (0.8605) Acc D Fake: 1.761% 
Loss D: 1.386 
Loss G: 0.6103 (0.5620) Acc G: 98.239% 
LR: 2.000e-04 

2023-03-01 13:45:14,708 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.5984 (0.6108) Acc D Real: 100.000% 
Loss D Fake: 0.7863 (0.8598) Acc D Fake: 1.745% 
Loss D: 1.385 
Loss G: 0.6112 (0.5625) Acc G: 98.255% 
LR: 2.000e-04 

2023-03-01 13:45:14,715 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.5981 (0.6107) Acc D Real: 100.000% 
Loss D Fake: 0.7851 (0.8591) Acc D Fake: 1.728% 
Loss D: 1.383 
Loss G: 0.6121 (0.5630) Acc G: 98.272% 
LR: 2.000e-04 

2023-03-01 13:45:14,722 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.5976 (0.6106) Acc D Real: 100.000% 
Loss D Fake: 0.7840 (0.8584) Acc D Fake: 1.713% 
Loss D: 1.382 
Loss G: 0.6130 (0.5634) Acc G: 98.287% 
LR: 2.000e-04 

2023-03-01 13:45:14,730 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.5977 (0.6105) Acc D Real: 100.000% 
Loss D Fake: 0.7828 (0.8577) Acc D Fake: 1.697% 
Loss D: 1.380 
Loss G: 0.6139 (0.5639) Acc G: 98.303% 
LR: 2.000e-04 

2023-03-01 13:45:14,737 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.5970 (0.6103) Acc D Real: 100.000% 
Loss D Fake: 0.7816 (0.8570) Acc D Fake: 1.682% 
Loss D: 1.379 
Loss G: 0.6148 (0.5643) Acc G: 98.318% 
LR: 2.000e-04 

2023-03-01 13:45:14,746 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.5966 (0.6102) Acc D Real: 100.000% 
Loss D Fake: 0.7805 (0.8563) Acc D Fake: 1.667% 
Loss D: 1.377 
Loss G: 0.6157 (0.5648) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-01 13:45:14,755 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.5963 (0.6101) Acc D Real: 100.000% 
Loss D Fake: 0.7793 (0.8557) Acc D Fake: 1.652% 
Loss D: 1.376 
Loss G: 0.6167 (0.5653) Acc G: 98.348% 
LR: 2.000e-04 

2023-03-01 13:45:14,764 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.5960 (0.6100) Acc D Real: 100.000% 
Loss D Fake: 0.7781 (0.8550) Acc D Fake: 1.637% 
Loss D: 1.374 
Loss G: 0.6176 (0.5657) Acc G: 98.363% 
LR: 2.000e-04 

2023-03-01 13:45:14,773 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.5956 (0.6099) Acc D Real: 100.000% 
Loss D Fake: 0.7770 (0.8543) Acc D Fake: 1.623% 
Loss D: 1.373 
Loss G: 0.6186 (0.5662) Acc G: 98.377% 
LR: 2.000e-04 

2023-03-01 13:45:14,781 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.5955 (0.6097) Acc D Real: 100.000% 
Loss D Fake: 0.7758 (0.8536) Acc D Fake: 1.609% 
Loss D: 1.371 
Loss G: 0.6195 (0.5666) Acc G: 98.391% 
LR: 2.000e-04 

2023-03-01 13:45:14,788 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.5947 (0.6096) Acc D Real: 100.000% 
Loss D Fake: 0.7746 (0.8530) Acc D Fake: 1.595% 
Loss D: 1.369 
Loss G: 0.6204 (0.5671) Acc G: 98.405% 
LR: 2.000e-04 

2023-03-01 13:45:14,796 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.5946 (0.6095) Acc D Real: 100.000% 
Loss D Fake: 0.7734 (0.8523) Acc D Fake: 1.582% 
Loss D: 1.368 
Loss G: 0.6214 (0.5676) Acc G: 98.418% 
LR: 2.000e-04 

2023-03-01 13:45:14,803 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.5938 (0.6093) Acc D Real: 100.000% 
Loss D Fake: 0.7722 (0.8516) Acc D Fake: 1.569% 
Loss D: 1.366 
Loss G: 0.6224 (0.5680) Acc G: 98.431% 
LR: 2.000e-04 

2023-03-01 13:45:14,811 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.5941 (0.6092) Acc D Real: 100.000% 
Loss D Fake: 0.7710 (0.8509) Acc D Fake: 1.556% 
Loss D: 1.365 
Loss G: 0.6233 (0.5685) Acc G: 98.444% 
LR: 2.000e-04 

2023-03-01 13:45:14,818 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.5937 (0.6091) Acc D Real: 100.000% 
Loss D Fake: 0.7699 (0.8503) Acc D Fake: 1.543% 
Loss D: 1.364 
Loss G: 0.6243 (0.5689) Acc G: 98.457% 
LR: 2.000e-04 

2023-03-01 13:45:14,826 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.5934 (0.6090) Acc D Real: 100.000% 
Loss D Fake: 0.7687 (0.8496) Acc D Fake: 1.530% 
Loss D: 1.362 
Loss G: 0.6253 (0.5694) Acc G: 98.470% 
LR: 2.000e-04 

2023-03-01 13:45:14,833 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.5924 (0.6088) Acc D Real: 100.000% 
Loss D Fake: 0.7675 (0.8489) Acc D Fake: 1.518% 
Loss D: 1.360 
Loss G: 0.6263 (0.5699) Acc G: 98.482% 
LR: 2.000e-04 

2023-03-01 13:45:14,840 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.5923 (0.6087) Acc D Real: 100.000% 
Loss D Fake: 0.7663 (0.8483) Acc D Fake: 1.505% 
Loss D: 1.359 
Loss G: 0.6273 (0.5703) Acc G: 98.495% 
LR: 2.000e-04 

2023-03-01 13:45:14,847 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.5925 (0.6086) Acc D Real: 100.000% 
Loss D Fake: 0.7651 (0.8476) Acc D Fake: 1.493% 
Loss D: 1.358 
Loss G: 0.6283 (0.5708) Acc G: 98.507% 
LR: 2.000e-04 

2023-03-01 13:45:14,854 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.5917 (0.6084) Acc D Real: 100.000% 
Loss D Fake: 0.7639 (0.8469) Acc D Fake: 1.481% 
Loss D: 1.356 
Loss G: 0.6293 (0.5713) Acc G: 98.519% 
LR: 2.000e-04 

2023-03-01 13:45:14,861 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.5915 (0.6083) Acc D Real: 100.000% 
Loss D Fake: 0.7627 (0.8463) Acc D Fake: 1.470% 
Loss D: 1.354 
Loss G: 0.6303 (0.5717) Acc G: 98.530% 
LR: 2.000e-04 

2023-03-01 13:45:14,870 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.5904 (0.6082) Acc D Real: 100.000% 
Loss D Fake: 0.7615 (0.8456) Acc D Fake: 1.458% 
Loss D: 1.352 
Loss G: 0.6313 (0.5722) Acc G: 98.542% 
LR: 2.000e-04 

2023-03-01 13:45:14,877 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.5903 (0.6080) Acc D Real: 100.000% 
Loss D Fake: 0.7603 (0.8449) Acc D Fake: 1.447% 
Loss D: 1.351 
Loss G: 0.6323 (0.5726) Acc G: 98.553% 
LR: 2.000e-04 

2023-03-01 13:45:14,884 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.5898 (0.6079) Acc D Real: 100.000% 
Loss D Fake: 0.7590 (0.8443) Acc D Fake: 1.436% 
Loss D: 1.349 
Loss G: 0.6334 (0.5731) Acc G: 98.564% 
LR: 2.000e-04 

2023-03-01 13:45:14,891 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.5894 (0.6077) Acc D Real: 100.000% 
Loss D Fake: 0.7578 (0.8436) Acc D Fake: 1.425% 
Loss D: 1.347 
Loss G: 0.6344 (0.5736) Acc G: 98.575% 
LR: 2.000e-04 

2023-03-01 13:45:14,899 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.5890 (0.6076) Acc D Real: 100.000% 
Loss D Fake: 0.7565 (0.8430) Acc D Fake: 1.414% 
Loss D: 1.346 
Loss G: 0.6355 (0.5741) Acc G: 98.586% 
LR: 2.000e-04 

2023-03-01 13:45:14,906 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.5899 (0.6075) Acc D Real: 100.000% 
Loss D Fake: 0.7553 (0.8423) Acc D Fake: 1.404% 
Loss D: 1.345 
Loss G: 0.6365 (0.5745) Acc G: 98.596% 
LR: 2.000e-04 

2023-03-01 13:45:14,913 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.5877 (0.6073) Acc D Real: 100.000% 
Loss D Fake: 0.7540 (0.8416) Acc D Fake: 1.393% 
Loss D: 1.342 
Loss G: 0.6376 (0.5750) Acc G: 98.607% 
LR: 2.000e-04 

2023-03-01 13:45:14,920 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.5880 (0.6072) Acc D Real: 100.000% 
Loss D Fake: 0.7528 (0.8410) Acc D Fake: 1.383% 
Loss D: 1.341 
Loss G: 0.6387 (0.5755) Acc G: 98.617% 
LR: 2.000e-04 

2023-03-01 13:45:14,928 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.5876 (0.6070) Acc D Real: 100.000% 
Loss D Fake: 0.7515 (0.8403) Acc D Fake: 1.373% 
Loss D: 1.339 
Loss G: 0.6398 (0.5759) Acc G: 98.627% 
LR: 2.000e-04 

2023-03-01 13:45:14,935 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.5870 (0.6069) Acc D Real: 100.000% 
Loss D Fake: 0.7502 (0.8397) Acc D Fake: 1.363% 
Loss D: 1.337 
Loss G: 0.6409 (0.5764) Acc G: 98.637% 
LR: 2.000e-04 

2023-03-01 13:45:14,943 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.5870 (0.6067) Acc D Real: 100.000% 
Loss D Fake: 0.7489 (0.8390) Acc D Fake: 1.353% 
Loss D: 1.336 
Loss G: 0.6421 (0.5769) Acc G: 98.635% 
LR: 2.000e-04 

2023-03-01 13:45:14,950 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.5864 (0.6066) Acc D Real: 100.000% 
Loss D Fake: 0.7476 (0.8384) Acc D Fake: 1.367% 
Loss D: 1.334 
Loss G: 0.6432 (0.5774) Acc G: 98.621% 
LR: 2.000e-04 

2023-03-01 13:45:14,957 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.5856 (0.6064) Acc D Real: 100.000% 
Loss D Fake: 0.7463 (0.8377) Acc D Fake: 1.381% 
Loss D: 1.332 
Loss G: 0.6444 (0.5778) Acc G: 98.607% 
LR: 2.000e-04 

2023-03-01 13:45:14,964 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.5863 (0.6063) Acc D Real: 100.000% 
Loss D Fake: 0.7450 (0.8370) Acc D Fake: 1.395% 
Loss D: 1.331 
Loss G: 0.6455 (0.5783) Acc G: 98.582% 
LR: 2.000e-04 

2023-03-01 13:45:14,972 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.5843 (0.6061) Acc D Real: 100.000% 
Loss D Fake: 0.7437 (0.8364) Acc D Fake: 1.420% 
Loss D: 1.328 
Loss G: 0.6467 (0.5788) Acc G: 98.556% 
LR: 2.000e-04 

2023-03-01 13:45:14,980 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.5838 (0.6060) Acc D Real: 100.000% 
Loss D Fake: 0.7424 (0.8357) Acc D Fake: 1.445% 
Loss D: 1.326 
Loss G: 0.6479 (0.5793) Acc G: 98.531% 
LR: 2.000e-04 

2023-03-01 13:45:14,987 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.5841 (0.6058) Acc D Real: 100.000% 
Loss D Fake: 0.7410 (0.8351) Acc D Fake: 1.470% 
Loss D: 1.325 
Loss G: 0.6491 (0.5798) Acc G: 98.507% 
LR: 2.000e-04 

2023-03-01 13:45:14,995 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.5828 (0.6057) Acc D Real: 100.000% 
Loss D Fake: 0.7396 (0.8344) Acc D Fake: 1.494% 
Loss D: 1.322 
Loss G: 0.6504 (0.5803) Acc G: 98.471% 
LR: 2.000e-04 

2023-03-01 13:45:15,002 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.5840 (0.6055) Acc D Real: 100.000% 
Loss D Fake: 0.7382 (0.8338) Acc D Fake: 1.553% 
Loss D: 1.322 
Loss G: 0.6516 (0.5807) Acc G: 98.413% 
LR: 2.000e-04 

2023-03-01 13:45:15,009 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.5829 (0.6054) Acc D Real: 100.000% 
Loss D Fake: 0.7369 (0.8331) Acc D Fake: 1.610% 
Loss D: 1.320 
Loss G: 0.6529 (0.5812) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-01 13:45:15,016 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.5826 (0.6052) Acc D Real: 100.000% 
Loss D Fake: 0.7355 (0.8324) Acc D Fake: 1.689% 
Loss D: 1.318 
Loss G: 0.6542 (0.5817) Acc G: 98.255% 
LR: 2.000e-04 

2023-03-01 13:45:15,023 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.5817 (0.6051) Acc D Real: 100.000% 
Loss D Fake: 0.7340 (0.8318) Acc D Fake: 1.767% 
Loss D: 1.316 
Loss G: 0.6555 (0.5822) Acc G: 98.177% 
LR: 2.000e-04 

2023-03-01 13:45:15,030 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.5811 (0.6049) Acc D Real: 100.000% 
Loss D Fake: 0.7326 (0.8311) Acc D Fake: 1.856% 
Loss D: 1.314 
Loss G: 0.6568 (0.5827) Acc G: 98.078% 
LR: 2.000e-04 

2023-03-01 13:45:15,037 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.5804 (0.6047) Acc D Real: 100.000% 
Loss D Fake: 0.7312 (0.8305) Acc D Fake: 1.954% 
Loss D: 1.312 
Loss G: 0.6581 (0.5832) Acc G: 97.980% 
LR: 2.000e-04 

2023-03-01 13:45:15,045 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.5801 (0.6046) Acc D Real: 100.000% 
Loss D Fake: 0.7297 (0.8298) Acc D Fake: 2.061% 
Loss D: 1.310 
Loss G: 0.6595 (0.5837) Acc G: 97.873% 
LR: 2.000e-04 

2023-03-01 13:45:15,053 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.5791 (0.6044) Acc D Real: 100.000% 
Loss D Fake: 0.7282 (0.8291) Acc D Fake: 2.168% 
Loss D: 1.307 
Loss G: 0.6609 (0.5842) Acc G: 97.767% 
LR: 2.000e-04 

2023-03-01 13:45:15,061 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.5802 (0.6043) Acc D Real: 100.000% 
Loss D Fake: 0.7267 (0.8285) Acc D Fake: 2.294% 
Loss D: 1.307 
Loss G: 0.6623 (0.5847) Acc G: 97.641% 
LR: 2.000e-04 

2023-03-01 13:45:15,068 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.5782 (0.6041) Acc D Real: 100.000% 
Loss D Fake: 0.7252 (0.8278) Acc D Fake: 2.419% 
Loss D: 1.303 
Loss G: 0.6637 (0.5852) Acc G: 97.505% 
LR: 2.000e-04 

2023-03-01 13:45:15,076 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.5765 (0.6039) Acc D Real: 100.000% 
Loss D Fake: 0.7236 (0.8271) Acc D Fake: 2.553% 
Loss D: 1.300 
Loss G: 0.6652 (0.5858) Acc G: 97.372% 
LR: 2.000e-04 

2023-03-01 13:45:15,083 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.5777 (0.6037) Acc D Real: 100.000% 
Loss D Fake: 0.7220 (0.8265) Acc D Fake: 2.686% 
Loss D: 1.300 
Loss G: 0.6667 (0.5863) Acc G: 97.229% 
LR: 2.000e-04 

2023-03-01 13:45:15,091 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.5765 (0.6036) Acc D Real: 100.000% 
Loss D Fake: 0.7204 (0.8258) Acc D Fake: 2.838% 
Loss D: 1.297 
Loss G: 0.6682 (0.5868) Acc G: 97.068% 
LR: 2.000e-04 

2023-03-01 13:45:15,098 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.5764 (0.6034) Acc D Real: 100.000% 
Loss D Fake: 0.7188 (0.8251) Acc D Fake: 2.998% 
Loss D: 1.295 
Loss G: 0.6698 (0.5873) Acc G: 96.908% 
LR: 2.000e-04 

2023-03-01 13:45:15,105 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.5772 (0.6032) Acc D Real: 100.000% 
Loss D Fake: 0.7172 (0.8244) Acc D Fake: 3.156% 
Loss D: 1.294 
Loss G: 0.6714 (0.5878) Acc G: 96.750% 
LR: 2.000e-04 

2023-03-01 13:45:15,112 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.5771 (0.6031) Acc D Real: 100.000% 
Loss D Fake: 0.7155 (0.8238) Acc D Fake: 3.313% 
Loss D: 1.293 
Loss G: 0.6729 (0.5884) Acc G: 96.584% 
LR: 2.000e-04 

2023-03-01 13:45:15,120 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.5762 (0.6029) Acc D Real: 100.000% 
Loss D Fake: 0.7139 (0.8231) Acc D Fake: 3.477% 
Loss D: 1.290 
Loss G: 0.6745 (0.5889) Acc G: 96.409% 
LR: 2.000e-04 

2023-03-01 13:45:15,127 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.5773 (0.6027) Acc D Real: 100.000% 
Loss D Fake: 0.7122 (0.8224) Acc D Fake: 3.650% 
Loss D: 1.290 
Loss G: 0.6761 (0.5894) Acc G: 96.237% 
LR: 2.000e-04 

2023-03-01 13:45:15,134 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.5751 (0.6026) Acc D Real: 100.000% 
Loss D Fake: 0.7106 (0.8217) Acc D Fake: 3.821% 
Loss D: 1.286 
Loss G: 0.6778 (0.5900) Acc G: 96.047% 
LR: 2.000e-04 

2023-03-01 13:45:15,141 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.5754 (0.6024) Acc D Real: 100.000% 
Loss D Fake: 0.7089 (0.8210) Acc D Fake: 4.020% 
Loss D: 1.284 
Loss G: 0.6794 (0.5905) Acc G: 95.848% 
LR: 2.000e-04 

2023-03-01 13:45:15,149 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.5718 (0.6022) Acc D Real: 100.000% 
Loss D Fake: 0.7072 (0.8204) Acc D Fake: 4.217% 
Loss D: 1.279 
Loss G: 0.6811 (0.5911) Acc G: 95.653% 
LR: 2.000e-04 

2023-03-01 13:45:15,156 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.5725 (0.6021) Acc D Real: 100.000% 
Loss D Fake: 0.7054 (0.8197) Acc D Fake: 4.421% 
Loss D: 1.278 
Loss G: 0.6829 (0.5916) Acc G: 95.449% 
LR: 2.000e-04 

2023-03-01 13:45:15,163 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.5710 (0.6019) Acc D Real: 100.000% 
Loss D Fake: 0.7036 (0.8190) Acc D Fake: 4.623% 
Loss D: 1.275 
Loss G: 0.6847 (0.5922) Acc G: 95.248% 
LR: 2.000e-04 

2023-03-01 13:45:15,171 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.5723 (0.6017) Acc D Real: 100.000% 
Loss D Fake: 0.7018 (0.8183) Acc D Fake: 4.832% 
Loss D: 1.274 
Loss G: 0.6865 (0.5927) Acc G: 95.039% 
LR: 2.000e-04 

2023-03-01 13:45:15,179 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.5706 (0.6015) Acc D Real: 100.000% 
Loss D Fake: 0.7000 (0.8176) Acc D Fake: 5.039% 
Loss D: 1.271 
Loss G: 0.6884 (0.5933) Acc G: 94.824% 
LR: 2.000e-04 

2023-03-01 13:45:15,187 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.5707 (0.6013) Acc D Real: 100.000% 
Loss D Fake: 0.6981 (0.8169) Acc D Fake: 5.253% 
Loss D: 1.269 
Loss G: 0.6903 (0.5939) Acc G: 94.600% 
LR: 2.000e-04 

2023-03-01 13:45:15,195 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.5692 (0.6011) Acc D Real: 100.000% 
Loss D Fake: 0.6962 (0.8162) Acc D Fake: 5.484% 
Loss D: 1.265 
Loss G: 0.6922 (0.5944) Acc G: 94.360% 
LR: 2.000e-04 

2023-03-01 13:45:15,202 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.5705 (0.6010) Acc D Real: 100.000% 
Loss D Fake: 0.6942 (0.8155) Acc D Fake: 5.723% 
Loss D: 1.265 
Loss G: 0.6942 (0.5950) Acc G: 94.123% 
LR: 2.000e-04 

2023-03-01 13:45:15,209 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.5671 (0.6008) Acc D Real: 100.000% 
Loss D Fake: 0.6923 (0.8148) Acc D Fake: 5.958% 
Loss D: 1.259 
Loss G: 0.6963 (0.5956) Acc G: 93.889% 
LR: 2.000e-04 

2023-03-01 13:45:15,216 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.5672 (0.6006) Acc D Real: 100.000% 
Loss D Fake: 0.6902 (0.8141) Acc D Fake: 6.200% 
Loss D: 1.257 
Loss G: 0.6984 (0.5962) Acc G: 93.629% 
LR: 2.000e-04 

2023-03-01 13:45:15,224 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.5658 (0.6004) Acc D Real: 100.000% 
Loss D Fake: 0.6882 (0.8133) Acc D Fake: 6.458% 
Loss D: 1.254 
Loss G: 0.7005 (0.5968) Acc G: 93.371% 
LR: 2.000e-04 

2023-03-01 13:45:15,231 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.5666 (0.6002) Acc D Real: 100.000% 
Loss D Fake: 0.6861 (0.8126) Acc D Fake: 6.714% 
Loss D: 1.253 
Loss G: 0.7027 (0.5974) Acc G: 93.107% 
LR: 2.000e-04 

2023-03-01 13:45:15,238 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.5661 (0.6000) Acc D Real: 100.000% 
Loss D Fake: 0.6839 (0.8119) Acc D Fake: 6.976% 
Loss D: 1.250 
Loss G: 0.7050 (0.5980) Acc G: 92.828% 
LR: 2.000e-04 

2023-03-01 13:45:15,245 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.5644 (0.5998) Acc D Real: 100.000% 
Loss D Fake: 0.6817 (0.8112) Acc D Fake: 7.253% 
Loss D: 1.246 
Loss G: 0.7073 (0.5986) Acc G: 92.542% 
LR: 2.000e-04 

2023-03-01 13:45:15,253 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.5649 (0.5996) Acc D Real: 100.000% 
Loss D Fake: 0.6795 (0.8104) Acc D Fake: 7.537% 
Loss D: 1.244 
Loss G: 0.7096 (0.5992) Acc G: 92.250% 
LR: 2.000e-04 

2023-03-01 13:45:15,260 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.5641 (0.5994) Acc D Real: 100.000% 
Loss D Fake: 0.6773 (0.8097) Acc D Fake: 7.845% 
Loss D: 1.241 
Loss G: 0.7120 (0.5998) Acc G: 91.934% 
LR: 2.000e-04 

2023-03-01 13:45:15,267 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.5629 (0.5992) Acc D Real: 100.000% 
Loss D Fake: 0.6750 (0.8090) Acc D Fake: 8.168% 
Loss D: 1.238 
Loss G: 0.7145 (0.6005) Acc G: 91.603% 
LR: 2.000e-04 

2023-03-01 13:45:15,275 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.5622 (0.5990) Acc D Real: 100.000% 
Loss D Fake: 0.6726 (0.8082) Acc D Fake: 8.525% 
Loss D: 1.235 
Loss G: 0.7170 (0.6011) Acc G: 91.239% 
LR: 2.000e-04 

2023-03-01 13:45:15,282 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.5614 (0.5988) Acc D Real: 100.000% 
Loss D Fake: 0.6702 (0.8075) Acc D Fake: 8.886% 
Loss D: 1.232 
Loss G: 0.7196 (0.6017) Acc G: 90.870% 
LR: 2.000e-04 

2023-03-01 13:45:15,290 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.5612 (0.5986) Acc D Real: 100.000% 
Loss D Fake: 0.6678 (0.8067) Acc D Fake: 9.261% 
Loss D: 1.229 
Loss G: 0.7223 (0.6024) Acc G: 90.495% 
LR: 2.000e-04 

2023-03-01 13:45:15,298 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.5608 (0.5984) Acc D Real: 100.000% 
Loss D Fake: 0.6653 (0.8060) Acc D Fake: 9.651% 
Loss D: 1.226 
Loss G: 0.7250 (0.6030) Acc G: 90.099% 
LR: 2.000e-04 

2023-03-01 13:45:15,306 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.5582 (0.5982) Acc D Real: 100.000% 
Loss D Fake: 0.6628 (0.8052) Acc D Fake: 10.045% 
Loss D: 1.221 
Loss G: 0.7278 (0.6037) Acc G: 89.697% 
LR: 2.000e-04 

2023-03-01 13:45:15,313 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.5603 (0.5980) Acc D Real: 100.000% 
Loss D Fake: 0.6602 (0.8044) Acc D Fake: 10.443% 
Loss D: 1.221 
Loss G: 0.7306 (0.6044) Acc G: 89.300% 
LR: 2.000e-04 

2023-03-01 13:45:15,320 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.5582 (0.5978) Acc D Real: 100.000% 
Loss D Fake: 0.6576 (0.8036) Acc D Fake: 10.838% 
Loss D: 1.216 
Loss G: 0.7336 (0.6051) Acc G: 88.907% 
LR: 2.000e-04 

2023-03-01 13:45:15,328 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.5576 (0.5976) Acc D Real: 100.000% 
Loss D Fake: 0.6549 (0.8029) Acc D Fake: 11.228% 
Loss D: 1.213 
Loss G: 0.7366 (0.6058) Acc G: 88.518% 
LR: 2.000e-04 

2023-03-01 13:45:15,336 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.5559 (0.5973) Acc D Real: 100.000% 
Loss D Fake: 0.6522 (0.8021) Acc D Fake: 11.614% 
Loss D: 1.208 
Loss G: 0.7397 (0.6065) Acc G: 88.133% 
LR: 2.000e-04 

2023-03-01 13:45:15,344 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.5550 (0.5971) Acc D Real: 100.000% 
Loss D Fake: 0.6494 (0.8013) Acc D Fake: 11.997% 
Loss D: 1.204 
Loss G: 0.7429 (0.6072) Acc G: 87.743% 
LR: 2.000e-04 

2023-03-01 13:45:15,351 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.5542 (0.5969) Acc D Real: 100.000% 
Loss D Fake: 0.6465 (0.8005) Acc D Fake: 12.383% 
Loss D: 1.201 
Loss G: 0.7462 (0.6079) Acc G: 87.358% 
LR: 2.000e-04 

2023-03-01 13:45:15,359 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.5515 (0.5967) Acc D Real: 100.000% 
Loss D Fake: 0.6436 (0.7997) Acc D Fake: 12.766% 
Loss D: 1.195 
Loss G: 0.7496 (0.6086) Acc G: 86.976% 
LR: 2.000e-04 

2023-03-01 13:45:15,366 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.5512 (0.5964) Acc D Real: 100.000% 
Loss D Fake: 0.6405 (0.7988) Acc D Fake: 13.145% 
Loss D: 1.192 
Loss G: 0.7531 (0.6094) Acc G: 86.598% 
LR: 2.000e-04 

2023-03-01 13:45:15,374 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.5519 (0.5962) Acc D Real: 100.000% 
Loss D Fake: 0.6375 (0.7980) Acc D Fake: 13.520% 
Loss D: 1.189 
Loss G: 0.7567 (0.6101) Acc G: 86.224% 
LR: 2.000e-04 

2023-03-01 13:45:15,382 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.5519 (0.5960) Acc D Real: 100.000% 
Loss D Fake: 0.6343 (0.7972) Acc D Fake: 13.892% 
Loss D: 1.186 
Loss G: 0.7604 (0.6109) Acc G: 85.854% 
LR: 2.000e-04 

2023-03-01 13:45:15,389 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.5489 (0.5957) Acc D Real: 100.000% 
Loss D Fake: 0.6311 (0.7964) Acc D Fake: 14.259% 
Loss D: 1.180 
Loss G: 0.7642 (0.6117) Acc G: 85.488% 
LR: 2.000e-04 

2023-03-01 13:45:15,397 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.5497 (0.5955) Acc D Real: 100.000% 
Loss D Fake: 0.6278 (0.7955) Acc D Fake: 14.623% 
Loss D: 1.178 
Loss G: 0.7681 (0.6124) Acc G: 85.117% 
LR: 2.000e-04 

2023-03-01 13:45:15,404 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.5471 (0.5953) Acc D Real: 100.000% 
Loss D Fake: 0.6245 (0.7947) Acc D Fake: 14.992% 
Loss D: 1.172 
Loss G: 0.7722 (0.6132) Acc G: 84.750% 
LR: 2.000e-04 

2023-03-01 13:45:15,412 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.5483 (0.5950) Acc D Real: 100.000% 
Loss D Fake: 0.6211 (0.7938) Acc D Fake: 15.357% 
Loss D: 1.169 
Loss G: 0.7763 (0.6140) Acc G: 84.386% 
LR: 2.000e-04 

2023-03-01 13:45:15,420 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.5474 (0.5948) Acc D Real: 100.000% 
Loss D Fake: 0.6176 (0.7929) Acc D Fake: 15.718% 
Loss D: 1.165 
Loss G: 0.7805 (0.6149) Acc G: 84.026% 
LR: 2.000e-04 

2023-03-01 13:45:15,427 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.5475 (0.5946) Acc D Real: 100.000% 
Loss D Fake: 0.6141 (0.7920) Acc D Fake: 16.076% 
Loss D: 1.162 
Loss G: 0.7849 (0.6157) Acc G: 83.670% 
LR: 2.000e-04 

2023-03-01 13:45:15,435 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.5450 (0.5943) Acc D Real: 100.000% 
Loss D Fake: 0.6105 (0.7911) Acc D Fake: 16.430% 
Loss D: 1.155 
Loss G: 0.7893 (0.6166) Acc G: 83.317% 
LR: 2.000e-04 

2023-03-01 13:45:15,442 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.5403 (0.5941) Acc D Real: 100.000% 
Loss D Fake: 0.6069 (0.7902) Acc D Fake: 16.780% 
Loss D: 1.147 
Loss G: 0.7939 (0.6174) Acc G: 82.967% 
LR: 2.000e-04 

2023-03-01 13:45:15,450 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.5430 (0.5938) Acc D Real: 100.000% 
Loss D Fake: 0.6031 (0.7893) Acc D Fake: 17.128% 
Loss D: 1.146 
Loss G: 0.7986 (0.6183) Acc G: 82.621% 
LR: 2.000e-04 

2023-03-01 13:45:15,457 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.5401 (0.5935) Acc D Real: 100.000% 
Loss D Fake: 0.5994 (0.7884) Acc D Fake: 17.472% 
Loss D: 1.139 
Loss G: 0.8035 (0.6192) Acc G: 82.279% 
LR: 2.000e-04 

2023-03-01 13:45:15,464 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.5397 (0.5933) Acc D Real: 100.000% 
Loss D Fake: 0.5955 (0.7875) Acc D Fake: 17.812% 
Loss D: 1.135 
Loss G: 0.8085 (0.6201) Acc G: 81.939% 
LR: 2.000e-04 

2023-03-01 13:45:15,472 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.5406 (0.5930) Acc D Real: 100.000% 
Loss D Fake: 0.5916 (0.7866) Acc D Fake: 18.150% 
Loss D: 1.132 
Loss G: 0.8135 (0.6210) Acc G: 81.595% 
LR: 2.000e-04 

2023-03-01 13:45:15,479 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.5341 (0.5928) Acc D Real: 100.000% 
Loss D Fake: 0.5876 (0.7856) Acc D Fake: 18.493% 
Loss D: 1.122 
Loss G: 0.8188 (0.6220) Acc G: 81.254% 
LR: 2.000e-04 

2023-03-01 13:45:15,487 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.5332 (0.5925) Acc D Real: 100.000% 
Loss D Fake: 0.5836 (0.7847) Acc D Fake: 18.831% 
Loss D: 1.117 
Loss G: 0.8242 (0.6229) Acc G: 80.916% 
LR: 2.000e-04 

2023-03-01 13:45:15,494 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.5345 (0.5922) Acc D Real: 100.000% 
Loss D Fake: 0.5795 (0.7837) Acc D Fake: 19.167% 
Loss D: 1.114 
Loss G: 0.8297 (0.6239) Acc G: 80.582% 
LR: 2.000e-04 

2023-03-01 13:45:15,501 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.5300 (0.5919) Acc D Real: 100.000% 
Loss D Fake: 0.5753 (0.7827) Acc D Fake: 19.500% 
Loss D: 1.105 
Loss G: 0.8353 (0.6249) Acc G: 80.250% 
LR: 2.000e-04 

2023-03-01 13:45:15,509 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.5318 (0.5916) Acc D Real: 100.000% 
Loss D Fake: 0.5711 (0.7817) Acc D Fake: 19.829% 
Loss D: 1.103 
Loss G: 0.8411 (0.6259) Acc G: 79.922% 
LR: 2.000e-04 

2023-03-01 13:45:15,516 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.5277 (0.5913) Acc D Real: 100.000% 
Loss D Fake: 0.5669 (0.7807) Acc D Fake: 20.156% 
Loss D: 1.095 
Loss G: 0.8470 (0.6269) Acc G: 79.597% 
LR: 2.000e-04 

2023-03-01 13:45:15,524 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.5277 (0.5910) Acc D Real: 100.000% 
Loss D Fake: 0.5626 (0.7797) Acc D Fake: 20.479% 
Loss D: 1.090 
Loss G: 0.8531 (0.6280) Acc G: 79.275% 
LR: 2.000e-04 

2023-03-01 13:45:15,531 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.5275 (0.5907) Acc D Real: 100.000% 
Loss D Fake: 0.5582 (0.7787) Acc D Fake: 20.799% 
Loss D: 1.086 
Loss G: 0.8592 (0.6291) Acc G: 78.955% 
LR: 2.000e-04 

2023-03-01 13:45:15,538 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.5196 (0.5904) Acc D Real: 100.000% 
Loss D Fake: 0.5539 (0.7777) Acc D Fake: 21.117% 
Loss D: 1.073 
Loss G: 0.8655 (0.6301) Acc G: 78.639% 
LR: 2.000e-04 

2023-03-01 13:45:15,546 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.5197 (0.5901) Acc D Real: 100.000% 
Loss D Fake: 0.5494 (0.7766) Acc D Fake: 21.431% 
Loss D: 1.069 
Loss G: 0.8720 (0.6312) Acc G: 78.326% 
LR: 2.000e-04 

2023-03-01 13:45:15,553 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.5201 (0.5898) Acc D Real: 100.000% 
Loss D Fake: 0.5450 (0.7756) Acc D Fake: 21.743% 
Loss D: 1.065 
Loss G: 0.8786 (0.6324) Acc G: 78.015% 
LR: 2.000e-04 

2023-03-01 13:45:15,560 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.5172 (0.5894) Acc D Real: 100.000% 
Loss D Fake: 0.5405 (0.7745) Acc D Fake: 22.052% 
Loss D: 1.058 
Loss G: 0.8853 (0.6335) Acc G: 77.707% 
LR: 2.000e-04 

2023-03-01 13:45:15,568 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.5151 (0.5891) Acc D Real: 100.000% 
Loss D Fake: 0.5359 (0.7734) Acc D Fake: 22.358% 
Loss D: 1.051 
Loss G: 0.8922 (0.6347) Acc G: 77.402% 
LR: 2.000e-04 

2023-03-01 13:45:15,575 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.5182 (0.5888) Acc D Real: 100.000% 
Loss D Fake: 0.5313 (0.7723) Acc D Fake: 22.661% 
Loss D: 1.050 
Loss G: 0.8992 (0.6359) Acc G: 77.100% 
LR: 2.000e-04 

2023-03-01 13:45:15,583 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.5128 (0.5885) Acc D Real: 100.000% 
Loss D Fake: 0.5268 (0.7712) Acc D Fake: 22.962% 
Loss D: 1.040 
Loss G: 0.9064 (0.6371) Acc G: 76.801% 
LR: 2.000e-04 

2023-03-01 13:45:15,590 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.5114 (0.5881) Acc D Real: 100.000% 
Loss D Fake: 0.5222 (0.7701) Acc D Fake: 23.260% 
Loss D: 1.034 
Loss G: 0.9137 (0.6383) Acc G: 76.504% 
LR: 2.000e-04 

2023-03-01 13:45:15,597 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.5078 (0.5878) Acc D Real: 100.000% 
Loss D Fake: 0.5175 (0.7690) Acc D Fake: 23.334% 
Loss D: 1.025 
Loss G: 0.9211 (0.6396) Acc G: 76.430% 
LR: 2.000e-04 

2023-03-01 13:45:15,609 -                train: [    INFO] - 
Epoch: 3/20
2023-03-01 13:45:15,827 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.5066 (0.5065) Acc D Real: 100.000% 
Loss D Fake: 0.5081 (0.5104) Acc D Fake: 90.833% 
Loss D: 1.015 
Loss G: 0.9366 (0.9327) Acc G: 9.167% 
LR: 2.000e-04 

2023-03-01 13:45:15,834 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.5016 (0.5049) Acc D Real: 100.000% 
Loss D Fake: 0.5033 (0.5081) Acc D Fake: 91.111% 
Loss D: 1.005 
Loss G: 0.9446 (0.9367) Acc G: 8.889% 
LR: 2.000e-04 

2023-03-01 13:45:15,841 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4959 (0.5026) Acc D Real: 100.000% 
Loss D Fake: 0.4985 (0.5057) Acc D Fake: 91.250% 
Loss D: 0.994 
Loss G: 0.9529 (0.9407) Acc G: 8.750% 
LR: 2.000e-04 

2023-03-01 13:45:15,856 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4907 (0.5002) Acc D Real: 100.000% 
Loss D Fake: 0.4936 (0.5032) Acc D Fake: 91.333% 
Loss D: 0.984 
Loss G: 0.9614 (0.9449) Acc G: 8.667% 
LR: 2.000e-04 

2023-03-01 13:45:15,863 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4956 (0.4995) Acc D Real: 100.000% 
Loss D Fake: 0.4886 (0.5008) Acc D Fake: 91.389% 
Loss D: 0.984 
Loss G: 0.9702 (0.9491) Acc G: 8.611% 
LR: 2.000e-04 

2023-03-01 13:45:15,870 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4882 (0.4979) Acc D Real: 100.000% 
Loss D Fake: 0.4835 (0.4983) Acc D Fake: 91.429% 
Loss D: 0.972 
Loss G: 0.9793 (0.9534) Acc G: 8.571% 
LR: 2.000e-04 

2023-03-01 13:45:15,876 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4891 (0.4968) Acc D Real: 100.000% 
Loss D Fake: 0.4784 (0.4958) Acc D Fake: 91.458% 
Loss D: 0.967 
Loss G: 0.9887 (0.9578) Acc G: 8.542% 
LR: 2.000e-04 

2023-03-01 13:45:15,883 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4824 (0.4952) Acc D Real: 100.000% 
Loss D Fake: 0.4732 (0.4933) Acc D Fake: 91.481% 
Loss D: 0.956 
Loss G: 0.9984 (0.9623) Acc G: 8.519% 
LR: 2.000e-04 

2023-03-01 13:45:15,890 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4773 (0.4934) Acc D Real: 100.000% 
Loss D Fake: 0.4679 (0.4908) Acc D Fake: 91.500% 
Loss D: 0.945 
Loss G: 1.0085 (0.9669) Acc G: 8.500% 
LR: 2.000e-04 

2023-03-01 13:45:15,897 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4729 (0.4915) Acc D Real: 100.000% 
Loss D Fake: 0.4624 (0.4882) Acc D Fake: 91.515% 
Loss D: 0.935 
Loss G: 1.0189 (0.9717) Acc G: 8.485% 
LR: 2.000e-04 

2023-03-01 13:45:15,903 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4728 (0.4900) Acc D Real: 100.000% 
Loss D Fake: 0.4569 (0.4856) Acc D Fake: 91.528% 
Loss D: 0.930 
Loss G: 1.0298 (0.9765) Acc G: 8.472% 
LR: 2.000e-04 

2023-03-01 13:45:15,910 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4699 (0.4884) Acc D Real: 100.000% 
Loss D Fake: 0.4513 (0.4830) Acc D Fake: 91.538% 
Loss D: 0.921 
Loss G: 1.0411 (0.9815) Acc G: 8.462% 
LR: 2.000e-04 

2023-03-01 13:45:15,917 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4691 (0.4870) Acc D Real: 100.000% 
Loss D Fake: 0.4455 (0.4803) Acc D Fake: 91.548% 
Loss D: 0.915 
Loss G: 1.0529 (0.9866) Acc G: 8.452% 
LR: 2.000e-04 

2023-03-01 13:45:15,924 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4627 (0.4854) Acc D Real: 100.000% 
Loss D Fake: 0.4397 (0.4776) Acc D Fake: 91.556% 
Loss D: 0.902 
Loss G: 1.0652 (0.9918) Acc G: 8.444% 
LR: 2.000e-04 

2023-03-01 13:45:15,930 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4551 (0.4835) Acc D Real: 100.000% 
Loss D Fake: 0.4337 (0.4748) Acc D Fake: 91.562% 
Loss D: 0.889 
Loss G: 1.0780 (0.9972) Acc G: 8.438% 
LR: 2.000e-04 

2023-03-01 13:45:15,938 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4563 (0.4819) Acc D Real: 100.000% 
Loss D Fake: 0.4276 (0.4721) Acc D Fake: 91.569% 
Loss D: 0.884 
Loss G: 1.0915 (1.0028) Acc G: 8.431% 
LR: 2.000e-04 

2023-03-01 13:45:15,944 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4474 (0.4800) Acc D Real: 100.000% 
Loss D Fake: 0.4213 (0.4692) Acc D Fake: 91.574% 
Loss D: 0.869 
Loss G: 1.1057 (1.0085) Acc G: 8.426% 
LR: 2.000e-04 

2023-03-01 13:45:15,951 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4412 (0.4780) Acc D Real: 100.000% 
Loss D Fake: 0.4148 (0.4664) Acc D Fake: 91.579% 
Loss D: 0.856 
Loss G: 1.1207 (1.0144) Acc G: 8.421% 
LR: 2.000e-04 

2023-03-01 13:45:15,958 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4416 (0.4761) Acc D Real: 100.000% 
Loss D Fake: 0.4081 (0.4635) Acc D Fake: 91.583% 
Loss D: 0.850 
Loss G: 1.1367 (1.0205) Acc G: 8.417% 
LR: 2.000e-04 

2023-03-01 13:45:15,965 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4376 (0.4743) Acc D Real: 100.000% 
Loss D Fake: 0.4011 (0.4605) Acc D Fake: 91.587% 
Loss D: 0.839 
Loss G: 1.1538 (1.0268) Acc G: 8.413% 
LR: 2.000e-04 

2023-03-01 13:45:15,971 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4294 (0.4723) Acc D Real: 100.000% 
Loss D Fake: 0.3939 (0.4575) Acc D Fake: 91.591% 
Loss D: 0.823 
Loss G: 1.1722 (1.0335) Acc G: 8.409% 
LR: 2.000e-04 

2023-03-01 13:45:15,978 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4251 (0.4702) Acc D Real: 100.000% 
Loss D Fake: 0.3863 (0.4544) Acc D Fake: 91.594% 
Loss D: 0.811 
Loss G: 1.1923 (1.0404) Acc G: 8.406% 
LR: 2.000e-04 

2023-03-01 13:45:15,985 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4185 (0.4681) Acc D Real: 100.000% 
Loss D Fake: 0.3782 (0.4512) Acc D Fake: 91.597% 
Loss D: 0.797 
Loss G: 1.2145 (1.0476) Acc G: 8.403% 
LR: 2.000e-04 

2023-03-01 13:45:15,992 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4193 (0.4661) Acc D Real: 100.000% 
Loss D Fake: 0.3696 (0.4479) Acc D Fake: 91.600% 
Loss D: 0.789 
Loss G: 1.2393 (1.0553) Acc G: 8.400% 
LR: 2.000e-04 

2023-03-01 13:45:15,999 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4142 (0.4641) Acc D Real: 100.000% 
Loss D Fake: 0.3603 (0.4446) Acc D Fake: 91.603% 
Loss D: 0.774 
Loss G: 1.2676 (1.0635) Acc G: 8.397% 
LR: 2.000e-04 

2023-03-01 13:45:16,006 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4024 (0.4618) Acc D Real: 100.000% 
Loss D Fake: 0.3502 (0.4411) Acc D Fake: 91.605% 
Loss D: 0.753 
Loss G: 1.3000 (1.0722) Acc G: 8.395% 
LR: 2.000e-04 

2023-03-01 13:45:16,013 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4033 (0.4597) Acc D Real: 100.000% 
Loss D Fake: 0.3391 (0.4374) Acc D Fake: 91.607% 
Loss D: 0.742 
Loss G: 1.3370 (1.0817) Acc G: 8.393% 
LR: 2.000e-04 

2023-03-01 13:45:16,020 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3952 (0.4575) Acc D Real: 100.000% 
Loss D Fake: 0.3275 (0.4336) Acc D Fake: 91.609% 
Loss D: 0.723 
Loss G: 1.3774 (1.0919) Acc G: 8.391% 
LR: 2.000e-04 

2023-03-01 13:45:16,028 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3915 (0.4553) Acc D Real: 100.000% 
Loss D Fake: 0.3161 (0.4297) Acc D Fake: 91.611% 
Loss D: 0.708 
Loss G: 1.4180 (1.1027) Acc G: 8.389% 
LR: 2.000e-04 

2023-03-01 13:45:16,035 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3834 (0.4530) Acc D Real: 100.000% 
Loss D Fake: 0.3058 (0.4257) Acc D Fake: 91.613% 
Loss D: 0.689 
Loss G: 1.4550 (1.1141) Acc G: 8.387% 
LR: 2.000e-04 

2023-03-01 13:45:16,042 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3789 (0.4507) Acc D Real: 100.000% 
Loss D Fake: 0.2973 (0.4217) Acc D Fake: 91.615% 
Loss D: 0.676 
Loss G: 1.4861 (1.1257) Acc G: 8.385% 
LR: 2.000e-04 

2023-03-01 13:45:16,049 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3716 (0.4483) Acc D Real: 100.000% 
Loss D Fake: 0.2905 (0.4177) Acc D Fake: 91.616% 
Loss D: 0.662 
Loss G: 1.5117 (1.1374) Acc G: 8.384% 
LR: 2.000e-04 

2023-03-01 13:45:16,056 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3638 (0.4458) Acc D Real: 100.000% 
Loss D Fake: 0.2851 (0.4138) Acc D Fake: 91.618% 
Loss D: 0.649 
Loss G: 1.5333 (1.1491) Acc G: 8.382% 
LR: 2.000e-04 

2023-03-01 13:45:16,064 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3571 (0.4433) Acc D Real: 100.000% 
Loss D Fake: 0.2805 (0.4100) Acc D Fake: 91.619% 
Loss D: 0.638 
Loss G: 1.5526 (1.1606) Acc G: 8.381% 
LR: 2.000e-04 

2023-03-01 13:45:16,071 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3505 (0.4407) Acc D Real: 100.000% 
Loss D Fake: 0.2764 (0.4063) Acc D Fake: 91.620% 
Loss D: 0.627 
Loss G: 1.5706 (1.1720) Acc G: 8.380% 
LR: 2.000e-04 

2023-03-01 13:45:16,079 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3493 (0.4382) Acc D Real: 100.000% 
Loss D Fake: 0.2725 (0.4027) Acc D Fake: 91.622% 
Loss D: 0.622 
Loss G: 1.5883 (1.1832) Acc G: 8.378% 
LR: 2.000e-04 

2023-03-01 13:45:16,086 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3450 (0.4358) Acc D Real: 100.000% 
Loss D Fake: 0.2688 (0.3992) Acc D Fake: 91.623% 
Loss D: 0.614 
Loss G: 1.6060 (1.1944) Acc G: 8.377% 
LR: 2.000e-04 

2023-03-01 13:45:16,093 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3412 (0.4333) Acc D Real: 100.000% 
Loss D Fake: 0.2652 (0.3957) Acc D Fake: 91.624% 
Loss D: 0.606 
Loss G: 1.6239 (1.2054) Acc G: 8.376% 
LR: 2.000e-04 

2023-03-01 13:45:16,100 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3294 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 0.2616 (0.3924) Acc D Fake: 91.625% 
Loss D: 0.591 
Loss G: 1.6420 (1.2163) Acc G: 8.375% 
LR: 2.000e-04 

2023-03-01 13:45:16,107 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3270 (0.4282) Acc D Real: 100.000% 
Loss D Fake: 0.2582 (0.3891) Acc D Fake: 91.626% 
Loss D: 0.585 
Loss G: 1.6603 (1.2271) Acc G: 8.374% 
LR: 2.000e-04 

2023-03-01 13:45:16,114 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3203 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 0.2548 (0.3859) Acc D Fake: 91.627% 
Loss D: 0.575 
Loss G: 1.6787 (1.2379) Acc G: 8.373% 
LR: 2.000e-04 

2023-03-01 13:45:16,122 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3146 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 0.2515 (0.3828) Acc D Fake: 91.628% 
Loss D: 0.566 
Loss G: 1.6970 (1.2486) Acc G: 8.372% 
LR: 2.000e-04 

2023-03-01 13:45:16,129 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3062 (0.4204) Acc D Real: 100.000% 
Loss D Fake: 0.2483 (0.3797) Acc D Fake: 91.629% 
Loss D: 0.555 
Loss G: 1.7153 (1.2592) Acc G: 8.371% 
LR: 2.000e-04 

2023-03-01 13:45:16,136 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3020 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 0.2452 (0.3767) Acc D Fake: 91.630% 
Loss D: 0.547 
Loss G: 1.7333 (1.2697) Acc G: 8.370% 
LR: 2.000e-04 

2023-03-01 13:45:16,143 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2940 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 0.2422 (0.3738) Acc D Fake: 91.630% 
Loss D: 0.536 
Loss G: 1.7512 (1.2802) Acc G: 8.370% 
LR: 2.000e-04 

2023-03-01 13:45:16,150 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.2929 (0.4125) Acc D Real: 100.000% 
Loss D Fake: 0.2394 (0.3710) Acc D Fake: 91.631% 
Loss D: 0.532 
Loss G: 1.7688 (1.2906) Acc G: 8.369% 
LR: 2.000e-04 

2023-03-01 13:45:16,158 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.2828 (0.4098) Acc D Real: 100.000% 
Loss D Fake: 0.2367 (0.3682) Acc D Fake: 91.632% 
Loss D: 0.519 
Loss G: 1.7861 (1.3009) Acc G: 8.368% 
LR: 2.000e-04 

2023-03-01 13:45:16,165 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2743 (0.4070) Acc D Real: 100.000% 
Loss D Fake: 0.2341 (0.3654) Acc D Fake: 91.633% 
Loss D: 0.508 
Loss G: 1.8031 (1.3111) Acc G: 8.367% 
LR: 2.000e-04 

2023-03-01 13:45:16,173 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2717 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 0.2317 (0.3627) Acc D Fake: 91.633% 
Loss D: 0.503 
Loss G: 1.8199 (1.3213) Acc G: 8.367% 
LR: 2.000e-04 

2023-03-01 13:45:16,180 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2682 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 0.2293 (0.3601) Acc D Fake: 91.634% 
Loss D: 0.497 
Loss G: 1.8362 (1.3314) Acc G: 8.366% 
LR: 2.000e-04 

2023-03-01 13:45:16,187 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2630 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 0.2271 (0.3576) Acc D Fake: 91.635% 
Loss D: 0.490 
Loss G: 1.8523 (1.3414) Acc G: 8.365% 
LR: 2.000e-04 

2023-03-01 13:45:16,195 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2540 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 0.2250 (0.3551) Acc D Fake: 91.635% 
Loss D: 0.479 
Loss G: 1.8679 (1.3514) Acc G: 8.365% 
LR: 2.000e-04 

2023-03-01 13:45:16,203 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2513 (0.3935) Acc D Real: 100.000% 
Loss D Fake: 0.2230 (0.3526) Acc D Fake: 91.636% 
Loss D: 0.474 
Loss G: 1.8833 (1.3612) Acc G: 8.364% 
LR: 2.000e-04 

2023-03-01 13:45:16,211 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2440 (0.3908) Acc D Real: 100.000% 
Loss D Fake: 0.2211 (0.3502) Acc D Fake: 91.636% 
Loss D: 0.465 
Loss G: 1.8982 (1.3710) Acc G: 8.364% 
LR: 2.000e-04 

2023-03-01 13:45:16,218 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2399 (0.3881) Acc D Real: 100.000% 
Loss D Fake: 0.2193 (0.3479) Acc D Fake: 91.637% 
Loss D: 0.459 
Loss G: 1.9128 (1.3806) Acc G: 8.363% 
LR: 2.000e-04 

2023-03-01 13:45:16,225 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2339 (0.3854) Acc D Real: 100.000% 
Loss D Fake: 0.2177 (0.3456) Acc D Fake: 91.637% 
Loss D: 0.452 
Loss G: 1.9269 (1.3902) Acc G: 8.363% 
LR: 2.000e-04 

2023-03-01 13:45:16,233 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2285 (0.3827) Acc D Real: 100.000% 
Loss D Fake: 0.2162 (0.3434) Acc D Fake: 91.638% 
Loss D: 0.445 
Loss G: 1.9407 (1.3997) Acc G: 8.362% 
LR: 2.000e-04 

2023-03-01 13:45:16,240 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.2215 (0.3800) Acc D Real: 100.000% 
Loss D Fake: 0.2148 (0.3412) Acc D Fake: 91.638% 
Loss D: 0.436 
Loss G: 1.9540 (1.4091) Acc G: 8.362% 
LR: 2.000e-04 

2023-03-01 13:45:16,248 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.2167 (0.3773) Acc D Real: 100.000% 
Loss D Fake: 0.2135 (0.3391) Acc D Fake: 91.639% 
Loss D: 0.430 
Loss G: 1.9668 (1.4184) Acc G: 8.361% 
LR: 2.000e-04 

2023-03-01 13:45:16,256 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.2139 (0.3746) Acc D Real: 100.000% 
Loss D Fake: 0.2123 (0.3370) Acc D Fake: 91.639% 
Loss D: 0.426 
Loss G: 1.9790 (1.4276) Acc G: 8.361% 
LR: 2.000e-04 

2023-03-01 13:45:16,263 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2093 (0.3719) Acc D Real: 100.000% 
Loss D Fake: 0.2113 (0.3350) Acc D Fake: 91.640% 
Loss D: 0.421 
Loss G: 1.9906 (1.4367) Acc G: 8.360% 
LR: 2.000e-04 

2023-03-01 13:45:16,271 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2045 (0.3693) Acc D Real: 100.000% 
Loss D Fake: 0.2105 (0.3330) Acc D Fake: 91.614% 
Loss D: 0.415 
Loss G: 2.0016 (1.4456) Acc G: 8.386% 
LR: 2.000e-04 

2023-03-01 13:45:16,278 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.2013 (0.3666) Acc D Real: 100.000% 
Loss D Fake: 0.2098 (0.3311) Acc D Fake: 91.589% 
Loss D: 0.411 
Loss G: 2.0117 (1.4545) Acc G: 8.411% 
LR: 2.000e-04 

2023-03-01 13:45:16,285 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.2005 (0.3641) Acc D Real: 100.000% 
Loss D Fake: 0.2093 (0.3292) Acc D Fake: 91.564% 
Loss D: 0.410 
Loss G: 2.0208 (1.4632) Acc G: 8.436% 
LR: 2.000e-04 

2023-03-01 13:45:16,293 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1920 (0.3615) Acc D Real: 100.000% 
Loss D Fake: 0.2090 (0.3274) Acc D Fake: 91.540% 
Loss D: 0.401 
Loss G: 2.0288 (1.4718) Acc G: 8.460% 
LR: 2.000e-04 

2023-03-01 13:45:16,300 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1898 (0.3589) Acc D Real: 100.000% 
Loss D Fake: 0.2090 (0.3256) Acc D Fake: 91.517% 
Loss D: 0.399 
Loss G: 2.0355 (1.4802) Acc G: 8.483% 
LR: 2.000e-04 

2023-03-01 13:45:16,307 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1847 (0.3564) Acc D Real: 100.000% 
Loss D Fake: 0.2093 (0.3239) Acc D Fake: 91.495% 
Loss D: 0.394 
Loss G: 2.0404 (1.4884) Acc G: 8.505% 
LR: 2.000e-04 

2023-03-01 13:45:16,315 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.1825 (0.3538) Acc D Real: 100.000% 
Loss D Fake: 0.2100 (0.3222) Acc D Fake: 91.473% 
Loss D: 0.393 
Loss G: 2.0432 (1.4965) Acc G: 8.527% 
LR: 2.000e-04 

2023-03-01 13:45:16,322 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1786 (0.3513) Acc D Real: 100.000% 
Loss D Fake: 0.2112 (0.3207) Acc D Fake: 91.452% 
Loss D: 0.390 
Loss G: 2.0432 (1.5043) Acc G: 8.548% 
LR: 2.000e-04 

2023-03-01 13:45:16,330 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.1757 (0.3489) Acc D Real: 100.000% 
Loss D Fake: 0.2131 (0.3191) Acc D Fake: 91.432% 
Loss D: 0.389 
Loss G: 2.0393 (1.5118) Acc G: 8.568% 
LR: 2.000e-04 

2023-03-01 13:45:16,337 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1711 (0.3464) Acc D Real: 100.000% 
Loss D Fake: 0.2158 (0.3177) Acc D Fake: 91.412% 
Loss D: 0.387 
Loss G: 2.0302 (1.5190) Acc G: 8.588% 
LR: 2.000e-04 

2023-03-01 13:45:16,345 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.1695 (0.3440) Acc D Real: 100.000% 
Loss D Fake: 0.2197 (0.3164) Acc D Fake: 91.370% 
Loss D: 0.389 
Loss G: 2.0137 (1.5258) Acc G: 8.630% 
LR: 2.000e-04 

2023-03-01 13:45:16,352 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1664 (0.3416) Acc D Real: 100.000% 
Loss D Fake: 0.2255 (0.3151) Acc D Fake: 91.329% 
Loss D: 0.392 
Loss G: 1.9864 (1.5320) Acc G: 8.671% 
LR: 2.000e-04 

2023-03-01 13:45:16,359 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.1608 (0.3392) Acc D Real: 100.000% 
Loss D Fake: 0.2340 (0.3141) Acc D Fake: 91.289% 
Loss D: 0.395 
Loss G: 1.9431 (1.5375) Acc G: 8.711% 
LR: 2.000e-04 

2023-03-01 13:45:16,367 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.1608 (0.3368) Acc D Real: 100.000% 
Loss D Fake: 0.2468 (0.3132) Acc D Fake: 91.250% 
Loss D: 0.408 
Loss G: 1.8772 (1.5420) Acc G: 8.750% 
LR: 2.000e-04 

2023-03-01 13:45:16,374 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.1566 (0.3345) Acc D Real: 100.000% 
Loss D Fake: 0.2657 (0.3126) Acc D Fake: 91.190% 
Loss D: 0.422 
Loss G: 1.7832 (1.5451) Acc G: 8.810% 
LR: 2.000e-04 

2023-03-01 13:45:16,382 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1551 (0.3322) Acc D Real: 100.000% 
Loss D Fake: 0.2918 (0.3123) Acc D Fake: 91.132% 
Loss D: 0.447 
Loss G: 1.6635 (1.5466) Acc G: 8.868% 
LR: 2.000e-04 

2023-03-01 13:45:16,389 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.1538 (0.3299) Acc D Real: 100.000% 
Loss D Fake: 0.3248 (0.3124) Acc D Fake: 91.055% 
Loss D: 0.479 
Loss G: 1.5298 (1.5464) Acc G: 8.945% 
LR: 2.000e-04 

2023-03-01 13:45:16,397 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1500 (0.3277) Acc D Real: 100.000% 
Loss D Fake: 0.3653 (0.3131) Acc D Fake: 90.979% 
Loss D: 0.515 
Loss G: 1.3944 (1.5445) Acc G: 9.021% 
LR: 2.000e-04 

2023-03-01 13:45:16,404 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.1460 (0.3254) Acc D Real: 100.000% 
Loss D Fake: 0.4145 (0.3144) Acc D Fake: 90.885% 
Loss D: 0.560 
Loss G: 1.2745 (1.5412) Acc G: 9.095% 
LR: 2.000e-04 

2023-03-01 13:45:16,412 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.1465 (0.3232) Acc D Real: 100.000% 
Loss D Fake: 0.4617 (0.3162) Acc D Fake: 90.793% 
Loss D: 0.608 
Loss G: 1.2030 (1.5370) Acc G: 9.187% 
LR: 2.000e-04 

2023-03-01 13:45:16,419 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.1465 (0.3211) Acc D Real: 100.000% 
Loss D Fake: 0.4681 (0.3180) Acc D Fake: 90.703% 
Loss D: 0.615 
Loss G: 1.1872 (1.5328) Acc G: 9.257% 
LR: 2.000e-04 

2023-03-01 13:45:16,427 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.1466 (0.3190) Acc D Real: 100.000% 
Loss D Fake: 0.4495 (0.3195) Acc D Fake: 90.615% 
Loss D: 0.596 
Loss G: 1.1979 (1.5288) Acc G: 9.325% 
LR: 2.000e-04 

2023-03-01 13:45:16,434 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.1447 (0.3170) Acc D Real: 100.000% 
Loss D Fake: 0.4319 (0.3209) Acc D Fake: 90.549% 
Loss D: 0.577 
Loss G: 1.2156 (1.5252) Acc G: 9.392% 
LR: 2.000e-04 

2023-03-01 13:45:16,441 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1430 (0.3150) Acc D Real: 100.000% 
Loss D Fake: 0.4179 (0.3220) Acc D Fake: 90.484% 
Loss D: 0.561 
Loss G: 1.2351 (1.5218) Acc G: 9.457% 
LR: 2.000e-04 

2023-03-01 13:45:16,449 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.1510 (0.3131) Acc D Real: 100.000% 
Loss D Fake: 0.4067 (0.3230) Acc D Fake: 90.421% 
Loss D: 0.558 
Loss G: 1.2546 (1.5187) Acc G: 9.521% 
LR: 2.000e-04 

2023-03-01 13:45:16,456 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.1441 (0.3112) Acc D Real: 100.000% 
Loss D Fake: 0.3976 (0.3238) Acc D Fake: 90.360% 
Loss D: 0.542 
Loss G: 1.2738 (1.5159) Acc G: 9.564% 
LR: 2.000e-04 

2023-03-01 13:45:16,464 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.1574 (0.3094) Acc D Real: 99.994% 
Loss D Fake: 0.3904 (0.3246) Acc D Fake: 90.300% 
Loss D: 0.548 
Loss G: 1.2905 (1.5134) Acc G: 9.607% 
LR: 2.000e-04 

2023-03-01 13:45:16,471 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.1725 (0.3079) Acc D Real: 99.963% 
Loss D Fake: 0.3857 (0.3252) Acc D Fake: 90.258% 
Loss D: 0.558 
Loss G: 1.3014 (1.5110) Acc G: 9.648% 
LR: 2.000e-04 

2023-03-01 13:45:16,479 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.1445 (0.3061) Acc D Real: 99.963% 
Loss D Fake: 0.3835 (0.3259) Acc D Fake: 90.211% 
Loss D: 0.528 
Loss G: 1.3097 (1.5088) Acc G: 9.689% 
LR: 2.000e-04 

2023-03-01 13:45:16,486 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.1432 (0.3043) Acc D Real: 99.964% 
Loss D Fake: 0.3825 (0.3265) Acc D Fake: 90.154% 
Loss D: 0.526 
Loss G: 1.3156 (1.5067) Acc G: 9.728% 
LR: 2.000e-04 

2023-03-01 13:45:16,494 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.1507 (0.3027) Acc D Real: 99.964% 
Loss D Fake: 0.3830 (0.3271) Acc D Fake: 90.099% 
Loss D: 0.534 
Loss G: 1.3169 (1.5047) Acc G: 9.767% 
LR: 2.000e-04 

2023-03-01 13:45:16,501 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.1605 (0.3012) Acc D Real: 99.963% 
Loss D Fake: 0.3876 (0.3278) Acc D Fake: 90.044% 
Loss D: 0.548 
Loss G: 1.2974 (1.5025) Acc G: 9.823% 
LR: 2.000e-04 

2023-03-01 13:45:16,509 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.1699 (0.2998) Acc D Real: 99.931% 
Loss D Fake: 0.4027 (0.3285) Acc D Fake: 89.991% 
Loss D: 0.573 
Loss G: 1.2538 (1.4999) Acc G: 9.877% 
LR: 2.000e-04 

2023-03-01 13:45:16,516 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.1393 (0.2981) Acc D Real: 99.932% 
Loss D Fake: 0.4284 (0.3296) Acc D Fake: 89.939% 
Loss D: 0.568 
Loss G: 1.1975 (1.4967) Acc G: 9.931% 
LR: 2.000e-04 

2023-03-01 13:45:16,523 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1509 (0.2966) Acc D Real: 99.931% 
Loss D Fake: 0.4661 (0.3310) Acc D Fake: 89.871% 
Loss D: 0.617 
Loss G: 1.1302 (1.4929) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-01 13:45:16,531 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.1361 (0.2950) Acc D Real: 99.932% 
Loss D Fake: 0.5257 (0.3330) Acc D Fake: 89.787% 
Loss D: 0.662 
Loss G: 1.0960 (1.4889) Acc G: 10.068% 
LR: 2.000e-04 

2023-03-01 13:45:16,538 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.1545 (0.2935) Acc D Real: 99.926% 
Loss D Fake: 0.5347 (0.3350) Acc D Fake: 89.705% 
Loss D: 0.689 
Loss G: 1.1084 (1.4850) Acc G: 10.135% 
LR: 2.000e-04 

2023-03-01 13:45:16,546 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2191 (0.2928) Acc D Real: 99.874% 
Loss D Fake: 0.5078 (0.3367) Acc D Fake: 89.642% 
Loss D: 0.727 
Loss G: 1.1188 (1.4814) Acc G: 10.200% 
LR: 2.000e-04 

2023-03-01 13:45:16,555 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2724 (0.2926) Acc D Real: 99.796% 
Loss D Fake: 0.5027 (0.3384) Acc D Fake: 89.579% 
Loss D: 0.775 
Loss G: 1.1017 (1.4776) Acc G: 10.248% 
LR: 2.000e-04 

2023-03-01 13:45:16,564 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2539 (0.2922) Acc D Real: 99.742% 
Loss D Fake: 0.5211 (0.3402) Acc D Fake: 89.518% 
Loss D: 0.775 
Loss G: 1.0550 (1.4735) Acc G: 10.294% 
LR: 2.000e-04 

2023-03-01 13:45:16,572 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4764 (0.2940) Acc D Real: 99.542% 
Loss D Fake: 0.5928 (0.3426) Acc D Fake: 89.458% 
Loss D: 1.069 
Loss G: 0.9914 (1.4688) Acc G: 10.340% 
LR: 2.000e-04 

2023-03-01 13:45:16,581 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.5247 (0.2962) Acc D Real: 99.333% 
Loss D Fake: 1.1786 (0.3507) Acc D Fake: 88.854% 
Loss D: 1.703 
Loss G: 1.0115 (1.4644) Acc G: 10.385% 
LR: 2.000e-04 

2023-03-01 13:45:16,589 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.9488 (0.3024) Acc D Real: 98.909% 
Loss D Fake: 0.5144 (0.3522) Acc D Fake: 88.817% 
Loss D: 1.463 
Loss G: 1.0515 (1.4605) Acc G: 10.413% 
LR: 2.000e-04 

2023-03-01 13:45:16,598 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 1.0306 (0.3093) Acc D Real: 98.445% 
Loss D Fake: 0.4960 (0.3536) Acc D Fake: 88.781% 
Loss D: 1.527 
Loss G: 1.0299 (1.4564) Acc G: 10.440% 
LR: 2.000e-04 

2023-03-01 13:45:16,606 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 1.2384 (0.3180) Acc D Real: 97.898% 
Loss D Fake: 0.5050 (0.3550) Acc D Fake: 88.746% 
Loss D: 1.743 
Loss G: 0.9926 (1.4521) Acc G: 10.467% 
LR: 2.000e-04 

2023-03-01 13:45:16,614 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 1.2290 (0.3264) Acc D Real: 97.377% 
Loss D Fake: 0.5214 (0.3565) Acc D Fake: 88.711% 
Loss D: 1.750 
Loss G: 0.9531 (1.4475) Acc G: 10.509% 
LR: 2.000e-04 

2023-03-01 13:45:16,623 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 1.7081 (0.3391) Acc D Real: 96.642% 
Loss D Fake: 0.5493 (0.3583) Acc D Fake: 88.662% 
Loss D: 2.257 
Loss G: 0.8965 (1.4424) Acc G: 10.566% 
LR: 2.000e-04 

2023-03-01 13:45:16,632 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 1.3116 (0.3479) Acc D Real: 96.048% 
Loss D Fake: 0.6245 (0.3607) Acc D Fake: 88.598% 
Loss D: 1.936 
Loss G: 0.8254 (1.4368) Acc G: 10.636% 
LR: 2.000e-04 

2023-03-01 13:45:16,640 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 1.5414 (0.3587) Acc D Real: 95.369% 
Loss D Fake: 1.5183 (0.3712) Acc D Fake: 87.905% 
Loss D: 3.060 
Loss G: 0.8779 (1.4318) Acc G: 10.706% 
LR: 2.000e-04 

2023-03-01 13:45:16,648 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 1.5387 (0.3692) Acc D Real: 94.649% 
Loss D Fake: 0.5419 (0.3727) Acc D Fake: 87.850% 
Loss D: 2.081 
Loss G: 0.9457 (1.4274) Acc G: 10.759% 
LR: 2.000e-04 

2023-03-01 13:45:16,655 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 1.5267 (0.3795) Acc D Real: 93.951% 
Loss D Fake: 0.5120 (0.3739) Acc D Fake: 87.810% 
Loss D: 2.039 
Loss G: 0.9688 (1.4234) Acc G: 10.811% 
LR: 2.000e-04 

2023-03-01 13:45:16,664 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 1.5004 (0.3893) Acc D Real: 93.242% 
Loss D Fake: 0.5024 (0.3750) Acc D Fake: 87.770% 
Loss D: 2.003 
Loss G: 0.9736 (1.4194) Acc G: 10.863% 
LR: 2.000e-04 

2023-03-01 13:45:16,671 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 1.3930 (0.3980) Acc D Real: 92.566% 
Loss D Fake: 0.5013 (0.3761) Acc D Fake: 87.732% 
Loss D: 1.894 
Loss G: 0.9691 (1.4155) Acc G: 10.913% 
LR: 2.000e-04 

2023-03-01 13:45:16,679 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 1.3567 (0.4063) Acc D Real: 91.878% 
Loss D Fake: 0.5046 (0.3772) Acc D Fake: 87.694% 
Loss D: 1.861 
Loss G: 0.9597 (1.4116) Acc G: 10.963% 
LR: 2.000e-04 

2023-03-01 13:45:16,686 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 1.2675 (0.4137) Acc D Real: 91.205% 
Loss D Fake: 0.5103 (0.3784) Acc D Fake: 87.657% 
Loss D: 1.778 
Loss G: 0.9479 (1.4076) Acc G: 11.011% 
LR: 2.000e-04 

2023-03-01 13:45:16,694 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 1.2514 (0.4208) Acc D Real: 90.539% 
Loss D Fake: 0.5172 (0.3796) Acc D Fake: 87.620% 
Loss D: 1.769 
Loss G: 0.9350 (1.4036) Acc G: 11.059% 
LR: 2.000e-04 

2023-03-01 13:45:16,702 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 1.2068 (0.4274) Acc D Real: 89.884% 
Loss D Fake: 0.5249 (0.3808) Acc D Fake: 87.584% 
Loss D: 1.732 
Loss G: 0.9218 (1.3995) Acc G: 11.106% 
LR: 2.000e-04 

2023-03-01 13:45:16,709 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 1.1890 (0.4337) Acc D Real: 89.240% 
Loss D Fake: 0.5329 (0.3820) Acc D Fake: 87.549% 
Loss D: 1.722 
Loss G: 0.9087 (1.3955) Acc G: 11.153% 
LR: 2.000e-04 

2023-03-01 13:45:16,718 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 1.1569 (0.4397) Acc D Real: 88.609% 
Loss D Fake: 0.5410 (0.3834) Acc D Fake: 87.514% 
Loss D: 1.698 
Loss G: 0.8959 (1.3913) Acc G: 11.198% 
LR: 2.000e-04 

2023-03-01 13:45:16,725 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 1.1316 (0.4454) Acc D Real: 87.987% 
Loss D Fake: 0.5491 (0.3847) Acc D Fake: 87.480% 
Loss D: 1.681 
Loss G: 0.8836 (1.3872) Acc G: 11.243% 
LR: 2.000e-04 

2023-03-01 13:45:16,733 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 1.1125 (0.4508) Acc D Real: 87.376% 
Loss D Fake: 0.5570 (0.3861) Acc D Fake: 87.446% 
Loss D: 1.670 
Loss G: 0.8718 (1.3830) Acc G: 11.300% 
LR: 2.000e-04 

2023-03-01 13:45:16,741 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 1.0898 (0.4559) Acc D Real: 86.777% 
Loss D Fake: 0.5648 (0.3876) Acc D Fake: 87.399% 
Loss D: 1.655 
Loss G: 0.8605 (1.3788) Acc G: 11.357% 
LR: 2.000e-04 

2023-03-01 13:45:16,748 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 1.0970 (0.4611) Acc D Real: 86.179% 
Loss D Fake: 0.5724 (0.3890) Acc D Fake: 87.353% 
Loss D: 1.669 
Loss G: 0.8499 (1.3745) Acc G: 11.413% 
LR: 2.000e-04 

2023-03-01 13:45:16,756 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 1.0596 (0.4658) Acc D Real: 85.597% 
Loss D Fake: 0.5798 (0.3906) Acc D Fake: 87.308% 
Loss D: 1.639 
Loss G: 0.8397 (1.3703) Acc G: 11.468% 
LR: 2.000e-04 

2023-03-01 13:45:16,764 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 1.0365 (0.4703) Acc D Real: 85.029% 
Loss D Fake: 0.5869 (0.3921) Acc D Fake: 87.264% 
Loss D: 1.623 
Loss G: 0.8301 (1.3660) Acc G: 11.522% 
LR: 2.000e-04 

2023-03-01 13:45:16,771 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 1.0450 (0.4748) Acc D Real: 84.462% 
Loss D Fake: 0.5937 (0.3937) Acc D Fake: 87.220% 
Loss D: 1.639 
Loss G: 0.8210 (1.3618) Acc G: 11.575% 
LR: 2.000e-04 

2023-03-01 13:45:16,778 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 1.0120 (0.4790) Acc D Real: 83.911% 
Loss D Fake: 0.6004 (0.3953) Acc D Fake: 87.177% 
Loss D: 1.612 
Loss G: 0.8124 (1.3575) Acc G: 11.628% 
LR: 2.000e-04 

2023-03-01 13:45:16,786 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.9965 (0.4829) Acc D Real: 83.369% 
Loss D Fake: 0.6068 (0.3969) Acc D Fake: 87.122% 
Loss D: 1.603 
Loss G: 0.8042 (1.3533) Acc G: 11.692% 
LR: 2.000e-04 

2023-03-01 13:45:16,793 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.9995 (0.4869) Acc D Real: 82.834% 
Loss D Fake: 0.6129 (0.3986) Acc D Fake: 87.067% 
Loss D: 1.612 
Loss G: 0.7964 (1.3490) Acc G: 11.755% 
LR: 2.000e-04 

2023-03-01 13:45:16,800 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.9817 (0.4906) Acc D Real: 82.309% 
Loss D Fake: 0.6189 (0.4002) Acc D Fake: 87.014% 
Loss D: 1.601 
Loss G: 0.7890 (1.3448) Acc G: 11.818% 
LR: 2.000e-04 

2023-03-01 13:45:16,808 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.9787 (0.4943) Acc D Real: 81.790% 
Loss D Fake: 0.6246 (0.4019) Acc D Fake: 86.961% 
Loss D: 1.603 
Loss G: 0.7820 (1.3405) Acc G: 11.879% 
LR: 2.000e-04 

2023-03-01 13:45:16,815 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.9717 (0.4979) Acc D Real: 81.281% 
Loss D Fake: 0.6301 (0.4036) Acc D Fake: 86.909% 
Loss D: 1.602 
Loss G: 0.7753 (1.3363) Acc G: 11.940% 
LR: 2.000e-04 

2023-03-01 13:45:16,823 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.9584 (0.5013) Acc D Real: 80.782% 
Loss D Fake: 0.6354 (0.4053) Acc D Fake: 86.858% 
Loss D: 1.594 
Loss G: 0.7689 (1.3321) Acc G: 12.000% 
LR: 2.000e-04 

2023-03-01 13:45:16,830 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.9511 (0.5046) Acc D Real: 80.291% 
Loss D Fake: 0.6405 (0.4071) Acc D Fake: 86.795% 
Loss D: 1.592 
Loss G: 0.7629 (1.3279) Acc G: 12.071% 
LR: 2.000e-04 

2023-03-01 13:45:16,837 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.9356 (0.5077) Acc D Real: 79.808% 
Loss D Fake: 0.6455 (0.4088) Acc D Fake: 86.734% 
Loss D: 1.581 
Loss G: 0.7571 (1.3238) Acc G: 12.141% 
LR: 2.000e-04 

2023-03-01 13:45:16,844 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.9457 (0.5109) Acc D Real: 79.327% 
Loss D Fake: 0.6503 (0.4105) Acc D Fake: 86.673% 
Loss D: 1.596 
Loss G: 0.7516 (1.3196) Acc G: 12.210% 
LR: 2.000e-04 

2023-03-01 13:45:16,851 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.9262 (0.5139) Acc D Real: 78.858% 
Loss D Fake: 0.6549 (0.4123) Acc D Fake: 86.613% 
Loss D: 1.581 
Loss G: 0.7463 (1.3155) Acc G: 12.278% 
LR: 2.000e-04 

2023-03-01 13:45:16,859 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.9142 (0.5168) Acc D Real: 78.399% 
Loss D Fake: 0.6594 (0.4141) Acc D Fake: 86.554% 
Loss D: 1.574 
Loss G: 0.7412 (1.3114) Acc G: 12.345% 
LR: 2.000e-04 

2023-03-01 13:45:16,866 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.9045 (0.5195) Acc D Real: 77.946% 
Loss D Fake: 0.6637 (0.4158) Acc D Fake: 86.495% 
Loss D: 1.568 
Loss G: 0.7363 (1.3073) Acc G: 12.411% 
LR: 2.000e-04 

2023-03-01 13:45:16,873 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.9114 (0.5223) Acc D Real: 77.496% 
Loss D Fake: 0.6680 (0.4176) Acc D Fake: 86.429% 
Loss D: 1.579 
Loss G: 0.7315 (1.3033) Acc G: 12.488% 
LR: 2.000e-04 

2023-03-01 13:45:16,880 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.8938 (0.5249) Acc D Real: 77.056% 
Loss D Fake: 0.6721 (0.4194) Acc D Fake: 86.361% 
Loss D: 1.566 
Loss G: 0.7270 (1.2992) Acc G: 12.564% 
LR: 2.000e-04 

2023-03-01 13:45:16,887 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.8880 (0.5274) Acc D Real: 76.623% 
Loss D Fake: 0.6760 (0.4212) Acc D Fake: 86.293% 
Loss D: 1.564 
Loss G: 0.7227 (1.2952) Acc G: 12.639% 
LR: 2.000e-04 

2023-03-01 13:45:16,895 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.8833 (0.5298) Acc D Real: 76.196% 
Loss D Fake: 0.6799 (0.4230) Acc D Fake: 86.227% 
Loss D: 1.563 
Loss G: 0.7184 (1.2912) Acc G: 12.712% 
LR: 2.000e-04 

2023-03-01 13:45:16,902 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.8795 (0.5322) Acc D Real: 75.775% 
Loss D Fake: 0.6837 (0.4247) Acc D Fake: 86.162% 
Loss D: 1.563 
Loss G: 0.7144 (1.2873) Acc G: 12.785% 
LR: 2.000e-04 

2023-03-01 13:45:16,909 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.8765 (0.5346) Acc D Real: 75.359% 
Loss D Fake: 0.6874 (0.4265) Acc D Fake: 86.097% 
Loss D: 1.564 
Loss G: 0.7104 (1.2834) Acc G: 12.857% 
LR: 2.000e-04 

2023-03-01 13:45:16,917 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.8621 (0.5368) Acc D Real: 74.952% 
Loss D Fake: 0.6910 (0.4283) Acc D Fake: 86.022% 
Loss D: 1.553 
Loss G: 0.7066 (1.2795) Acc G: 12.939% 
LR: 2.000e-04 

2023-03-01 13:45:16,924 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.8450 (0.5389) Acc D Real: 74.555% 
Loss D Fake: 0.6945 (0.4301) Acc D Fake: 85.948% 
Loss D: 1.539 
Loss G: 0.7028 (1.2756) Acc G: 13.020% 
LR: 2.000e-04 

2023-03-01 13:45:16,931 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.8636 (0.5410) Acc D Real: 74.158% 
Loss D Fake: 0.6980 (0.4319) Acc D Fake: 85.875% 
Loss D: 1.562 
Loss G: 0.6991 (1.2718) Acc G: 13.100% 
LR: 2.000e-04 

2023-03-01 13:45:16,939 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.8526 (0.5431) Acc D Real: 73.767% 
Loss D Fake: 0.7015 (0.4337) Acc D Fake: 85.803% 
Loss D: 1.554 
Loss G: 0.6955 (1.2679) Acc G: 13.178% 
LR: 2.000e-04 

2023-03-01 13:45:16,946 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.8390 (0.5450) Acc D Real: 73.385% 
Loss D Fake: 0.7049 (0.4355) Acc D Fake: 85.732% 
Loss D: 1.544 
Loss G: 0.6920 (1.2642) Acc G: 13.267% 
LR: 2.000e-04 

2023-03-01 13:45:16,953 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.8547 (0.5471) Acc D Real: 73.003% 
Loss D Fake: 0.7082 (0.4372) Acc D Fake: 85.651% 
Loss D: 1.563 
Loss G: 0.6886 (1.2604) Acc G: 13.355% 
LR: 2.000e-04 

2023-03-01 13:45:16,960 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.8240 (0.5489) Acc D Real: 72.633% 
Loss D Fake: 0.7114 (0.4390) Acc D Fake: 85.571% 
Loss D: 1.535 
Loss G: 0.6853 (1.2567) Acc G: 13.441% 
LR: 2.000e-04 

2023-03-01 13:45:16,968 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.8354 (0.5507) Acc D Real: 72.265% 
Loss D Fake: 0.7146 (0.4408) Acc D Fake: 85.492% 
Loss D: 1.550 
Loss G: 0.6821 (1.2530) Acc G: 13.527% 
LR: 2.000e-04 

2023-03-01 13:45:16,975 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.8389 (0.5526) Acc D Real: 71.898% 
Loss D Fake: 0.7178 (0.4426) Acc D Fake: 85.414% 
Loss D: 1.557 
Loss G: 0.6789 (1.2493) Acc G: 13.621% 
LR: 2.000e-04 

2023-03-01 13:45:16,983 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.8153 (0.5542) Acc D Real: 71.542% 
Loss D Fake: 0.7209 (0.4444) Acc D Fake: 85.326% 
Loss D: 1.536 
Loss G: 0.6758 (1.2456) Acc G: 13.715% 
LR: 2.000e-04 

2023-03-01 13:45:16,990 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.7978 (0.5558) Acc D Real: 71.200% 
Loss D Fake: 0.7241 (0.4461) Acc D Fake: 85.240% 
Loss D: 1.522 
Loss G: 0.6724 (1.2420) Acc G: 13.808% 
LR: 2.000e-04 

2023-03-01 13:45:16,997 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.8107 (0.5574) Acc D Real: 70.853% 
Loss D Fake: 0.7276 (0.4479) Acc D Fake: 85.144% 
Loss D: 1.538 
Loss G: 0.6691 (1.2384) Acc G: 13.910% 
LR: 2.000e-04 

2023-03-01 13:45:17,004 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.8170 (0.5590) Acc D Real: 70.507% 
Loss D Fake: 0.7309 (0.4497) Acc D Fake: 85.049% 
Loss D: 1.548 
Loss G: 0.6658 (1.2348) Acc G: 14.021% 
LR: 2.000e-04 

2023-03-01 13:45:17,012 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.8099 (0.5605) Acc D Real: 70.168% 
Loss D Fake: 0.7341 (0.4514) Acc D Fake: 84.946% 
Loss D: 1.544 
Loss G: 0.6627 (1.2313) Acc G: 14.130% 
LR: 2.000e-04 

2023-03-01 13:45:17,019 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.8003 (0.5620) Acc D Real: 69.835% 
Loss D Fake: 0.7373 (0.4532) Acc D Fake: 84.833% 
Loss D: 1.538 
Loss G: 0.6597 (1.2277) Acc G: 14.259% 
LR: 2.000e-04 

2023-03-01 13:45:17,027 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.8056 (0.5635) Acc D Real: 69.503% 
Loss D Fake: 0.7404 (0.4550) Acc D Fake: 84.701% 
Loss D: 1.546 
Loss G: 0.6567 (1.2242) Acc G: 14.396% 
LR: 2.000e-04 

2023-03-01 13:45:17,034 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.7831 (0.5649) Acc D Real: 69.183% 
Loss D Fake: 0.7435 (0.4567) Acc D Fake: 84.551% 
Loss D: 1.527 
Loss G: 0.6538 (1.2207) Acc G: 14.563% 
LR: 2.000e-04 

2023-03-01 13:45:17,041 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.7966 (0.5663) Acc D Real: 68.867% 
Loss D Fake: 0.7465 (0.4585) Acc D Fake: 84.382% 
Loss D: 1.543 
Loss G: 0.6509 (1.2173) Acc G: 14.757% 
LR: 2.000e-04 

2023-03-01 13:45:17,048 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.7771 (0.5675) Acc D Real: 68.557% 
Loss D Fake: 0.7495 (0.4602) Acc D Fake: 84.185% 
Loss D: 1.527 
Loss G: 0.6481 (1.2139) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-01 13:45:17,055 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.7858 (0.5688) Acc D Real: 68.245% 
Loss D Fake: 0.7524 (0.4620) Acc D Fake: 83.970% 
Loss D: 1.538 
Loss G: 0.6454 (1.2105) Acc G: 15.209% 
LR: 2.000e-04 

2023-03-01 13:45:17,063 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.7762 (0.5701) Acc D Real: 67.940% 
Loss D Fake: 0.7553 (0.4637) Acc D Fake: 83.728% 
Loss D: 1.532 
Loss G: 0.6426 (1.2071) Acc G: 15.477% 
LR: 2.000e-04 

2023-03-01 13:45:17,070 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.7445 (0.5711) Acc D Real: 67.657% 
Loss D Fake: 0.7583 (0.4655) Acc D Fake: 83.429% 
Loss D: 1.503 
Loss G: 0.6398 (1.2037) Acc G: 15.938% 
LR: 2.000e-04 

2023-03-01 13:45:17,078 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.7698 (0.5723) Acc D Real: 67.359% 
Loss D Fake: 0.7613 (0.4672) Acc D Fake: 82.939% 
Loss D: 1.531 
Loss G: 0.6371 (1.2004) Acc G: 16.433% 
LR: 2.000e-04 

2023-03-01 13:45:17,085 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.7559 (0.5734) Acc D Real: 67.070% 
Loss D Fake: 0.7643 (0.4689) Acc D Fake: 82.454% 
Loss D: 1.520 
Loss G: 0.6343 (1.1971) Acc G: 16.921% 
LR: 2.000e-04 

2023-03-01 13:45:17,092 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.7492 (0.5744) Acc D Real: 66.787% 
Loss D Fake: 0.7673 (0.4707) Acc D Fake: 81.974% 
Loss D: 1.516 
Loss G: 0.6316 (1.1938) Acc G: 17.404% 
LR: 2.000e-04 

2023-03-01 13:45:17,100 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.7479 (0.5754) Acc D Real: 66.507% 
Loss D Fake: 0.7702 (0.4724) Acc D Fake: 81.500% 
Loss D: 1.518 
Loss G: 0.6290 (1.1905) Acc G: 17.882% 
LR: 2.000e-04 

2023-03-01 13:45:17,107 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.7536 (0.5764) Acc D Real: 66.228% 
Loss D Fake: 0.7732 (0.4741) Acc D Fake: 81.032% 
Loss D: 1.527 
Loss G: 0.6264 (1.1873) Acc G: 18.354% 
LR: 2.000e-04 

2023-03-01 13:45:17,114 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.7400 (0.5773) Acc D Real: 65.957% 
Loss D Fake: 0.7760 (0.4759) Acc D Fake: 80.569% 
Loss D: 1.516 
Loss G: 0.6238 (1.1841) Acc G: 18.820% 
LR: 2.000e-04 

2023-03-01 13:45:17,122 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.7379 (0.5782) Acc D Real: 65.700% 
Loss D Fake: 0.7789 (0.4776) Acc D Fake: 80.111% 
Loss D: 1.517 
Loss G: 0.6212 (1.1809) Acc G: 19.281% 
LR: 2.000e-04 

2023-03-01 13:45:17,129 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.7338 (0.5791) Acc D Real: 65.439% 
Loss D Fake: 0.7818 (0.4793) Acc D Fake: 79.659% 
Loss D: 1.516 
Loss G: 0.6186 (1.1777) Acc G: 19.738% 
LR: 2.000e-04 

2023-03-01 13:45:17,136 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.7376 (0.5800) Acc D Real: 65.179% 
Loss D Fake: 0.7847 (0.4810) Acc D Fake: 79.211% 
Loss D: 1.522 
Loss G: 0.6160 (1.1745) Acc G: 20.188% 
LR: 2.000e-04 

2023-03-01 13:45:17,143 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.7259 (0.5808) Acc D Real: 64.926% 
Loss D Fake: 0.7876 (0.4827) Acc D Fake: 78.769% 
Loss D: 1.514 
Loss G: 0.6135 (1.1714) Acc G: 20.634% 
LR: 2.000e-04 

2023-03-01 13:45:17,152 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.7321 (0.5817) Acc D Real: 64.670% 
Loss D Fake: 0.7905 (0.4844) Acc D Fake: 78.331% 
Loss D: 1.523 
Loss G: 0.6110 (1.1683) Acc G: 21.075% 
LR: 2.000e-04 

2023-03-01 13:45:17,160 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.7181 (0.5824) Acc D Real: 64.424% 
Loss D Fake: 0.7934 (0.4861) Acc D Fake: 77.898% 
Loss D: 1.511 
Loss G: 0.6085 (1.1652) Acc G: 21.511% 
LR: 2.000e-04 

2023-03-01 13:45:17,167 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.7172 (0.5832) Acc D Real: 64.182% 
Loss D Fake: 0.7963 (0.4879) Acc D Fake: 77.470% 
Loss D: 1.513 
Loss G: 0.6060 (1.1621) Acc G: 21.943% 
LR: 2.000e-04 

2023-03-01 13:45:17,175 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.7173 (0.5839) Acc D Real: 63.940% 
Loss D Fake: 0.7992 (0.4896) Acc D Fake: 77.047% 
Loss D: 1.517 
Loss G: 0.6034 (1.1591) Acc G: 22.369% 
LR: 2.000e-04 

2023-03-01 13:45:17,183 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.7144 (0.5846) Acc D Real: 63.704% 
Loss D Fake: 0.8021 (0.4913) Acc D Fake: 76.628% 
Loss D: 1.517 
Loss G: 0.6009 (1.1560) Acc G: 22.791% 
LR: 2.000e-04 

2023-03-01 13:45:17,191 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.7084 (0.5853) Acc D Real: 63.484% 
Loss D Fake: 0.8051 (0.4929) Acc D Fake: 76.214% 
Loss D: 1.513 
Loss G: 0.5985 (1.1530) Acc G: 23.208% 
LR: 2.000e-04 

2023-03-01 13:45:17,198 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.6949 (0.5859) Acc D Real: 63.275% 
Loss D Fake: 0.8080 (0.4946) Acc D Fake: 75.804% 
Loss D: 1.503 
Loss G: 0.5960 (1.1500) Acc G: 23.621% 
LR: 2.000e-04 

2023-03-01 13:45:17,205 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.7012 (0.5865) Acc D Real: 63.054% 
Loss D Fake: 0.8110 (0.4963) Acc D Fake: 75.399% 
Loss D: 1.512 
Loss G: 0.5935 (1.1471) Acc G: 24.030% 
LR: 2.000e-04 

2023-03-01 13:45:17,213 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.6941 (0.5871) Acc D Real: 62.845% 
Loss D Fake: 0.8139 (0.4980) Acc D Fake: 74.998% 
Loss D: 1.508 
Loss G: 0.5910 (1.1441) Acc G: 24.434% 
LR: 2.000e-04 

2023-03-01 13:45:17,221 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.6944 (0.5876) Acc D Real: 62.652% 
Loss D Fake: 0.8169 (0.4997) Acc D Fake: 74.601% 
Loss D: 1.511 
Loss G: 0.5885 (1.1412) Acc G: 24.834% 
LR: 2.000e-04 

2023-03-01 13:45:17,228 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.6856 (0.5881) Acc D Real: 62.477% 
Loss D Fake: 0.8199 (0.5014) Acc D Fake: 74.208% 
Loss D: 1.505 
Loss G: 0.5860 (1.1382) Acc G: 25.229% 
LR: 2.000e-04 

2023-03-01 13:45:17,236 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.6811 (0.5886) Acc D Real: 62.323% 
Loss D Fake: 0.8229 (0.5031) Acc D Fake: 73.820% 
Loss D: 1.504 
Loss G: 0.5836 (1.1353) Acc G: 25.621% 
LR: 2.000e-04 

2023-03-01 13:45:17,243 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.6814 (0.5891) Acc D Real: 62.180% 
Loss D Fake: 0.8259 (0.5048) Acc D Fake: 73.435% 
Loss D: 1.507 
Loss G: 0.5811 (1.1324) Acc G: 26.008% 
LR: 2.000e-04 

2023-03-01 13:45:17,251 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.6782 (0.5896) Acc D Real: 62.059% 
Loss D Fake: 0.8290 (0.5064) Acc D Fake: 73.055% 
Loss D: 1.507 
Loss G: 0.5786 (1.1296) Acc G: 26.391% 
LR: 2.000e-04 

2023-03-01 13:45:17,258 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.6661 (0.5900) Acc D Real: 62.008% 
Loss D Fake: 0.8320 (0.5081) Acc D Fake: 72.678% 
Loss D: 1.498 
Loss G: 0.5762 (1.1267) Acc G: 26.771% 
LR: 2.000e-04 

2023-03-01 13:45:17,266 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.6683 (0.5904) Acc D Real: 61.954% 
Loss D Fake: 0.8351 (0.5098) Acc D Fake: 72.306% 
Loss D: 1.503 
Loss G: 0.5737 (1.1239) Acc G: 27.146% 
LR: 2.000e-04 

2023-03-01 13:45:17,273 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.6639 (0.5907) Acc D Real: 61.910% 
Loss D Fake: 0.8382 (0.5115) Acc D Fake: 71.937% 
Loss D: 1.502 
Loss G: 0.5712 (1.1211) Acc G: 27.518% 
LR: 2.000e-04 

2023-03-01 13:45:17,281 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.6535 (0.5911) Acc D Real: 61.947% 
Loss D Fake: 0.8413 (0.5131) Acc D Fake: 71.571% 
Loss D: 1.495 
Loss G: 0.5688 (1.1183) Acc G: 27.886% 
LR: 2.000e-04 

2023-03-01 13:45:17,288 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.6560 (0.5914) Acc D Real: 61.956% 
Loss D Fake: 0.8444 (0.5148) Acc D Fake: 71.210% 
Loss D: 1.500 
Loss G: 0.5663 (1.1155) Acc G: 28.250% 
LR: 2.000e-04 

2023-03-01 13:45:17,296 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.6518 (0.5917) Acc D Real: 61.978% 
Loss D Fake: 0.8475 (0.5165) Acc D Fake: 70.852% 
Loss D: 1.499 
Loss G: 0.5639 (1.1127) Acc G: 28.611% 
LR: 2.000e-04 

2023-03-01 13:45:17,303 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.6495 (0.5920) Acc D Real: 62.033% 
Loss D Fake: 0.8507 (0.5182) Acc D Fake: 70.498% 
Loss D: 1.500 
Loss G: 0.5614 (1.1099) Acc G: 28.968% 
LR: 2.000e-04 

2023-03-01 13:45:17,311 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.6443 (0.5922) Acc D Real: 62.112% 
Loss D Fake: 0.8538 (0.5198) Acc D Fake: 70.147% 
Loss D: 1.498 
Loss G: 0.5590 (1.1072) Acc G: 29.321% 
LR: 2.000e-04 

2023-03-01 13:45:17,318 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.6478 (0.5925) Acc D Real: 62.187% 
Loss D Fake: 0.8570 (0.5215) Acc D Fake: 69.800% 
Loss D: 1.505 
Loss G: 0.5566 (1.1045) Acc G: 29.671% 
LR: 2.000e-04 

2023-03-01 13:45:17,326 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.6383 (0.5927) Acc D Real: 62.310% 
Loss D Fake: 0.8601 (0.5232) Acc D Fake: 69.456% 
Loss D: 1.498 
Loss G: 0.5541 (1.1018) Acc G: 30.017% 
LR: 2.000e-04 

2023-03-01 13:45:17,333 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.6384 (0.5930) Acc D Real: 62.436% 
Loss D Fake: 0.8633 (0.5248) Acc D Fake: 69.116% 
Loss D: 1.502 
Loss G: 0.5517 (1.0991) Acc G: 30.360% 
LR: 2.000e-04 

2023-03-01 13:45:17,340 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.6349 (0.5932) Acc D Real: 62.570% 
Loss D Fake: 0.8665 (0.5265) Acc D Fake: 68.778% 
Loss D: 1.501 
Loss G: 0.5493 (1.0964) Acc G: 30.700% 
LR: 2.000e-04 

2023-03-01 13:45:17,348 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.6249 (0.5933) Acc D Real: 62.717% 
Loss D Fake: 0.8697 (0.5282) Acc D Fake: 68.445% 
Loss D: 1.495 
Loss G: 0.5469 (1.0937) Acc G: 31.037% 
LR: 2.000e-04 

2023-03-01 13:45:17,355 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.6323 (0.5935) Acc D Real: 62.829% 
Loss D Fake: 0.8730 (0.5298) Acc D Fake: 68.114% 
Loss D: 1.505 
Loss G: 0.5445 (1.0911) Acc G: 31.370% 
LR: 2.000e-04 

2023-03-01 13:45:17,363 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.6258 (0.5937) Acc D Real: 62.986% 
Loss D Fake: 0.8762 (0.5315) Acc D Fake: 67.786% 
Loss D: 1.502 
Loss G: 0.5421 (1.0884) Acc G: 31.700% 
LR: 2.000e-04 

2023-03-01 13:45:17,370 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.6161 (0.5938) Acc D Real: 63.158% 
Loss D Fake: 0.8794 (0.5332) Acc D Fake: 67.462% 
Loss D: 1.496 
Loss G: 0.5397 (1.0858) Acc G: 32.027% 
LR: 2.000e-04 

2023-03-01 13:45:17,377 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.6154 (0.5939) Acc D Real: 63.332% 
Loss D Fake: 0.8826 (0.5348) Acc D Fake: 67.141% 
Loss D: 1.498 
Loss G: 0.5373 (1.0832) Acc G: 32.350% 
LR: 2.000e-04 

2023-03-01 13:45:17,385 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.6132 (0.5940) Acc D Real: 63.481% 
Loss D Fake: 0.8859 (0.5365) Acc D Fake: 66.823% 
Loss D: 1.499 
Loss G: 0.5350 (1.0806) Acc G: 32.671% 
LR: 2.000e-04 

2023-03-01 13:45:17,393 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.6088 (0.5940) Acc D Real: 63.636% 
Loss D Fake: 0.8892 (0.5382) Acc D Fake: 66.507% 
Loss D: 1.498 
Loss G: 0.5326 (1.0780) Acc G: 32.988% 
LR: 2.000e-04 

2023-03-01 13:45:17,401 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.6026 (0.5941) Acc D Real: 63.801% 
Loss D Fake: 0.8924 (0.5398) Acc D Fake: 66.195% 
Loss D: 1.495 
Loss G: 0.5303 (1.0754) Acc G: 33.303% 
LR: 2.000e-04 

2023-03-01 13:45:17,409 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.6064 (0.5941) Acc D Real: 63.971% 
Loss D Fake: 0.8957 (0.5415) Acc D Fake: 65.886% 
Loss D: 1.502 
Loss G: 0.5280 (1.0729) Acc G: 33.615% 
LR: 2.000e-04 

2023-03-01 13:45:17,417 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.6036 (0.5942) Acc D Real: 64.138% 
Loss D Fake: 0.8989 (0.5431) Acc D Fake: 65.579% 
Loss D: 1.503 
Loss G: 0.5256 (1.0703) Acc G: 33.923% 
LR: 2.000e-04 

2023-03-01 13:45:17,424 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.6055 (0.5942) Acc D Real: 64.304% 
Loss D Fake: 0.9022 (0.5448) Acc D Fake: 65.276% 
Loss D: 1.508 
Loss G: 0.5233 (1.0678) Acc G: 34.229% 
LR: 2.000e-04 

2023-03-01 13:45:17,431 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.5980 (0.5943) Acc D Real: 64.469% 
Loss D Fake: 0.9055 (0.5465) Acc D Fake: 64.975% 
Loss D: 1.503 
Loss G: 0.5210 (1.0653) Acc G: 34.532% 
LR: 2.000e-04 

2023-03-01 13:45:17,439 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.5938 (0.5942) Acc D Real: 64.632% 
Loss D Fake: 0.9088 (0.5481) Acc D Fake: 64.677% 
Loss D: 1.503 
Loss G: 0.5187 (1.0628) Acc G: 34.833% 
LR: 2.000e-04 

2023-03-01 13:45:17,446 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.5963 (0.5943) Acc D Real: 64.793% 
Loss D Fake: 0.9120 (0.5498) Acc D Fake: 64.382% 
Loss D: 1.508 
Loss G: 0.5165 (1.0603) Acc G: 35.130% 
LR: 2.000e-04 

2023-03-01 13:45:17,453 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.5807 (0.5942) Acc D Real: 64.953% 
Loss D Fake: 0.9153 (0.5515) Acc D Fake: 64.089% 
Loss D: 1.496 
Loss G: 0.5142 (1.0578) Acc G: 35.425% 
LR: 2.000e-04 

2023-03-01 13:45:17,461 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.5811 (0.5941) Acc D Real: 65.112% 
Loss D Fake: 0.9186 (0.5531) Acc D Fake: 63.799% 
Loss D: 1.500 
Loss G: 0.5120 (1.0553) Acc G: 35.717% 
LR: 2.000e-04 

2023-03-01 13:45:17,468 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.5830 (0.5941) Acc D Real: 65.269% 
Loss D Fake: 0.9219 (0.5548) Acc D Fake: 63.512% 
Loss D: 1.505 
Loss G: 0.5097 (1.0529) Acc G: 36.007% 
LR: 2.000e-04 

2023-03-01 13:45:17,476 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.5755 (0.5940) Acc D Real: 65.425% 
Loss D Fake: 0.9251 (0.5564) Acc D Fake: 63.227% 
Loss D: 1.501 
Loss G: 0.5075 (1.0504) Acc G: 36.294% 
LR: 2.000e-04 

2023-03-01 13:45:17,483 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.5754 (0.5939) Acc D Real: 65.579% 
Loss D Fake: 0.9284 (0.5581) Acc D Fake: 62.945% 
Loss D: 1.504 
Loss G: 0.5053 (1.0480) Acc G: 36.578% 
LR: 2.000e-04 

2023-03-01 13:45:17,491 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.5644 (0.5938) Acc D Real: 65.732% 
Loss D Fake: 0.9317 (0.5598) Acc D Fake: 62.665% 
Loss D: 1.496 
Loss G: 0.5032 (1.0456) Acc G: 36.860% 
LR: 2.000e-04 

2023-03-01 13:45:17,498 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.5725 (0.5937) Acc D Real: 65.770% 
Loss D Fake: 0.9349 (0.5614) Acc D Fake: 62.595% 
Loss D: 1.507 
Loss G: 0.5010 (1.0432) Acc G: 36.930% 
LR: 2.000e-04 

2023-03-01 13:45:17,512 -                train: [    INFO] - 
Epoch: 4/20
2023-03-01 13:45:17,716 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.5648 (0.5645) Acc D Real: 100.000% 
Loss D Fake: 0.9414 (0.9398) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4967 (0.4978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,725 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.5543 (0.5611) Acc D Real: 100.000% 
Loss D Fake: 0.9446 (0.9414) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4946 (0.4967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,734 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.5580 (0.5603) Acc D Real: 100.000% 
Loss D Fake: 0.9479 (0.9430) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4925 (0.4957) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,751 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.5522 (0.5587) Acc D Real: 100.000% 
Loss D Fake: 0.9511 (0.9446) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4905 (0.4946) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,758 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.5488 (0.5570) Acc D Real: 100.000% 
Loss D Fake: 0.9543 (0.9462) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4884 (0.4936) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,765 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.5545 (0.5567) Acc D Real: 100.000% 
Loss D Fake: 0.9574 (0.9478) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4864 (0.4926) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,772 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.5445 (0.5551) Acc D Real: 100.000% 
Loss D Fake: 0.9606 (0.9494) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4844 (0.4916) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,779 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.5439 (0.5539) Acc D Real: 100.000% 
Loss D Fake: 0.9638 (0.9510) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4824 (0.4905) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,786 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.5351 (0.5520) Acc D Real: 100.000% 
Loss D Fake: 0.9669 (0.9526) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4804 (0.4895) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,793 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.5335 (0.5503) Acc D Real: 100.000% 
Loss D Fake: 0.9700 (0.9542) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4785 (0.4885) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,800 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.5320 (0.5488) Acc D Real: 100.000% 
Loss D Fake: 0.9731 (0.9558) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4766 (0.4875) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,807 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.5289 (0.5473) Acc D Real: 100.000% 
Loss D Fake: 0.9762 (0.9573) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4747 (0.4865) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,815 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.5286 (0.5459) Acc D Real: 100.000% 
Loss D Fake: 0.9792 (0.9589) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4728 (0.4856) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,822 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.5280 (0.5447) Acc D Real: 100.000% 
Loss D Fake: 0.9822 (0.9605) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4710 (0.4846) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,829 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.5214 (0.5433) Acc D Real: 100.000% 
Loss D Fake: 0.9852 (0.9620) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4692 (0.4836) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,837 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.5253 (0.5422) Acc D Real: 100.000% 
Loss D Fake: 0.9882 (0.9636) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4674 (0.4827) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,844 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.5205 (0.5410) Acc D Real: 100.000% 
Loss D Fake: 0.9912 (0.9651) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4656 (0.4817) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,850 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.5146 (0.5396) Acc D Real: 100.000% 
Loss D Fake: 0.9941 (0.9666) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4639 (0.4808) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,858 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.5112 (0.5382) Acc D Real: 100.000% 
Loss D Fake: 0.9970 (0.9681) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4622 (0.4799) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,865 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.5130 (0.5370) Acc D Real: 100.000% 
Loss D Fake: 0.9999 (0.9696) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4605 (0.4789) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,872 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.5105 (0.5358) Acc D Real: 100.000% 
Loss D Fake: 1.0027 (0.9711) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4588 (0.4780) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,879 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.5071 (0.5345) Acc D Real: 100.000% 
Loss D Fake: 1.0055 (0.9726) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4572 (0.4771) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,886 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.5036 (0.5333) Acc D Real: 100.000% 
Loss D Fake: 1.0083 (0.9741) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4556 (0.4762) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,893 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.5045 (0.5321) Acc D Real: 100.000% 
Loss D Fake: 1.0111 (0.9756) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4540 (0.4753) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,900 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.5012 (0.5309) Acc D Real: 100.000% 
Loss D Fake: 1.0138 (0.9771) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4524 (0.4744) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,907 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4934 (0.5295) Acc D Real: 100.000% 
Loss D Fake: 1.0165 (0.9785) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4509 (0.4736) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,914 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4995 (0.5285) Acc D Real: 100.000% 
Loss D Fake: 1.0191 (0.9800) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4494 (0.4727) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,922 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4929 (0.5272) Acc D Real: 100.000% 
Loss D Fake: 1.0217 (0.9814) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4479 (0.4719) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,930 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4881 (0.5259) Acc D Real: 100.000% 
Loss D Fake: 1.0243 (0.9829) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4464 (0.4710) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,937 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4865 (0.5247) Acc D Real: 100.000% 
Loss D Fake: 1.0269 (0.9843) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4450 (0.4702) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,945 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4876 (0.5235) Acc D Real: 100.000% 
Loss D Fake: 1.0294 (0.9857) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4436 (0.4693) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,952 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4821 (0.5222) Acc D Real: 100.000% 
Loss D Fake: 1.0318 (0.9871) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4422 (0.4685) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,959 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4859 (0.5212) Acc D Real: 100.000% 
Loss D Fake: 1.0343 (0.9885) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4409 (0.4677) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,967 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4780 (0.5199) Acc D Real: 100.000% 
Loss D Fake: 1.0366 (0.9898) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4396 (0.4669) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,975 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4779 (0.5188) Acc D Real: 100.000% 
Loss D Fake: 1.0390 (0.9912) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4383 (0.4661) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,983 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4774 (0.5177) Acc D Real: 100.000% 
Loss D Fake: 1.0413 (0.9926) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4370 (0.4653) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,990 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4747 (0.5165) Acc D Real: 100.000% 
Loss D Fake: 1.0436 (0.9939) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4358 (0.4645) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:17,998 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4733 (0.5154) Acc D Real: 100.000% 
Loss D Fake: 1.0458 (0.9952) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4346 (0.4638) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,006 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4734 (0.5144) Acc D Real: 100.000% 
Loss D Fake: 1.0480 (0.9966) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4334 (0.4630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,013 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4692 (0.5133) Acc D Real: 100.000% 
Loss D Fake: 1.0502 (0.9979) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4322 (0.4623) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,021 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4678 (0.5122) Acc D Real: 100.000% 
Loss D Fake: 1.0523 (0.9992) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4311 (0.4615) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,028 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4656 (0.5111) Acc D Real: 100.000% 
Loss D Fake: 1.0544 (1.0004) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4300 (0.4608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,036 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4624 (0.5100) Acc D Real: 100.000% 
Loss D Fake: 1.0564 (1.0017) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4289 (0.4601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,043 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4658 (0.5090) Acc D Real: 100.000% 
Loss D Fake: 1.0584 (1.0030) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4278 (0.4593) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,051 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4627 (0.5080) Acc D Real: 100.000% 
Loss D Fake: 1.0603 (1.0042) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4268 (0.4586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,058 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4605 (0.5070) Acc D Real: 100.000% 
Loss D Fake: 1.0623 (1.0055) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4258 (0.4579) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,065 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4551 (0.5059) Acc D Real: 100.000% 
Loss D Fake: 1.0641 (1.0067) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4248 (0.4572) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,073 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4543 (0.5049) Acc D Real: 100.000% 
Loss D Fake: 1.0660 (1.0079) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4239 (0.4566) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,080 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4575 (0.5039) Acc D Real: 100.000% 
Loss D Fake: 1.0678 (1.0091) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4229 (0.4559) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,088 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4546 (0.5029) Acc D Real: 100.000% 
Loss D Fake: 1.0695 (1.0103) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4220 (0.4552) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,095 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4505 (0.5019) Acc D Real: 100.000% 
Loss D Fake: 1.0712 (1.0114) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4211 (0.4546) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,103 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4519 (0.5010) Acc D Real: 100.000% 
Loss D Fake: 1.0729 (1.0126) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4202 (0.4539) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,110 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4484 (0.5000) Acc D Real: 100.000% 
Loss D Fake: 1.0746 (1.0138) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4194 (0.4533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,118 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4490 (0.4991) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.0149) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4186 (0.4527) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,125 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4461 (0.4981) Acc D Real: 100.000% 
Loss D Fake: 1.0777 (1.0160) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4178 (0.4520) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,133 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4467 (0.4972) Acc D Real: 100.000% 
Loss D Fake: 1.0792 (1.0171) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4170 (0.4514) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,140 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4438 (0.4963) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.0182) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4162 (0.4508) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,147 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4432 (0.4954) Acc D Real: 100.000% 
Loss D Fake: 1.0822 (1.0193) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4155 (0.4502) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,155 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4418 (0.4945) Acc D Real: 100.000% 
Loss D Fake: 1.0836 (1.0204) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4148 (0.4496) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,162 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4403 (0.4936) Acc D Real: 100.000% 
Loss D Fake: 1.0850 (1.0214) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4141 (0.4490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,170 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4381 (0.4927) Acc D Real: 100.000% 
Loss D Fake: 1.0863 (1.0225) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4134 (0.4485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,177 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4381 (0.4919) Acc D Real: 100.000% 
Loss D Fake: 1.0876 (1.0235) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4127 (0.4479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,184 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4374 (0.4910) Acc D Real: 100.000% 
Loss D Fake: 1.0889 (1.0245) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4121 (0.4473) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,192 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4375 (0.4902) Acc D Real: 100.000% 
Loss D Fake: 1.0901 (1.0255) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4115 (0.4468) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,199 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4326 (0.4893) Acc D Real: 100.000% 
Loss D Fake: 1.0914 (1.0265) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4109 (0.4462) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,207 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4353 (0.4885) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.0275) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4103 (0.4457) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,214 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4329 (0.4877) Acc D Real: 100.000% 
Loss D Fake: 1.0937 (1.0285) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4097 (0.4452) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,222 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4335 (0.4869) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.0295) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4092 (0.4447) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,229 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4311 (0.4861) Acc D Real: 100.000% 
Loss D Fake: 1.0959 (1.0304) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4086 (0.4441) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,237 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4285 (0.4853) Acc D Real: 100.000% 
Loss D Fake: 1.0969 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4081 (0.4436) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,244 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4305 (0.4845) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.0323) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4076 (0.4431) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,252 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4278 (0.4838) Acc D Real: 100.000% 
Loss D Fake: 1.0989 (1.0332) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4071 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,259 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4279 (0.4830) Acc D Real: 100.000% 
Loss D Fake: 1.0999 (1.0341) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4066 (0.4422) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,267 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4274 (0.4823) Acc D Real: 100.000% 
Loss D Fake: 1.1009 (1.0350) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4062 (0.4417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,275 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4264 (0.4815) Acc D Real: 100.000% 
Loss D Fake: 1.1018 (1.0359) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4057 (0.4412) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,283 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4251 (0.4808) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.0367) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4053 (0.4407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,290 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4254 (0.4801) Acc D Real: 100.000% 
Loss D Fake: 1.1036 (1.0376) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4048 (0.4403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,298 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4244 (0.4794) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.0384) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4044 (0.4398) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,305 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4227 (0.4787) Acc D Real: 100.000% 
Loss D Fake: 1.1053 (1.0393) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4040 (0.4394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,313 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4230 (0.4780) Acc D Real: 100.000% 
Loss D Fake: 1.1061 (1.0401) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4036 (0.4389) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,320 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4205 (0.4773) Acc D Real: 100.000% 
Loss D Fake: 1.1069 (1.0409) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4032 (0.4385) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,327 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4202 (0.4766) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.0417) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4028 (0.4381) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,334 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4194 (0.4759) Acc D Real: 100.000% 
Loss D Fake: 1.1084 (1.0425) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4025 (0.4376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,342 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4183 (0.4752) Acc D Real: 100.000% 
Loss D Fake: 1.1091 (1.0433) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4021 (0.4372) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,349 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4175 (0.4746) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.0441) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4018 (0.4368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,356 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4163 (0.4739) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.0448) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4015 (0.4364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,363 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4169 (0.4733) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.0456) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4011 (0.4360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,371 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4168 (0.4726) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.0463) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4008 (0.4356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,378 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4135 (0.4720) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.0470) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4005 (0.4352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,385 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4145 (0.4713) Acc D Real: 100.000% 
Loss D Fake: 1.1130 (1.0478) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4003 (0.4348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,392 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4139 (0.4707) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.0485) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4000 (0.4345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,400 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4129 (0.4701) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.0492) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3997 (0.4341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,407 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4124 (0.4695) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.0499) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3995 (0.4337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,415 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4118 (0.4689) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.0506) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3992 (0.4333) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,422 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4117 (0.4683) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.0513) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3990 (0.4330) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,429 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4099 (0.4677) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.0519) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3987 (0.4326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,438 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4091 (0.4671) Acc D Real: 100.000% 
Loss D Fake: 1.1166 (1.0526) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3985 (0.4323) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,446 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4102 (0.4665) Acc D Real: 100.000% 
Loss D Fake: 1.1170 (1.0532) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3983 (0.4319) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,453 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4078 (0.4659) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3981 (0.4316) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,461 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4082 (0.4653) Acc D Real: 100.000% 
Loss D Fake: 1.1179 (1.0545) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3979 (0.4313) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,468 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4075 (0.4648) Acc D Real: 100.000% 
Loss D Fake: 1.1183 (1.0551) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3977 (0.4309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,476 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4066 (0.4642) Acc D Real: 100.000% 
Loss D Fake: 1.1187 (1.0558) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3975 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,483 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4064 (0.4637) Acc D Real: 100.000% 
Loss D Fake: 1.1191 (1.0564) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3973 (0.4303) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,491 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4053 (0.4631) Acc D Real: 100.000% 
Loss D Fake: 1.1195 (1.0570) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3971 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,498 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4046 (0.4625) Acc D Real: 100.000% 
Loss D Fake: 1.1198 (1.0576) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3969 (0.4297) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,505 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4046 (0.4620) Acc D Real: 100.000% 
Loss D Fake: 1.1202 (1.0581) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3968 (0.4294) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,513 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4043 (0.4615) Acc D Real: 100.000% 
Loss D Fake: 1.1205 (1.0587) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3966 (0.4291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,520 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4039 (0.4609) Acc D Real: 100.000% 
Loss D Fake: 1.1209 (1.0593) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3965 (0.4288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,528 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4035 (0.4604) Acc D Real: 100.000% 
Loss D Fake: 1.1212 (1.0599) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3963 (0.4285) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,535 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4020 (0.4599) Acc D Real: 100.000% 
Loss D Fake: 1.1215 (1.0604) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3961 (0.4282) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,543 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4022 (0.4594) Acc D Real: 100.000% 
Loss D Fake: 1.1219 (1.0610) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3960 (0.4279) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,550 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4014 (0.4589) Acc D Real: 100.000% 
Loss D Fake: 1.1222 (1.0615) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3958 (0.4276) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,557 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4012 (0.4584) Acc D Real: 100.000% 
Loss D Fake: 1.1225 (1.0620) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3957 (0.4273) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,564 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4003 (0.4579) Acc D Real: 100.000% 
Loss D Fake: 1.1228 (1.0626) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3955 (0.4270) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,571 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3995 (0.4574) Acc D Real: 100.000% 
Loss D Fake: 1.1230 (1.0631) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3954 (0.4268) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,579 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3993 (0.4569) Acc D Real: 100.000% 
Loss D Fake: 1.1233 (1.0636) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3953 (0.4265) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,586 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3985 (0.4564) Acc D Real: 100.000% 
Loss D Fake: 1.1236 (1.0641) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3952 (0.4262) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,593 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3982 (0.4559) Acc D Real: 100.000% 
Loss D Fake: 1.1238 (1.0646) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3951 (0.4260) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,601 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3976 (0.4554) Acc D Real: 100.000% 
Loss D Fake: 1.1240 (1.0651) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3949 (0.4257) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,608 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3975 (0.4549) Acc D Real: 100.000% 
Loss D Fake: 1.1243 (1.0656) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3948 (0.4255) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,615 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3965 (0.4544) Acc D Real: 100.000% 
Loss D Fake: 1.1245 (1.0661) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3947 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,622 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3960 (0.4540) Acc D Real: 100.000% 
Loss D Fake: 1.1247 (1.0666) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3946 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,630 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3957 (0.4535) Acc D Real: 100.000% 
Loss D Fake: 1.1249 (1.0670) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3945 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,637 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3954 (0.4530) Acc D Real: 100.000% 
Loss D Fake: 1.1251 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3945 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,644 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3948 (0.4526) Acc D Real: 100.000% 
Loss D Fake: 1.1252 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3944 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,651 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3943 (0.4521) Acc D Real: 100.000% 
Loss D Fake: 1.1254 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3943 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,659 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3940 (0.4516) Acc D Real: 100.000% 
Loss D Fake: 1.1256 (1.0688) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3942 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,666 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3935 (0.4512) Acc D Real: 100.000% 
Loss D Fake: 1.1258 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3941 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,674 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3930 (0.4507) Acc D Real: 100.000% 
Loss D Fake: 1.1259 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3941 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,682 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3924 (0.4503) Acc D Real: 100.000% 
Loss D Fake: 1.1260 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3940 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,690 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3919 (0.4499) Acc D Real: 100.000% 
Loss D Fake: 1.1262 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3939 (0.4229) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,697 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3913 (0.4494) Acc D Real: 100.000% 
Loss D Fake: 1.1263 (1.0710) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3938 (0.4226) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,704 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3910 (0.4490) Acc D Real: 100.000% 
Loss D Fake: 1.1265 (1.0714) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3938 (0.4224) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,712 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3905 (0.4485) Acc D Real: 100.000% 
Loss D Fake: 1.1266 (1.0718) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3937 (0.4222) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,720 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3906 (0.4481) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0722) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3936 (0.4220) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,727 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3895 (0.4477) Acc D Real: 100.000% 
Loss D Fake: 1.1269 (1.0726) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3935 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,734 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3893 (0.4473) Acc D Real: 100.000% 
Loss D Fake: 1.1271 (1.0730) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3935 (0.4216) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,742 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3886 (0.4468) Acc D Real: 100.000% 
Loss D Fake: 1.1272 (1.0734) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3934 (0.4214) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,749 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3888 (0.4464) Acc D Real: 100.000% 
Loss D Fake: 1.1274 (1.0738) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3933 (0.4212) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,756 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3875 (0.4460) Acc D Real: 100.000% 
Loss D Fake: 1.1275 (1.0742) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3933 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,764 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3871 (0.4456) Acc D Real: 100.000% 
Loss D Fake: 1.1276 (1.0745) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3932 (0.4208) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,771 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3870 (0.4452) Acc D Real: 100.000% 
Loss D Fake: 1.1278 (1.0749) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3932 (0.4206) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,780 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3866 (0.4448) Acc D Real: 100.000% 
Loss D Fake: 1.1279 (1.0753) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3931 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,788 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3856 (0.4444) Acc D Real: 100.000% 
Loss D Fake: 1.1280 (1.0757) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3930 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,795 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3854 (0.4440) Acc D Real: 100.000% 
Loss D Fake: 1.1281 (1.0760) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3930 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,803 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3846 (0.4436) Acc D Real: 100.000% 
Loss D Fake: 1.1282 (1.0764) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.3929 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,810 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3844 (0.4432) Acc D Real: 100.000% 
Loss D Fake: 1.1283 (1.0767) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.3929 (0.4197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,817 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3829 (0.4428) Acc D Real: 100.000% 
Loss D Fake: 1.1285 (1.0771) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.3928 (0.4195) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,825 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3837 (0.4424) Acc D Real: 100.000% 
Loss D Fake: 1.1286 (1.0774) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.3927 (0.4193) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,832 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3838 (0.4420) Acc D Real: 100.000% 
Loss D Fake: 1.1287 (1.0777) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.3927 (0.4191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,841 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3822 (0.4416) Acc D Real: 100.000% 
Loss D Fake: 1.1288 (1.0781) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.3926 (0.4190) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,849 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3812 (0.4412) Acc D Real: 100.000% 
Loss D Fake: 1.1290 (1.0784) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.3926 (0.4188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,858 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3811 (0.4408) Acc D Real: 100.000% 
Loss D Fake: 1.1291 (1.0787) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.3925 (0.4186) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,867 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3798 (0.4404) Acc D Real: 100.000% 
Loss D Fake: 1.1292 (1.0791) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.3924 (0.4185) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,876 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3805 (0.4400) Acc D Real: 100.000% 
Loss D Fake: 1.1294 (1.0794) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.3924 (0.4183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,883 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3785 (0.4396) Acc D Real: 100.000% 
Loss D Fake: 1.1295 (1.0797) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.3923 (0.4181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,890 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3797 (0.4393) Acc D Real: 100.000% 
Loss D Fake: 1.1296 (1.0800) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.3922 (0.4180) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,898 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3779 (0.4389) Acc D Real: 100.000% 
Loss D Fake: 1.1298 (1.0803) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.3922 (0.4178) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,905 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3771 (0.4385) Acc D Real: 100.000% 
Loss D Fake: 1.1299 (1.0807) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.3921 (0.4176) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,913 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3776 (0.4381) Acc D Real: 100.000% 
Loss D Fake: 1.1301 (1.0810) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.3920 (0.4175) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,920 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3757 (0.4377) Acc D Real: 100.000% 
Loss D Fake: 1.1302 (1.0813) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.3920 (0.4173) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,928 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3772 (0.4373) Acc D Real: 100.000% 
Loss D Fake: 1.1304 (1.0816) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.3919 (0.4172) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,935 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3764 (0.4370) Acc D Real: 100.000% 
Loss D Fake: 1.1305 (1.0819) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.3918 (0.4170) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,943 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3746 (0.4366) Acc D Real: 100.000% 
Loss D Fake: 1.1307 (1.0822) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.3917 (0.4169) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,951 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3736 (0.4362) Acc D Real: 100.000% 
Loss D Fake: 1.1308 (1.0825) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.3917 (0.4167) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,959 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3743 (0.4358) Acc D Real: 100.000% 
Loss D Fake: 1.1310 (1.0827) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.3916 (0.4166) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,967 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3711 (0.4355) Acc D Real: 100.000% 
Loss D Fake: 1.1312 (1.0830) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.3915 (0.4164) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,974 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3728 (0.4351) Acc D Real: 100.000% 
Loss D Fake: 1.1314 (1.0833) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.3914 (0.4163) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,981 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3723 (0.4347) Acc D Real: 100.000% 
Loss D Fake: 1.1315 (1.0836) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.3913 (0.4161) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,989 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3704 (0.4343) Acc D Real: 100.000% 
Loss D Fake: 1.1317 (1.0839) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.3912 (0.4160) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:18,996 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3680 (0.4340) Acc D Real: 100.000% 
Loss D Fake: 1.1319 (1.0842) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.3911 (0.4158) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,003 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3698 (0.4336) Acc D Real: 100.000% 
Loss D Fake: 1.1321 (1.0844) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.3910 (0.4157) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,010 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3701 (0.4332) Acc D Real: 100.000% 
Loss D Fake: 1.1323 (1.0847) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.3909 (0.4155) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,018 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3686 (0.4329) Acc D Real: 100.000% 
Loss D Fake: 1.1326 (1.0850) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.3908 (0.4154) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,025 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3685 (0.4325) Acc D Real: 100.000% 
Loss D Fake: 1.1328 (1.0853) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.3907 (0.4153) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,032 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3682 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.1330 (1.0855) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.3906 (0.4151) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,040 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3651 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.1332 (1.0858) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.3905 (0.4150) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,049 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3665 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.1334 (1.0861) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.3904 (0.4148) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,057 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3616 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.1336 (1.0863) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.3903 (0.4147) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,065 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3636 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.1338 (1.0866) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.3902 (0.4146) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,074 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3645 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.1341 (1.0868) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.3901 (0.4144) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,083 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3637 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.1343 (1.0871) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.3900 (0.4143) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,091 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3595 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.1346 (1.0874) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.3898 (0.4142) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,099 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3606 (0.4291) Acc D Real: 100.000% 
Loss D Fake: 1.1349 (1.0876) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.3897 (0.4140) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,106 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3615 (0.4288) Acc D Real: 100.000% 
Loss D Fake: 1.1352 (1.0879) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.3896 (0.4139) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,113 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3587 (0.4284) Acc D Real: 100.000% 
Loss D Fake: 1.1355 (1.0881) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.3894 (0.4138) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,121 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3594 (0.4280) Acc D Real: 100.000% 
Loss D Fake: 1.1358 (1.0884) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.3893 (0.4136) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,128 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3547 (0.4277) Acc D Real: 100.000% 
Loss D Fake: 1.1361 (1.0886) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.3891 (0.4135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,135 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3543 (0.4273) Acc D Real: 100.000% 
Loss D Fake: 1.1364 (1.0889) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.3890 (0.4134) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,143 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3532 (0.4269) Acc D Real: 100.000% 
Loss D Fake: 1.1368 (1.0891) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.3888 (0.4133) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,150 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3537 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.1372 (1.0894) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.3886 (0.4131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,157 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3509 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.1376 (1.0896) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.3884 (0.4130) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,164 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3563 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.1381 (1.0899) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.3882 (0.4129) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,172 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3481 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.1385 (1.0901) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.3880 (0.4127) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,179 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3462 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.1390 (1.0904) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.3877 (0.4126) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,186 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3431 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.1395 (1.0906) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.3875 (0.4125) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,193 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3394 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.1400 (1.0909) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.3872 (0.4124) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,200 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3444 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.1406 (1.0911) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.3869 (0.4122) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,208 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3407 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.1412 (1.0914) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.3866 (0.4121) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,215 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3378 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.1419 (1.0916) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.3863 (0.4120) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,223 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3373 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.1426 (1.0919) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.3860 (0.4118) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,230 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3343 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.1434 (1.0921) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.3856 (0.4117) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,237 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3347 (0.4216) Acc D Real: 100.000% 
Loss D Fake: 1.1442 (1.0924) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.3852 (0.4116) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,244 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3325 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.1450 (1.0927) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.3848 (0.4115) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,251 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3273 (0.4207) Acc D Real: 100.000% 
Loss D Fake: 1.1460 (1.0929) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.3844 (0.4113) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,258 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3294 (0.4202) Acc D Real: 100.000% 
Loss D Fake: 1.1470 (1.0932) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.3839 (0.4112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,266 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3240 (0.4198) Acc D Real: 100.000% 
Loss D Fake: 1.1481 (1.0934) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.3834 (0.4111) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,273 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3199 (0.4193) Acc D Real: 100.000% 
Loss D Fake: 1.1493 (1.0937) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.3828 (0.4109) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,280 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3177 (0.4188) Acc D Real: 100.000% 
Loss D Fake: 1.1506 (1.0940) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.3822 (0.4108) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,288 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3148 (0.4183) Acc D Real: 100.000% 
Loss D Fake: 1.1520 (1.0943) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.3815 (0.4106) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,295 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3091 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.1535 (1.0945) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.3808 (0.4105) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,303 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3052 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.1552 (1.0948) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.3800 (0.4104) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,311 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.2992 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.1571 (1.0951) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.3791 (0.4102) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,318 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3029 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.1593 (1.0954) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.3781 (0.4101) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,325 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2935 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.1617 (1.0957) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.3769 (0.4099) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,332 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2762 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.1644 (1.0960) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.3757 (0.4098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,339 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2702 (0.4143) Acc D Real: 100.000% 
Loss D Fake: 1.1672 (1.0964) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.3745 (0.4096) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,349 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2664 (0.4136) Acc D Real: 100.000% 
Loss D Fake: 1.1700 (1.0967) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.3734 (0.4094) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,357 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2441 (0.4129) Acc D Real: 100.000% 
Loss D Fake: 1.1725 (1.0970) Acc D Fake: 0.000% 
Loss D: 1.417 
Loss G: 0.3726 (0.4093) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,365 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2483 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.1737 (1.0974) Acc D Fake: 0.000% 
Loss D: 1.422 
Loss G: 0.3724 (0.4091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,373 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2474 (0.4114) Acc D Real: 100.000% 
Loss D Fake: 1.1739 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.421 
Loss G: 0.3724 (0.4089) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,381 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2276 (0.4106) Acc D Real: 100.000% 
Loss D Fake: 1.1731 (1.0981) Acc D Fake: 0.000% 
Loss D: 1.401 
Loss G: 0.3735 (0.4088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,390 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2166 (0.4097) Acc D Real: 100.000% 
Loss D Fake: 1.1691 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.3761 (0.4086) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,398 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.2097 (0.4088) Acc D Real: 100.000% 
Loss D Fake: 1.1610 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.371 
Loss G: 0.3807 (0.4085) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,406 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2083 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1489 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.357 
Loss G: 0.3868 (0.4084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,419 -                train: [    INFO] - Best Loss 0.983 to 0.958
2023-03-01 13:45:19,420 -                train: [    INFO] - 
Epoch: 5/20
2023-03-01 13:45:19,610 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2096 (0.2137) Acc D Real: 100.000% 
Loss D Fake: 1.1206 (1.1277) Acc D Fake: 0.000% 
Loss D: 1.330 
Loss G: 0.4001 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,617 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.2098 (0.2124) Acc D Real: 100.000% 
Loss D Fake: 1.1061 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.316 
Loss G: 0.4072 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,626 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2239 (0.2153) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.316 
Loss G: 0.4135 (0.4036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,659 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2052 (0.2132) Acc D Real: 100.000% 
Loss D Fake: 1.0804 (1.1068) Acc D Fake: 0.000% 
Loss D: 1.286 
Loss G: 0.4194 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,666 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2123 (0.2131) Acc D Real: 100.000% 
Loss D Fake: 1.0693 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.282 
Loss G: 0.4248 (0.4097) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,672 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2186 (0.2139) Acc D Real: 100.000% 
Loss D Fake: 1.0597 (1.0947) Acc D Fake: 0.000% 
Loss D: 1.278 
Loss G: 0.4295 (0.4126) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,679 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2131 (0.2138) Acc D Real: 100.000% 
Loss D Fake: 1.0520 (1.0894) Acc D Fake: 0.000% 
Loss D: 1.265 
Loss G: 0.4330 (0.4151) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,686 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2042 (0.2127) Acc D Real: 100.000% 
Loss D Fake: 1.0462 (1.0846) Acc D Fake: 0.000% 
Loss D: 1.250 
Loss G: 0.4360 (0.4174) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,693 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1921 (0.2107) Acc D Real: 100.000% 
Loss D Fake: 1.0404 (1.0802) Acc D Fake: 0.000% 
Loss D: 1.233 
Loss G: 0.4394 (0.4196) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,699 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1924 (0.2090) Acc D Real: 100.000% 
Loss D Fake: 1.0338 (1.0759) Acc D Fake: 0.000% 
Loss D: 1.226 
Loss G: 0.4433 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,706 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1986 (0.2081) Acc D Real: 100.000% 
Loss D Fake: 1.0269 (1.0719) Acc D Fake: 0.000% 
Loss D: 1.226 
Loss G: 0.4469 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,714 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.1940 (0.2070) Acc D Real: 100.000% 
Loss D Fake: 1.0207 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.215 
Loss G: 0.4503 (0.4259) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,722 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.1923 (0.2060) Acc D Real: 100.000% 
Loss D Fake: 1.0149 (1.0641) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.4536 (0.4279) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,730 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.1980 (0.2055) Acc D Real: 100.000% 
Loss D Fake: 1.0097 (1.0605) Acc D Fake: 0.000% 
Loss D: 1.208 
Loss G: 0.4563 (0.4298) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,737 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.1951 (0.2048) Acc D Real: 100.000% 
Loss D Fake: 1.0056 (1.0571) Acc D Fake: 0.000% 
Loss D: 1.201 
Loss G: 0.4585 (0.4316) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,745 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.2051 (0.2048) Acc D Real: 100.000% 
Loss D Fake: 1.0023 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.4601 (0.4333) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,753 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1812 (0.2035) Acc D Real: 100.000% 
Loss D Fake: 0.9996 (1.0508) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.4620 (0.4349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,761 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.1875 (0.2027) Acc D Real: 100.000% 
Loss D Fake: 0.9962 (1.0480) Acc D Fake: 0.000% 
Loss D: 1.184 
Loss G: 0.4640 (0.4364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,768 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1746 (0.2013) Acc D Real: 100.000% 
Loss D Fake: 0.9924 (1.0452) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.4667 (0.4379) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,776 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.1723 (0.1999) Acc D Real: 100.000% 
Loss D Fake: 0.9873 (1.0424) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.4702 (0.4394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,783 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1817 (0.1991) Acc D Real: 100.000% 
Loss D Fake: 0.9814 (1.0397) Acc D Fake: 0.000% 
Loss D: 1.163 
Loss G: 0.4739 (0.4410) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,791 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1734 (0.1980) Acc D Real: 100.000% 
Loss D Fake: 0.9753 (1.0369) Acc D Fake: 0.000% 
Loss D: 1.149 
Loss G: 0.4777 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,799 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.1842 (0.1974) Acc D Real: 100.000% 
Loss D Fake: 0.9695 (1.0340) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.4809 (0.4442) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,806 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1701 (0.1963) Acc D Real: 100.000% 
Loss D Fake: 0.9647 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.4840 (0.4458) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,814 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.1686 (0.1952) Acc D Real: 100.000% 
Loss D Fake: 0.9595 (1.0285) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.4874 (0.4474) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,821 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1681 (0.1942) Acc D Real: 100.000% 
Loss D Fake: 0.9542 (1.0258) Acc D Fake: 0.000% 
Loss D: 1.122 
Loss G: 0.4908 (0.4490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,829 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1645 (0.1932) Acc D Real: 100.000% 
Loss D Fake: 0.9490 (1.0230) Acc D Fake: 0.000% 
Loss D: 1.114 
Loss G: 0.4942 (0.4506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,837 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1734 (0.1925) Acc D Real: 100.000% 
Loss D Fake: 0.9439 (1.0203) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.4976 (0.4522) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,845 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.1883 (0.1923) Acc D Real: 100.000% 
Loss D Fake: 0.9392 (1.0176) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.5002 (0.4538) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,854 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.1732 (0.1917) Acc D Real: 100.000% 
Loss D Fake: 0.9361 (1.0150) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.5019 (0.4554) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,862 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1816 (0.1914) Acc D Real: 100.000% 
Loss D Fake: 0.9340 (1.0124) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.5031 (0.4569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,870 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.1656 (0.1906) Acc D Real: 100.000% 
Loss D Fake: 0.9324 (1.0100) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.5043 (0.4583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,878 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1710 (0.1900) Acc D Real: 100.000% 
Loss D Fake: 0.9307 (1.0077) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.5054 (0.4597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,886 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1691 (0.1894) Acc D Real: 100.000% 
Loss D Fake: 0.9290 (1.0054) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.5066 (0.4610) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,894 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1662 (0.1888) Acc D Real: 100.000% 
Loss D Fake: 0.9274 (1.0033) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.5077 (0.4623) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,901 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.1559 (0.1879) Acc D Real: 100.000% 
Loss D Fake: 0.9258 (1.0012) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.5090 (0.4636) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,909 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1479 (0.1869) Acc D Real: 100.000% 
Loss D Fake: 0.9235 (0.9991) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.5111 (0.4648) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,916 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1597 (0.1862) Acc D Real: 100.000% 
Loss D Fake: 0.9203 (0.9971) Acc D Fake: 0.000% 
Loss D: 1.080 
Loss G: 0.5132 (0.4661) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,923 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.1501 (0.1853) Acc D Real: 100.000% 
Loss D Fake: 0.9171 (0.9951) Acc D Fake: 0.000% 
Loss D: 1.067 
Loss G: 0.5156 (0.4673) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,931 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1553 (0.1845) Acc D Real: 100.000% 
Loss D Fake: 0.9136 (0.9931) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.5181 (0.4686) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,938 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1500 (0.1837) Acc D Real: 100.000% 
Loss D Fake: 0.9102 (0.9911) Acc D Fake: 0.000% 
Loss D: 1.060 
Loss G: 0.5205 (0.4698) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,947 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1429 (0.1828) Acc D Real: 100.000% 
Loss D Fake: 0.9067 (0.9892) Acc D Fake: 0.000% 
Loss D: 1.050 
Loss G: 0.5233 (0.4710) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,954 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.1449 (0.1819) Acc D Real: 100.000% 
Loss D Fake: 0.9024 (0.9872) Acc D Fake: 0.000% 
Loss D: 1.047 
Loss G: 0.5266 (0.4723) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,962 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.1594 (0.1814) Acc D Real: 100.000% 
Loss D Fake: 0.8980 (0.9852) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.5294 (0.4736) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,969 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.1427 (0.1806) Acc D Real: 100.000% 
Loss D Fake: 0.8942 (0.9832) Acc D Fake: 0.000% 
Loss D: 1.037 
Loss G: 0.5324 (0.4749) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,977 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1552 (0.1800) Acc D Real: 100.000% 
Loss D Fake: 0.8902 (0.9813) Acc D Fake: 0.000% 
Loss D: 1.045 
Loss G: 0.5353 (0.4761) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,984 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.1404 (0.1792) Acc D Real: 100.000% 
Loss D Fake: 0.8863 (0.9793) Acc D Fake: 0.000% 
Loss D: 1.027 
Loss G: 0.5381 (0.4774) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:19,992 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.1351 (0.1783) Acc D Real: 100.000% 
Loss D Fake: 0.8824 (0.9773) Acc D Fake: 0.000% 
Loss D: 1.017 
Loss G: 0.5412 (0.4787) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,001 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.1407 (0.1775) Acc D Real: 100.000% 
Loss D Fake: 0.8783 (0.9753) Acc D Fake: 0.000% 
Loss D: 1.019 
Loss G: 0.5442 (0.4800) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,008 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.1614 (0.1772) Acc D Real: 100.000% 
Loss D Fake: 0.8747 (0.9734) Acc D Fake: 0.000% 
Loss D: 1.036 
Loss G: 0.5466 (0.4813) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,016 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.1378 (0.1765) Acc D Real: 100.000% 
Loss D Fake: 0.8717 (0.9714) Acc D Fake: 0.000% 
Loss D: 1.009 
Loss G: 0.5490 (0.4826) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,023 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.1418 (0.1758) Acc D Real: 100.000% 
Loss D Fake: 0.8687 (0.9695) Acc D Fake: 0.000% 
Loss D: 1.010 
Loss G: 0.5512 (0.4839) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,031 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.1348 (0.1751) Acc D Real: 100.000% 
Loss D Fake: 0.8659 (0.9675) Acc D Fake: 0.000% 
Loss D: 1.001 
Loss G: 0.5535 (0.4852) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,041 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.1364 (0.1744) Acc D Real: 100.000% 
Loss D Fake: 0.8629 (0.9656) Acc D Fake: 0.000% 
Loss D: 0.999 
Loss G: 0.5558 (0.4865) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,050 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.1297 (0.1736) Acc D Real: 100.000% 
Loss D Fake: 0.8598 (0.9638) Acc D Fake: 0.000% 
Loss D: 0.990 
Loss G: 0.5583 (0.4878) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,057 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.1322 (0.1728) Acc D Real: 100.000% 
Loss D Fake: 0.8566 (0.9619) Acc D Fake: 0.000% 
Loss D: 0.989 
Loss G: 0.5609 (0.4891) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,064 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.1453 (0.1724) Acc D Real: 100.000% 
Loss D Fake: 0.8535 (0.9600) Acc D Fake: 0.000% 
Loss D: 0.999 
Loss G: 0.5632 (0.4904) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,072 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.1384 (0.1718) Acc D Real: 100.000% 
Loss D Fake: 0.8508 (0.9582) Acc D Fake: 0.000% 
Loss D: 0.989 
Loss G: 0.5652 (0.4916) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,079 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.1372 (0.1712) Acc D Real: 100.000% 
Loss D Fake: 0.8486 (0.9563) Acc D Fake: 0.000% 
Loss D: 0.986 
Loss G: 0.5669 (0.4929) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,089 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.1325 (0.1706) Acc D Real: 100.000% 
Loss D Fake: 0.8466 (0.9545) Acc D Fake: 0.000% 
Loss D: 0.979 
Loss G: 0.5686 (0.4941) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,096 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.1278 (0.1699) Acc D Real: 100.000% 
Loss D Fake: 0.8445 (0.9528) Acc D Fake: 0.000% 
Loss D: 0.972 
Loss G: 0.5704 (0.4953) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,104 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.1313 (0.1693) Acc D Real: 100.000% 
Loss D Fake: 0.8423 (0.9510) Acc D Fake: 0.000% 
Loss D: 0.974 
Loss G: 0.5722 (0.4966) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,111 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.1277 (0.1686) Acc D Real: 100.000% 
Loss D Fake: 0.8400 (0.9493) Acc D Fake: 0.000% 
Loss D: 0.968 
Loss G: 0.5742 (0.4978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,118 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.1262 (0.1680) Acc D Real: 100.000% 
Loss D Fake: 0.8377 (0.9475) Acc D Fake: 0.000% 
Loss D: 0.964 
Loss G: 0.5762 (0.4990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,126 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1268 (0.1673) Acc D Real: 100.000% 
Loss D Fake: 0.8353 (0.9458) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5781 (0.5002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,133 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1252 (0.1667) Acc D Real: 100.000% 
Loss D Fake: 0.8331 (0.9442) Acc D Fake: 0.000% 
Loss D: 0.958 
Loss G: 0.5799 (0.5014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,140 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1289 (0.1662) Acc D Real: 100.000% 
Loss D Fake: 0.8309 (0.9425) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.5818 (0.5026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,148 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.1258 (0.1656) Acc D Real: 100.000% 
Loss D Fake: 0.8287 (0.9408) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.5836 (0.5037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,155 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1224 (0.1650) Acc D Real: 100.000% 
Loss D Fake: 0.8266 (0.9392) Acc D Fake: 0.000% 
Loss D: 0.949 
Loss G: 0.5854 (0.5049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,163 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.1223 (0.1644) Acc D Real: 100.000% 
Loss D Fake: 0.8245 (0.9376) Acc D Fake: 0.000% 
Loss D: 0.947 
Loss G: 0.5872 (0.5061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,172 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1256 (0.1638) Acc D Real: 100.000% 
Loss D Fake: 0.8224 (0.9360) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.5890 (0.5072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,179 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.1341 (0.1634) Acc D Real: 100.000% 
Loss D Fake: 0.8204 (0.9344) Acc D Fake: 0.000% 
Loss D: 0.954 
Loss G: 0.5908 (0.5084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,187 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1240 (0.1629) Acc D Real: 100.000% 
Loss D Fake: 0.8183 (0.9328) Acc D Fake: 0.000% 
Loss D: 0.942 
Loss G: 0.5925 (0.5095) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,194 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.1201 (0.1623) Acc D Real: 100.000% 
Loss D Fake: 0.8163 (0.9313) Acc D Fake: 0.000% 
Loss D: 0.936 
Loss G: 0.5944 (0.5106) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,202 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.1339 (0.1619) Acc D Real: 100.000% 
Loss D Fake: 0.8142 (0.9298) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.5962 (0.5118) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,209 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.1194 (0.1614) Acc D Real: 100.000% 
Loss D Fake: 0.8122 (0.9282) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.5980 (0.5129) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,217 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1188 (0.1608) Acc D Real: 100.000% 
Loss D Fake: 0.8101 (0.9267) Acc D Fake: 0.000% 
Loss D: 0.929 
Loss G: 0.5999 (0.5140) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,224 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.1181 (0.1603) Acc D Real: 100.000% 
Loss D Fake: 0.8079 (0.9252) Acc D Fake: 0.000% 
Loss D: 0.926 
Loss G: 0.6018 (0.5151) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,233 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1148 (0.1597) Acc D Real: 100.000% 
Loss D Fake: 0.8058 (0.9237) Acc D Fake: 0.000% 
Loss D: 0.921 
Loss G: 0.6037 (0.5162) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,241 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.1142 (0.1592) Acc D Real: 100.000% 
Loss D Fake: 0.8035 (0.9222) Acc D Fake: 0.021% 
Loss D: 0.918 
Loss G: 0.6057 (0.5173) Acc G: 99.938% 
LR: 2.000e-04 

2023-03-01 13:45:20,248 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.1207 (0.1587) Acc D Real: 100.000% 
Loss D Fake: 0.8016 (0.9208) Acc D Fake: 0.081% 
Loss D: 0.922 
Loss G: 0.6070 (0.5184) Acc G: 99.878% 
LR: 2.000e-04 

2023-03-01 13:45:20,256 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.1150 (0.1582) Acc D Real: 100.000% 
Loss D Fake: 0.8004 (0.9193) Acc D Fake: 0.161% 
Loss D: 0.915 
Loss G: 0.6082 (0.5195) Acc G: 99.779% 
LR: 2.000e-04 

2023-03-01 13:45:20,263 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.1458 (0.1580) Acc D Real: 100.000% 
Loss D Fake: 0.7991 (0.9179) Acc D Fake: 0.258% 
Loss D: 0.945 
Loss G: 0.6094 (0.5206) Acc G: 99.683% 
LR: 2.000e-04 

2023-03-01 13:45:20,271 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.1175 (0.1575) Acc D Real: 100.000% 
Loss D Fake: 0.7979 (0.9165) Acc D Fake: 0.373% 
Loss D: 0.915 
Loss G: 0.6106 (0.5216) Acc G: 99.569% 
LR: 2.000e-04 

2023-03-01 13:45:20,278 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1120 (0.1570) Acc D Real: 100.000% 
Loss D Fake: 0.7966 (0.9151) Acc D Fake: 0.484% 
Loss D: 0.909 
Loss G: 0.6120 (0.5227) Acc G: 99.457% 
LR: 2.000e-04 

2023-03-01 13:45:20,285 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.1274 (0.1567) Acc D Real: 99.998% 
Loss D Fake: 0.7949 (0.9137) Acc D Fake: 0.594% 
Loss D: 0.922 
Loss G: 0.6136 (0.5237) Acc G: 99.330% 
LR: 2.000e-04 

2023-03-01 13:45:20,293 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.1123 (0.1562) Acc D Real: 99.998% 
Loss D Fake: 0.7931 (0.9123) Acc D Fake: 0.720% 
Loss D: 0.905 
Loss G: 0.6153 (0.5248) Acc G: 99.186% 
LR: 2.000e-04 

2023-03-01 13:45:20,301 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.1136 (0.1557) Acc D Real: 99.998% 
Loss D Fake: 0.7911 (0.9110) Acc D Fake: 0.861% 
Loss D: 0.905 
Loss G: 0.6172 (0.5258) Acc G: 99.045% 
LR: 2.000e-04 

2023-03-01 13:45:20,308 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.1132 (0.1552) Acc D Real: 99.998% 
Loss D Fake: 0.7890 (0.9096) Acc D Fake: 1.000% 
Loss D: 0.902 
Loss G: 0.6192 (0.5268) Acc G: 98.907% 
LR: 2.000e-04 

2023-03-01 13:45:20,315 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.1169 (0.1548) Acc D Real: 99.998% 
Loss D Fake: 0.7868 (0.9083) Acc D Fake: 1.154% 
Loss D: 0.904 
Loss G: 0.6211 (0.5279) Acc G: 98.755% 
LR: 2.000e-04 

2023-03-01 13:45:20,323 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.1265 (0.1545) Acc D Real: 99.994% 
Loss D Fake: 0.7848 (0.9069) Acc D Fake: 1.304% 
Loss D: 0.911 
Loss G: 0.6229 (0.5289) Acc G: 98.587% 
LR: 2.000e-04 

2023-03-01 13:45:20,330 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.1135 (0.1541) Acc D Real: 99.994% 
Loss D Fake: 0.7829 (0.9056) Acc D Fake: 1.470% 
Loss D: 0.896 
Loss G: 0.6248 (0.5299) Acc G: 98.423% 
LR: 2.000e-04 

2023-03-01 13:45:20,339 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.1116 (0.1536) Acc D Real: 99.994% 
Loss D Fake: 0.7809 (0.9043) Acc D Fake: 1.631% 
Loss D: 0.892 
Loss G: 0.6266 (0.5310) Acc G: 98.262% 
LR: 2.000e-04 

2023-03-01 13:45:20,347 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.1141 (0.1532) Acc D Real: 99.994% 
Loss D Fake: 0.7789 (0.9029) Acc D Fake: 1.807% 
Loss D: 0.893 
Loss G: 0.6285 (0.5320) Acc G: 98.088% 
LR: 2.000e-04 

2023-03-01 13:45:20,356 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.1124 (0.1528) Acc D Real: 99.994% 
Loss D Fake: 0.7769 (0.9016) Acc D Fake: 1.979% 
Loss D: 0.889 
Loss G: 0.6304 (0.5330) Acc G: 97.917% 
LR: 2.000e-04 

2023-03-01 13:45:20,364 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1125 (0.1523) Acc D Real: 99.994% 
Loss D Fake: 0.7749 (0.9003) Acc D Fake: 2.148% 
Loss D: 0.887 
Loss G: 0.6322 (0.5340) Acc G: 97.742% 
LR: 2.000e-04 

2023-03-01 13:45:20,372 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.1090 (0.1519) Acc D Real: 99.994% 
Loss D Fake: 0.7730 (0.8990) Acc D Fake: 2.330% 
Loss D: 0.882 
Loss G: 0.6341 (0.5351) Acc G: 97.561% 
LR: 2.000e-04 

2023-03-01 13:45:20,379 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.1097 (0.1515) Acc D Real: 99.994% 
Loss D Fake: 0.7710 (0.8977) Acc D Fake: 2.508% 
Loss D: 0.881 
Loss G: 0.6360 (0.5361) Acc G: 97.383% 
LR: 2.000e-04 

2023-03-01 13:45:20,387 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.1070 (0.1510) Acc D Real: 99.994% 
Loss D Fake: 0.7689 (0.8964) Acc D Fake: 2.717% 
Loss D: 0.876 
Loss G: 0.6380 (0.5371) Acc G: 97.076% 
LR: 2.000e-04 

2023-03-01 13:45:20,394 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.1061 (0.1506) Acc D Real: 99.994% 
Loss D Fake: 0.7667 (0.8952) Acc D Fake: 3.069% 
Loss D: 0.873 
Loss G: 0.6401 (0.5381) Acc G: 96.692% 
LR: 2.000e-04 

2023-03-01 13:45:20,401 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.1149 (0.1502) Acc D Real: 99.994% 
Loss D Fake: 0.7647 (0.8939) Acc D Fake: 3.464% 
Loss D: 0.880 
Loss G: 0.6418 (0.5391) Acc G: 96.300% 
LR: 2.000e-04 

2023-03-01 13:45:20,408 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.1144 (0.1499) Acc D Real: 99.994% 
Loss D Fake: 0.7631 (0.8926) Acc D Fake: 3.867% 
Loss D: 0.877 
Loss G: 0.6433 (0.5401) Acc G: 95.899% 
LR: 2.000e-04 

2023-03-01 13:45:20,416 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.1079 (0.1495) Acc D Real: 99.994% 
Loss D Fake: 0.7617 (0.8913) Acc D Fake: 4.263% 
Loss D: 0.870 
Loss G: 0.6448 (0.5411) Acc G: 95.474% 
LR: 2.000e-04 

2023-03-01 13:45:20,423 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.1049 (0.1491) Acc D Real: 99.995% 
Loss D Fake: 0.7601 (0.8901) Acc D Fake: 4.683% 
Loss D: 0.865 
Loss G: 0.6465 (0.5421) Acc G: 95.041% 
LR: 2.000e-04 

2023-03-01 13:45:20,430 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.1062 (0.1487) Acc D Real: 99.995% 
Loss D Fake: 0.7584 (0.8889) Acc D Fake: 5.110% 
Loss D: 0.865 
Loss G: 0.6483 (0.5431) Acc G: 94.616% 
LR: 2.000e-04 

2023-03-01 13:45:20,438 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.1048 (0.1482) Acc D Real: 99.995% 
Loss D Fake: 0.7565 (0.8876) Acc D Fake: 5.561% 
Loss D: 0.861 
Loss G: 0.6502 (0.5442) Acc G: 94.168% 
LR: 2.000e-04 

2023-03-01 13:45:20,445 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.1070 (0.1479) Acc D Real: 99.995% 
Loss D Fake: 0.7545 (0.8864) Acc D Fake: 6.034% 
Loss D: 0.861 
Loss G: 0.6522 (0.5452) Acc G: 93.697% 
LR: 2.000e-04 

2023-03-01 13:45:20,452 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.1077 (0.1475) Acc D Real: 99.995% 
Loss D Fake: 0.7525 (0.8852) Acc D Fake: 6.498% 
Loss D: 0.860 
Loss G: 0.6541 (0.5462) Acc G: 93.220% 
LR: 2.000e-04 

2023-03-01 13:45:20,459 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.1072 (0.1471) Acc D Real: 99.995% 
Loss D Fake: 0.7505 (0.8839) Acc D Fake: 6.970% 
Loss D: 0.858 
Loss G: 0.6561 (0.5471) Acc G: 92.751% 
LR: 2.000e-04 

2023-03-01 13:45:20,467 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.1037 (0.1467) Acc D Real: 99.995% 
Loss D Fake: 0.7485 (0.8827) Acc D Fake: 7.447% 
Loss D: 0.852 
Loss G: 0.6581 (0.5481) Acc G: 92.261% 
LR: 2.000e-04 

2023-03-01 13:45:20,474 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.1064 (0.1464) Acc D Real: 99.995% 
Loss D Fake: 0.7464 (0.8815) Acc D Fake: 7.932% 
Loss D: 0.853 
Loss G: 0.6602 (0.5491) Acc G: 91.764% 
LR: 2.000e-04 

2023-03-01 13:45:20,482 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.1082 (0.1460) Acc D Real: 99.995% 
Loss D Fake: 0.7444 (0.8803) Acc D Fake: 8.422% 
Loss D: 0.853 
Loss G: 0.6620 (0.5501) Acc G: 91.247% 
LR: 2.000e-04 

2023-03-01 13:45:20,489 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.1066 (0.1457) Acc D Real: 99.995% 
Loss D Fake: 0.7427 (0.8791) Acc D Fake: 8.933% 
Loss D: 0.849 
Loss G: 0.6638 (0.5511) Acc G: 90.710% 
LR: 2.000e-04 

2023-03-01 13:45:20,496 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.1051 (0.1453) Acc D Real: 99.995% 
Loss D Fake: 0.7409 (0.8779) Acc D Fake: 9.478% 
Loss D: 0.846 
Loss G: 0.6657 (0.5521) Acc G: 90.168% 
LR: 2.000e-04 

2023-03-01 13:45:20,505 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.1206 (0.1451) Acc D Real: 99.988% 
Loss D Fake: 0.7392 (0.8767) Acc D Fake: 10.014% 
Loss D: 0.860 
Loss G: 0.6675 (0.5531) Acc G: 89.635% 
LR: 2.000e-04 

2023-03-01 13:45:20,513 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.1065 (0.1448) Acc D Real: 99.988% 
Loss D Fake: 0.7374 (0.8755) Acc D Fake: 10.541% 
Loss D: 0.844 
Loss G: 0.6694 (0.5541) Acc G: 89.111% 
LR: 2.000e-04 

2023-03-01 13:45:20,521 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.1022 (0.1444) Acc D Real: 99.988% 
Loss D Fake: 0.7355 (0.8743) Acc D Fake: 11.059% 
Loss D: 0.838 
Loss G: 0.6713 (0.5551) Acc G: 88.581% 
LR: 2.000e-04 

2023-03-01 13:45:20,528 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.1018 (0.1441) Acc D Real: 99.988% 
Loss D Fake: 0.7335 (0.8731) Acc D Fake: 11.583% 
Loss D: 0.835 
Loss G: 0.6734 (0.5561) Acc G: 88.061% 
LR: 2.000e-04 

2023-03-01 13:45:20,535 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.1038 (0.1437) Acc D Real: 99.988% 
Loss D Fake: 0.7315 (0.8719) Acc D Fake: 12.097% 
Loss D: 0.835 
Loss G: 0.6755 (0.5571) Acc G: 87.549% 
LR: 2.000e-04 

2023-03-01 13:45:20,543 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.1017 (0.1434) Acc D Real: 99.988% 
Loss D Fake: 0.7295 (0.8708) Acc D Fake: 12.603% 
Loss D: 0.831 
Loss G: 0.6776 (0.5581) Acc G: 87.046% 
LR: 2.000e-04 

2023-03-01 13:45:20,550 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.1023 (0.1431) Acc D Real: 99.988% 
Loss D Fake: 0.7274 (0.8696) Acc D Fake: 13.101% 
Loss D: 0.830 
Loss G: 0.6797 (0.5591) Acc G: 86.551% 
LR: 2.000e-04 

2023-03-01 13:45:20,558 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.1035 (0.1427) Acc D Real: 99.989% 
Loss D Fake: 0.7254 (0.8684) Acc D Fake: 13.591% 
Loss D: 0.829 
Loss G: 0.6819 (0.5601) Acc G: 86.065% 
LR: 2.000e-04 

2023-03-01 13:45:20,565 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.1023 (0.1424) Acc D Real: 99.989% 
Loss D Fake: 0.7233 (0.8672) Acc D Fake: 14.073% 
Loss D: 0.826 
Loss G: 0.6840 (0.5611) Acc G: 85.586% 
LR: 2.000e-04 

2023-03-01 13:45:20,573 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.1242 (0.1423) Acc D Real: 99.974% 
Loss D Fake: 0.7214 (0.8661) Acc D Fake: 14.547% 
Loss D: 0.846 
Loss G: 0.6859 (0.5621) Acc G: 85.114% 
LR: 2.000e-04 

2023-03-01 13:45:20,580 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.1044 (0.1420) Acc D Real: 99.974% 
Loss D Fake: 0.7197 (0.8649) Acc D Fake: 15.013% 
Loss D: 0.824 
Loss G: 0.6877 (0.5631) Acc G: 84.650% 
LR: 2.000e-04 

2023-03-01 13:45:20,588 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.1038 (0.1417) Acc D Real: 99.975% 
Loss D Fake: 0.7181 (0.8638) Acc D Fake: 15.472% 
Loss D: 0.822 
Loss G: 0.6895 (0.5641) Acc G: 84.194% 
LR: 2.000e-04 

2023-03-01 13:45:20,595 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.1049 (0.1414) Acc D Real: 99.975% 
Loss D Fake: 0.7165 (0.8626) Acc D Fake: 15.924% 
Loss D: 0.821 
Loss G: 0.6914 (0.5651) Acc G: 83.744% 
LR: 2.000e-04 

2023-03-01 13:45:20,603 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.1022 (0.1411) Acc D Real: 99.975% 
Loss D Fake: 0.7148 (0.8615) Acc D Fake: 16.370% 
Loss D: 0.817 
Loss G: 0.6932 (0.5661) Acc G: 83.302% 
LR: 2.000e-04 

2023-03-01 13:45:20,611 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.0967 (0.1407) Acc D Real: 99.975% 
Loss D Fake: 0.7131 (0.8603) Acc D Fake: 16.808% 
Loss D: 0.810 
Loss G: 0.6953 (0.5671) Acc G: 82.866% 
LR: 2.000e-04 

2023-03-01 13:45:20,618 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1185 (0.1406) Acc D Real: 99.959% 
Loss D Fake: 0.7111 (0.8592) Acc D Fake: 17.239% 
Loss D: 0.830 
Loss G: 0.6974 (0.5681) Acc G: 82.437% 
LR: 2.000e-04 

2023-03-01 13:45:20,626 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.0975 (0.1402) Acc D Real: 99.959% 
Loss D Fake: 0.7092 (0.8580) Acc D Fake: 17.664% 
Loss D: 0.807 
Loss G: 0.6996 (0.5691) Acc G: 82.015% 
LR: 2.000e-04 

2023-03-01 13:45:20,633 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.1005 (0.1399) Acc D Real: 99.960% 
Loss D Fake: 0.7071 (0.8569) Acc D Fake: 18.083% 
Loss D: 0.808 
Loss G: 0.7019 (0.5701) Acc G: 81.586% 
LR: 2.000e-04 

2023-03-01 13:45:20,640 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.1360 (0.1399) Acc D Real: 99.927% 
Loss D Fake: 0.7049 (0.8558) Acc D Fake: 18.507% 
Loss D: 0.841 
Loss G: 0.7042 (0.5711) Acc G: 81.164% 
LR: 2.000e-04 

2023-03-01 13:45:20,648 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.1018 (0.1396) Acc D Real: 99.927% 
Loss D Fake: 0.7030 (0.8546) Acc D Fake: 18.926% 
Loss D: 0.805 
Loss G: 0.7063 (0.5721) Acc G: 80.748% 
LR: 2.000e-04 

2023-03-01 13:45:20,655 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.0979 (0.1393) Acc D Real: 99.928% 
Loss D Fake: 0.7011 (0.8535) Acc D Fake: 19.338% 
Loss D: 0.799 
Loss G: 0.7084 (0.5731) Acc G: 80.338% 
LR: 2.000e-04 

2023-03-01 13:45:20,662 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1011 (0.1390) Acc D Real: 99.928% 
Loss D Fake: 0.6992 (0.8524) Acc D Fake: 19.745% 
Loss D: 0.800 
Loss G: 0.7106 (0.5741) Acc G: 79.934% 
LR: 2.000e-04 

2023-03-01 13:45:20,669 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.0994 (0.1388) Acc D Real: 99.929% 
Loss D Fake: 0.6972 (0.8513) Acc D Fake: 20.145% 
Loss D: 0.797 
Loss G: 0.7128 (0.5751) Acc G: 79.536% 
LR: 2.000e-04 

2023-03-01 13:45:20,677 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.0994 (0.1385) Acc D Real: 99.929% 
Loss D Fake: 0.6952 (0.8501) Acc D Fake: 20.540% 
Loss D: 0.795 
Loss G: 0.7151 (0.5761) Acc G: 79.143% 
LR: 2.000e-04 

2023-03-01 13:45:20,684 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.1024 (0.1382) Acc D Real: 99.930% 
Loss D Fake: 0.6931 (0.8490) Acc D Fake: 20.929% 
Loss D: 0.796 
Loss G: 0.7174 (0.5771) Acc G: 78.757% 
LR: 2.000e-04 

2023-03-01 13:45:20,691 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.0992 (0.1379) Acc D Real: 99.930% 
Loss D Fake: 0.6912 (0.8479) Acc D Fake: 21.312% 
Loss D: 0.790 
Loss G: 0.7196 (0.5781) Acc G: 78.375% 
LR: 2.000e-04 

2023-03-01 13:45:20,699 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.0957 (0.1376) Acc D Real: 99.931% 
Loss D Fake: 0.6892 (0.8468) Acc D Fake: 21.690% 
Loss D: 0.785 
Loss G: 0.7220 (0.5791) Acc G: 78.000% 
LR: 2.000e-04 

2023-03-01 13:45:20,706 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.1178 (0.1375) Acc D Real: 99.916% 
Loss D Fake: 0.6871 (0.8457) Acc D Fake: 22.063% 
Loss D: 0.805 
Loss G: 0.7243 (0.5801) Acc G: 77.629% 
LR: 2.000e-04 

2023-03-01 13:45:20,713 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1179 (0.1374) Acc D Real: 99.901% 
Loss D Fake: 0.6851 (0.8445) Acc D Fake: 22.431% 
Loss D: 0.803 
Loss G: 0.7266 (0.5812) Acc G: 77.263% 
LR: 2.000e-04 

2023-03-01 13:45:20,720 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.0981 (0.1371) Acc D Real: 99.902% 
Loss D Fake: 0.6831 (0.8434) Acc D Fake: 22.793% 
Loss D: 0.781 
Loss G: 0.7289 (0.5822) Acc G: 76.903% 
LR: 2.000e-04 

2023-03-01 13:45:20,728 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.0962 (0.1368) Acc D Real: 99.903% 
Loss D Fake: 0.6811 (0.8423) Acc D Fake: 23.151% 
Loss D: 0.777 
Loss G: 0.7314 (0.5832) Acc G: 76.548% 
LR: 2.000e-04 

2023-03-01 13:45:20,737 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.0976 (0.1365) Acc D Real: 99.903% 
Loss D Fake: 0.6790 (0.8412) Acc D Fake: 23.503% 
Loss D: 0.777 
Loss G: 0.7339 (0.5842) Acc G: 76.197% 
LR: 2.000e-04 

2023-03-01 13:45:20,745 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.1160 (0.1364) Acc D Real: 99.890% 
Loss D Fake: 0.6768 (0.8401) Acc D Fake: 23.851% 
Loss D: 0.793 
Loss G: 0.7363 (0.5852) Acc G: 75.840% 
LR: 2.000e-04 

2023-03-01 13:45:20,753 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.1175 (0.1363) Acc D Real: 99.875% 
Loss D Fake: 0.6748 (0.8390) Acc D Fake: 24.206% 
Loss D: 0.792 
Loss G: 0.7386 (0.5863) Acc G: 75.488% 
LR: 2.000e-04 

2023-03-01 13:45:20,760 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.0996 (0.1360) Acc D Real: 99.875% 
Loss D Fake: 0.6728 (0.8379) Acc D Fake: 24.556% 
Loss D: 0.772 
Loss G: 0.7410 (0.5873) Acc G: 75.140% 
LR: 2.000e-04 

2023-03-01 13:45:20,768 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.0997 (0.1358) Acc D Real: 99.876% 
Loss D Fake: 0.6708 (0.8368) Acc D Fake: 24.901% 
Loss D: 0.771 
Loss G: 0.7434 (0.5883) Acc G: 74.797% 
LR: 2.000e-04 

2023-03-01 13:45:20,775 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.0960 (0.1355) Acc D Real: 99.877% 
Loss D Fake: 0.6688 (0.8357) Acc D Fake: 25.241% 
Loss D: 0.765 
Loss G: 0.7459 (0.5894) Acc G: 74.458% 
LR: 2.000e-04 

2023-03-01 13:45:20,783 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.0956 (0.1353) Acc D Real: 99.878% 
Loss D Fake: 0.6667 (0.8346) Acc D Fake: 25.577% 
Loss D: 0.762 
Loss G: 0.7484 (0.5904) Acc G: 74.124% 
LR: 2.000e-04 

2023-03-01 13:45:20,790 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.1143 (0.1351) Acc D Real: 99.863% 
Loss D Fake: 0.6646 (0.8335) Acc D Fake: 25.909% 
Loss D: 0.779 
Loss G: 0.7510 (0.5915) Acc G: 73.794% 
LR: 2.000e-04 

2023-03-01 13:45:20,799 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.0963 (0.1349) Acc D Real: 99.864% 
Loss D Fake: 0.6624 (0.8324) Acc D Fake: 26.237% 
Loss D: 0.759 
Loss G: 0.7536 (0.5925) Acc G: 73.469% 
LR: 2.000e-04 

2023-03-01 13:45:20,806 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.0942 (0.1346) Acc D Real: 99.865% 
Loss D Fake: 0.6603 (0.8313) Acc D Fake: 26.560% 
Loss D: 0.754 
Loss G: 0.7563 (0.5936) Acc G: 73.147% 
LR: 2.000e-04 

2023-03-01 13:45:20,814 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.0970 (0.1344) Acc D Real: 99.866% 
Loss D Fake: 0.6580 (0.8302) Acc D Fake: 26.879% 
Loss D: 0.755 
Loss G: 0.7590 (0.5946) Acc G: 72.830% 
LR: 2.000e-04 

2023-03-01 13:45:20,821 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.0973 (0.1341) Acc D Real: 99.866% 
Loss D Fake: 0.6559 (0.8291) Acc D Fake: 27.194% 
Loss D: 0.753 
Loss G: 0.7616 (0.5957) Acc G: 72.517% 
LR: 2.000e-04 

2023-03-01 13:45:20,828 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.0938 (0.1339) Acc D Real: 99.867% 
Loss D Fake: 0.6537 (0.8280) Acc D Fake: 27.505% 
Loss D: 0.748 
Loss G: 0.7643 (0.5967) Acc G: 72.207% 
LR: 2.000e-04 

2023-03-01 13:45:20,836 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.0960 (0.1337) Acc D Real: 99.868% 
Loss D Fake: 0.6516 (0.8268) Acc D Fake: 27.812% 
Loss D: 0.748 
Loss G: 0.7668 (0.5978) Acc G: 71.902% 
LR: 2.000e-04 

2023-03-01 13:45:20,843 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.1143 (0.1335) Acc D Real: 99.855% 
Loss D Fake: 0.6498 (0.8257) Acc D Fake: 28.116% 
Loss D: 0.764 
Loss G: 0.7693 (0.5989) Acc G: 71.600% 
LR: 2.000e-04 

2023-03-01 13:45:20,851 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.0978 (0.1333) Acc D Real: 99.856% 
Loss D Fake: 0.6479 (0.8247) Acc D Fake: 28.416% 
Loss D: 0.746 
Loss G: 0.7717 (0.5999) Acc G: 71.302% 
LR: 2.000e-04 

2023-03-01 13:45:20,858 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.1131 (0.1332) Acc D Real: 99.849% 
Loss D Fake: 0.6471 (0.8236) Acc D Fake: 28.712% 
Loss D: 0.760 
Loss G: 0.7709 (0.6010) Acc G: 71.008% 
LR: 2.000e-04 

2023-03-01 13:45:20,866 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.0951 (0.1330) Acc D Real: 99.850% 
Loss D Fake: 0.6502 (0.8225) Acc D Fake: 28.994% 
Loss D: 0.745 
Loss G: 0.7687 (0.6020) Acc G: 70.728% 
LR: 2.000e-04 

2023-03-01 13:45:20,873 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.0921 (0.1327) Acc D Real: 99.851% 
Loss D Fake: 0.6531 (0.8215) Acc D Fake: 29.273% 
Loss D: 0.745 
Loss G: 0.7669 (0.6030) Acc G: 70.461% 
LR: 2.000e-04 

2023-03-01 13:45:20,881 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.0940 (0.1325) Acc D Real: 99.852% 
Loss D Fake: 0.6553 (0.8205) Acc D Fake: 29.538% 
Loss D: 0.749 
Loss G: 0.7662 (0.6040) Acc G: 70.197% 
LR: 2.000e-04 

2023-03-01 13:45:20,889 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.0912 (0.1322) Acc D Real: 99.853% 
Loss D Fake: 0.6560 (0.8195) Acc D Fake: 29.800% 
Loss D: 0.747 
Loss G: 0.7673 (0.6050) Acc G: 69.936% 
LR: 2.000e-04 

2023-03-01 13:45:20,897 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.0894 (0.1320) Acc D Real: 99.854% 
Loss D Fake: 0.6544 (0.8185) Acc D Fake: 30.060% 
Loss D: 0.744 
Loss G: 0.7705 (0.6059) Acc G: 69.679% 
LR: 2.000e-04 

2023-03-01 13:45:20,904 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.0899 (0.1317) Acc D Real: 99.855% 
Loss D Fake: 0.6508 (0.8175) Acc D Fake: 30.316% 
Loss D: 0.741 
Loss G: 0.7753 (0.6069) Acc G: 69.424% 
LR: 2.000e-04 

2023-03-01 13:45:20,911 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.0911 (0.1315) Acc D Real: 99.855% 
Loss D Fake: 0.6459 (0.8165) Acc D Fake: 30.569% 
Loss D: 0.737 
Loss G: 0.7810 (0.6080) Acc G: 69.163% 
LR: 2.000e-04 

2023-03-01 13:45:20,919 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.0917 (0.1313) Acc D Real: 99.856% 
Loss D Fake: 0.6406 (0.8155) Acc D Fake: 30.828% 
Loss D: 0.732 
Loss G: 0.7868 (0.6090) Acc G: 68.904% 
LR: 2.000e-04 

2023-03-01 13:45:20,927 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.0921 (0.1310) Acc D Real: 99.857% 
Loss D Fake: 0.6354 (0.8144) Acc D Fake: 31.085% 
Loss D: 0.727 
Loss G: 0.7924 (0.6101) Acc G: 68.639% 
LR: 2.000e-04 

2023-03-01 13:45:20,935 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.0926 (0.1308) Acc D Real: 99.858% 
Loss D Fake: 0.6305 (0.8134) Acc D Fake: 31.349% 
Loss D: 0.723 
Loss G: 0.7977 (0.6112) Acc G: 68.378% 
LR: 2.000e-04 

2023-03-01 13:45:20,943 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.0943 (0.1306) Acc D Real: 99.859% 
Loss D Fake: 0.6261 (0.8123) Acc D Fake: 31.609% 
Loss D: 0.720 
Loss G: 0.8026 (0.6123) Acc G: 68.119% 
LR: 2.000e-04 

2023-03-01 13:45:20,950 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.1111 (0.1305) Acc D Real: 99.850% 
Loss D Fake: 0.6224 (0.8112) Acc D Fake: 31.867% 
Loss D: 0.734 
Loss G: 0.8063 (0.6134) Acc G: 67.853% 
LR: 2.000e-04 

2023-03-01 13:45:20,958 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.0946 (0.1303) Acc D Real: 99.851% 
Loss D Fake: 0.6198 (0.8101) Acc D Fake: 32.131% 
Loss D: 0.714 
Loss G: 0.8096 (0.6145) Acc G: 67.591% 
LR: 2.000e-04 

2023-03-01 13:45:20,966 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.0937 (0.1301) Acc D Real: 99.852% 
Loss D Fake: 0.6172 (0.8090) Acc D Fake: 32.392% 
Loss D: 0.711 
Loss G: 0.8130 (0.6156) Acc G: 67.331% 
LR: 2.000e-04 

2023-03-01 13:45:20,974 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.1145 (0.1300) Acc D Real: 99.839% 
Loss D Fake: 0.6146 (0.8079) Acc D Fake: 32.650% 
Loss D: 0.729 
Loss G: 0.8164 (0.6167) Acc G: 67.075% 
LR: 2.000e-04 

2023-03-01 13:45:20,982 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.0923 (0.1298) Acc D Real: 99.840% 
Loss D Fake: 0.6121 (0.8068) Acc D Fake: 32.905% 
Loss D: 0.704 
Loss G: 0.8199 (0.6179) Acc G: 66.821% 
LR: 2.000e-04 

2023-03-01 13:45:20,989 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.0946 (0.1296) Acc D Real: 99.841% 
Loss D Fake: 0.6095 (0.8057) Acc D Fake: 33.157% 
Loss D: 0.704 
Loss G: 0.8233 (0.6190) Acc G: 66.570% 
LR: 2.000e-04 

2023-03-01 13:45:20,997 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.0908 (0.1294) Acc D Real: 99.841% 
Loss D Fake: 0.6070 (0.8046) Acc D Fake: 33.407% 
Loss D: 0.698 
Loss G: 0.8267 (0.6202) Acc G: 66.322% 
LR: 2.000e-04 

2023-03-01 13:45:21,004 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.0976 (0.1292) Acc D Real: 99.842% 
Loss D Fake: 0.6045 (0.8035) Acc D Fake: 33.654% 
Loss D: 0.702 
Loss G: 0.8299 (0.6213) Acc G: 66.077% 
LR: 2.000e-04 

2023-03-01 13:45:21,012 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.0945 (0.1290) Acc D Real: 99.843% 
Loss D Fake: 0.6024 (0.8025) Acc D Fake: 33.898% 
Loss D: 0.697 
Loss G: 0.8330 (0.6225) Acc G: 65.834% 
LR: 2.000e-04 

2023-03-01 13:45:21,020 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.0956 (0.1288) Acc D Real: 99.844% 
Loss D Fake: 0.6003 (0.8014) Acc D Fake: 34.139% 
Loss D: 0.696 
Loss G: 0.8361 (0.6236) Acc G: 65.594% 
LR: 2.000e-04 

2023-03-01 13:45:21,027 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.0946 (0.1286) Acc D Real: 99.845% 
Loss D Fake: 0.5982 (0.8003) Acc D Fake: 34.378% 
Loss D: 0.693 
Loss G: 0.8392 (0.6248) Acc G: 65.357% 
LR: 2.000e-04 

2023-03-01 13:45:21,034 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.0931 (0.1284) Acc D Real: 99.846% 
Loss D Fake: 0.5960 (0.7992) Acc D Fake: 34.615% 
Loss D: 0.689 
Loss G: 0.8424 (0.6260) Acc G: 65.122% 
LR: 2.000e-04 

2023-03-01 13:45:21,042 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.0909 (0.1282) Acc D Real: 99.847% 
Loss D Fake: 0.5938 (0.7981) Acc D Fake: 34.848% 
Loss D: 0.685 
Loss G: 0.8457 (0.6271) Acc G: 64.889% 
LR: 2.000e-04 

2023-03-01 13:45:21,049 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.0924 (0.1281) Acc D Real: 99.847% 
Loss D Fake: 0.5915 (0.7970) Acc D Fake: 35.080% 
Loss D: 0.684 
Loss G: 0.8491 (0.6283) Acc G: 64.660% 
LR: 2.000e-04 

2023-03-01 13:45:21,056 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.0939 (0.1279) Acc D Real: 99.848% 
Loss D Fake: 0.5892 (0.7959) Acc D Fake: 35.309% 
Loss D: 0.683 
Loss G: 0.8525 (0.6295) Acc G: 64.432% 
LR: 2.000e-04 

2023-03-01 13:45:21,063 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.0933 (0.1277) Acc D Real: 99.849% 
Loss D Fake: 0.5869 (0.7948) Acc D Fake: 35.544% 
Loss D: 0.680 
Loss G: 0.8559 (0.6307) Acc G: 64.198% 
LR: 2.000e-04 

2023-03-01 13:45:21,071 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.0937 (0.1275) Acc D Real: 99.850% 
Loss D Fake: 0.5847 (0.7937) Acc D Fake: 35.777% 
Loss D: 0.678 
Loss G: 0.8594 (0.6319) Acc G: 63.967% 
LR: 2.000e-04 

2023-03-01 13:45:21,078 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.0930 (0.1273) Acc D Real: 99.851% 
Loss D Fake: 0.5825 (0.7926) Acc D Fake: 36.007% 
Loss D: 0.675 
Loss G: 0.8627 (0.6331) Acc G: 63.738% 
LR: 2.000e-04 

2023-03-01 13:45:21,086 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.0951 (0.1272) Acc D Real: 99.851% 
Loss D Fake: 0.5803 (0.7915) Acc D Fake: 36.235% 
Loss D: 0.675 
Loss G: 0.8661 (0.6343) Acc G: 63.511% 
LR: 2.000e-04 

2023-03-01 13:45:21,093 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.0976 (0.1270) Acc D Real: 99.852% 
Loss D Fake: 0.5783 (0.7904) Acc D Fake: 36.460% 
Loss D: 0.676 
Loss G: 0.8690 (0.6355) Acc G: 63.287% 
LR: 2.000e-04 

2023-03-01 13:45:21,101 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.1375 (0.1271) Acc D Real: 99.828% 
Loss D Fake: 0.5768 (0.7893) Acc D Fake: 36.684% 
Loss D: 0.714 
Loss G: 0.8717 (0.6367) Acc G: 63.065% 
LR: 2.000e-04 

2023-03-01 13:45:21,108 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.0929 (0.1269) Acc D Real: 99.829% 
Loss D Fake: 0.5752 (0.7882) Acc D Fake: 36.905% 
Loss D: 0.668 
Loss G: 0.8746 (0.6379) Acc G: 62.845% 
LR: 2.000e-04 

2023-03-01 13:45:21,115 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.1153 (0.1268) Acc D Real: 99.817% 
Loss D Fake: 0.5735 (0.7871) Acc D Fake: 37.124% 
Loss D: 0.689 
Loss G: 0.8776 (0.6392) Acc G: 62.628% 
LR: 2.000e-04 

2023-03-01 13:45:21,123 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.0902 (0.1267) Acc D Real: 99.818% 
Loss D Fake: 0.5717 (0.7860) Acc D Fake: 37.340% 
Loss D: 0.662 
Loss G: 0.8808 (0.6404) Acc G: 62.412% 
LR: 2.000e-04 

2023-03-01 13:45:21,130 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.0899 (0.1265) Acc D Real: 99.819% 
Loss D Fake: 0.5697 (0.7849) Acc D Fake: 37.554% 
Loss D: 0.660 
Loss G: 0.8843 (0.6416) Acc G: 62.199% 
LR: 2.000e-04 

2023-03-01 13:45:21,137 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.0910 (0.1263) Acc D Real: 99.820% 
Loss D Fake: 0.5674 (0.7838) Acc D Fake: 37.767% 
Loss D: 0.658 
Loss G: 0.8879 (0.6428) Acc G: 61.988% 
LR: 2.000e-04 

2023-03-01 13:45:21,144 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.0909 (0.1261) Acc D Real: 99.821% 
Loss D Fake: 0.5651 (0.7827) Acc D Fake: 37.977% 
Loss D: 0.656 
Loss G: 0.8918 (0.6441) Acc G: 61.779% 
LR: 2.000e-04 

2023-03-01 13:45:21,152 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.0885 (0.1259) Acc D Real: 99.822% 
Loss D Fake: 0.5626 (0.7816) Acc D Fake: 38.185% 
Loss D: 0.651 
Loss G: 0.8958 (0.6453) Acc G: 61.573% 
LR: 2.000e-04 

2023-03-01 13:45:21,159 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.1097 (0.1258) Acc D Real: 99.815% 
Loss D Fake: 0.5605 (0.7806) Acc D Fake: 38.391% 
Loss D: 0.670 
Loss G: 0.8983 (0.6466) Acc G: 61.368% 
LR: 2.000e-04 

2023-03-01 13:45:21,166 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.0869 (0.1257) Acc D Real: 99.815% 
Loss D Fake: 0.5597 (0.7795) Acc D Fake: 38.595% 
Loss D: 0.647 
Loss G: 0.9007 (0.6478) Acc G: 61.165% 
LR: 2.000e-04 

2023-03-01 13:45:21,174 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.0905 (0.1255) Acc D Real: 99.816% 
Loss D Fake: 0.5584 (0.7784) Acc D Fake: 38.796% 
Loss D: 0.649 
Loss G: 0.9036 (0.6491) Acc G: 60.972% 
LR: 2.000e-04 

2023-03-01 13:45:21,181 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.0906 (0.1253) Acc D Real: 99.817% 
Loss D Fake: 0.5568 (0.7773) Acc D Fake: 38.988% 
Loss D: 0.647 
Loss G: 0.9069 (0.6503) Acc G: 60.781% 
LR: 2.000e-04 

2023-03-01 13:45:21,188 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.1107 (0.1252) Acc D Real: 99.806% 
Loss D Fake: 0.5548 (0.7762) Acc D Fake: 39.178% 
Loss D: 0.665 
Loss G: 0.9106 (0.6516) Acc G: 60.584% 
LR: 2.000e-04 

2023-03-01 13:45:21,196 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.0867 (0.1251) Acc D Real: 99.807% 
Loss D Fake: 0.5523 (0.7752) Acc D Fake: 39.374% 
Loss D: 0.639 
Loss G: 0.9149 (0.6528) Acc G: 60.389% 
LR: 2.000e-04 

2023-03-01 13:45:21,203 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.0866 (0.1249) Acc D Real: 99.808% 
Loss D Fake: 0.5495 (0.7741) Acc D Fake: 39.569% 
Loss D: 0.636 
Loss G: 0.9195 (0.6541) Acc G: 60.195% 
LR: 2.000e-04 

2023-03-01 13:45:21,210 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.0875 (0.1247) Acc D Real: 99.809% 
Loss D Fake: 0.5464 (0.7730) Acc D Fake: 39.761% 
Loss D: 0.634 
Loss G: 0.9244 (0.6554) Acc G: 60.004% 
LR: 2.000e-04 

2023-03-01 13:45:21,217 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.0901 (0.1245) Acc D Real: 99.810% 
Loss D Fake: 0.5433 (0.7719) Acc D Fake: 39.952% 
Loss D: 0.633 
Loss G: 0.9292 (0.6567) Acc G: 59.814% 
LR: 2.000e-04 

2023-03-01 13:45:21,224 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.0909 (0.1244) Acc D Real: 99.811% 
Loss D Fake: 0.5403 (0.7708) Acc D Fake: 40.141% 
Loss D: 0.631 
Loss G: 0.9339 (0.6580) Acc G: 59.627% 
LR: 2.000e-04 

2023-03-01 13:45:21,232 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.0870 (0.1242) Acc D Real: 99.812% 
Loss D Fake: 0.5374 (0.7697) Acc D Fake: 40.328% 
Loss D: 0.624 
Loss G: 0.9387 (0.6593) Acc G: 59.441% 
LR: 2.000e-04 

2023-03-01 13:45:21,239 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.0888 (0.1240) Acc D Real: 99.813% 
Loss D Fake: 0.5344 (0.7686) Acc D Fake: 40.514% 
Loss D: 0.623 
Loss G: 0.9436 (0.6606) Acc G: 59.256% 
LR: 2.000e-04 

2023-03-01 13:45:21,247 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.0878 (0.1239) Acc D Real: 99.813% 
Loss D Fake: 0.5314 (0.7675) Acc D Fake: 40.697% 
Loss D: 0.619 
Loss G: 0.9485 (0.6620) Acc G: 59.074% 
LR: 2.000e-04 

2023-03-01 13:45:21,254 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.1141 (0.1238) Acc D Real: 99.803% 
Loss D Fake: 0.5285 (0.7664) Acc D Fake: 40.879% 
Loss D: 0.643 
Loss G: 0.9533 (0.6633) Acc G: 58.893% 
LR: 2.000e-04 

2023-03-01 13:45:21,262 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.1161 (0.1238) Acc D Real: 99.793% 
Loss D Fake: 0.5258 (0.7653) Acc D Fake: 41.067% 
Loss D: 0.642 
Loss G: 0.9579 (0.6647) Acc G: 58.706% 
LR: 2.000e-04 

2023-03-01 13:45:21,269 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.0907 (0.1236) Acc D Real: 99.794% 
Loss D Fake: 0.5232 (0.7642) Acc D Fake: 41.253% 
Loss D: 0.614 
Loss G: 0.9624 (0.6661) Acc G: 58.521% 
LR: 2.000e-04 

2023-03-01 13:45:21,277 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.0885 (0.1235) Acc D Real: 99.795% 
Loss D Fake: 0.5206 (0.7631) Acc D Fake: 41.438% 
Loss D: 0.609 
Loss G: 0.9669 (0.6674) Acc G: 58.337% 
LR: 2.000e-04 

2023-03-01 13:45:21,284 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.0892 (0.1233) Acc D Real: 99.795% 
Loss D Fake: 0.5182 (0.7620) Acc D Fake: 41.621% 
Loss D: 0.607 
Loss G: 0.9714 (0.6688) Acc G: 58.155% 
LR: 2.000e-04 

2023-03-01 13:45:21,292 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.0901 (0.1232) Acc D Real: 99.796% 
Loss D Fake: 0.5157 (0.7609) Acc D Fake: 41.802% 
Loss D: 0.606 
Loss G: 0.9759 (0.6702) Acc G: 57.975% 
LR: 2.000e-04 

2023-03-01 13:45:21,300 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.0880 (0.1230) Acc D Real: 99.797% 
Loss D Fake: 0.5134 (0.7597) Acc D Fake: 41.982% 
Loss D: 0.601 
Loss G: 0.9804 (0.6716) Acc G: 57.797% 
LR: 2.000e-04 

2023-03-01 13:45:21,307 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.0877 (0.1228) Acc D Real: 99.798% 
Loss D Fake: 0.5109 (0.7586) Acc D Fake: 42.159% 
Loss D: 0.599 
Loss G: 0.9850 (0.6730) Acc G: 57.620% 
LR: 2.000e-04 

2023-03-01 13:45:21,315 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.0893 (0.1227) Acc D Real: 99.799% 
Loss D Fake: 0.5085 (0.7575) Acc D Fake: 42.336% 
Loss D: 0.598 
Loss G: 0.9897 (0.6744) Acc G: 57.444% 
LR: 2.000e-04 

2023-03-01 13:45:21,322 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.0914 (0.1226) Acc D Real: 99.800% 
Loss D Fake: 0.5061 (0.7564) Acc D Fake: 42.511% 
Loss D: 0.598 
Loss G: 0.9943 (0.6758) Acc G: 57.270% 
LR: 2.000e-04 

2023-03-01 13:45:21,329 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.0859 (0.1224) Acc D Real: 99.800% 
Loss D Fake: 0.5037 (0.7553) Acc D Fake: 42.554% 
Loss D: 0.590 
Loss G: 0.9990 (0.6773) Acc G: 57.227% 
LR: 2.000e-04 

2023-03-01 13:45:21,343 -                train: [    INFO] - Best Loss 0.958 to 0.777
2023-03-01 13:45:21,343 -                train: [    INFO] - 
Epoch: 6/20
2023-03-01 13:45:21,581 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.1161 (0.1274) Acc D Real: 96.562% 
Loss D Fake: 0.5001 (0.5008) Acc D Fake: 81.667% 
Loss D: 0.616 
Loss G: 1.0065 (1.0047) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,589 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.0877 (0.1142) Acc D Real: 97.708% 
Loss D Fake: 0.4986 (0.5001) Acc D Fake: 81.667% 
Loss D: 0.586 
Loss G: 1.0103 (1.0066) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,597 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.0862 (0.1072) Acc D Real: 98.281% 
Loss D Fake: 0.4969 (0.4993) Acc D Fake: 81.667% 
Loss D: 0.583 
Loss G: 1.0144 (1.0085) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,614 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.0871 (0.1032) Acc D Real: 98.625% 
Loss D Fake: 0.4949 (0.4984) Acc D Fake: 81.667% 
Loss D: 0.582 
Loss G: 1.0189 (1.0106) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,621 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.0860 (0.1003) Acc D Real: 98.854% 
Loss D Fake: 0.4928 (0.4975) Acc D Fake: 81.667% 
Loss D: 0.579 
Loss G: 1.0236 (1.0128) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,628 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.0876 (0.0985) Acc D Real: 99.018% 
Loss D Fake: 0.4904 (0.4965) Acc D Fake: 81.667% 
Loss D: 0.578 
Loss G: 1.0285 (1.0150) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,636 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.0899 (0.0974) Acc D Real: 99.141% 
Loss D Fake: 0.4882 (0.4954) Acc D Fake: 81.667% 
Loss D: 0.578 
Loss G: 1.0329 (1.0172) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,643 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.1142 (0.0993) Acc D Real: 98.947% 
Loss D Fake: 0.4865 (0.4944) Acc D Fake: 81.667% 
Loss D: 0.601 
Loss G: 1.0372 (1.0195) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,650 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.0832 (0.0977) Acc D Real: 99.052% 
Loss D Fake: 0.4845 (0.4934) Acc D Fake: 81.667% 
Loss D: 0.568 
Loss G: 1.0420 (1.0217) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,657 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.0847 (0.0965) Acc D Real: 99.138% 
Loss D Fake: 0.4822 (0.4924) Acc D Fake: 81.667% 
Loss D: 0.567 
Loss G: 1.0472 (1.0240) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,664 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.0842 (0.0955) Acc D Real: 99.210% 
Loss D Fake: 0.4796 (0.4914) Acc D Fake: 81.667% 
Loss D: 0.564 
Loss G: 1.0527 (1.0264) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,671 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.0812 (0.0944) Acc D Real: 99.271% 
Loss D Fake: 0.4768 (0.4902) Acc D Fake: 81.667% 
Loss D: 0.558 
Loss G: 1.0586 (1.0289) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,678 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.0848 (0.0937) Acc D Real: 99.323% 
Loss D Fake: 0.4738 (0.4891) Acc D Fake: 81.667% 
Loss D: 0.559 
Loss G: 1.0647 (1.0314) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,685 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.1138 (0.0950) Acc D Real: 99.250% 
Loss D Fake: 0.4718 (0.4879) Acc D Fake: 81.667% 
Loss D: 0.586 
Loss G: 1.0670 (1.0338) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,692 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.0867 (0.0945) Acc D Real: 99.297% 
Loss D Fake: 0.4727 (0.4870) Acc D Fake: 81.667% 
Loss D: 0.559 
Loss G: 1.0683 (1.0360) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,700 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.0838 (0.0939) Acc D Real: 99.338% 
Loss D Fake: 0.4732 (0.4861) Acc D Fake: 81.667% 
Loss D: 0.557 
Loss G: 1.0706 (1.0380) Acc G: 18.431% 
LR: 2.000e-04 

2023-03-01 13:45:21,708 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1107 (0.0948) Acc D Real: 99.256% 
Loss D Fake: 0.4731 (0.4854) Acc D Fake: 81.574% 
Loss D: 0.584 
Loss G: 1.0728 (1.0399) Acc G: 18.519% 
LR: 2.000e-04 

2023-03-01 13:45:21,715 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.1437 (0.0974) Acc D Real: 99.038% 
Loss D Fake: 0.4730 (0.4848) Acc D Fake: 81.491% 
Loss D: 0.617 
Loss G: 1.0757 (1.0418) Acc G: 18.596% 
LR: 2.000e-04 

2023-03-01 13:45:21,722 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.0810 (0.0966) Acc D Real: 99.086% 
Loss D Fake: 0.4720 (0.4841) Acc D Fake: 81.417% 
Loss D: 0.553 
Loss G: 1.0803 (1.0437) Acc G: 18.667% 
LR: 2.000e-04 

2023-03-01 13:45:21,729 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.0826 (0.0959) Acc D Real: 99.129% 
Loss D Fake: 0.4694 (0.4834) Acc D Fake: 81.349% 
Loss D: 0.552 
Loss G: 1.0867 (1.0458) Acc G: 18.730% 
LR: 2.000e-04 

2023-03-01 13:45:21,735 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.0804 (0.0952) Acc D Real: 99.169% 
Loss D Fake: 0.4656 (0.4826) Acc D Fake: 81.288% 
Loss D: 0.546 
Loss G: 1.0945 (1.0480) Acc G: 18.788% 
LR: 2.000e-04 

2023-03-01 13:45:21,742 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.0824 (0.0946) Acc D Real: 99.205% 
Loss D Fake: 0.4612 (0.4817) Acc D Fake: 81.304% 
Loss D: 0.544 
Loss G: 1.1029 (1.0504) Acc G: 18.768% 
LR: 2.000e-04 

2023-03-01 13:45:21,749 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.0835 (0.0942) Acc D Real: 99.238% 
Loss D Fake: 0.4566 (0.4806) Acc D Fake: 81.319% 
Loss D: 0.540 
Loss G: 1.1114 (1.0529) Acc G: 18.750% 
LR: 2.000e-04 

2023-03-01 13:45:21,756 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.0840 (0.0938) Acc D Real: 99.269% 
Loss D Fake: 0.4520 (0.4795) Acc D Fake: 81.333% 
Loss D: 0.536 
Loss G: 1.1199 (1.0556) Acc G: 18.733% 
LR: 2.000e-04 

2023-03-01 13:45:21,763 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.0819 (0.0933) Acc D Real: 99.297% 
Loss D Fake: 0.4476 (0.4783) Acc D Fake: 81.346% 
Loss D: 0.529 
Loss G: 1.1283 (1.0584) Acc G: 18.718% 
LR: 2.000e-04 

2023-03-01 13:45:21,771 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.0870 (0.0931) Acc D Real: 99.323% 
Loss D Fake: 0.4434 (0.4770) Acc D Fake: 81.358% 
Loss D: 0.530 
Loss G: 1.1362 (1.0613) Acc G: 18.704% 
LR: 2.000e-04 

2023-03-01 13:45:21,780 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1176 (0.0940) Acc D Real: 99.256% 
Loss D Fake: 0.4397 (0.4756) Acc D Fake: 81.429% 
Loss D: 0.557 
Loss G: 1.1437 (1.0642) Acc G: 18.631% 
LR: 2.000e-04 

2023-03-01 13:45:21,788 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1184 (0.0948) Acc D Real: 99.194% 
Loss D Fake: 0.4363 (0.4743) Acc D Fake: 81.494% 
Loss D: 0.555 
Loss G: 1.1507 (1.0672) Acc G: 18.563% 
LR: 2.000e-04 

2023-03-01 13:45:21,796 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.1150 (0.0955) Acc D Real: 99.141% 
Loss D Fake: 0.4334 (0.4729) Acc D Fake: 81.556% 
Loss D: 0.548 
Loss G: 1.1570 (1.0702) Acc G: 18.500% 
LR: 2.000e-04 

2023-03-01 13:45:21,805 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.0845 (0.0951) Acc D Real: 99.168% 
Loss D Fake: 0.4309 (0.4716) Acc D Fake: 81.613% 
Loss D: 0.515 
Loss G: 1.1632 (1.0732) Acc G: 18.441% 
LR: 2.000e-04 

2023-03-01 13:45:21,813 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1171 (0.0958) Acc D Real: 99.113% 
Loss D Fake: 0.4284 (0.4702) Acc D Fake: 81.667% 
Loss D: 0.545 
Loss G: 1.1693 (1.0762) Acc G: 18.385% 
LR: 2.000e-04 

2023-03-01 13:45:21,822 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.0823 (0.0954) Acc D Real: 99.140% 
Loss D Fake: 0.4259 (0.4689) Acc D Fake: 81.717% 
Loss D: 0.508 
Loss G: 1.1756 (1.0792) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-01 13:45:21,830 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.0841 (0.0951) Acc D Real: 99.165% 
Loss D Fake: 0.4234 (0.4675) Acc D Fake: 81.765% 
Loss D: 0.507 
Loss G: 1.1821 (1.0822) Acc G: 18.284% 
LR: 2.000e-04 

2023-03-01 13:45:21,837 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.0871 (0.0948) Acc D Real: 99.189% 
Loss D Fake: 0.4209 (0.4662) Acc D Fake: 81.810% 
Loss D: 0.508 
Loss G: 1.1885 (1.0853) Acc G: 18.238% 
LR: 2.000e-04 

2023-03-01 13:45:21,845 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.0841 (0.0945) Acc D Real: 99.212% 
Loss D Fake: 0.4185 (0.4649) Acc D Fake: 81.852% 
Loss D: 0.503 
Loss G: 1.1949 (1.0883) Acc G: 18.194% 
LR: 2.000e-04 

2023-03-01 13:45:21,852 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.1206 (0.0952) Acc D Real: 99.169% 
Loss D Fake: 0.4164 (0.4636) Acc D Fake: 81.892% 
Loss D: 0.537 
Loss G: 1.2002 (1.0914) Acc G: 18.153% 
LR: 2.000e-04 

2023-03-01 13:45:21,859 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.0855 (0.0950) Acc D Real: 99.191% 
Loss D Fake: 0.4150 (0.4623) Acc D Fake: 81.930% 
Loss D: 0.501 
Loss G: 1.2054 (1.0944) Acc G: 18.114% 
LR: 2.000e-04 

2023-03-01 13:45:21,867 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.0812 (0.0946) Acc D Real: 99.212% 
Loss D Fake: 0.4135 (0.4610) Acc D Fake: 81.966% 
Loss D: 0.495 
Loss G: 1.2109 (1.0973) Acc G: 18.077% 
LR: 2.000e-04 

2023-03-01 13:45:21,875 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.1200 (0.0953) Acc D Real: 99.167% 
Loss D Fake: 0.4119 (0.4598) Acc D Fake: 82.000% 
Loss D: 0.532 
Loss G: 1.2163 (1.1003) Acc G: 18.042% 
LR: 2.000e-04 

2023-03-01 13:45:21,882 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1170 (0.0958) Acc D Real: 99.129% 
Loss D Fake: 0.4106 (0.4586) Acc D Fake: 82.033% 
Loss D: 0.528 
Loss G: 1.2210 (1.1033) Acc G: 18.008% 
LR: 2.000e-04 

2023-03-01 13:45:21,889 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.0799 (0.0954) Acc D Real: 99.149% 
Loss D Fake: 0.4095 (0.4574) Acc D Fake: 82.063% 
Loss D: 0.489 
Loss G: 1.2262 (1.1062) Acc G: 17.976% 
LR: 2.000e-04 

2023-03-01 13:45:21,897 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.0816 (0.0951) Acc D Real: 99.169% 
Loss D Fake: 0.4080 (0.4563) Acc D Fake: 82.093% 
Loss D: 0.490 
Loss G: 1.2321 (1.1091) Acc G: 17.946% 
LR: 2.000e-04 

2023-03-01 13:45:21,904 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.0833 (0.0948) Acc D Real: 99.188% 
Loss D Fake: 0.4060 (0.4552) Acc D Fake: 82.121% 
Loss D: 0.489 
Loss G: 1.2385 (1.1121) Acc G: 17.917% 
LR: 2.000e-04 

2023-03-01 13:45:21,912 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.0847 (0.0946) Acc D Real: 99.206% 
Loss D Fake: 0.4039 (0.4540) Acc D Fake: 82.148% 
Loss D: 0.489 
Loss G: 1.2452 (1.1150) Acc G: 17.889% 
LR: 2.000e-04 

2023-03-01 13:45:21,919 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.0833 (0.0944) Acc D Real: 99.223% 
Loss D Fake: 0.4016 (0.4529) Acc D Fake: 82.174% 
Loss D: 0.485 
Loss G: 1.2522 (1.1180) Acc G: 17.862% 
LR: 2.000e-04 

2023-03-01 13:45:21,927 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.0827 (0.0941) Acc D Real: 99.240% 
Loss D Fake: 0.3991 (0.4517) Acc D Fake: 82.199% 
Loss D: 0.482 
Loss G: 1.2595 (1.1210) Acc G: 17.837% 
LR: 2.000e-04 

2023-03-01 13:45:21,934 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.0807 (0.0938) Acc D Real: 99.256% 
Loss D Fake: 0.3965 (0.4506) Acc D Fake: 82.222% 
Loss D: 0.477 
Loss G: 1.2672 (1.1241) Acc G: 17.812% 
LR: 2.000e-04 

2023-03-01 13:45:21,941 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.1545 (0.0951) Acc D Real: 99.163% 
Loss D Fake: 0.3938 (0.4494) Acc D Fake: 82.245% 
Loss D: 0.548 
Loss G: 1.2747 (1.1271) Acc G: 17.789% 
LR: 2.000e-04 

2023-03-01 13:45:21,949 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.1183 (0.0955) Acc D Real: 99.128% 
Loss D Fake: 0.3912 (0.4483) Acc D Fake: 82.267% 
Loss D: 0.509 
Loss G: 1.2821 (1.1302) Acc G: 17.767% 
LR: 2.000e-04 

2023-03-01 13:45:21,957 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.0790 (0.0952) Acc D Real: 99.145% 
Loss D Fake: 0.3885 (0.4471) Acc D Fake: 82.288% 
Loss D: 0.468 
Loss G: 1.2897 (1.1334) Acc G: 17.745% 
LR: 2.000e-04 

2023-03-01 13:45:21,964 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.0825 (0.0950) Acc D Real: 99.162% 
Loss D Fake: 0.3858 (0.4459) Acc D Fake: 82.308% 
Loss D: 0.468 
Loss G: 1.2975 (1.1365) Acc G: 17.724% 
LR: 2.000e-04 

2023-03-01 13:45:21,971 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.0835 (0.0947) Acc D Real: 99.177% 
Loss D Fake: 0.3831 (0.4447) Acc D Fake: 82.327% 
Loss D: 0.467 
Loss G: 1.3053 (1.1397) Acc G: 17.704% 
LR: 2.000e-04 

2023-03-01 13:45:21,979 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.1199 (0.0952) Acc D Real: 99.147% 
Loss D Fake: 0.3807 (0.4435) Acc D Fake: 82.346% 
Loss D: 0.501 
Loss G: 1.3122 (1.1429) Acc G: 17.685% 
LR: 2.000e-04 

2023-03-01 13:45:21,986 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.0840 (0.0950) Acc D Real: 99.163% 
Loss D Fake: 0.3789 (0.4424) Acc D Fake: 82.364% 
Loss D: 0.463 
Loss G: 1.3188 (1.1461) Acc G: 17.667% 
LR: 2.000e-04 

2023-03-01 13:45:21,994 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.0832 (0.0948) Acc D Real: 99.178% 
Loss D Fake: 0.3771 (0.4412) Acc D Fake: 82.381% 
Loss D: 0.460 
Loss G: 1.3255 (1.1493) Acc G: 17.649% 
LR: 2.000e-04 

2023-03-01 13:45:22,001 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.0800 (0.0945) Acc D Real: 99.192% 
Loss D Fake: 0.3753 (0.4400) Acc D Fake: 82.398% 
Loss D: 0.455 
Loss G: 1.3326 (1.1525) Acc G: 17.632% 
LR: 2.000e-04 

2023-03-01 13:45:22,009 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.0820 (0.0943) Acc D Real: 99.206% 
Loss D Fake: 0.3732 (0.4389) Acc D Fake: 82.414% 
Loss D: 0.455 
Loss G: 1.3399 (1.1557) Acc G: 17.615% 
LR: 2.000e-04 

2023-03-01 13:45:22,016 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.0809 (0.0941) Acc D Real: 99.220% 
Loss D Fake: 0.3711 (0.4377) Acc D Fake: 82.429% 
Loss D: 0.452 
Loss G: 1.3476 (1.1590) Acc G: 17.599% 
LR: 2.000e-04 

2023-03-01 13:45:22,023 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.1226 (0.0946) Acc D Real: 99.193% 
Loss D Fake: 0.3692 (0.4366) Acc D Fake: 82.444% 
Loss D: 0.492 
Loss G: 1.3540 (1.1622) Acc G: 17.583% 
LR: 2.000e-04 

2023-03-01 13:45:22,031 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.1182 (0.0950) Acc D Real: 99.164% 
Loss D Fake: 0.3680 (0.4355) Acc D Fake: 82.459% 
Loss D: 0.486 
Loss G: 1.3601 (1.1655) Acc G: 17.568% 
LR: 2.000e-04 

2023-03-01 13:45:22,038 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.1193 (0.0953) Acc D Real: 99.136% 
Loss D Fake: 0.3666 (0.4344) Acc D Fake: 82.473% 
Loss D: 0.486 
Loss G: 1.3665 (1.1687) Acc G: 17.554% 
LR: 2.000e-04 

2023-03-01 13:45:22,045 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.0824 (0.0951) Acc D Real: 99.149% 
Loss D Fake: 0.3651 (0.4333) Acc D Fake: 82.487% 
Loss D: 0.447 
Loss G: 1.3732 (1.1720) Acc G: 17.540% 
LR: 2.000e-04 

2023-03-01 13:45:22,053 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.0809 (0.0949) Acc D Real: 99.163% 
Loss D Fake: 0.3633 (0.4322) Acc D Fake: 82.500% 
Loss D: 0.444 
Loss G: 1.3805 (1.1752) Acc G: 17.526% 
LR: 2.000e-04 

2023-03-01 13:45:22,060 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.0792 (0.0947) Acc D Real: 99.175% 
Loss D Fake: 0.3613 (0.4311) Acc D Fake: 82.513% 
Loss D: 0.440 
Loss G: 1.3884 (1.1785) Acc G: 17.513% 
LR: 2.000e-04 

2023-03-01 13:45:22,068 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.0829 (0.0945) Acc D Real: 99.188% 
Loss D Fake: 0.3592 (0.4300) Acc D Fake: 82.525% 
Loss D: 0.442 
Loss G: 1.3957 (1.1818) Acc G: 17.500% 
LR: 2.000e-04 

2023-03-01 13:45:22,075 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.0786 (0.0943) Acc D Real: 99.200% 
Loss D Fake: 0.3576 (0.4289) Acc D Fake: 82.537% 
Loss D: 0.436 
Loss G: 1.4030 (1.1851) Acc G: 17.488% 
LR: 2.000e-04 

2023-03-01 13:45:22,082 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.0795 (0.0940) Acc D Real: 99.212% 
Loss D Fake: 0.3557 (0.4278) Acc D Fake: 82.549% 
Loss D: 0.435 
Loss G: 1.4111 (1.1884) Acc G: 17.475% 
LR: 2.000e-04 

2023-03-01 13:45:22,089 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.0804 (0.0938) Acc D Real: 99.223% 
Loss D Fake: 0.3535 (0.4268) Acc D Fake: 82.560% 
Loss D: 0.434 
Loss G: 1.4195 (1.1918) Acc G: 17.464% 
LR: 2.000e-04 

2023-03-01 13:45:22,097 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.0796 (0.0936) Acc D Real: 99.234% 
Loss D Fake: 0.3511 (0.4257) Acc D Fake: 82.571% 
Loss D: 0.431 
Loss G: 1.4284 (1.1952) Acc G: 17.452% 
LR: 2.000e-04 

2023-03-01 13:45:22,104 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.0822 (0.0935) Acc D Real: 99.245% 
Loss D Fake: 0.3486 (0.4246) Acc D Fake: 82.582% 
Loss D: 0.431 
Loss G: 1.4372 (1.1986) Acc G: 17.441% 
LR: 2.000e-04 

2023-03-01 13:45:22,111 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1175 (0.0938) Acc D Real: 99.223% 
Loss D Fake: 0.3466 (0.4235) Acc D Fake: 82.593% 
Loss D: 0.464 
Loss G: 1.4445 (1.2020) Acc G: 17.431% 
LR: 2.000e-04 

2023-03-01 13:45:22,119 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.1626 (0.0948) Acc D Real: 99.164% 
Loss D Fake: 0.3456 (0.4224) Acc D Fake: 82.603% 
Loss D: 0.508 
Loss G: 1.4501 (1.2054) Acc G: 17.420% 
LR: 2.000e-04 

2023-03-01 13:45:22,126 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1193 (0.0951) Acc D Real: 99.142% 
Loss D Fake: 0.3451 (0.4214) Acc D Fake: 82.613% 
Loss D: 0.464 
Loss G: 1.4555 (1.2088) Acc G: 17.410% 
LR: 2.000e-04 

2023-03-01 13:45:22,133 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.0804 (0.0949) Acc D Real: 99.153% 
Loss D Fake: 0.3444 (0.4204) Acc D Fake: 82.622% 
Loss D: 0.425 
Loss G: 1.4617 (1.2121) Acc G: 17.400% 
LR: 2.000e-04 

2023-03-01 13:45:22,141 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.0774 (0.0947) Acc D Real: 99.165% 
Loss D Fake: 0.3430 (0.4193) Acc D Fake: 82.632% 
Loss D: 0.420 
Loss G: 1.4693 (1.2155) Acc G: 17.390% 
LR: 2.000e-04 

2023-03-01 13:45:22,148 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.0779 (0.0944) Acc D Real: 99.175% 
Loss D Fake: 0.3409 (0.4183) Acc D Fake: 82.641% 
Loss D: 0.419 
Loss G: 1.4779 (1.2189) Acc G: 17.381% 
LR: 2.000e-04 

2023-03-01 13:45:22,156 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.0805 (0.0943) Acc D Real: 99.186% 
Loss D Fake: 0.3385 (0.4173) Acc D Fake: 82.650% 
Loss D: 0.419 
Loss G: 1.4871 (1.2224) Acc G: 17.372% 
LR: 2.000e-04 

2023-03-01 13:45:22,163 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.0755 (0.0940) Acc D Real: 99.196% 
Loss D Fake: 0.3359 (0.4163) Acc D Fake: 82.658% 
Loss D: 0.411 
Loss G: 1.4969 (1.2258) Acc G: 17.363% 
LR: 2.000e-04 

2023-03-01 13:45:22,171 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.0809 (0.0939) Acc D Real: 99.206% 
Loss D Fake: 0.3329 (0.4152) Acc D Fake: 82.667% 
Loss D: 0.414 
Loss G: 1.5070 (1.2293) Acc G: 17.354% 
LR: 2.000e-04 

2023-03-01 13:45:22,178 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.0775 (0.0937) Acc D Real: 99.216% 
Loss D Fake: 0.3300 (0.4142) Acc D Fake: 82.675% 
Loss D: 0.408 
Loss G: 1.5174 (1.2329) Acc G: 17.346% 
LR: 2.000e-04 

2023-03-01 13:45:22,187 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.1213 (0.0940) Acc D Real: 99.196% 
Loss D Fake: 0.3272 (0.4131) Acc D Fake: 82.683% 
Loss D: 0.448 
Loss G: 1.5268 (1.2365) Acc G: 17.337% 
LR: 2.000e-04 

2023-03-01 13:45:22,195 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.0778 (0.0938) Acc D Real: 99.206% 
Loss D Fake: 0.3249 (0.4121) Acc D Fake: 82.691% 
Loss D: 0.403 
Loss G: 1.5363 (1.2401) Acc G: 17.329% 
LR: 2.000e-04 

2023-03-01 13:45:22,204 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.1276 (0.0942) Acc D Real: 99.184% 
Loss D Fake: 0.3225 (0.4110) Acc D Fake: 82.698% 
Loss D: 0.450 
Loss G: 1.5450 (1.2437) Acc G: 17.321% 
LR: 2.000e-04 

2023-03-01 13:45:22,212 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.1279 (0.0946) Acc D Real: 99.162% 
Loss D Fake: 0.3211 (0.4099) Acc D Fake: 82.706% 
Loss D: 0.449 
Loss G: 1.5510 (1.2473) Acc G: 17.314% 
LR: 2.000e-04 

2023-03-01 13:45:22,220 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1253 (0.0950) Acc D Real: 99.141% 
Loss D Fake: 0.3209 (0.4089) Acc D Fake: 82.713% 
Loss D: 0.446 
Loss G: 1.5564 (1.2509) Acc G: 17.306% 
LR: 2.000e-04 

2023-03-01 13:45:22,229 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.0812 (0.0948) Acc D Real: 99.151% 
Loss D Fake: 0.3204 (0.4079) Acc D Fake: 82.720% 
Loss D: 0.402 
Loss G: 1.5625 (1.2545) Acc G: 17.299% 
LR: 2.000e-04 

2023-03-01 13:45:22,237 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.0810 (0.0946) Acc D Real: 99.161% 
Loss D Fake: 0.3196 (0.4069) Acc D Fake: 82.727% 
Loss D: 0.401 
Loss G: 1.5695 (1.2581) Acc G: 17.292% 
LR: 2.000e-04 

2023-03-01 13:45:22,245 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.0803 (0.0945) Acc D Real: 99.170% 
Loss D Fake: 0.3185 (0.4059) Acc D Fake: 82.734% 
Loss D: 0.399 
Loss G: 1.5769 (1.2617) Acc G: 17.285% 
LR: 2.000e-04 

2023-03-01 13:45:22,253 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.0770 (0.0943) Acc D Real: 99.179% 
Loss D Fake: 0.3172 (0.4049) Acc D Fake: 82.741% 
Loss D: 0.394 
Loss G: 1.5853 (1.2653) Acc G: 17.278% 
LR: 2.000e-04 

2023-03-01 13:45:22,260 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.0774 (0.0941) Acc D Real: 99.188% 
Loss D Fake: 0.3152 (0.4039) Acc D Fake: 82.747% 
Loss D: 0.393 
Loss G: 1.5947 (1.2689) Acc G: 17.271% 
LR: 2.000e-04 

2023-03-01 13:45:22,267 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.0796 (0.0939) Acc D Real: 99.197% 
Loss D Fake: 0.3132 (0.4029) Acc D Fake: 82.754% 
Loss D: 0.393 
Loss G: 1.6030 (1.2725) Acc G: 17.264% 
LR: 2.000e-04 

2023-03-01 13:45:22,275 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.0764 (0.0938) Acc D Real: 99.206% 
Loss D Fake: 0.3119 (0.4019) Acc D Fake: 82.760% 
Loss D: 0.388 
Loss G: 1.6118 (1.2762) Acc G: 17.258% 
LR: 2.000e-04 

2023-03-01 13:45:22,283 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.0870 (0.0937) Acc D Real: 99.214% 
Loss D Fake: 0.3118 (0.4010) Acc D Fake: 82.766% 
Loss D: 0.399 
Loss G: 1.6113 (1.2797) Acc G: 17.252% 
LR: 2.000e-04 

2023-03-01 13:45:22,290 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.0776 (0.0935) Acc D Real: 99.223% 
Loss D Fake: 0.3172 (0.4001) Acc D Fake: 82.772% 
Loss D: 0.395 
Loss G: 1.6084 (1.2832) Acc G: 17.253% 
LR: 2.000e-04 

2023-03-01 13:45:22,297 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.0756 (0.0933) Acc D Real: 99.231% 
Loss D Fake: 0.3214 (0.3993) Acc D Fake: 82.760% 
Loss D: 0.397 
Loss G: 1.6093 (1.2866) Acc G: 17.265% 
LR: 2.000e-04 

2023-03-01 13:45:22,304 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1227 (0.0936) Acc D Real: 99.212% 
Loss D Fake: 0.3227 (0.3985) Acc D Fake: 82.749% 
Loss D: 0.445 
Loss G: 1.6157 (1.2900) Acc G: 17.276% 
LR: 2.000e-04 

2023-03-01 13:45:22,312 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.0746 (0.0934) Acc D Real: 99.220% 
Loss D Fake: 0.3204 (0.3977) Acc D Fake: 82.738% 
Loss D: 0.395 
Loss G: 1.6286 (1.2934) Acc G: 17.286% 
LR: 2.000e-04 

2023-03-01 13:45:22,319 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.0768 (0.0933) Acc D Real: 99.228% 
Loss D Fake: 0.3150 (0.3969) Acc D Fake: 82.727% 
Loss D: 0.392 
Loss G: 1.6453 (1.2970) Acc G: 17.280% 
LR: 2.000e-04 

2023-03-01 13:45:22,327 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.0762 (0.0931) Acc D Real: 99.235% 
Loss D Fake: 0.3086 (0.3960) Acc D Fake: 82.733% 
Loss D: 0.385 
Loss G: 1.6628 (1.3007) Acc G: 17.274% 
LR: 2.000e-04 

2023-03-01 13:45:22,334 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.0770 (0.0929) Acc D Real: 99.243% 
Loss D Fake: 0.3024 (0.3951) Acc D Fake: 82.739% 
Loss D: 0.379 
Loss G: 1.6798 (1.3044) Acc G: 17.268% 
LR: 2.000e-04 

2023-03-01 13:45:22,342 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.0756 (0.0928) Acc D Real: 99.250% 
Loss D Fake: 0.2968 (0.3941) Acc D Fake: 82.745% 
Loss D: 0.372 
Loss G: 1.6958 (1.3082) Acc G: 17.248% 
LR: 2.000e-04 

2023-03-01 13:45:22,349 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.1303 (0.0931) Acc D Real: 99.232% 
Loss D Fake: 0.2919 (0.3931) Acc D Fake: 82.767% 
Loss D: 0.422 
Loss G: 1.7099 (1.3121) Acc G: 17.226% 
LR: 2.000e-04 

2023-03-01 13:45:22,356 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.0816 (0.0930) Acc D Real: 99.239% 
Loss D Fake: 0.2880 (0.3921) Acc D Fake: 82.788% 
Loss D: 0.370 
Loss G: 1.7225 (1.3161) Acc G: 17.205% 
LR: 2.000e-04 

2023-03-01 13:45:22,364 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.1406 (0.0935) Acc D Real: 99.221% 
Loss D Fake: 0.2851 (0.3911) Acc D Fake: 82.810% 
Loss D: 0.426 
Loss G: 1.7320 (1.3201) Acc G: 17.184% 
LR: 2.000e-04 

2023-03-01 13:45:22,371 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.1822 (0.0943) Acc D Real: 99.181% 
Loss D Fake: 0.2847 (0.3901) Acc D Fake: 82.830% 
Loss D: 0.467 
Loss G: 1.7334 (1.3240) Acc G: 17.163% 
LR: 2.000e-04 

2023-03-01 13:45:22,378 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.0823 (0.0942) Acc D Real: 99.189% 
Loss D Fake: 0.2880 (0.3891) Acc D Fake: 82.842% 
Loss D: 0.370 
Loss G: 1.7318 (1.3278) Acc G: 17.158% 
LR: 2.000e-04 

2023-03-01 13:45:22,385 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.0775 (0.0940) Acc D Real: 99.196% 
Loss D Fake: 0.2915 (0.3882) Acc D Fake: 82.846% 
Loss D: 0.369 
Loss G: 1.7313 (1.3315) Acc G: 17.154% 
LR: 2.000e-04 

2023-03-01 13:45:22,393 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.0800 (0.0939) Acc D Real: 99.203% 
Loss D Fake: 0.2944 (0.3873) Acc D Fake: 82.851% 
Loss D: 0.374 
Loss G: 1.7315 (1.3352) Acc G: 17.149% 
LR: 2.000e-04 

2023-03-01 13:45:22,400 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.0810 (0.0938) Acc D Real: 99.211% 
Loss D Fake: 0.2970 (0.3865) Acc D Fake: 82.855% 
Loss D: 0.378 
Loss G: 1.7338 (1.3388) Acc G: 17.145% 
LR: 2.000e-04 

2023-03-01 13:45:22,408 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.0748 (0.0936) Acc D Real: 99.218% 
Loss D Fake: 0.2978 (0.3857) Acc D Fake: 82.859% 
Loss D: 0.373 
Loss G: 1.7411 (1.3424) Acc G: 17.141% 
LR: 2.000e-04 

2023-03-01 13:45:22,415 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.0777 (0.0935) Acc D Real: 99.225% 
Loss D Fake: 0.2959 (0.3849) Acc D Fake: 82.864% 
Loss D: 0.374 
Loss G: 1.7527 (1.3461) Acc G: 17.136% 
LR: 2.000e-04 

2023-03-01 13:45:22,423 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.0782 (0.0934) Acc D Real: 99.232% 
Loss D Fake: 0.2922 (0.3841) Acc D Fake: 82.868% 
Loss D: 0.370 
Loss G: 1.7670 (1.3498) Acc G: 17.132% 
LR: 2.000e-04 

2023-03-01 13:45:22,430 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.1810 (0.0941) Acc D Real: 99.192% 
Loss D Fake: 0.2880 (0.3833) Acc D Fake: 82.872% 
Loss D: 0.469 
Loss G: 1.7802 (1.3536) Acc G: 17.128% 
LR: 2.000e-04 

2023-03-01 13:45:22,439 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.1319 (0.0945) Acc D Real: 99.176% 
Loss D Fake: 0.2843 (0.3824) Acc D Fake: 82.876% 
Loss D: 0.416 
Loss G: 1.7926 (1.3574) Acc G: 17.124% 
LR: 2.000e-04 

2023-03-01 13:45:22,446 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.1336 (0.0948) Acc D Real: 99.161% 
Loss D Fake: 0.2810 (0.3815) Acc D Fake: 82.880% 
Loss D: 0.415 
Loss G: 1.8026 (1.3612) Acc G: 17.120% 
LR: 2.000e-04 

2023-03-01 13:45:22,454 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.0846 (0.0947) Acc D Real: 99.168% 
Loss D Fake: 0.2796 (0.3807) Acc D Fake: 82.884% 
Loss D: 0.364 
Loss G: 1.8076 (1.3651) Acc G: 17.116% 
LR: 2.000e-04 

2023-03-01 13:45:22,461 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.0817 (0.0946) Acc D Real: 99.175% 
Loss D Fake: 0.2804 (0.3798) Acc D Fake: 82.888% 
Loss D: 0.362 
Loss G: 1.8120 (1.3688) Acc G: 17.112% 
LR: 2.000e-04 

2023-03-01 13:45:22,468 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.1293 (0.0949) Acc D Real: 99.162% 
Loss D Fake: 0.2824 (0.3790) Acc D Fake: 82.891% 
Loss D: 0.412 
Loss G: 1.8070 (1.3725) Acc G: 17.109% 
LR: 2.000e-04 

2023-03-01 13:45:22,476 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.1332 (0.0952) Acc D Real: 99.147% 
Loss D Fake: 0.2893 (0.3782) Acc D Fake: 82.881% 
Loss D: 0.423 
Loss G: 1.8002 (1.3761) Acc G: 17.119% 
LR: 2.000e-04 

2023-03-01 13:45:22,483 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.0766 (0.0950) Acc D Real: 99.154% 
Loss D Fake: 0.2942 (0.3775) Acc D Fake: 82.871% 
Loss D: 0.371 
Loss G: 1.8009 (1.3796) Acc G: 17.129% 
LR: 2.000e-04 

2023-03-01 13:45:22,491 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.0781 (0.0949) Acc D Real: 99.161% 
Loss D Fake: 0.2941 (0.3769) Acc D Fake: 82.861% 
Loss D: 0.372 
Loss G: 1.8119 (1.3831) Acc G: 17.139% 
LR: 2.000e-04 

2023-03-01 13:45:22,498 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.1331 (0.0952) Acc D Real: 99.146% 
Loss D Fake: 0.2893 (0.3761) Acc D Fake: 82.851% 
Loss D: 0.422 
Loss G: 1.8288 (1.3868) Acc G: 17.135% 
LR: 2.000e-04 

2023-03-01 13:45:22,505 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.0845 (0.0951) Acc D Real: 99.153% 
Loss D Fake: 0.2831 (0.3754) Acc D Fake: 82.855% 
Loss D: 0.368 
Loss G: 1.8459 (1.3905) Acc G: 17.131% 
LR: 2.000e-04 

2023-03-01 13:45:22,513 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.0857 (0.0951) Acc D Real: 99.160% 
Loss D Fake: 0.2779 (0.3746) Acc D Fake: 82.859% 
Loss D: 0.364 
Loss G: 1.8602 (1.3942) Acc G: 17.128% 
LR: 2.000e-04 

2023-03-01 13:45:22,521 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.0798 (0.0949) Acc D Real: 99.166% 
Loss D Fake: 0.2741 (0.3738) Acc D Fake: 82.863% 
Loss D: 0.354 
Loss G: 1.8734 (1.3980) Acc G: 17.124% 
LR: 2.000e-04 

2023-03-01 13:45:22,528 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.0845 (0.0949) Acc D Real: 99.173% 
Loss D Fake: 0.2709 (0.3730) Acc D Fake: 82.867% 
Loss D: 0.355 
Loss G: 1.8846 (1.4019) Acc G: 17.120% 
LR: 2.000e-04 

2023-03-01 13:45:22,536 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.0874 (0.0948) Acc D Real: 99.179% 
Loss D Fake: 0.2692 (0.3722) Acc D Fake: 82.870% 
Loss D: 0.357 
Loss G: 1.8929 (1.4057) Acc G: 17.117% 
LR: 2.000e-04 

2023-03-01 13:45:22,543 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.2053 (0.0957) Acc D Real: 99.144% 
Loss D Fake: 0.2712 (0.3714) Acc D Fake: 82.874% 
Loss D: 0.476 
Loss G: 1.8835 (1.4094) Acc G: 17.113% 
LR: 2.000e-04 

2023-03-01 13:45:22,551 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.2395 (0.0968) Acc D Real: 99.092% 
Loss D Fake: 0.2933 (0.3708) Acc D Fake: 82.865% 
Loss D: 0.533 
Loss G: 1.7086 (1.4117) Acc G: 17.187% 
LR: 2.000e-04 

2023-03-01 13:45:22,558 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1373 (0.0971) Acc D Real: 99.078% 
Loss D Fake: 4.4182 (0.4017) Acc D Fake: 82.232% 
Loss D: 4.556 
Loss G: 0.1117 (1.4018) Acc G: 17.819% 
LR: 2.000e-04 

2023-03-01 13:45:22,566 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1338 (0.0973) Acc D Real: 99.065% 
Loss D Fake: 4.3043 (0.4313) Acc D Fake: 81.609% 
Loss D: 4.438 
Loss G: 1.8273 (1.4050) Acc G: 17.835% 
LR: 2.000e-04 

2023-03-01 13:45:22,573 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.0901 (0.0973) Acc D Real: 99.072% 
Loss D Fake: 0.2762 (0.4301) Acc D Fake: 81.622% 
Loss D: 0.366 
Loss G: 1.9217 (1.4089) Acc G: 17.814% 
LR: 2.000e-04 

2023-03-01 13:45:22,581 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.6196 (0.1012) Acc D Real: 98.894% 
Loss D Fake: 0.2733 (0.4289) Acc D Fake: 81.635% 
Loss D: 0.893 
Loss G: 0.1295 (1.3993) Acc G: 18.427% 
LR: 2.000e-04 

2023-03-01 13:45:22,588 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.1454 (0.1015) Acc D Real: 98.882% 
Loss D Fake: 4.3188 (0.4578) Acc D Fake: 81.030% 
Loss D: 4.464 
Loss G: 0.0872 (1.3896) Acc G: 19.032% 
LR: 2.000e-04 

2023-03-01 13:45:22,596 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.0839 (0.1014) Acc D Real: 98.891% 
Loss D Fake: 4.3654 (0.4865) Acc D Fake: 80.434% 
Loss D: 4.449 
Loss G: 0.0806 (1.3800) Acc G: 19.627% 
LR: 2.000e-04 

2023-03-01 13:45:22,603 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.0799 (0.1012) Acc D Real: 98.899% 
Loss D Fake: 4.3119 (0.5144) Acc D Fake: 79.847% 
Loss D: 4.392 
Loss G: 0.0787 (1.3705) Acc G: 20.214% 
LR: 2.000e-04 

2023-03-01 13:45:22,611 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.0818 (0.1011) Acc D Real: 98.906% 
Loss D Fake: 4.2169 (0.5412) Acc D Fake: 79.269% 
Loss D: 4.299 
Loss G: 0.0788 (1.3611) Acc G: 20.792% 
LR: 2.000e-04 

2023-03-01 13:45:22,618 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.0791 (0.1009) Acc D Real: 98.914% 
Loss D Fake: 4.0977 (0.5668) Acc D Fake: 78.698% 
Loss D: 4.177 
Loss G: 0.0802 (1.3519) Acc G: 21.362% 
LR: 2.000e-04 

2023-03-01 13:45:22,626 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.0813 (0.1008) Acc D Real: 98.922% 
Loss D Fake: 3.9616 (0.5911) Acc D Fake: 78.136% 
Loss D: 4.043 
Loss G: 0.0825 (1.3429) Acc G: 21.923% 
LR: 2.000e-04 

2023-03-01 13:45:22,633 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.0847 (0.1007) Acc D Real: 98.929% 
Loss D Fake: 3.8131 (0.6139) Acc D Fake: 77.582% 
Loss D: 3.898 
Loss G: 0.0856 (1.3339) Acc G: 22.477% 
LR: 2.000e-04 

2023-03-01 13:45:22,641 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.0886 (0.1006) Acc D Real: 98.937% 
Loss D Fake: 3.6550 (0.6353) Acc D Fake: 77.036% 
Loss D: 3.744 
Loss G: 0.0896 (1.3252) Acc G: 23.023% 
LR: 2.000e-04 

2023-03-01 13:45:22,648 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.0929 (0.1005) Acc D Real: 98.944% 
Loss D Fake: 3.4902 (0.6553) Acc D Fake: 76.497% 
Loss D: 3.583 
Loss G: 0.0945 (1.3166) Acc G: 23.561% 
LR: 2.000e-04 

2023-03-01 13:45:22,656 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.0970 (0.1005) Acc D Real: 98.951% 
Loss D Fake: 3.3221 (0.6738) Acc D Fake: 75.966% 
Loss D: 3.419 
Loss G: 0.1002 (1.3081) Acc G: 24.092% 
LR: 2.000e-04 

2023-03-01 13:45:22,664 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.1031 (0.1005) Acc D Real: 98.959% 
Loss D Fake: 3.1545 (0.6909) Acc D Fake: 75.442% 
Loss D: 3.258 
Loss G: 0.1069 (1.2998) Acc G: 24.616% 
LR: 2.000e-04 

2023-03-01 13:45:22,673 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.1094 (0.1006) Acc D Real: 98.966% 
Loss D Fake: 2.9910 (0.7067) Acc D Fake: 74.925% 
Loss D: 3.100 
Loss G: 0.1146 (1.2917) Acc G: 25.132% 
LR: 2.000e-04 

2023-03-01 13:45:22,680 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1156 (0.1007) Acc D Real: 98.973% 
Loss D Fake: 2.8337 (0.7212) Acc D Fake: 74.415% 
Loss D: 2.949 
Loss G: 0.1232 (1.2838) Acc G: 25.641% 
LR: 2.000e-04 

2023-03-01 13:45:22,688 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.1274 (0.1009) Acc D Real: 98.980% 
Loss D Fake: 2.6832 (0.7344) Acc D Fake: 73.913% 
Loss D: 2.811 
Loss G: 0.1328 (1.2760) Acc G: 26.144% 
LR: 2.000e-04 

2023-03-01 13:45:22,696 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.1431 (0.1012) Acc D Real: 98.985% 
Loss D Fake: 2.5398 (0.7465) Acc D Fake: 73.417% 
Loss D: 2.683 
Loss G: 0.1434 (1.2684) Acc G: 26.639% 
LR: 2.000e-04 

2023-03-01 13:45:22,703 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.1483 (0.1015) Acc D Real: 98.990% 
Loss D Fake: 2.4028 (0.7576) Acc D Fake: 72.927% 
Loss D: 2.551 
Loss G: 0.1551 (1.2610) Acc G: 27.128% 
LR: 2.000e-04 

2023-03-01 13:45:22,710 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.1615 (0.1019) Acc D Real: 98.995% 
Loss D Fake: 2.2718 (0.7676) Acc D Fake: 72.444% 
Loss D: 2.433 
Loss G: 0.1680 (1.2537) Acc G: 27.611% 
LR: 2.000e-04 

2023-03-01 13:45:22,718 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.1752 (0.1024) Acc D Real: 98.995% 
Loss D Fake: 2.1469 (0.7767) Acc D Fake: 71.968% 
Loss D: 2.322 
Loss G: 0.1821 (1.2467) Acc G: 28.087% 
LR: 2.000e-04 

2023-03-01 13:45:22,725 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.1866 (0.1029) Acc D Real: 98.995% 
Loss D Fake: 2.0286 (0.7849) Acc D Fake: 71.497% 
Loss D: 2.215 
Loss G: 0.1972 (1.2398) Acc G: 28.557% 
LR: 2.000e-04 

2023-03-01 13:45:22,732 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.2038 (0.1036) Acc D Real: 98.986% 
Loss D Fake: 1.9178 (0.7922) Acc D Fake: 71.033% 
Loss D: 2.122 
Loss G: 0.2133 (1.2332) Acc G: 29.010% 
LR: 2.000e-04 

2023-03-01 13:45:22,739 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.2181 (0.1043) Acc D Real: 98.978% 
Loss D Fake: 1.8152 (0.7988) Acc D Fake: 70.585% 
Loss D: 2.033 
Loss G: 0.2301 (1.2267) Acc G: 29.447% 
LR: 2.000e-04 

2023-03-01 13:45:22,747 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.2389 (0.1052) Acc D Real: 98.959% 
Loss D Fake: 1.7211 (0.8047) Acc D Fake: 70.154% 
Loss D: 1.960 
Loss G: 0.2473 (1.2204) Acc G: 29.878% 
LR: 2.000e-04 

2023-03-01 13:45:22,754 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.2543 (0.1061) Acc D Real: 98.934% 
Loss D Fake: 1.6352 (0.8100) Acc D Fake: 69.729% 
Loss D: 1.889 
Loss G: 0.2648 (1.2143) Acc G: 30.293% 
LR: 2.000e-04 

2023-03-01 13:45:22,761 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2696 (0.1071) Acc D Real: 98.907% 
Loss D Fake: 1.5572 (0.8147) Acc D Fake: 69.319% 
Loss D: 1.827 
Loss G: 0.2824 (1.2084) Acc G: 30.702% 
LR: 2.000e-04 

2023-03-01 13:45:22,769 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2935 (0.1083) Acc D Real: 98.871% 
Loss D Fake: 1.4866 (0.8190) Acc D Fake: 68.914% 
Loss D: 1.780 
Loss G: 0.2998 (1.2027) Acc G: 31.107% 
LR: 2.000e-04 

2023-03-01 13:45:22,776 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3053 (0.1095) Acc D Real: 98.840% 
Loss D Fake: 1.4231 (0.8227) Acc D Fake: 68.515% 
Loss D: 1.728 
Loss G: 0.3168 (1.1972) Acc G: 31.506% 
LR: 2.000e-04 

2023-03-01 13:45:22,783 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3256 (0.1109) Acc D Real: 98.805% 
Loss D Fake: 1.3662 (0.8261) Acc D Fake: 68.120% 
Loss D: 1.692 
Loss G: 0.3332 (1.1918) Acc G: 31.900% 
LR: 2.000e-04 

2023-03-01 13:45:22,790 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3394 (0.1123) Acc D Real: 98.770% 
Loss D Fake: 1.3155 (0.8291) Acc D Fake: 67.731% 
Loss D: 1.655 
Loss G: 0.3490 (1.1866) Acc G: 32.290% 
LR: 2.000e-04 

2023-03-01 13:45:22,797 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3549 (0.1138) Acc D Real: 98.736% 
Loss D Fake: 1.2705 (0.8318) Acc D Fake: 67.346% 
Loss D: 1.625 
Loss G: 0.3638 (1.1816) Acc G: 32.664% 
LR: 2.000e-04 

2023-03-01 13:45:22,804 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3719 (0.1154) Acc D Real: 98.698% 
Loss D Fake: 1.2307 (0.8343) Acc D Fake: 66.976% 
Loss D: 1.603 
Loss G: 0.3776 (1.1767) Acc G: 33.034% 
LR: 2.000e-04 

2023-03-01 13:45:22,812 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3821 (0.1170) Acc D Real: 98.663% 
Loss D Fake: 1.1959 (0.8365) Acc D Fake: 66.610% 
Loss D: 1.578 
Loss G: 0.3903 (1.1719) Acc G: 33.400% 
LR: 2.000e-04 

2023-03-01 13:45:22,819 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3966 (0.1187) Acc D Real: 98.624% 
Loss D Fake: 1.1655 (0.8385) Acc D Fake: 66.249% 
Loss D: 1.562 
Loss G: 0.4019 (1.1672) Acc G: 33.761% 
LR: 2.000e-04 

2023-03-01 13:45:22,826 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4092 (0.1204) Acc D Real: 98.585% 
Loss D Fake: 1.1392 (0.8403) Acc D Fake: 65.893% 
Loss D: 1.548 
Loss G: 0.4122 (1.1627) Acc G: 34.107% 
LR: 2.000e-04 

2023-03-01 13:45:22,833 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4180 (0.1222) Acc D Real: 98.547% 
Loss D Fake: 1.1166 (0.8419) Acc D Fake: 65.550% 
Loss D: 1.535 
Loss G: 0.4213 (1.1583) Acc G: 34.450% 
LR: 2.000e-04 

2023-03-01 13:45:22,841 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4253 (0.1240) Acc D Real: 98.510% 
Loss D Fake: 1.0975 (0.8434) Acc D Fake: 65.211% 
Loss D: 1.523 
Loss G: 0.4292 (1.1540) Acc G: 34.789% 
LR: 2.000e-04 

2023-03-01 13:45:22,849 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4331 (0.1258) Acc D Real: 98.473% 
Loss D Fake: 1.0813 (0.8448) Acc D Fake: 64.877% 
Loss D: 1.514 
Loss G: 0.4360 (1.1498) Acc G: 35.123% 
LR: 2.000e-04 

2023-03-01 13:45:22,858 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4413 (0.1276) Acc D Real: 98.434% 
Loss D Fake: 1.0679 (0.8461) Acc D Fake: 64.546% 
Loss D: 1.509 
Loss G: 0.4416 (1.1456) Acc G: 35.454% 
LR: 2.000e-04 

2023-03-01 13:45:22,865 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4446 (0.1295) Acc D Real: 98.397% 
Loss D Fake: 1.0569 (0.8473) Acc D Fake: 64.219% 
Loss D: 1.502 
Loss G: 0.4463 (1.1416) Acc G: 35.781% 
LR: 2.000e-04 

2023-03-01 13:45:22,872 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4518 (0.1313) Acc D Real: 98.359% 
Loss D Fake: 1.0481 (0.8485) Acc D Fake: 63.896% 
Loss D: 1.500 
Loss G: 0.4500 (1.1376) Acc G: 36.104% 
LR: 2.000e-04 

2023-03-01 13:45:22,880 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4541 (0.1332) Acc D Real: 98.323% 
Loss D Fake: 1.0411 (0.8496) Acc D Fake: 63.577% 
Loss D: 1.495 
Loss G: 0.4528 (1.1336) Acc G: 36.423% 
LR: 2.000e-04 

2023-03-01 13:45:22,887 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4530 (0.1350) Acc D Real: 98.290% 
Loss D Fake: 1.0357 (0.8507) Acc D Fake: 63.261% 
Loss D: 1.489 
Loss G: 0.4550 (1.1298) Acc G: 36.739% 
LR: 2.000e-04 

2023-03-01 13:45:22,895 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4577 (0.1369) Acc D Real: 98.256% 
Loss D Fake: 1.0316 (0.8517) Acc D Fake: 62.949% 
Loss D: 1.489 
Loss G: 0.4565 (1.1259) Acc G: 37.051% 
LR: 2.000e-04 

2023-03-01 13:45:22,902 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4569 (0.1387) Acc D Real: 98.223% 
Loss D Fake: 1.0288 (0.8527) Acc D Fake: 62.641% 
Loss D: 1.486 
Loss G: 0.4575 (1.1222) Acc G: 37.359% 
LR: 2.000e-04 

2023-03-01 13:45:22,910 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4619 (0.1405) Acc D Real: 98.188% 
Loss D Fake: 1.0269 (0.8537) Acc D Fake: 62.336% 
Loss D: 1.489 
Loss G: 0.4580 (1.1184) Acc G: 37.664% 
LR: 2.000e-04 

2023-03-01 13:45:22,917 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4621 (0.1423) Acc D Real: 98.156% 
Loss D Fake: 1.0258 (0.8546) Acc D Fake: 62.034% 
Loss D: 1.488 
Loss G: 0.4581 (1.1147) Acc G: 37.966% 
LR: 2.000e-04 

2023-03-01 13:45:22,925 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4628 (0.1441) Acc D Real: 98.122% 
Loss D Fake: 1.0255 (0.8556) Acc D Fake: 61.736% 
Loss D: 1.488 
Loss G: 0.4579 (1.1111) Acc G: 38.274% 
LR: 2.000e-04 

2023-03-01 13:45:22,933 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4574 (0.1458) Acc D Real: 98.094% 
Loss D Fake: 1.0257 (0.8565) Acc D Fake: 61.431% 
Loss D: 1.483 
Loss G: 0.4574 (1.1075) Acc G: 38.578% 
LR: 2.000e-04 

2023-03-01 13:45:22,940 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4575 (0.1475) Acc D Real: 98.065% 
Loss D Fake: 1.0263 (0.8575) Acc D Fake: 61.130% 
Loss D: 1.484 
Loss G: 0.4568 (1.1039) Acc G: 38.879% 
LR: 2.000e-04 

2023-03-01 13:45:22,949 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4583 (0.1492) Acc D Real: 98.036% 
Loss D Fake: 1.0272 (0.8584) Acc D Fake: 60.824% 
Loss D: 1.485 
Loss G: 0.4560 (1.1004) Acc G: 39.185% 
LR: 2.000e-04 

2023-03-01 13:45:22,957 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4581 (0.1509) Acc D Real: 98.008% 
Loss D Fake: 1.0285 (0.8593) Acc D Fake: 60.521% 
Loss D: 1.487 
Loss G: 0.4551 (1.0969) Acc G: 39.489% 
LR: 2.000e-04 

2023-03-01 13:45:22,964 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4541 (0.1525) Acc D Real: 97.982% 
Loss D Fake: 1.0300 (0.8602) Acc D Fake: 60.220% 
Loss D: 1.484 
Loss G: 0.4540 (1.0934) Acc G: 39.789% 
LR: 2.000e-04 

2023-03-01 13:45:22,972 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4541 (0.1541) Acc D Real: 97.956% 
Loss D Fake: 1.0316 (0.8612) Acc D Fake: 59.924% 
Loss D: 1.486 
Loss G: 0.4529 (1.0899) Acc G: 40.086% 
LR: 2.000e-04 

2023-03-01 13:45:22,979 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4511 (0.1557) Acc D Real: 97.932% 
Loss D Fake: 1.0334 (0.8621) Acc D Fake: 59.630% 
Loss D: 1.484 
Loss G: 0.4518 (1.0865) Acc G: 40.379% 
LR: 2.000e-04 

2023-03-01 13:45:22,987 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4525 (0.1573) Acc D Real: 97.907% 
Loss D Fake: 1.0352 (0.8630) Acc D Fake: 59.339% 
Loss D: 1.488 
Loss G: 0.4507 (1.0831) Acc G: 40.670% 
LR: 2.000e-04 

2023-03-01 13:45:22,995 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4502 (0.1589) Acc D Real: 97.882% 
Loss D Fake: 1.0370 (0.8639) Acc D Fake: 59.052% 
Loss D: 1.487 
Loss G: 0.4496 (1.0798) Acc G: 40.957% 
LR: 2.000e-04 

2023-03-01 13:45:23,003 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4498 (0.1604) Acc D Real: 97.857% 
Loss D Fake: 1.0388 (0.8648) Acc D Fake: 58.767% 
Loss D: 1.489 
Loss G: 0.4485 (1.0765) Acc G: 41.242% 
LR: 2.000e-04 

2023-03-01 13:45:23,011 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4487 (0.1619) Acc D Real: 97.834% 
Loss D Fake: 1.0405 (0.8658) Acc D Fake: 58.486% 
Loss D: 1.489 
Loss G: 0.4475 (1.0732) Acc G: 41.523% 
LR: 2.000e-04 

2023-03-01 13:45:23,018 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4503 (0.1634) Acc D Real: 97.809% 
Loss D Fake: 1.0421 (0.8667) Acc D Fake: 58.207% 
Loss D: 1.492 
Loss G: 0.4466 (1.0699) Acc G: 41.802% 
LR: 2.000e-04 

2023-03-01 13:45:23,026 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4447 (0.1649) Acc D Real: 97.788% 
Loss D Fake: 1.0436 (0.8676) Acc D Fake: 57.932% 
Loss D: 1.488 
Loss G: 0.4456 (1.0667) Acc G: 42.077% 
LR: 2.000e-04 

2023-03-01 13:45:23,033 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4446 (0.1663) Acc D Real: 97.766% 
Loss D Fake: 1.0451 (0.8685) Acc D Fake: 57.659% 
Loss D: 1.490 
Loss G: 0.4448 (1.0635) Acc G: 42.350% 
LR: 2.000e-04 

2023-03-01 13:45:23,040 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4444 (0.1677) Acc D Real: 97.744% 
Loss D Fake: 1.0466 (0.8694) Acc D Fake: 57.389% 
Loss D: 1.491 
Loss G: 0.4439 (1.0603) Acc G: 42.620% 
LR: 2.000e-04 

2023-03-01 13:45:23,048 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4394 (0.1691) Acc D Real: 97.726% 
Loss D Fake: 1.0479 (0.8703) Acc D Fake: 57.121% 
Loss D: 1.487 
Loss G: 0.4432 (1.0571) Acc G: 42.887% 
LR: 2.000e-04 

2023-03-01 13:45:23,056 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4382 (0.1705) Acc D Real: 97.708% 
Loss D Fake: 1.0490 (0.8712) Acc D Fake: 56.857% 
Loss D: 1.487 
Loss G: 0.4425 (1.0540) Acc G: 43.152% 
LR: 2.000e-04 

2023-03-01 13:45:23,063 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4395 (0.1718) Acc D Real: 97.690% 
Loss D Fake: 1.0501 (0.8721) Acc D Fake: 56.595% 
Loss D: 1.490 
Loss G: 0.4419 (1.0509) Acc G: 43.414% 
LR: 2.000e-04 

2023-03-01 13:45:23,071 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4407 (0.1732) Acc D Real: 97.671% 
Loss D Fake: 1.0512 (0.8730) Acc D Fake: 56.336% 
Loss D: 1.492 
Loss G: 0.4412 (1.0479) Acc G: 43.673% 
LR: 2.000e-04 

2023-03-01 13:45:23,078 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4372 (0.1745) Acc D Real: 97.654% 
Loss D Fake: 1.0522 (0.8739) Acc D Fake: 56.079% 
Loss D: 1.489 
Loss G: 0.4407 (1.0448) Acc G: 43.930% 
LR: 2.000e-04 

2023-03-01 13:45:23,085 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4363 (0.1758) Acc D Real: 97.637% 
Loss D Fake: 1.0532 (0.8748) Acc D Fake: 55.825% 
Loss D: 1.489 
Loss G: 0.4401 (1.0418) Acc G: 44.184% 
LR: 2.000e-04 

2023-03-01 13:45:23,093 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4360 (0.1771) Acc D Real: 97.620% 
Loss D Fake: 1.0543 (0.8757) Acc D Fake: 55.573% 
Loss D: 1.490 
Loss G: 0.4394 (1.0388) Acc G: 44.435% 
LR: 2.000e-04 

2023-03-01 13:45:23,101 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4335 (0.1784) Acc D Real: 97.603% 
Loss D Fake: 1.0559 (0.8766) Acc D Fake: 55.324% 
Loss D: 1.489 
Loss G: 0.4388 (1.0359) Acc G: 44.684% 
LR: 2.000e-04 

2023-03-01 13:45:23,108 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4388 (0.1796) Acc D Real: 97.585% 
Loss D Fake: 1.0572 (0.8775) Acc D Fake: 55.077% 
Loss D: 1.496 
Loss G: 0.4384 (1.0330) Acc G: 44.931% 
LR: 2.000e-04 

2023-03-01 13:45:23,115 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4287 (0.1808) Acc D Real: 97.570% 
Loss D Fake: 1.0577 (0.8784) Acc D Fake: 54.833% 
Loss D: 1.486 
Loss G: 0.4382 (1.0301) Acc G: 45.175% 
LR: 2.000e-04 

2023-03-01 13:45:23,122 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4272 (0.1820) Acc D Real: 97.557% 
Loss D Fake: 1.0583 (0.8792) Acc D Fake: 54.591% 
Loss D: 1.486 
Loss G: 0.4380 (1.0272) Acc G: 45.417% 
LR: 2.000e-04 

2023-03-01 13:45:23,129 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4310 (0.1832) Acc D Real: 97.542% 
Loss D Fake: 1.0585 (0.8801) Acc D Fake: 54.344% 
Loss D: 1.490 
Loss G: 0.4383 (1.0243) Acc G: 45.665% 
LR: 2.000e-04 

2023-03-01 13:45:23,136 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4247 (0.1844) Acc D Real: 97.529% 
Loss D Fake: 1.0574 (0.8810) Acc D Fake: 54.098% 
Loss D: 1.482 
Loss G: 0.4383 (1.0215) Acc G: 45.910% 
LR: 2.000e-04 

2023-03-01 13:45:23,144 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4247 (0.1856) Acc D Real: 97.515% 
Loss D Fake: 1.0583 (0.8818) Acc D Fake: 53.855% 
Loss D: 1.483 
Loss G: 0.4378 (1.0187) Acc G: 46.153% 
LR: 2.000e-04 

2023-03-01 13:45:23,151 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4275 (0.1867) Acc D Real: 97.501% 
Loss D Fake: 1.0598 (0.8827) Acc D Fake: 53.615% 
Loss D: 1.487 
Loss G: 0.4377 (1.0160) Acc G: 46.393% 
LR: 2.000e-04 

2023-03-01 13:45:23,158 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4209 (0.1878) Acc D Real: 97.491% 
Loss D Fake: 1.0604 (0.8835) Acc D Fake: 53.377% 
Loss D: 1.481 
Loss G: 0.4374 (1.0132) Acc G: 46.632% 
LR: 2.000e-04 

2023-03-01 13:45:23,165 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4262 (0.1889) Acc D Real: 97.478% 
Loss D Fake: 1.0612 (0.8843) Acc D Fake: 53.140% 
Loss D: 1.487 
Loss G: 0.4380 (1.0105) Acc G: 46.868% 
LR: 2.000e-04 

2023-03-01 13:45:23,173 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4249 (0.1901) Acc D Real: 97.466% 
Loss D Fake: 1.0572 (0.8852) Acc D Fake: 52.907% 
Loss D: 1.482 
Loss G: 0.4393 (1.0078) Acc G: 47.101% 
LR: 2.000e-04 

2023-03-01 13:45:23,180 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4331 (0.1912) Acc D Real: 97.452% 
Loss D Fake: 1.0547 (0.8859) Acc D Fake: 52.675% 
Loss D: 1.488 
Loss G: 0.4398 (1.0052) Acc G: 47.333% 
LR: 2.000e-04 

2023-03-01 13:45:23,187 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4286 (0.1923) Acc D Real: 97.439% 
Loss D Fake: 1.0545 (0.8867) Acc D Fake: 52.445% 
Loss D: 1.483 
Loss G: 0.4405 (1.0025) Acc G: 47.562% 
LR: 2.000e-04 

2023-03-01 13:45:23,194 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4250 (0.1934) Acc D Real: 97.428% 
Loss D Fake: 1.0522 (0.8875) Acc D Fake: 52.218% 
Loss D: 1.477 
Loss G: 0.4414 (0.9999) Acc G: 47.790% 
LR: 2.000e-04 

2023-03-01 13:45:23,202 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4315 (0.1945) Acc D Real: 97.416% 
Loss D Fake: 1.0510 (0.8882) Acc D Fake: 51.993% 
Loss D: 1.483 
Loss G: 0.4417 (0.9974) Acc G: 48.015% 
LR: 2.000e-04 

2023-03-01 13:45:23,209 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4258 (0.1955) Acc D Real: 97.406% 
Loss D Fake: 1.0513 (0.8890) Acc D Fake: 51.770% 
Loss D: 1.477 
Loss G: 0.4428 (0.9948) Acc G: 48.238% 
LR: 2.000e-04 

2023-03-01 13:45:23,216 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4282 (0.1966) Acc D Real: 97.397% 
Loss D Fake: 1.0471 (0.8897) Acc D Fake: 51.548% 
Loss D: 1.475 
Loss G: 0.4441 (0.9923) Acc G: 48.459% 
LR: 2.000e-04 

2023-03-01 13:45:23,223 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4299 (0.1977) Acc D Real: 97.386% 
Loss D Fake: 1.0450 (0.8904) Acc D Fake: 51.329% 
Loss D: 1.475 
Loss G: 0.4447 (0.9898) Acc G: 48.679% 
LR: 2.000e-04 

2023-03-01 13:45:23,231 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4379 (0.1987) Acc D Real: 97.375% 
Loss D Fake: 1.0454 (0.8911) Acc D Fake: 51.112% 
Loss D: 1.483 
Loss G: 0.4454 (0.9874) Acc G: 48.896% 
LR: 2.000e-04 

2023-03-01 13:45:23,238 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4293 (0.1998) Acc D Real: 97.367% 
Loss D Fake: 1.0430 (0.8918) Acc D Fake: 50.897% 
Loss D: 1.472 
Loss G: 0.4461 (0.9849) Acc G: 49.111% 
LR: 2.000e-04 

2023-03-01 13:45:23,246 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4347 (0.2008) Acc D Real: 97.358% 
Loss D Fake: 1.0402 (0.8925) Acc D Fake: 50.684% 
Loss D: 1.475 
Loss G: 0.4477 (0.9825) Acc G: 49.324% 
LR: 2.000e-04 

2023-03-01 13:45:23,253 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4369 (0.2019) Acc D Real: 97.348% 
Loss D Fake: 1.0353 (0.8931) Acc D Fake: 50.472% 
Loss D: 1.472 
Loss G: 0.4484 (0.9801) Acc G: 49.535% 
LR: 2.000e-04 

2023-03-01 13:45:23,260 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4385 (0.2029) Acc D Real: 97.338% 
Loss D Fake: 1.0367 (0.8938) Acc D Fake: 50.263% 
Loss D: 1.475 
Loss G: 0.4476 (0.9778) Acc G: 49.745% 
LR: 2.000e-04 

2023-03-01 13:45:23,267 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4434 (0.2040) Acc D Real: 97.337% 
Loss D Fake: 1.0388 (0.8944) Acc D Fake: 50.211% 
Loss D: 1.482 
Loss G: 0.4486 (0.9754) Acc G: 49.797% 
LR: 2.000e-04 

2023-03-01 13:45:23,279 -                train: [    INFO] - 
Epoch: 7/20
2023-03-01 13:45:23,483 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4325 (0.4366) Acc D Real: 95.443% 
Loss D Fake: 1.0394 (1.0366) Acc D Fake: 3.333% 
Loss D: 1.472 
Loss G: 0.4473 (0.4479) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,490 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4386 (0.4373) Acc D Real: 95.469% 
Loss D Fake: 1.0379 (1.0370) Acc D Fake: 3.333% 
Loss D: 1.476 
Loss G: 0.4488 (0.4482) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,499 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4408 (0.4382) Acc D Real: 95.534% 
Loss D Fake: 1.0320 (1.0358) Acc D Fake: 3.333% 
Loss D: 1.473 
Loss G: 0.4490 (0.4484) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,517 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4446 (0.4395) Acc D Real: 95.562% 
Loss D Fake: 1.0352 (1.0357) Acc D Fake: 3.333% 
Loss D: 1.480 
Loss G: 0.4470 (0.4481) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,524 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4326 (0.4383) Acc D Real: 95.668% 
Loss D Fake: 1.0437 (1.0370) Acc D Fake: 3.333% 
Loss D: 1.476 
Loss G: 0.4477 (0.4480) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,531 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4364 (0.4380) Acc D Real: 95.759% 
Loss D Fake: 1.0347 (1.0367) Acc D Fake: 3.333% 
Loss D: 1.471 
Loss G: 0.4466 (0.4478) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,538 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4394 (0.4382) Acc D Real: 95.853% 
Loss D Fake: 1.0442 (1.0376) Acc D Fake: 3.333% 
Loss D: 1.484 
Loss G: 0.4473 (0.4478) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,545 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4424 (0.4387) Acc D Real: 95.903% 
Loss D Fake: 1.0322 (1.0370) Acc D Fake: 3.333% 
Loss D: 1.475 
Loss G: 0.4488 (0.4479) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,553 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4438 (0.4392) Acc D Real: 95.995% 
Loss D Fake: 1.0316 (1.0365) Acc D Fake: 3.333% 
Loss D: 1.475 
Loss G: 0.4479 (0.4479) Acc G: 96.833% 
LR: 2.000e-04 

2023-03-01 13:45:23,560 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4431 (0.4395) Acc D Real: 96.117% 
Loss D Fake: 1.0371 (1.0365) Acc D Fake: 3.182% 
Loss D: 1.480 
Loss G: 0.4442 (0.4475) Acc G: 96.970% 
LR: 2.000e-04 

2023-03-01 13:45:23,567 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4441 (0.4399) Acc D Real: 96.198% 
Loss D Fake: 1.0510 (1.0377) Acc D Fake: 3.056% 
Loss D: 1.495 
Loss G: 0.4470 (0.4475) Acc G: 97.083% 
LR: 2.000e-04 

2023-03-01 13:45:23,574 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4443 (0.4403) Acc D Real: 96.278% 
Loss D Fake: 1.0293 (1.0371) Acc D Fake: 2.821% 
Loss D: 1.474 
Loss G: 0.4500 (0.4477) Acc G: 97.308% 
LR: 2.000e-04 

2023-03-01 13:45:23,581 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4534 (0.4412) Acc D Real: 96.369% 
Loss D Fake: 1.0267 (1.0363) Acc D Fake: 2.619% 
Loss D: 1.480 
Loss G: 0.4502 (0.4479) Acc G: 97.500% 
LR: 2.000e-04 

2023-03-01 13:45:23,587 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4492 (0.4417) Acc D Real: 96.458% 
Loss D Fake: 1.0272 (1.0357) Acc D Fake: 2.444% 
Loss D: 1.476 
Loss G: 0.4494 (0.4480) Acc G: 97.667% 
LR: 2.000e-04 

2023-03-01 13:45:23,594 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4475 (0.4421) Acc D Real: 96.559% 
Loss D Fake: 1.0292 (1.0353) Acc D Fake: 2.292% 
Loss D: 1.477 
Loss G: 0.4480 (0.4480) Acc G: 97.812% 
LR: 2.000e-04 

2023-03-01 13:45:23,602 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4539 (0.4428) Acc D Real: 96.605% 
Loss D Fake: 1.0322 (1.0351) Acc D Fake: 2.157% 
Loss D: 1.486 
Loss G: 0.4463 (0.4479) Acc G: 97.941% 
LR: 2.000e-04 

2023-03-01 13:45:23,609 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4471 (0.4430) Acc D Real: 96.684% 
Loss D Fake: 1.0356 (1.0352) Acc D Fake: 2.037% 
Loss D: 1.483 
Loss G: 0.4442 (0.4477) Acc G: 98.056% 
LR: 2.000e-04 

2023-03-01 13:45:23,616 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4471 (0.4432) Acc D Real: 96.752% 
Loss D Fake: 1.0408 (1.0355) Acc D Fake: 1.930% 
Loss D: 1.488 
Loss G: 0.4408 (0.4473) Acc G: 98.158% 
LR: 2.000e-04 

2023-03-01 13:45:23,623 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4384 (0.4430) Acc D Real: 96.820% 
Loss D Fake: 1.0529 (1.0363) Acc D Fake: 1.833% 
Loss D: 1.491 
Loss G: 0.4328 (0.4466) Acc G: 98.250% 
LR: 2.000e-04 

2023-03-01 13:45:23,629 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4322 (0.4425) Acc D Real: 96.873% 
Loss D Fake: 1.1012 (1.0394) Acc D Fake: 1.746% 
Loss D: 1.533 
Loss G: 0.4404 (0.4463) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-01 13:45:23,636 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4445 (0.4426) Acc D Real: 96.932% 
Loss D Fake: 1.0393 (1.0394) Acc D Fake: 1.667% 
Loss D: 1.484 
Loss G: 0.4439 (0.4462) Acc G: 98.409% 
LR: 2.000e-04 

2023-03-01 13:45:23,643 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4474 (0.4428) Acc D Real: 96.986% 
Loss D Fake: 1.0370 (1.0393) Acc D Fake: 1.594% 
Loss D: 1.484 
Loss G: 0.4441 (0.4461) Acc G: 98.478% 
LR: 2.000e-04 

2023-03-01 13:45:23,650 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4479 (0.4430) Acc D Real: 97.033% 
Loss D Fake: 1.0376 (1.0392) Acc D Fake: 1.528% 
Loss D: 1.485 
Loss G: 0.4434 (0.4460) Acc G: 98.542% 
LR: 2.000e-04 

2023-03-01 13:45:23,657 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4460 (0.4431) Acc D Real: 97.096% 
Loss D Fake: 1.0392 (1.0392) Acc D Fake: 1.467% 
Loss D: 1.485 
Loss G: 0.4423 (0.4458) Acc G: 98.600% 
LR: 2.000e-04 

2023-03-01 13:45:23,665 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4461 (0.4432) Acc D Real: 97.151% 
Loss D Fake: 1.0412 (1.0393) Acc D Fake: 1.410% 
Loss D: 1.487 
Loss G: 0.4412 (0.4456) Acc G: 98.654% 
LR: 2.000e-04 

2023-03-01 13:45:23,672 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4445 (0.4433) Acc D Real: 97.209% 
Loss D Fake: 1.0434 (1.0395) Acc D Fake: 1.358% 
Loss D: 1.488 
Loss G: 0.4399 (0.4454) Acc G: 98.704% 
LR: 2.000e-04 

2023-03-01 13:45:23,678 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4435 (0.4433) Acc D Real: 97.262% 
Loss D Fake: 1.0457 (1.0397) Acc D Fake: 1.310% 
Loss D: 1.489 
Loss G: 0.4387 (0.4452) Acc G: 98.750% 
LR: 2.000e-04 

2023-03-01 13:45:23,686 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4424 (0.4433) Acc D Real: 97.302% 
Loss D Fake: 1.0479 (1.0400) Acc D Fake: 1.264% 
Loss D: 1.490 
Loss G: 0.4375 (0.4449) Acc G: 98.793% 
LR: 2.000e-04 

2023-03-01 13:45:23,693 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4411 (0.4432) Acc D Real: 97.342% 
Loss D Fake: 1.0501 (1.0403) Acc D Fake: 1.222% 
Loss D: 1.491 
Loss G: 0.4364 (0.4447) Acc G: 98.833% 
LR: 2.000e-04 

2023-03-01 13:45:23,702 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4401 (0.4431) Acc D Real: 97.387% 
Loss D Fake: 1.0521 (1.0407) Acc D Fake: 1.183% 
Loss D: 1.492 
Loss G: 0.4354 (0.4444) Acc G: 98.871% 
LR: 2.000e-04 

2023-03-01 13:45:23,711 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4385 (0.4429) Acc D Real: 97.438% 
Loss D Fake: 1.0539 (1.0411) Acc D Fake: 1.146% 
Loss D: 1.492 
Loss G: 0.4344 (0.4440) Acc G: 98.906% 
LR: 2.000e-04 

2023-03-01 13:45:23,720 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4380 (0.4428) Acc D Real: 97.479% 
Loss D Fake: 1.0557 (1.0415) Acc D Fake: 1.111% 
Loss D: 1.494 
Loss G: 0.4336 (0.4437) Acc G: 98.939% 
LR: 2.000e-04 

2023-03-01 13:45:23,727 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4376 (0.4426) Acc D Real: 97.517% 
Loss D Fake: 1.0572 (1.0420) Acc D Fake: 1.078% 
Loss D: 1.495 
Loss G: 0.4328 (0.4434) Acc G: 98.971% 
LR: 2.000e-04 

2023-03-01 13:45:23,734 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4361 (0.4424) Acc D Real: 97.561% 
Loss D Fake: 1.0586 (1.0425) Acc D Fake: 1.048% 
Loss D: 1.495 
Loss G: 0.4321 (0.4431) Acc G: 99.000% 
LR: 2.000e-04 

2023-03-01 13:45:23,741 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4354 (0.4423) Acc D Real: 97.609% 
Loss D Fake: 1.0598 (1.0430) Acc D Fake: 1.019% 
Loss D: 1.495 
Loss G: 0.4314 (0.4428) Acc G: 99.028% 
LR: 2.000e-04 

2023-03-01 13:45:23,749 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4353 (0.4421) Acc D Real: 97.642% 
Loss D Fake: 1.0609 (1.0434) Acc D Fake: 0.991% 
Loss D: 1.496 
Loss G: 0.4309 (0.4424) Acc G: 99.054% 
LR: 2.000e-04 

2023-03-01 13:45:23,756 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4341 (0.4419) Acc D Real: 97.693% 
Loss D Fake: 1.0619 (1.0439) Acc D Fake: 0.965% 
Loss D: 1.496 
Loss G: 0.4304 (0.4421) Acc G: 99.079% 
LR: 2.000e-04 

2023-03-01 13:45:23,764 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4341 (0.4417) Acc D Real: 97.742% 
Loss D Fake: 1.0627 (1.0444) Acc D Fake: 0.940% 
Loss D: 1.497 
Loss G: 0.4300 (0.4418) Acc G: 99.103% 
LR: 2.000e-04 

2023-03-01 13:45:23,771 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4329 (0.4414) Acc D Real: 97.790% 
Loss D Fake: 1.0634 (1.0449) Acc D Fake: 0.917% 
Loss D: 1.496 
Loss G: 0.4296 (0.4415) Acc G: 99.125% 
LR: 2.000e-04 

2023-03-01 13:45:23,778 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4321 (0.4412) Acc D Real: 97.842% 
Loss D Fake: 1.0640 (1.0454) Acc D Fake: 0.894% 
Loss D: 1.496 
Loss G: 0.4293 (0.4412) Acc G: 99.146% 
LR: 2.000e-04 

2023-03-01 13:45:23,785 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4320 (0.4410) Acc D Real: 97.888% 
Loss D Fake: 1.0644 (1.0458) Acc D Fake: 0.873% 
Loss D: 1.496 
Loss G: 0.4291 (0.4409) Acc G: 99.167% 
LR: 2.000e-04 

2023-03-01 13:45:23,793 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4315 (0.4408) Acc D Real: 97.934% 
Loss D Fake: 1.0649 (1.0462) Acc D Fake: 0.853% 
Loss D: 1.496 
Loss G: 0.4288 (0.4406) Acc G: 99.186% 
LR: 2.000e-04 

2023-03-01 13:45:23,800 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4315 (0.4406) Acc D Real: 97.979% 
Loss D Fake: 1.0652 (1.0467) Acc D Fake: 0.833% 
Loss D: 1.497 
Loss G: 0.4286 (0.4404) Acc G: 99.205% 
LR: 2.000e-04 

2023-03-01 13:45:23,807 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4311 (0.4403) Acc D Real: 98.024% 
Loss D Fake: 1.0655 (1.0471) Acc D Fake: 0.815% 
Loss D: 1.497 
Loss G: 0.4285 (0.4401) Acc G: 99.222% 
LR: 2.000e-04 

2023-03-01 13:45:23,814 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4309 (0.4401) Acc D Real: 98.067% 
Loss D Fake: 1.0657 (1.0475) Acc D Fake: 0.797% 
Loss D: 1.497 
Loss G: 0.4284 (0.4398) Acc G: 99.239% 
LR: 2.000e-04 

2023-03-01 13:45:23,822 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4306 (0.4399) Acc D Real: 98.108% 
Loss D Fake: 1.0659 (1.0479) Acc D Fake: 0.780% 
Loss D: 1.496 
Loss G: 0.4283 (0.4396) Acc G: 99.255% 
LR: 2.000e-04 

2023-03-01 13:45:23,829 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4316 (0.4398) Acc D Real: 98.143% 
Loss D Fake: 1.0660 (1.0483) Acc D Fake: 0.764% 
Loss D: 1.498 
Loss G: 0.4282 (0.4394) Acc G: 99.271% 
LR: 2.000e-04 

2023-03-01 13:45:23,836 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4299 (0.4396) Acc D Real: 98.181% 
Loss D Fake: 1.0661 (1.0486) Acc D Fake: 0.748% 
Loss D: 1.496 
Loss G: 0.4281 (0.4391) Acc G: 99.286% 
LR: 2.000e-04 

2023-03-01 13:45:23,844 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4306 (0.4394) Acc D Real: 98.217% 
Loss D Fake: 1.0661 (1.0490) Acc D Fake: 0.733% 
Loss D: 1.497 
Loss G: 0.4281 (0.4389) Acc G: 99.300% 
LR: 2.000e-04 

2023-03-01 13:45:23,851 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4305 (0.4392) Acc D Real: 98.252% 
Loss D Fake: 1.0662 (1.0493) Acc D Fake: 0.719% 
Loss D: 1.497 
Loss G: 0.4280 (0.4387) Acc G: 99.314% 
LR: 2.000e-04 

2023-03-01 13:45:23,858 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4305 (0.4390) Acc D Real: 98.284% 
Loss D Fake: 1.0662 (1.0496) Acc D Fake: 0.705% 
Loss D: 1.497 
Loss G: 0.4280 (0.4385) Acc G: 99.327% 
LR: 2.000e-04 

2023-03-01 13:45:23,865 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4299 (0.4389) Acc D Real: 98.317% 
Loss D Fake: 1.0662 (1.0500) Acc D Fake: 0.692% 
Loss D: 1.496 
Loss G: 0.4279 (0.4383) Acc G: 99.340% 
LR: 2.000e-04 

2023-03-01 13:45:23,873 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4302 (0.4387) Acc D Real: 98.348% 
Loss D Fake: 1.0662 (1.0503) Acc D Fake: 0.679% 
Loss D: 1.496 
Loss G: 0.4279 (0.4381) Acc G: 99.352% 
LR: 2.000e-04 

2023-03-01 13:45:23,880 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4304 (0.4386) Acc D Real: 98.378% 
Loss D Fake: 1.0662 (1.0505) Acc D Fake: 0.667% 
Loss D: 1.497 
Loss G: 0.4279 (0.4379) Acc G: 99.364% 
LR: 2.000e-04 

2023-03-01 13:45:23,887 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4300 (0.4384) Acc D Real: 98.407% 
Loss D Fake: 1.0662 (1.0508) Acc D Fake: 0.655% 
Loss D: 1.496 
Loss G: 0.4279 (0.4377) Acc G: 99.375% 
LR: 2.000e-04 

2023-03-01 13:45:23,894 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4295 (0.4383) Acc D Real: 98.435% 
Loss D Fake: 1.0661 (1.0511) Acc D Fake: 0.643% 
Loss D: 1.496 
Loss G: 0.4279 (0.4376) Acc G: 99.386% 
LR: 2.000e-04 

2023-03-01 13:45:23,902 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4302 (0.4381) Acc D Real: 98.462% 
Loss D Fake: 1.0661 (1.0514) Acc D Fake: 0.632% 
Loss D: 1.496 
Loss G: 0.4278 (0.4374) Acc G: 99.397% 
LR: 2.000e-04 

2023-03-01 13:45:23,909 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4303 (0.4380) Acc D Real: 98.488% 
Loss D Fake: 1.0661 (1.0516) Acc D Fake: 0.621% 
Loss D: 1.496 
Loss G: 0.4278 (0.4372) Acc G: 99.407% 
LR: 2.000e-04 

2023-03-01 13:45:23,916 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4292 (0.4378) Acc D Real: 98.513% 
Loss D Fake: 1.0661 (1.0518) Acc D Fake: 0.611% 
Loss D: 1.495 
Loss G: 0.4278 (0.4371) Acc G: 99.417% 
LR: 2.000e-04 

2023-03-01 13:45:23,923 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4296 (0.4377) Acc D Real: 98.537% 
Loss D Fake: 1.0661 (1.0521) Acc D Fake: 0.601% 
Loss D: 1.496 
Loss G: 0.4278 (0.4369) Acc G: 99.426% 
LR: 2.000e-04 

2023-03-01 13:45:23,931 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4294 (0.4376) Acc D Real: 98.561% 
Loss D Fake: 1.0661 (1.0523) Acc D Fake: 0.591% 
Loss D: 1.496 
Loss G: 0.4277 (0.4368) Acc G: 99.435% 
LR: 2.000e-04 

2023-03-01 13:45:23,938 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4290 (0.4374) Acc D Real: 98.584% 
Loss D Fake: 1.0661 (1.0525) Acc D Fake: 0.582% 
Loss D: 1.495 
Loss G: 0.4277 (0.4366) Acc G: 99.444% 
LR: 2.000e-04 

2023-03-01 13:45:23,945 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4294 (0.4373) Acc D Real: 98.606% 
Loss D Fake: 1.0661 (1.0527) Acc D Fake: 0.573% 
Loss D: 1.496 
Loss G: 0.4277 (0.4365) Acc G: 99.453% 
LR: 2.000e-04 

2023-03-01 13:45:23,953 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4289 (0.4372) Acc D Real: 98.627% 
Loss D Fake: 1.0661 (1.0529) Acc D Fake: 0.564% 
Loss D: 1.495 
Loss G: 0.4276 (0.4364) Acc G: 99.462% 
LR: 2.000e-04 

2023-03-01 13:45:23,960 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4293 (0.4371) Acc D Real: 98.648% 
Loss D Fake: 1.0662 (1.0531) Acc D Fake: 0.556% 
Loss D: 1.496 
Loss G: 0.4276 (0.4362) Acc G: 99.470% 
LR: 2.000e-04 

2023-03-01 13:45:23,967 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4291 (0.4369) Acc D Real: 98.668% 
Loss D Fake: 1.0662 (1.0533) Acc D Fake: 0.547% 
Loss D: 1.495 
Loss G: 0.4275 (0.4361) Acc G: 99.478% 
LR: 2.000e-04 

2023-03-01 13:45:23,975 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4288 (0.4368) Acc D Real: 98.686% 
Loss D Fake: 1.0663 (1.0535) Acc D Fake: 0.539% 
Loss D: 1.495 
Loss G: 0.4275 (0.4360) Acc G: 99.485% 
LR: 2.000e-04 

2023-03-01 13:45:23,982 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4293 (0.4367) Acc D Real: 98.705% 
Loss D Fake: 1.0663 (1.0537) Acc D Fake: 0.531% 
Loss D: 1.496 
Loss G: 0.4274 (0.4358) Acc G: 99.493% 
LR: 2.000e-04 

2023-03-01 13:45:23,989 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4289 (0.4366) Acc D Real: 98.724% 
Loss D Fake: 1.0664 (1.0539) Acc D Fake: 0.524% 
Loss D: 1.495 
Loss G: 0.4274 (0.4357) Acc G: 99.500% 
LR: 2.000e-04 

2023-03-01 13:45:23,996 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4286 (0.4365) Acc D Real: 98.742% 
Loss D Fake: 1.0665 (1.0541) Acc D Fake: 0.516% 
Loss D: 1.495 
Loss G: 0.4273 (0.4356) Acc G: 99.507% 
LR: 2.000e-04 

2023-03-01 13:45:24,004 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4283 (0.4364) Acc D Real: 98.759% 
Loss D Fake: 1.0666 (1.0542) Acc D Fake: 0.509% 
Loss D: 1.495 
Loss G: 0.4272 (0.4355) Acc G: 99.514% 
LR: 2.000e-04 

2023-03-01 13:45:24,011 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4283 (0.4363) Acc D Real: 98.776% 
Loss D Fake: 1.0667 (1.0544) Acc D Fake: 0.502% 
Loss D: 1.495 
Loss G: 0.4272 (0.4354) Acc G: 99.521% 
LR: 2.000e-04 

2023-03-01 13:45:24,018 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4280 (0.4362) Acc D Real: 98.793% 
Loss D Fake: 1.0668 (1.0546) Acc D Fake: 0.495% 
Loss D: 1.495 
Loss G: 0.4271 (0.4353) Acc G: 99.527% 
LR: 2.000e-04 

2023-03-01 13:45:24,025 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4281 (0.4360) Acc D Real: 98.809% 
Loss D Fake: 1.0669 (1.0547) Acc D Fake: 0.489% 
Loss D: 1.495 
Loss G: 0.4270 (0.4351) Acc G: 99.533% 
LR: 2.000e-04 

2023-03-01 13:45:24,033 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4277 (0.4359) Acc D Real: 98.825% 
Loss D Fake: 1.0670 (1.0549) Acc D Fake: 0.482% 
Loss D: 1.495 
Loss G: 0.4269 (0.4350) Acc G: 99.539% 
LR: 2.000e-04 

2023-03-01 13:45:24,040 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4276 (0.4358) Acc D Real: 98.840% 
Loss D Fake: 1.0671 (1.0551) Acc D Fake: 0.476% 
Loss D: 1.495 
Loss G: 0.4269 (0.4349) Acc G: 99.545% 
LR: 2.000e-04 

2023-03-01 13:45:24,047 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4276 (0.4357) Acc D Real: 98.855% 
Loss D Fake: 1.0672 (1.0552) Acc D Fake: 0.470% 
Loss D: 1.495 
Loss G: 0.4268 (0.4348) Acc G: 99.551% 
LR: 2.000e-04 

2023-03-01 13:45:24,054 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4275 (0.4356) Acc D Real: 98.869% 
Loss D Fake: 1.0673 (1.0554) Acc D Fake: 0.464% 
Loss D: 1.495 
Loss G: 0.4267 (0.4347) Acc G: 99.557% 
LR: 2.000e-04 

2023-03-01 13:45:24,062 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4273 (0.4355) Acc D Real: 98.883% 
Loss D Fake: 1.0674 (1.0555) Acc D Fake: 0.458% 
Loss D: 1.495 
Loss G: 0.4266 (0.4346) Acc G: 99.562% 
LR: 2.000e-04 

2023-03-01 13:45:24,069 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4272 (0.4354) Acc D Real: 98.897% 
Loss D Fake: 1.0676 (1.0557) Acc D Fake: 0.453% 
Loss D: 1.495 
Loss G: 0.4265 (0.4345) Acc G: 99.568% 
LR: 2.000e-04 

2023-03-01 13:45:24,076 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4270 (0.4353) Acc D Real: 98.911% 
Loss D Fake: 1.0677 (1.0558) Acc D Fake: 0.447% 
Loss D: 1.495 
Loss G: 0.4264 (0.4344) Acc G: 99.573% 
LR: 2.000e-04 

2023-03-01 13:45:24,083 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4268 (0.4352) Acc D Real: 98.924% 
Loss D Fake: 1.0679 (1.0560) Acc D Fake: 0.442% 
Loss D: 1.495 
Loss G: 0.4263 (0.4343) Acc G: 99.578% 
LR: 2.000e-04 

2023-03-01 13:45:24,091 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4266 (0.4351) Acc D Real: 98.937% 
Loss D Fake: 1.0681 (1.0561) Acc D Fake: 0.437% 
Loss D: 1.495 
Loss G: 0.4262 (0.4342) Acc G: 99.583% 
LR: 2.000e-04 

2023-03-01 13:45:24,098 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4266 (0.4350) Acc D Real: 98.949% 
Loss D Fake: 1.0683 (1.0563) Acc D Fake: 0.431% 
Loss D: 1.495 
Loss G: 0.4261 (0.4341) Acc G: 99.588% 
LR: 2.000e-04 

2023-03-01 13:45:24,105 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4263 (0.4349) Acc D Real: 98.961% 
Loss D Fake: 1.0685 (1.0564) Acc D Fake: 0.426% 
Loss D: 1.495 
Loss G: 0.4260 (0.4340) Acc G: 99.593% 
LR: 2.000e-04 

2023-03-01 13:45:24,112 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4260 (0.4348) Acc D Real: 98.973% 
Loss D Fake: 1.0687 (1.0565) Acc D Fake: 0.421% 
Loss D: 1.495 
Loss G: 0.4258 (0.4339) Acc G: 99.598% 
LR: 2.000e-04 

2023-03-01 13:45:24,119 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4261 (0.4347) Acc D Real: 98.985% 
Loss D Fake: 1.0689 (1.0567) Acc D Fake: 0.417% 
Loss D: 1.495 
Loss G: 0.4257 (0.4339) Acc G: 99.602% 
LR: 2.000e-04 

2023-03-01 13:45:24,127 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4257 (0.4346) Acc D Real: 98.996% 
Loss D Fake: 1.0692 (1.0568) Acc D Fake: 0.412% 
Loss D: 1.495 
Loss G: 0.4256 (0.4338) Acc G: 99.607% 
LR: 2.000e-04 

2023-03-01 13:45:24,136 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4251 (0.4345) Acc D Real: 99.008% 
Loss D Fake: 1.0694 (1.0570) Acc D Fake: 0.407% 
Loss D: 1.495 
Loss G: 0.4254 (0.4337) Acc G: 99.611% 
LR: 2.000e-04 

2023-03-01 13:45:24,144 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4252 (0.4344) Acc D Real: 99.018% 
Loss D Fake: 1.0697 (1.0571) Acc D Fake: 0.403% 
Loss D: 1.495 
Loss G: 0.4252 (0.4336) Acc G: 99.615% 
LR: 2.000e-04 

2023-03-01 13:45:24,153 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4248 (0.4343) Acc D Real: 99.029% 
Loss D Fake: 1.0700 (1.0572) Acc D Fake: 0.399% 
Loss D: 1.495 
Loss G: 0.4251 (0.4335) Acc G: 99.620% 
LR: 2.000e-04 

2023-03-01 13:45:24,161 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4240 (0.4342) Acc D Real: 99.040% 
Loss D Fake: 1.0704 (1.0574) Acc D Fake: 0.394% 
Loss D: 1.494 
Loss G: 0.4249 (0.4334) Acc G: 99.624% 
LR: 2.000e-04 

2023-03-01 13:45:24,169 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4244 (0.4341) Acc D Real: 99.050% 
Loss D Fake: 1.0708 (1.0575) Acc D Fake: 0.390% 
Loss D: 1.495 
Loss G: 0.4246 (0.4333) Acc G: 99.628% 
LR: 2.000e-04 

2023-03-01 13:45:24,177 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4239 (0.4340) Acc D Real: 99.060% 
Loss D Fake: 1.0712 (1.0577) Acc D Fake: 0.386% 
Loss D: 1.495 
Loss G: 0.4244 (0.4332) Acc G: 99.632% 
LR: 2.000e-04 

2023-03-01 13:45:24,185 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4231 (0.4339) Acc D Real: 99.070% 
Loss D Fake: 1.0717 (1.0578) Acc D Fake: 0.382% 
Loss D: 1.495 
Loss G: 0.4241 (0.4331) Acc G: 99.635% 
LR: 2.000e-04 

2023-03-01 13:45:24,193 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4232 (0.4337) Acc D Real: 99.079% 
Loss D Fake: 1.0722 (1.0580) Acc D Fake: 0.378% 
Loss D: 1.495 
Loss G: 0.4239 (0.4330) Acc G: 99.639% 
LR: 2.000e-04 

2023-03-01 13:45:24,201 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4224 (0.4336) Acc D Real: 99.089% 
Loss D Fake: 1.0728 (1.0581) Acc D Fake: 0.374% 
Loss D: 1.495 
Loss G: 0.4236 (0.4329) Acc G: 99.643% 
LR: 2.000e-04 

2023-03-01 13:45:24,210 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4221 (0.4335) Acc D Real: 99.098% 
Loss D Fake: 1.0734 (1.0583) Acc D Fake: 0.370% 
Loss D: 1.495 
Loss G: 0.4233 (0.4328) Acc G: 99.646% 
LR: 2.000e-04 

2023-03-01 13:45:24,218 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4225 (0.4334) Acc D Real: 99.107% 
Loss D Fake: 1.0740 (1.0584) Acc D Fake: 0.367% 
Loss D: 1.496 
Loss G: 0.4230 (0.4327) Acc G: 99.650% 
LR: 2.000e-04 

2023-03-01 13:45:24,226 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4209 (0.4333) Acc D Real: 99.116% 
Loss D Fake: 1.0745 (1.0586) Acc D Fake: 0.363% 
Loss D: 1.495 
Loss G: 0.4228 (0.4326) Acc G: 99.653% 
LR: 2.000e-04 

2023-03-01 13:45:24,233 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4212 (0.4332) Acc D Real: 99.124% 
Loss D Fake: 1.0750 (1.0587) Acc D Fake: 0.359% 
Loss D: 1.496 
Loss G: 0.4225 (0.4325) Acc G: 99.657% 
LR: 2.000e-04 

2023-03-01 13:45:24,241 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4206 (0.4330) Acc D Real: 99.133% 
Loss D Fake: 1.0754 (1.0589) Acc D Fake: 0.356% 
Loss D: 1.496 
Loss G: 0.4223 (0.4324) Acc G: 99.660% 
LR: 2.000e-04 

2023-03-01 13:45:24,248 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4215 (0.4329) Acc D Real: 99.141% 
Loss D Fake: 1.0757 (1.0591) Acc D Fake: 0.353% 
Loss D: 1.497 
Loss G: 0.4222 (0.4323) Acc G: 99.663% 
LR: 2.000e-04 

2023-03-01 13:45:24,255 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4193 (0.4328) Acc D Real: 99.149% 
Loss D Fake: 1.0759 (1.0592) Acc D Fake: 0.349% 
Loss D: 1.495 
Loss G: 0.4221 (0.4322) Acc G: 99.667% 
LR: 2.000e-04 

2023-03-01 13:45:24,262 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4199 (0.4327) Acc D Real: 99.157% 
Loss D Fake: 1.0760 (1.0594) Acc D Fake: 0.346% 
Loss D: 1.496 
Loss G: 0.4221 (0.4321) Acc G: 99.670% 
LR: 2.000e-04 

2023-03-01 13:45:24,270 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4195 (0.4326) Acc D Real: 99.165% 
Loss D Fake: 1.0760 (1.0595) Acc D Fake: 0.343% 
Loss D: 1.496 
Loss G: 0.4222 (0.4320) Acc G: 99.673% 
LR: 2.000e-04 

2023-03-01 13:45:24,277 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4198 (0.4324) Acc D Real: 99.173% 
Loss D Fake: 1.0758 (1.0597) Acc D Fake: 0.340% 
Loss D: 1.496 
Loss G: 0.4223 (0.4320) Acc G: 99.676% 
LR: 2.000e-04 

2023-03-01 13:45:24,284 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4189 (0.4323) Acc D Real: 99.181% 
Loss D Fake: 1.0755 (1.0598) Acc D Fake: 0.336% 
Loss D: 1.494 
Loss G: 0.4225 (0.4319) Acc G: 99.679% 
LR: 2.000e-04 

2023-03-01 13:45:24,291 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4202 (0.4322) Acc D Real: 99.188% 
Loss D Fake: 1.0751 (1.0600) Acc D Fake: 0.333% 
Loss D: 1.495 
Loss G: 0.4227 (0.4318) Acc G: 99.682% 
LR: 2.000e-04 

2023-03-01 13:45:24,298 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4204 (0.4321) Acc D Real: 99.195% 
Loss D Fake: 1.0745 (1.0601) Acc D Fake: 0.330% 
Loss D: 1.495 
Loss G: 0.4231 (0.4317) Acc G: 99.685% 
LR: 2.000e-04 

2023-03-01 13:45:24,306 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4203 (0.4320) Acc D Real: 99.202% 
Loss D Fake: 1.0737 (1.0602) Acc D Fake: 0.327% 
Loss D: 1.494 
Loss G: 0.4235 (0.4316) Acc G: 99.688% 
LR: 2.000e-04 

2023-03-01 13:45:24,315 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4192 (0.4319) Acc D Real: 99.210% 
Loss D Fake: 1.0728 (1.0603) Acc D Fake: 0.324% 
Loss D: 1.492 
Loss G: 0.4239 (0.4316) Acc G: 99.690% 
LR: 2.000e-04 

2023-03-01 13:45:24,323 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4208 (0.4318) Acc D Real: 99.216% 
Loss D Fake: 1.0720 (1.0604) Acc D Fake: 0.322% 
Loss D: 1.493 
Loss G: 0.4243 (0.4315) Acc G: 99.693% 
LR: 2.000e-04 

2023-03-01 13:45:24,332 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4235 (0.4317) Acc D Real: 99.223% 
Loss D Fake: 1.0711 (1.0605) Acc D Fake: 0.319% 
Loss D: 1.495 
Loss G: 0.4248 (0.4314) Acc G: 99.696% 
LR: 2.000e-04 

2023-03-01 13:45:24,340 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4219 (0.4316) Acc D Real: 99.230% 
Loss D Fake: 1.0700 (1.0606) Acc D Fake: 0.316% 
Loss D: 1.492 
Loss G: 0.4254 (0.4314) Acc G: 99.698% 
LR: 2.000e-04 

2023-03-01 13:45:24,348 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4226 (0.4315) Acc D Real: 99.237% 
Loss D Fake: 1.0687 (1.0607) Acc D Fake: 0.313% 
Loss D: 1.491 
Loss G: 0.4261 (0.4313) Acc G: 99.701% 
LR: 2.000e-04 

2023-03-01 13:45:24,356 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4233 (0.4315) Acc D Real: 99.243% 
Loss D Fake: 1.0674 (1.0607) Acc D Fake: 0.311% 
Loss D: 1.491 
Loss G: 0.4267 (0.4313) Acc G: 99.703% 
LR: 2.000e-04 

2023-03-01 13:45:24,364 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4240 (0.4314) Acc D Real: 99.249% 
Loss D Fake: 1.0662 (1.0608) Acc D Fake: 0.308% 
Loss D: 1.490 
Loss G: 0.4272 (0.4313) Acc G: 99.706% 
LR: 2.000e-04 

2023-03-01 13:45:24,371 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4230 (0.4313) Acc D Real: 99.256% 
Loss D Fake: 1.0652 (1.0608) Acc D Fake: 0.306% 
Loss D: 1.488 
Loss G: 0.4276 (0.4312) Acc G: 99.708% 
LR: 2.000e-04 

2023-03-01 13:45:24,378 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4222 (0.4313) Acc D Real: 99.262% 
Loss D Fake: 1.0647 (1.0609) Acc D Fake: 0.303% 
Loss D: 1.487 
Loss G: 0.4278 (0.4312) Acc G: 99.711% 
LR: 2.000e-04 

2023-03-01 13:45:24,386 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4257 (0.4312) Acc D Real: 99.268% 
Loss D Fake: 1.0646 (1.0609) Acc D Fake: 0.301% 
Loss D: 1.490 
Loss G: 0.4278 (0.4312) Acc G: 99.713% 
LR: 2.000e-04 

2023-03-01 13:45:24,393 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4262 (0.4312) Acc D Real: 99.274% 
Loss D Fake: 1.0643 (1.0609) Acc D Fake: 0.298% 
Loss D: 1.491 
Loss G: 0.4281 (0.4312) Acc G: 99.715% 
LR: 2.000e-04 

2023-03-01 13:45:24,400 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4234 (0.4311) Acc D Real: 99.280% 
Loss D Fake: 1.0637 (1.0609) Acc D Fake: 0.296% 
Loss D: 1.487 
Loss G: 0.4283 (0.4311) Acc G: 99.718% 
LR: 2.000e-04 

2023-03-01 13:45:24,407 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4228 (0.4310) Acc D Real: 99.285% 
Loss D Fake: 1.0634 (1.0610) Acc D Fake: 0.293% 
Loss D: 1.486 
Loss G: 0.4283 (0.4311) Acc G: 99.720% 
LR: 2.000e-04 

2023-03-01 13:45:24,414 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4233 (0.4310) Acc D Real: 99.291% 
Loss D Fake: 1.0637 (1.0610) Acc D Fake: 0.291% 
Loss D: 1.487 
Loss G: 0.4281 (0.4311) Acc G: 99.722% 
LR: 2.000e-04 

2023-03-01 13:45:24,422 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4255 (0.4309) Acc D Real: 99.297% 
Loss D Fake: 1.0641 (1.0610) Acc D Fake: 0.289% 
Loss D: 1.490 
Loss G: 0.4281 (0.4311) Acc G: 99.724% 
LR: 2.000e-04 

2023-03-01 13:45:24,429 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4251 (0.4309) Acc D Real: 99.302% 
Loss D Fake: 1.0640 (1.0610) Acc D Fake: 0.286% 
Loss D: 1.489 
Loss G: 0.4282 (0.4310) Acc G: 99.727% 
LR: 2.000e-04 

2023-03-01 13:45:24,436 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4246 (0.4308) Acc D Real: 99.308% 
Loss D Fake: 1.0636 (1.0610) Acc D Fake: 0.284% 
Loss D: 1.488 
Loss G: 0.4284 (0.4310) Acc G: 99.729% 
LR: 2.000e-04 

2023-03-01 13:45:24,443 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4246 (0.4308) Acc D Real: 99.313% 
Loss D Fake: 1.0631 (1.0611) Acc D Fake: 0.282% 
Loss D: 1.488 
Loss G: 0.4287 (0.4310) Acc G: 99.731% 
LR: 2.000e-04 

2023-03-01 13:45:24,451 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4240 (0.4307) Acc D Real: 99.318% 
Loss D Fake: 1.0624 (1.0611) Acc D Fake: 0.280% 
Loss D: 1.486 
Loss G: 0.4291 (0.4310) Acc G: 99.733% 
LR: 2.000e-04 

2023-03-01 13:45:24,458 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4254 (0.4307) Acc D Real: 99.323% 
Loss D Fake: 1.0618 (1.0611) Acc D Fake: 0.278% 
Loss D: 1.487 
Loss G: 0.4293 (0.4310) Acc G: 99.735% 
LR: 2.000e-04 

2023-03-01 13:45:24,466 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4248 (0.4307) Acc D Real: 99.328% 
Loss D Fake: 1.0615 (1.0611) Acc D Fake: 0.276% 
Loss D: 1.486 
Loss G: 0.4294 (0.4310) Acc G: 99.737% 
LR: 2.000e-04 

2023-03-01 13:45:24,474 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4246 (0.4306) Acc D Real: 99.333% 
Loss D Fake: 1.0615 (1.0611) Acc D Fake: 0.274% 
Loss D: 1.486 
Loss G: 0.4292 (0.4310) Acc G: 99.739% 
LR: 2.000e-04 

2023-03-01 13:45:24,481 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4244 (0.4306) Acc D Real: 99.338% 
Loss D Fake: 1.0621 (1.0611) Acc D Fake: 0.272% 
Loss D: 1.487 
Loss G: 0.4289 (0.4309) Acc G: 99.741% 
LR: 2.000e-04 

2023-03-01 13:45:24,488 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4249 (0.4305) Acc D Real: 99.343% 
Loss D Fake: 1.0629 (1.0611) Acc D Fake: 0.270% 
Loss D: 1.488 
Loss G: 0.4286 (0.4309) Acc G: 99.743% 
LR: 2.000e-04 

2023-03-01 13:45:24,496 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4246 (0.4305) Acc D Real: 99.348% 
Loss D Fake: 1.0634 (1.0611) Acc D Fake: 0.268% 
Loss D: 1.488 
Loss G: 0.4284 (0.4309) Acc G: 99.745% 
LR: 2.000e-04 

2023-03-01 13:45:24,503 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4230 (0.4304) Acc D Real: 99.353% 
Loss D Fake: 1.0638 (1.0611) Acc D Fake: 0.266% 
Loss D: 1.487 
Loss G: 0.4282 (0.4309) Acc G: 99.746% 
LR: 2.000e-04 

2023-03-01 13:45:24,511 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4234 (0.4304) Acc D Real: 99.357% 
Loss D Fake: 1.0642 (1.0612) Acc D Fake: 0.264% 
Loss D: 1.488 
Loss G: 0.4283 (0.4309) Acc G: 99.748% 
LR: 2.000e-04 

2023-03-01 13:45:24,519 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4232 (0.4303) Acc D Real: 99.362% 
Loss D Fake: 1.0637 (1.0612) Acc D Fake: 0.262% 
Loss D: 1.487 
Loss G: 0.4286 (0.4308) Acc G: 99.750% 
LR: 2.000e-04 

2023-03-01 13:45:24,526 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4217 (0.4303) Acc D Real: 99.367% 
Loss D Fake: 1.0631 (1.0612) Acc D Fake: 0.260% 
Loss D: 1.485 
Loss G: 0.4287 (0.4308) Acc G: 99.752% 
LR: 2.000e-04 

2023-03-01 13:45:24,533 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4245 (0.4302) Acc D Real: 99.371% 
Loss D Fake: 1.0629 (1.0612) Acc D Fake: 0.258% 
Loss D: 1.487 
Loss G: 0.4289 (0.4308) Acc G: 99.754% 
LR: 2.000e-04 

2023-03-01 13:45:24,540 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4190 (0.4302) Acc D Real: 99.375% 
Loss D Fake: 1.0631 (1.0612) Acc D Fake: 0.256% 
Loss D: 1.482 
Loss G: 0.4283 (0.4308) Acc G: 99.755% 
LR: 2.000e-04 

2023-03-01 13:45:24,548 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4254 (0.4301) Acc D Real: 99.380% 
Loss D Fake: 1.0646 (1.0612) Acc D Fake: 0.255% 
Loss D: 1.490 
Loss G: 0.4282 (0.4308) Acc G: 99.757% 
LR: 2.000e-04 

2023-03-01 13:45:24,555 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4208 (0.4301) Acc D Real: 99.384% 
Loss D Fake: 1.0644 (1.0613) Acc D Fake: 0.253% 
Loss D: 1.485 
Loss G: 0.4283 (0.4308) Acc G: 99.759% 
LR: 2.000e-04 

2023-03-01 13:45:24,562 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4244 (0.4300) Acc D Real: 99.388% 
Loss D Fake: 1.0639 (1.0613) Acc D Fake: 0.251% 
Loss D: 1.488 
Loss G: 0.4288 (0.4308) Acc G: 99.760% 
LR: 2.000e-04 

2023-03-01 13:45:24,570 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4241 (0.4300) Acc D Real: 99.392% 
Loss D Fake: 1.0624 (1.0613) Acc D Fake: 0.249% 
Loss D: 1.486 
Loss G: 0.4296 (0.4307) Acc G: 99.762% 
LR: 2.000e-04 

2023-03-01 13:45:24,578 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4244 (0.4299) Acc D Real: 99.396% 
Loss D Fake: 1.0604 (1.0613) Acc D Fake: 0.248% 
Loss D: 1.485 
Loss G: 0.4305 (0.4307) Acc G: 99.764% 
LR: 2.000e-04 

2023-03-01 13:45:24,587 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4287 (0.4299) Acc D Real: 99.401% 
Loss D Fake: 1.0581 (1.0613) Acc D Fake: 0.246% 
Loss D: 1.487 
Loss G: 0.4315 (0.4307) Acc G: 99.765% 
LR: 2.000e-04 

2023-03-01 13:45:24,595 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4250 (0.4299) Acc D Real: 99.405% 
Loss D Fake: 1.0567 (1.0612) Acc D Fake: 0.244% 
Loss D: 1.482 
Loss G: 0.4314 (0.4308) Acc G: 99.767% 
LR: 2.000e-04 

2023-03-01 13:45:24,603 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4275 (0.4299) Acc D Real: 99.408% 
Loss D Fake: 1.0577 (1.0612) Acc D Fake: 0.243% 
Loss D: 1.485 
Loss G: 0.4309 (0.4308) Acc G: 99.768% 
LR: 2.000e-04 

2023-03-01 13:45:24,611 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4266 (0.4299) Acc D Real: 99.412% 
Loss D Fake: 1.0588 (1.0612) Acc D Fake: 0.241% 
Loss D: 1.485 
Loss G: 0.4305 (0.4308) Acc G: 99.770% 
LR: 2.000e-04 

2023-03-01 13:45:24,618 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4261 (0.4298) Acc D Real: 99.416% 
Loss D Fake: 1.0597 (1.0612) Acc D Fake: 0.240% 
Loss D: 1.486 
Loss G: 0.4300 (0.4307) Acc G: 99.771% 
LR: 2.000e-04 

2023-03-01 13:45:24,625 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4270 (0.4298) Acc D Real: 99.420% 
Loss D Fake: 1.0605 (1.0612) Acc D Fake: 0.238% 
Loss D: 1.488 
Loss G: 0.4299 (0.4307) Acc G: 99.773% 
LR: 2.000e-04 

2023-03-01 13:45:24,634 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4230 (0.4298) Acc D Real: 99.424% 
Loss D Fake: 1.0612 (1.0612) Acc D Fake: 0.237% 
Loss D: 1.484 
Loss G: 0.4291 (0.4307) Acc G: 99.774% 
LR: 2.000e-04 

2023-03-01 13:45:24,641 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4216 (0.4297) Acc D Real: 99.427% 
Loss D Fake: 1.0637 (1.0612) Acc D Fake: 0.235% 
Loss D: 1.485 
Loss G: 0.4280 (0.4307) Acc G: 99.776% 
LR: 2.000e-04 

2023-03-01 13:45:24,648 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4242 (0.4297) Acc D Real: 99.431% 
Loss D Fake: 1.0655 (1.0612) Acc D Fake: 0.234% 
Loss D: 1.490 
Loss G: 0.4280 (0.4307) Acc G: 99.777% 
LR: 2.000e-04 

2023-03-01 13:45:24,656 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4176 (0.4296) Acc D Real: 99.435% 
Loss D Fake: 1.0654 (1.0612) Acc D Fake: 0.232% 
Loss D: 1.483 
Loss G: 0.4275 (0.4307) Acc G: 99.778% 
LR: 2.000e-04 

2023-03-01 13:45:24,663 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4217 (0.4296) Acc D Real: 99.438% 
Loss D Fake: 1.0668 (1.0613) Acc D Fake: 0.231% 
Loss D: 1.488 
Loss G: 0.4275 (0.4307) Acc G: 99.780% 
LR: 2.000e-04 

2023-03-01 13:45:24,671 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4237 (0.4295) Acc D Real: 99.442% 
Loss D Fake: 1.0652 (1.0613) Acc D Fake: 0.229% 
Loss D: 1.489 
Loss G: 0.4286 (0.4306) Acc G: 99.781% 
LR: 2.000e-04 

2023-03-01 13:45:24,678 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4219 (0.4295) Acc D Real: 99.445% 
Loss D Fake: 1.0622 (1.0613) Acc D Fake: 0.228% 
Loss D: 1.484 
Loss G: 0.4294 (0.4306) Acc G: 99.783% 
LR: 2.000e-04 

2023-03-01 13:45:24,685 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4204 (0.4294) Acc D Real: 99.449% 
Loss D Fake: 1.0616 (1.0613) Acc D Fake: 0.226% 
Loss D: 1.482 
Loss G: 0.4291 (0.4306) Acc G: 99.784% 
LR: 2.000e-04 

2023-03-01 13:45:24,693 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4230 (0.4294) Acc D Real: 99.452% 
Loss D Fake: 1.0631 (1.0613) Acc D Fake: 0.225% 
Loss D: 1.486 
Loss G: 0.4287 (0.4306) Acc G: 99.785% 
LR: 2.000e-04 

2023-03-01 13:45:24,700 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4248 (0.4294) Acc D Real: 99.455% 
Loss D Fake: 1.0628 (1.0613) Acc D Fake: 0.224% 
Loss D: 1.488 
Loss G: 0.4295 (0.4306) Acc G: 99.787% 
LR: 2.000e-04 

2023-03-01 13:45:24,708 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4250 (0.4293) Acc D Real: 99.459% 
Loss D Fake: 1.0603 (1.0613) Acc D Fake: 0.222% 
Loss D: 1.485 
Loss G: 0.4302 (0.4306) Acc G: 99.788% 
LR: 2.000e-04 

2023-03-01 13:45:24,715 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4252 (0.4293) Acc D Real: 99.462% 
Loss D Fake: 1.0592 (1.0613) Acc D Fake: 0.221% 
Loss D: 1.484 
Loss G: 0.4303 (0.4306) Acc G: 99.789% 
LR: 2.000e-04 

2023-03-01 13:45:24,722 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4241 (0.4293) Acc D Real: 99.465% 
Loss D Fake: 1.0602 (1.0613) Acc D Fake: 0.220% 
Loss D: 1.484 
Loss G: 0.4292 (0.4306) Acc G: 99.790% 
LR: 2.000e-04 

2023-03-01 13:45:24,729 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4235 (0.4292) Acc D Real: 99.468% 
Loss D Fake: 1.0637 (1.0613) Acc D Fake: 0.218% 
Loss D: 1.487 
Loss G: 0.4281 (0.4306) Acc G: 99.792% 
LR: 2.000e-04 

2023-03-01 13:45:24,736 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4219 (0.4292) Acc D Real: 99.471% 
Loss D Fake: 1.0657 (1.0614) Acc D Fake: 0.217% 
Loss D: 1.488 
Loss G: 0.4279 (0.4306) Acc G: 99.793% 
LR: 2.000e-04 

2023-03-01 13:45:24,744 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4167 (0.4291) Acc D Real: 99.475% 
Loss D Fake: 1.0653 (1.0614) Acc D Fake: 0.216% 
Loss D: 1.482 
Loss G: 0.4281 (0.4305) Acc G: 99.794% 
LR: 2.000e-04 

2023-03-01 13:45:24,752 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4199 (0.4291) Acc D Real: 99.478% 
Loss D Fake: 1.0641 (1.0614) Acc D Fake: 0.214% 
Loss D: 1.484 
Loss G: 0.4289 (0.4305) Acc G: 99.795% 
LR: 2.000e-04 

2023-03-01 13:45:24,761 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4216 (0.4290) Acc D Real: 99.481% 
Loss D Fake: 1.0614 (1.0614) Acc D Fake: 0.213% 
Loss D: 1.483 
Loss G: 0.4298 (0.4305) Acc G: 99.797% 
LR: 2.000e-04 

2023-03-01 13:45:24,770 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4258 (0.4290) Acc D Real: 99.484% 
Loss D Fake: 1.0594 (1.0614) Acc D Fake: 0.212% 
Loss D: 1.485 
Loss G: 0.4304 (0.4305) Acc G: 99.798% 
LR: 2.000e-04 

2023-03-01 13:45:24,779 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4240 (0.4290) Acc D Real: 99.487% 
Loss D Fake: 1.0586 (1.0614) Acc D Fake: 0.211% 
Loss D: 1.483 
Loss G: 0.4304 (0.4305) Acc G: 99.799% 
LR: 2.000e-04 

2023-03-01 13:45:24,787 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4237 (0.4289) Acc D Real: 99.490% 
Loss D Fake: 1.0597 (1.0614) Acc D Fake: 0.210% 
Loss D: 1.483 
Loss G: 0.4296 (0.4305) Acc G: 99.800% 
LR: 2.000e-04 

2023-03-01 13:45:24,794 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4267 (0.4289) Acc D Real: 99.492% 
Loss D Fake: 1.0619 (1.0614) Acc D Fake: 0.208% 
Loss D: 1.489 
Loss G: 0.4289 (0.4305) Acc G: 99.801% 
LR: 2.000e-04 

2023-03-01 13:45:24,801 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4195 (0.4289) Acc D Real: 99.495% 
Loss D Fake: 1.0636 (1.0614) Acc D Fake: 0.207% 
Loss D: 1.483 
Loss G: 0.4280 (0.4305) Acc G: 99.802% 
LR: 2.000e-04 

2023-03-01 13:45:24,810 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4261 (0.4289) Acc D Real: 99.498% 
Loss D Fake: 1.0647 (1.0614) Acc D Fake: 0.206% 
Loss D: 1.491 
Loss G: 0.4288 (0.4305) Acc G: 99.803% 
LR: 2.000e-04 

2023-03-01 13:45:24,818 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4217 (0.4288) Acc D Real: 99.501% 
Loss D Fake: 1.0619 (1.0614) Acc D Fake: 0.205% 
Loss D: 1.484 
Loss G: 0.4287 (0.4305) Acc G: 99.804% 
LR: 2.000e-04 

2023-03-01 13:45:24,826 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4216 (0.4288) Acc D Real: 99.504% 
Loss D Fake: 1.0648 (1.0614) Acc D Fake: 0.204% 
Loss D: 1.486 
Loss G: 0.4272 (0.4305) Acc G: 99.806% 
LR: 2.000e-04 

2023-03-01 13:45:24,834 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4125 (0.4287) Acc D Real: 99.507% 
Loss D Fake: 1.0683 (1.0614) Acc D Fake: 0.203% 
Loss D: 1.481 
Loss G: 0.4278 (0.4305) Acc G: 99.807% 
LR: 2.000e-04 

2023-03-01 13:45:24,842 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4213 (0.4287) Acc D Real: 99.509% 
Loss D Fake: 1.0619 (1.0614) Acc D Fake: 0.201% 
Loss D: 1.483 
Loss G: 0.4299 (0.4304) Acc G: 99.808% 
LR: 2.000e-04 

2023-03-01 13:45:24,851 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4224 (0.4286) Acc D Real: 99.512% 
Loss D Fake: 1.0588 (1.0614) Acc D Fake: 0.200% 
Loss D: 1.481 
Loss G: 0.4301 (0.4304) Acc G: 99.809% 
LR: 2.000e-04 

2023-03-01 13:45:24,858 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4203 (0.4286) Acc D Real: 99.515% 
Loss D Fake: 1.0600 (1.0614) Acc D Fake: 0.199% 
Loss D: 1.480 
Loss G: 0.4294 (0.4304) Acc G: 99.810% 
LR: 2.000e-04 

2023-03-01 13:45:24,865 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4235 (0.4285) Acc D Real: 99.517% 
Loss D Fake: 1.0620 (1.0614) Acc D Fake: 0.198% 
Loss D: 1.486 
Loss G: 0.4287 (0.4304) Acc G: 99.811% 
LR: 2.000e-04 

2023-03-01 13:45:24,872 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4202 (0.4285) Acc D Real: 99.520% 
Loss D Fake: 1.0638 (1.0614) Acc D Fake: 0.197% 
Loss D: 1.484 
Loss G: 0.4280 (0.4304) Acc G: 99.812% 
LR: 2.000e-04 

2023-03-01 13:45:24,879 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4229 (0.4285) Acc D Real: 99.522% 
Loss D Fake: 1.0646 (1.0615) Acc D Fake: 0.196% 
Loss D: 1.488 
Loss G: 0.4285 (0.4304) Acc G: 99.813% 
LR: 2.000e-04 

2023-03-01 13:45:24,887 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4247 (0.4284) Acc D Real: 99.525% 
Loss D Fake: 1.0613 (1.0615) Acc D Fake: 0.195% 
Loss D: 1.486 
Loss G: 0.4297 (0.4304) Acc G: 99.814% 
LR: 2.000e-04 

2023-03-01 13:45:24,894 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4249 (0.4284) Acc D Real: 99.527% 
Loss D Fake: 1.0593 (1.0614) Acc D Fake: 0.194% 
Loss D: 1.484 
Loss G: 0.4299 (0.4304) Acc G: 99.815% 
LR: 2.000e-04 

2023-03-01 13:45:24,901 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4216 (0.4284) Acc D Real: 99.530% 
Loss D Fake: 1.0605 (1.0614) Acc D Fake: 0.193% 
Loss D: 1.482 
Loss G: 0.4287 (0.4304) Acc G: 99.816% 
LR: 2.000e-04 

2023-03-01 13:45:24,908 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4189 (0.4283) Acc D Real: 99.532% 
Loss D Fake: 1.0655 (1.0615) Acc D Fake: 0.192% 
Loss D: 1.484 
Loss G: 0.4267 (0.4304) Acc G: 99.817% 
LR: 2.000e-04 

2023-03-01 13:45:24,915 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4197 (0.4283) Acc D Real: 99.535% 
Loss D Fake: 1.0675 (1.0615) Acc D Fake: 0.191% 
Loss D: 1.487 
Loss G: 0.4285 (0.4304) Acc G: 99.818% 
LR: 2.000e-04 

2023-03-01 13:45:24,923 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4136 (0.4282) Acc D Real: 99.537% 
Loss D Fake: 1.0613 (1.0615) Acc D Fake: 0.190% 
Loss D: 1.475 
Loss G: 0.4286 (0.4304) Acc G: 99.819% 
LR: 2.000e-04 

2023-03-01 13:45:24,930 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4215 (0.4282) Acc D Real: 99.540% 
Loss D Fake: 1.0641 (1.0615) Acc D Fake: 0.189% 
Loss D: 1.486 
Loss G: 0.4276 (0.4303) Acc G: 99.820% 
LR: 2.000e-04 

2023-03-01 13:45:24,937 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4149 (0.4281) Acc D Real: 99.542% 
Loss D Fake: 1.0696 (1.0615) Acc D Fake: 0.188% 
Loss D: 1.484 
Loss G: 0.4261 (0.4303) Acc G: 99.821% 
LR: 2.000e-04 

2023-03-01 13:45:24,945 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4180 (0.4281) Acc D Real: 99.544% 
Loss D Fake: 1.0683 (1.0616) Acc D Fake: 0.187% 
Loss D: 1.486 
Loss G: 0.4287 (0.4303) Acc G: 99.821% 
LR: 2.000e-04 

2023-03-01 13:45:24,952 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4242 (0.4280) Acc D Real: 99.547% 
Loss D Fake: 1.0583 (1.0616) Acc D Fake: 0.186% 
Loss D: 1.483 
Loss G: 0.4314 (0.4303) Acc G: 99.822% 
LR: 2.000e-04 

2023-03-01 13:45:24,960 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4225 (0.4280) Acc D Real: 99.549% 
Loss D Fake: 1.0550 (1.0615) Acc D Fake: 0.185% 
Loss D: 1.478 
Loss G: 0.4318 (0.4303) Acc G: 99.823% 
LR: 2.000e-04 

2023-03-01 13:45:24,968 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4280 (0.4280) Acc D Real: 99.551% 
Loss D Fake: 1.0549 (1.0615) Acc D Fake: 0.184% 
Loss D: 1.483 
Loss G: 0.4317 (0.4303) Acc G: 99.824% 
LR: 2.000e-04 

2023-03-01 13:45:24,976 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4271 (0.4280) Acc D Real: 99.553% 
Loss D Fake: 1.0554 (1.0615) Acc D Fake: 0.183% 
Loss D: 1.482 
Loss G: 0.4313 (0.4303) Acc G: 99.825% 
LR: 2.000e-04 

2023-03-01 13:45:24,983 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4263 (0.4280) Acc D Real: 99.556% 
Loss D Fake: 1.0571 (1.0614) Acc D Fake: 0.182% 
Loss D: 1.483 
Loss G: 0.4298 (0.4303) Acc G: 99.826% 
LR: 2.000e-04 

2023-03-01 13:45:24,991 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4173 (0.4280) Acc D Real: 99.558% 
Loss D Fake: 1.0630 (1.0615) Acc D Fake: 0.182% 
Loss D: 1.480 
Loss G: 0.4256 (0.4303) Acc G: 99.827% 
LR: 2.000e-04 

2023-03-01 13:45:24,998 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4205 (0.4279) Acc D Real: 99.560% 
Loss D Fake: 1.0809 (1.0616) Acc D Fake: 0.181% 
Loss D: 1.501 
Loss G: 0.4302 (0.4303) Acc G: 99.828% 
LR: 2.000e-04 

2023-03-01 13:45:25,006 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4221 (0.4279) Acc D Real: 99.562% 
Loss D Fake: 1.0546 (1.0615) Acc D Fake: 0.180% 
Loss D: 1.477 
Loss G: 0.4323 (0.4303) Acc G: 99.828% 
LR: 2.000e-04 

2023-03-01 13:45:25,013 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4280 (0.4279) Acc D Real: 99.564% 
Loss D Fake: 1.0535 (1.0615) Acc D Fake: 0.179% 
Loss D: 1.481 
Loss G: 0.4321 (0.4303) Acc G: 99.829% 
LR: 2.000e-04 

2023-03-01 13:45:25,020 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4278 (0.4279) Acc D Real: 99.566% 
Loss D Fake: 1.0542 (1.0614) Acc D Fake: 0.178% 
Loss D: 1.482 
Loss G: 0.4316 (0.4303) Acc G: 99.830% 
LR: 2.000e-04 

2023-03-01 13:45:25,028 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4272 (0.4279) Acc D Real: 99.568% 
Loss D Fake: 1.0554 (1.0614) Acc D Fake: 0.177% 
Loss D: 1.483 
Loss G: 0.4309 (0.4303) Acc G: 99.831% 
LR: 2.000e-04 

2023-03-01 13:45:25,035 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4261 (0.4279) Acc D Real: 99.571% 
Loss D Fake: 1.0567 (1.0614) Acc D Fake: 0.176% 
Loss D: 1.483 
Loss G: 0.4301 (0.4303) Acc G: 99.832% 
LR: 2.000e-04 

2023-03-01 13:45:25,042 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4259 (0.4279) Acc D Real: 99.573% 
Loss D Fake: 1.0581 (1.0614) Acc D Fake: 0.175% 
Loss D: 1.484 
Loss G: 0.4294 (0.4303) Acc G: 99.833% 
LR: 2.000e-04 

2023-03-01 13:45:25,049 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4249 (0.4279) Acc D Real: 99.575% 
Loss D Fake: 1.0595 (1.0614) Acc D Fake: 0.175% 
Loss D: 1.484 
Loss G: 0.4287 (0.4303) Acc G: 99.833% 
LR: 2.000e-04 

2023-03-01 13:45:25,056 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4243 (0.4278) Acc D Real: 99.577% 
Loss D Fake: 1.0608 (1.0614) Acc D Fake: 0.174% 
Loss D: 1.485 
Loss G: 0.4280 (0.4303) Acc G: 99.834% 
LR: 2.000e-04 

2023-03-01 13:45:25,063 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4234 (0.4278) Acc D Real: 99.579% 
Loss D Fake: 1.0621 (1.0614) Acc D Fake: 0.173% 
Loss D: 1.485 
Loss G: 0.4274 (0.4303) Acc G: 99.835% 
LR: 2.000e-04 

2023-03-01 13:45:25,070 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4230 (0.4278) Acc D Real: 99.581% 
Loss D Fake: 1.0632 (1.0614) Acc D Fake: 0.172% 
Loss D: 1.486 
Loss G: 0.4268 (0.4303) Acc G: 99.836% 
LR: 2.000e-04 

2023-03-01 13:45:25,077 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4221 (0.4278) Acc D Real: 99.583% 
Loss D Fake: 1.0643 (1.0614) Acc D Fake: 0.171% 
Loss D: 1.486 
Loss G: 0.4263 (0.4303) Acc G: 99.836% 
LR: 2.000e-04 

2023-03-01 13:45:25,085 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4216 (0.4277) Acc D Real: 99.585% 
Loss D Fake: 1.0653 (1.0614) Acc D Fake: 0.171% 
Loss D: 1.487 
Loss G: 0.4258 (0.4302) Acc G: 99.837% 
LR: 2.000e-04 

2023-03-01 13:45:25,092 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4212 (0.4277) Acc D Real: 99.586% 
Loss D Fake: 1.0661 (1.0614) Acc D Fake: 0.170% 
Loss D: 1.487 
Loss G: 0.4254 (0.4302) Acc G: 99.838% 
LR: 2.000e-04 

2023-03-01 13:45:25,099 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4203 (0.4277) Acc D Real: 99.588% 
Loss D Fake: 1.0669 (1.0615) Acc D Fake: 0.169% 
Loss D: 1.487 
Loss G: 0.4250 (0.4302) Acc G: 99.839% 
LR: 2.000e-04 

2023-03-01 13:45:25,106 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4201 (0.4276) Acc D Real: 99.590% 
Loss D Fake: 1.0676 (1.0615) Acc D Fake: 0.168% 
Loss D: 1.488 
Loss G: 0.4246 (0.4302) Acc G: 99.839% 
LR: 2.000e-04 

2023-03-01 13:45:25,114 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4198 (0.4276) Acc D Real: 99.592% 
Loss D Fake: 1.0682 (1.0615) Acc D Fake: 0.167% 
Loss D: 1.488 
Loss G: 0.4243 (0.4301) Acc G: 99.840% 
LR: 2.000e-04 

2023-03-01 13:45:25,121 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4193 (0.4276) Acc D Real: 99.594% 
Loss D Fake: 1.0687 (1.0615) Acc D Fake: 0.167% 
Loss D: 1.488 
Loss G: 0.4241 (0.4301) Acc G: 99.841% 
LR: 2.000e-04 

2023-03-01 13:45:25,128 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4190 (0.4275) Acc D Real: 99.596% 
Loss D Fake: 1.0692 (1.0616) Acc D Fake: 0.166% 
Loss D: 1.488 
Loss G: 0.4239 (0.4301) Acc G: 99.842% 
LR: 2.000e-04 

2023-03-01 13:45:25,135 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4188 (0.4275) Acc D Real: 99.598% 
Loss D Fake: 1.0696 (1.0616) Acc D Fake: 0.165% 
Loss D: 1.488 
Loss G: 0.4237 (0.4301) Acc G: 99.842% 
LR: 2.000e-04 

2023-03-01 13:45:25,142 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4186 (0.4274) Acc D Real: 99.599% 
Loss D Fake: 1.0699 (1.0617) Acc D Fake: 0.164% 
Loss D: 1.488 
Loss G: 0.4235 (0.4300) Acc G: 99.843% 
LR: 2.000e-04 

2023-03-01 13:45:25,150 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4181 (0.4274) Acc D Real: 99.601% 
Loss D Fake: 1.0701 (1.0617) Acc D Fake: 0.164% 
Loss D: 1.488 
Loss G: 0.4234 (0.4300) Acc G: 99.844% 
LR: 2.000e-04 

2023-03-01 13:45:25,157 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4179 (0.4274) Acc D Real: 99.603% 
Loss D Fake: 1.0703 (1.0617) Acc D Fake: 0.163% 
Loss D: 1.488 
Loss G: 0.4233 (0.4300) Acc G: 99.844% 
LR: 2.000e-04 

2023-03-01 13:45:25,164 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4177 (0.4273) Acc D Real: 99.603% 
Loss D Fake: 1.0705 (1.0618) Acc D Fake: 0.163% 
Loss D: 1.488 
Loss G: 0.4233 (0.4299) Acc G: 99.845% 
LR: 2.000e-04 

2023-03-01 13:45:25,175 -                train: [    INFO] - 
Epoch: 8/20
2023-03-01 13:45:25,381 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4178 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4231 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,389 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4177 (0.4177) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4231 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,396 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4172 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4231 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,414 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4174 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,421 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4172 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,428 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4172 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,435 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4173 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,442 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4172 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,449 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4173 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,456 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4171 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,463 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4163 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,470 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4170 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,477 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4168 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,484 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4167 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,490 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4166 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,497 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4168 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,504 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4164 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,511 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4166 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,518 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4163 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,524 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4163 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,531 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4162 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,538 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4163 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,545 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4162 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,552 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4164 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,559 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4161 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,565 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4156 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,573 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4159 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,580 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4155 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,587 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4156 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,595 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4158 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,602 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4155 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,609 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4152 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,617 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4155 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,624 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4157 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,631 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4147 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,638 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4158 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,646 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4150 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,653 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4149 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,660 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4152 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,668 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4151 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,675 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4145 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4230 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,682 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4147 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,690 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4150 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,697 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4147 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,704 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4153 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,712 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4142 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0707 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,719 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4146 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0706 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,728 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4149 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0706 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4231 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,736 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4142 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0706 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4232 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,744 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4134 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0705 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4232 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,752 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4145 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0705 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4232 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,760 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4142 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0704 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4233 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,769 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4146 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0703 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4233 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,777 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4145 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0702 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4234 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,785 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4143 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0701 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4235 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,793 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4139 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0699 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4235 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,800 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4141 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0698 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4236 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,808 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4146 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0696 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4237 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,816 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4144 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0695 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4238 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,824 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4142 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0693 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4239 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,832 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4143 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0692 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4239 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,839 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4140 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4240 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,847 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4139 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0689 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4241 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,854 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4138 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0688 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4242 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,862 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4142 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0686 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4243 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,869 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4144 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0685 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4243 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,876 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4144 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0684 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4244 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,884 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4137 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0682 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4245 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,891 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4128 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0681 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4246 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,899 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4134 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0679 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4247 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,907 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4127 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0677 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4248 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,914 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4139 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0675 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4250 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,921 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4141 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4251 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,928 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4140 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0670 (1.0702) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4252 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,937 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4143 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0669 (1.0702) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4253 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,945 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4134 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4254 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,953 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4136 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0665 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4255 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,961 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4146 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0663 (1.0700) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4256 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,970 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4146 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0700) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4257 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,978 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4138 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4257 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,986 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4140 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4257 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:25,994 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4151 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0663 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4257 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,002 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4140 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0665 (1.0698) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4256 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,011 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4134 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0698) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4255 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,019 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4137 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4256 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,026 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4119 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4257 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,033 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4114 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0665 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4259 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,041 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4144 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0696) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4260 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,048 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4142 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0661 (1.0696) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4261 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,055 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4136 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0661 (1.0696) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4261 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,063 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4134 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0661 (1.0695) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4261 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,070 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4148 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0695) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4260 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,077 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4149 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0665 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4259 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,085 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4129 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4258 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,092 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4136 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0670 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4257 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,099 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4138 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0673 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4257 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,106 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4115 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0674 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4257 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,114 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4148 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4258 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,121 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4122 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4260 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,128 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4119 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0670 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4261 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,135 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4147 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0669 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4261 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,142 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4153 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0670 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4261 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,149 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4181 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4260 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,156 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4135 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0675 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4260 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,164 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4134 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0675 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4260 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,171 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4146 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0677 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4258 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,178 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4131 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0683 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4255 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,185 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4115 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0688 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4255 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,192 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4144 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0688 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4255 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,199 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4137 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4255 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,206 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4117 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0689 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4258 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,213 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4165 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0681 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4262 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,220 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4170 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0673 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4267 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,228 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4181 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0663 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4273 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,235 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4147 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0649 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4280 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,242 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4187 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0636 (1.0690) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4286 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,250 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4195 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0628 (1.0690) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4288 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,257 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4187 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0627 (1.0689) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4289 (0.4244) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,264 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4182 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0689) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4291 (0.4244) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,271 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4214 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0621 (1.0688) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4290 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,279 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4197 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0688) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4288 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,286 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4200 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0631 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4283 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,293 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4189 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0643 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4277 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,301 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4188 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0657 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4268 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,308 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4209 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0674 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4263 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,316 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4208 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0679 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4263 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,324 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4224 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4269 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,331 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4255 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0654 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4279 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,339 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4226 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0631 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4287 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,346 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4259 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0618 (1.0685) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4292 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,353 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4246 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0607 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4296 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,361 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4250 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0600 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4296 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,368 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4218 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0607 (1.0683) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4288 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,376 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4254 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0683) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4281 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,383 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4243 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0634 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4276 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,391 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4228 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0642 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4272 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,399 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4206 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0652 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4265 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,406 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4207 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0668 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4256 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,413 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4245 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0682 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4252 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,421 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4230 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0684 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4252 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,428 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4229 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0681 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4254 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,436 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4232 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0672 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4260 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,444 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4227 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0658 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4265 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,453 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4254 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0646 (1.0681) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4272 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,461 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4255 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0632 (1.0681) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4278 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,468 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4251 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0620 (1.0681) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4282 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,476 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4267 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0613 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4285 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,484 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4261 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0608 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4286 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,491 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4268 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0607 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4285 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,499 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4269 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0609 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4284 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,506 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4258 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0611 (1.0678) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4281 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,513 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4262 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0616 (1.0678) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4278 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,521 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4265 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0620 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4276 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,529 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4246 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4273 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,537 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4262 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0630 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4271 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,545 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4245 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0634 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4268 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,554 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4247 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0638 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4266 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,562 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4244 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0641 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4264 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,570 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4247 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0645 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4262 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,578 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4244 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0648 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4260 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,586 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4246 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0651 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4258 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,593 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4234 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0654 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4256 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,600 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4236 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0658 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4254 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,607 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4234 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0662 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4251 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,615 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4227 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4248 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,622 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4222 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0673 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4245 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,629 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4219 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0678 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4242 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,636 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4208 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0684 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4239 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,643 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4211 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4235 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,651 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4203 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0696 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4232 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,658 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4196 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0702 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4229 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,665 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4194 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0708 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4226 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,672 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4196 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0713 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4223 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,679 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4179 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0719 (1.0676) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4221 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,686 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4192 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0724 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4218 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,694 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4187 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0729 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4215 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,701 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4179 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0734 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4213 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,708 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4170 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0738 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4211 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,715 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4174 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0742 (1.0678) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4209 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,723 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4169 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0745 (1.0678) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4207 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,730 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4165 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0749 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4205 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,737 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4167 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0751 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4204 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,744 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4157 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0754 (1.0679) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4203 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,752 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4163 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4202 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,759 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4161 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0758 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4201 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,766 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4160 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0760 (1.0681) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4200 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,773 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4151 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.0681) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4199 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,781 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4149 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0763 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,788 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4152 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0764 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4198 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,796 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4143 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0764 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,803 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4147 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0683) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,810 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4144 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0683) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,817 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4149 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,825 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4118 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4198 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,833 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4115 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0764 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4198 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,841 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4118 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0763 (1.0685) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4199 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,849 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4114 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0761 (1.0685) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4200 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,857 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4048 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0761 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4200 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,864 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3977 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0766 (1.0686) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4193 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,872 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3840 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4132 (0.4244) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,879 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3719 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.1720 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.544 
Loss G: 0.3734 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,887 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3635 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.1803 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.544 
Loss G: 0.3717 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,894 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3602 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.1784 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.539 
Loss G: 0.3752 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,902 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3743 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.1692 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.543 
Loss G: 0.3806 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,909 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3692 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.1515 (1.0711) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4256 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,917 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3782 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0572 (1.0711) Acc D Fake: 0.000% 
Loss D: 1.435 
Loss G: 0.4323 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,924 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4025 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0496 (1.0710) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4351 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,931 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4047 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0453 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4370 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,940 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3997 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0424 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4381 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,948 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3944 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0418 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4370 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,955 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3839 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0586 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4177 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,962 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3840 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.1289 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4370 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,970 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3804 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0358 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.416 
Loss G: 0.4438 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,977 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3946 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0288 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.423 
Loss G: 0.4462 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,984 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4084 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0258 (1.0702) Acc D Fake: 0.000% 
Loss D: 1.434 
Loss G: 0.4471 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,991 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4149 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0260 (1.0700) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4456 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:26,998 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4119 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0362 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4333 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,005 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3907 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.1226 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4421 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,013 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3921 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.0270 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.419 
Loss G: 0.4487 (0.4244) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,020 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3972 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0209 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.418 
Loss G: 0.4504 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,027 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4088 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0192 (1.0695) Acc D Fake: 0.000% 
Loss D: 1.428 
Loss G: 0.4508 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,035 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4010 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0199 (1.0692) Acc D Fake: 0.000% 
Loss D: 1.421 
Loss G: 0.4492 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,042 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3980 (0.4143) Acc D Real: 100.000% 
Loss D Fake: 1.0271 (1.0690) Acc D Fake: 0.000% 
Loss D: 1.425 
Loss G: 0.4449 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,049 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3910 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0418 (1.0689) Acc D Fake: 0.000% 
Loss D: 1.433 
Loss G: 0.4509 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,056 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4197 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0172 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4506 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,067 -                train: [    INFO] - 
Epoch: 9/20
2023-03-01 13:45:27,270 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3854 (0.3937) Acc D Real: 100.000% 
Loss D Fake: 1.0603 (1.0434) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4554 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,277 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4210 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.0104 (1.0324) Acc D Fake: 0.000% 
Loss D: 1.431 
Loss G: 0.4544 (0.4511) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,284 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4442 (0.4132) Acc D Real: 100.000% 
Loss D Fake: 1.0143 (1.0279) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4515 (0.4512) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,301 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4459 (0.4197) Acc D Real: 100.000% 
Loss D Fake: 1.0199 (1.0263) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4482 (0.4506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,307 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4426 (0.4235) Acc D Real: 100.000% 
Loss D Fake: 1.0260 (1.0262) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4447 (0.4496) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,315 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4396 (0.4258) Acc D Real: 100.000% 
Loss D Fake: 1.0323 (1.0271) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4413 (0.4484) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,322 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4367 (0.4272) Acc D Real: 100.000% 
Loss D Fake: 1.0385 (1.0285) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4380 (0.4471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,329 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4328 (0.4278) Acc D Real: 100.000% 
Loss D Fake: 1.0444 (1.0303) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4350 (0.4458) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,336 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4298 (0.4280) Acc D Real: 100.000% 
Loss D Fake: 1.0499 (1.0322) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4322 (0.4444) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,343 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4269 (0.4279) Acc D Real: 100.000% 
Loss D Fake: 1.0551 (1.0343) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4296 (0.4431) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,350 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4249 (0.4277) Acc D Real: 100.000% 
Loss D Fake: 1.0599 (1.0364) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4272 (0.4418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,357 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4232 (0.4273) Acc D Real: 100.000% 
Loss D Fake: 1.0641 (1.0386) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4252 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,364 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4198 (0.4268) Acc D Real: 100.000% 
Loss D Fake: 1.0677 (1.0406) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4234 (0.4393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,371 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4175 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.0711 (1.0427) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4218 (0.4381) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,377 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4180 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.0740 (1.0446) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4205 (0.4370) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,385 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4159 (0.4251) Acc D Real: 100.000% 
Loss D Fake: 1.0763 (1.0465) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4195 (0.4360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,392 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4149 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.0780 (1.0482) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4187 (0.4350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,399 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4138 (0.4239) Acc D Real: 100.000% 
Loss D Fake: 1.0794 (1.0499) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4181 (0.4341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,406 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4135 (0.4234) Acc D Real: 100.000% 
Loss D Fake: 1.0804 (1.0514) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4176 (0.4333) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,412 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4130 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.0814 (1.0528) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4172 (0.4325) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,419 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4122 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.0821 (1.0542) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4169 (0.4318) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,426 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4116 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.0826 (1.0554) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4167 (0.4312) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,433 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4114 (0.4215) Acc D Real: 100.000% 
Loss D Fake: 1.0830 (1.0566) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4164 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,440 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4110 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.0834 (1.0576) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4162 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,446 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4109 (0.4207) Acc D Real: 100.000% 
Loss D Fake: 1.0837 (1.0586) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4162 (0.4294) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,453 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4113 (0.4204) Acc D Real: 100.000% 
Loss D Fake: 1.0838 (1.0596) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4161 (0.4290) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,460 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4121 (0.4201) Acc D Real: 100.000% 
Loss D Fake: 1.0838 (1.0604) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4162 (0.4285) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,468 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4112 (0.4198) Acc D Real: 100.000% 
Loss D Fake: 1.0835 (1.0612) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4164 (0.4281) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,475 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4111 (0.4195) Acc D Real: 100.000% 
Loss D Fake: 1.0832 (1.0620) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4165 (0.4277) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,482 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4113 (0.4192) Acc D Real: 100.000% 
Loss D Fake: 1.0830 (1.0626) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4165 (0.4273) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,489 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4114 (0.4190) Acc D Real: 100.000% 
Loss D Fake: 1.0829 (1.0633) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4166 (0.4270) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,497 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4117 (0.4187) Acc D Real: 100.000% 
Loss D Fake: 1.0826 (1.0639) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4168 (0.4267) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,504 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4130 (0.4186) Acc D Real: 100.000% 
Loss D Fake: 1.0823 (1.0644) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4170 (0.4264) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,511 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4127 (0.4184) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.0649) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4172 (0.4261) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,519 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4129 (0.4183) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0654) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4174 (0.4259) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,526 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4126 (0.4181) Acc D Real: 100.000% 
Loss D Fake: 1.0808 (1.0658) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4177 (0.4257) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,533 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4126 (0.4180) Acc D Real: 100.000% 
Loss D Fake: 1.0804 (1.0662) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4178 (0.4255) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,540 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4131 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0801 (1.0665) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4180 (0.4253) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,548 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4131 (0.4177) Acc D Real: 100.000% 
Loss D Fake: 1.0799 (1.0668) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4181 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,555 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4125 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0797 (1.0672) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4181 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,563 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4129 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0797 (1.0675) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4181 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,571 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4132 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0796 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4181 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,579 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4132 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0795 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4182 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,586 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4138 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0794 (1.0683) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4183 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,594 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4143 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0791 (1.0685) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4185 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,602 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4136 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0786 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4187 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,609 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4148 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0782 (1.0689) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4189 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,616 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4144 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0778 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4191 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,624 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4142 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0773 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4193 (0.4238) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,631 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4136 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0769 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4195 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,639 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4147 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0695) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4197 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,646 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4145 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.0697) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4198 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,653 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4151 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0760 (1.0698) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4199 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,661 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4150 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0758 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4200 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,669 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4148 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.0700) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4200 (0.4234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,676 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4148 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0755 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4201 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,684 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4146 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0755 (1.0702) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4201 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,691 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4149 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0755 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4200 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,699 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4142 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0755 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4200 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,706 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4142 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.0704) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4200 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,713 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4144 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4199 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,720 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4145 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0758 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4198 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,728 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4145 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0759 (1.0707) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4198 (0.4229) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,735 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4136 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0761 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4197 (0.4229) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,743 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4143 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4196 (0.4228) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,750 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4148 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0764 (1.0709) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4195 (0.4228) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,758 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4139 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0765 (1.0710) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4194 (0.4227) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,765 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4141 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0767 (1.0711) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4193 (0.4227) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,772 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4138 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0769 (1.0712) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4192 (0.4226) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,779 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4125 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0770 (1.0713) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4192 (0.4226) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,788 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4132 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0771 (1.0714) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4191 (0.4225) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,796 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4132 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0773 (1.0714) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4190 (0.4225) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,804 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4134 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0774 (1.0715) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4190 (0.4224) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,812 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4134 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0775 (1.0716) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4189 (0.4224) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,819 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4133 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0776 (1.0717) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4188 (0.4223) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,826 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4128 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0778 (1.0718) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4187 (0.4223) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,833 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4132 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0779 (1.0718) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4187 (0.4223) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,841 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4129 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0781 (1.0719) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4186 (0.4222) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,848 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4120 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0783 (1.0720) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4185 (0.4222) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,856 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4120 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0784 (1.0721) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4184 (0.4221) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,863 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4123 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0786 (1.0722) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4183 (0.4221) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,871 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4112 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0787 (1.0722) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4183 (0.4220) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,878 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4124 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0788 (1.0723) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4182 (0.4220) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,886 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4126 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0789 (1.0724) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4182 (0.4219) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,893 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4117 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0790 (1.0725) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4181 (0.4219) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,900 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4114 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0791 (1.0725) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4180 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,908 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4116 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0792 (1.0726) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4180 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,915 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4119 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0793 (1.0727) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4179 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,923 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4120 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0794 (1.0728) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4179 (0.4217) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,931 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4103 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0795 (1.0728) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4178 (0.4217) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,939 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4109 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0796 (1.0729) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4178 (0.4216) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,946 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4105 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0797 (1.0730) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4178 (0.4216) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,953 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4094 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0797 (1.0731) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4177 (0.4215) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,960 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4107 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0798 (1.0731) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4177 (0.4215) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,968 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4119 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0798 (1.0732) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4177 (0.4215) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,975 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4110 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0799 (1.0733) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4177 (0.4214) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,982 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4109 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0799 (1.0733) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4176 (0.4214) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,989 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4102 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0800 (1.0734) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4176 (0.4214) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:27,997 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4108 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0801 (1.0735) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4176 (0.4213) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,004 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4096 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0801 (1.0735) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4175 (0.4213) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,012 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4104 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0802 (1.0736) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4175 (0.4212) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,020 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4116 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0802 (1.0737) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4175 (0.4212) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,028 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4106 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0803 (1.0737) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4174 (0.4212) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,036 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4086 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0804 (1.0738) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4174 (0.4211) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,044 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4103 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.0804 (1.0739) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4174 (0.4211) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,053 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4111 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.0805 (1.0739) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4174 (0.4211) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,061 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4109 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.0805 (1.0740) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4173 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,069 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4091 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.0806 (1.0740) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4173 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,077 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4103 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.0741) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4173 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,084 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4101 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.0742) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4172 (0.4209) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,091 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4081 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0808 (1.0742) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4172 (0.4209) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,098 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4090 (0.4143) Acc D Real: 100.000% 
Loss D Fake: 1.0808 (1.0743) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4172 (0.4209) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,106 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4093 (0.4143) Acc D Real: 100.000% 
Loss D Fake: 1.0808 (1.0743) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4172 (0.4208) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,113 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4097 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0809 (1.0744) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4172 (0.4208) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,121 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4095 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0809 (1.0744) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4171 (0.4208) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,128 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4089 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0810 (1.0745) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4171 (0.4207) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,135 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4098 (0.4141) Acc D Real: 100.000% 
Loss D Fake: 1.0810 (1.0746) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4171 (0.4207) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,142 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4063 (0.4141) Acc D Real: 100.000% 
Loss D Fake: 1.0810 (1.0746) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4171 (0.4207) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,149 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4093 (0.4140) Acc D Real: 100.000% 
Loss D Fake: 1.0810 (1.0747) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4171 (0.4206) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,156 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4077 (0.4140) Acc D Real: 100.000% 
Loss D Fake: 1.0811 (1.0747) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4171 (0.4206) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,163 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4084 (0.4139) Acc D Real: 100.000% 
Loss D Fake: 1.0811 (1.0748) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4171 (0.4206) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,170 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4097 (0.4139) Acc D Real: 100.000% 
Loss D Fake: 1.0811 (1.0748) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4171 (0.4206) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,177 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4097 (0.4139) Acc D Real: 100.000% 
Loss D Fake: 1.0812 (1.0749) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4171 (0.4205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,184 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4083 (0.4138) Acc D Real: 100.000% 
Loss D Fake: 1.0812 (1.0749) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4170 (0.4205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,191 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4066 (0.4138) Acc D Real: 100.000% 
Loss D Fake: 1.0812 (1.0750) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4170 (0.4205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,198 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4056 (0.4137) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0750) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4170 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,205 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4074 (0.4136) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0751) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4170 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,213 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4087 (0.4136) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0751) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4170 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,220 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4070 (0.4136) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0752) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4170 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,227 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4081 (0.4135) Acc D Real: 100.000% 
Loss D Fake: 1.0813 (1.0752) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4170 (0.4203) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,234 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4085 (0.4135) Acc D Real: 100.000% 
Loss D Fake: 1.0814 (1.0753) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4170 (0.4203) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,241 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4063 (0.4134) Acc D Real: 100.000% 
Loss D Fake: 1.0814 (1.0753) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4170 (0.4203) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,248 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4078 (0.4134) Acc D Real: 100.000% 
Loss D Fake: 1.0815 (1.0754) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4170 (0.4203) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,255 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4097 (0.4133) Acc D Real: 100.000% 
Loss D Fake: 1.0815 (1.0754) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4169 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,263 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4069 (0.4133) Acc D Real: 100.000% 
Loss D Fake: 1.0816 (1.0754) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4169 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,271 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4070 (0.4133) Acc D Real: 100.000% 
Loss D Fake: 1.0817 (1.0755) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4169 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,279 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4061 (0.4132) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.0755) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4168 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,286 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4057 (0.4131) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.0756) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4168 (0.4201) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,294 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4062 (0.4131) Acc D Real: 100.000% 
Loss D Fake: 1.0819 (1.0756) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4168 (0.4201) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,302 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4063 (0.4131) Acc D Real: 100.000% 
Loss D Fake: 1.0820 (1.0757) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4168 (0.4201) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,309 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4061 (0.4130) Acc D Real: 100.000% 
Loss D Fake: 1.0821 (1.0757) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4167 (0.4201) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,316 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4072 (0.4130) Acc D Real: 100.000% 
Loss D Fake: 1.0822 (1.0758) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4167 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,324 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4050 (0.4129) Acc D Real: 100.000% 
Loss D Fake: 1.0823 (1.0758) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4167 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,331 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4058 (0.4129) Acc D Real: 100.000% 
Loss D Fake: 1.0824 (1.0759) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4166 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,339 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4055 (0.4128) Acc D Real: 100.000% 
Loss D Fake: 1.0825 (1.0759) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4166 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,347 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4050 (0.4128) Acc D Real: 100.000% 
Loss D Fake: 1.0827 (1.0759) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4165 (0.4200) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,354 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4051 (0.4127) Acc D Real: 100.000% 
Loss D Fake: 1.0828 (1.0760) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4165 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,361 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4041 (0.4126) Acc D Real: 100.000% 
Loss D Fake: 1.0830 (1.0760) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4164 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,368 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4062 (0.4126) Acc D Real: 100.000% 
Loss D Fake: 1.0831 (1.0761) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4163 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,376 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4047 (0.4125) Acc D Real: 100.000% 
Loss D Fake: 1.0833 (1.0761) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4163 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,383 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4040 (0.4125) Acc D Real: 100.000% 
Loss D Fake: 1.0835 (1.0762) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4162 (0.4198) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,390 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4041 (0.4124) Acc D Real: 100.000% 
Loss D Fake: 1.0838 (1.0762) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4161 (0.4198) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,398 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4012 (0.4124) Acc D Real: 100.000% 
Loss D Fake: 1.0840 (1.0763) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4160 (0.4198) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,405 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4019 (0.4123) Acc D Real: 100.000% 
Loss D Fake: 1.0843 (1.0763) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4159 (0.4198) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,412 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4003 (0.4122) Acc D Real: 100.000% 
Loss D Fake: 1.0847 (1.0764) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4157 (0.4197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,419 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4024 (0.4122) Acc D Real: 100.000% 
Loss D Fake: 1.0851 (1.0764) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4156 (0.4197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,426 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4009 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0856 (1.0765) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4154 (0.4197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,434 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4022 (0.4120) Acc D Real: 100.000% 
Loss D Fake: 1.0862 (1.0766) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4151 (0.4197) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,441 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3958 (0.4119) Acc D Real: 100.000% 
Loss D Fake: 1.0868 (1.0766) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4149 (0.4196) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,448 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4006 (0.4119) Acc D Real: 100.000% 
Loss D Fake: 1.0876 (1.0767) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4145 (0.4196) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,455 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4020 (0.4118) Acc D Real: 100.000% 
Loss D Fake: 1.0885 (1.0768) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4142 (0.4196) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,463 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4000 (0.4117) Acc D Real: 100.000% 
Loss D Fake: 1.0894 (1.0768) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4138 (0.4195) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,470 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3957 (0.4116) Acc D Real: 100.000% 
Loss D Fake: 1.0907 (1.0769) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4131 (0.4195) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,478 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3973 (0.4115) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.0770) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4124 (0.4194) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,485 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3925 (0.4114) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.0771) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4112 (0.4194) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,492 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3907 (0.4113) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.0773) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4096 (0.4193) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,500 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3874 (0.4112) Acc D Real: 100.000% 
Loss D Fake: 1.1032 (1.0774) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4070 (0.4193) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,508 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3757 (0.4109) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.0776) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4028 (0.4192) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,516 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3652 (0.4107) Acc D Real: 100.000% 
Loss D Fake: 1.1270 (1.0779) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.3961 (0.4190) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,523 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3522 (0.4103) Acc D Real: 100.000% 
Loss D Fake: 1.1568 (1.0784) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.3872 (0.4188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,531 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3458 (0.4100) Acc D Real: 100.000% 
Loss D Fake: 1.1756 (1.0789) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3907 (0.4187) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,539 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3502 (0.4096) Acc D Real: 100.000% 
Loss D Fake: 1.1399 (1.0793) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.3982 (0.4186) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,546 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3595 (0.4093) Acc D Real: 100.000% 
Loss D Fake: 1.1305 (1.0796) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.3991 (0.4184) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,553 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3458 (0.4090) Acc D Real: 100.000% 
Loss D Fake: 1.1385 (1.0799) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.3968 (0.4183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,561 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3417 (0.4086) Acc D Real: 100.000% 
Loss D Fake: 1.1446 (1.0803) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4015 (0.4182) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,568 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3497 (0.4082) Acc D Real: 100.000% 
Loss D Fake: 1.1190 (1.0805) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4095 (0.4182) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,576 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3538 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.0806) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4133 (0.4181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,583 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3554 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1026 (1.0808) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4134 (0.4181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,591 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3403 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.1099 (1.0809) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4127 (0.4181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,598 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3314 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1049 (1.0811) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4207 (0.4181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,605 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3489 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.0790 (1.0810) Acc D Fake: 0.000% 
Loss D: 1.428 
Loss G: 0.4287 (0.4182) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,613 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3220 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.0694 (1.0810) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.4297 (0.4182) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,620 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3093 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.0848 (1.0810) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.4195 (0.4182) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,628 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3115 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.1457 (1.0813) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4323 (0.4183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,635 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3055 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.0408 (1.0811) Acc D Fake: 0.000% 
Loss D: 1.346 
Loss G: 0.4501 (0.4185) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,643 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3331 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.0231 (1.0808) Acc D Fake: 0.000% 
Loss D: 1.356 
Loss G: 0.4512 (0.4187) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,650 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3568 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.0331 (1.0806) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.4423 (0.4188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,658 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3323 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.0711 (1.0805) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.4106 (0.4187) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,665 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.2692 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.3217 (1.0818) Acc D Fake: 0.000% 
Loss D: 1.591 
Loss G: 0.4616 (0.4190) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,672 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3594 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 0.9606 (1.0811) Acc D Fake: 0.000% 
Loss D: 1.320 
Loss G: 0.4996 (0.4194) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,680 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4663 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 0.9231 (1.0803) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.5149 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,688 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4852 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 0.9054 (1.0794) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.5233 (0.4204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,695 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.5053 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 0.8956 (1.0785) Acc D Fake: 0.000% 
Loss D: 1.401 
Loss G: 0.5280 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,703 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.5119 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 0.8907 (1.0775) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.5301 (0.4215) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,712 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.5137 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 0.8892 (1.0765) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.5301 (0.4221) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,720 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.5110 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 0.8902 (1.0756) Acc D Fake: 0.000% 
Loss D: 1.401 
Loss G: 0.5287 (0.4226) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,728 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.5139 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 0.8932 (1.0747) Acc D Fake: 0.000% 
Loss D: 1.407 
Loss G: 0.5262 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,735 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.5115 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 0.8976 (1.0738) Acc D Fake: 0.000% 
Loss D: 1.409 
Loss G: 0.5227 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,744 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.5081 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 0.9033 (1.0729) Acc D Fake: 0.000% 
Loss D: 1.411 
Loss G: 0.5186 (0.4241) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,751 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4989 (0.4075) Acc D Real: 100.000% 
Loss D Fake: 0.9097 (1.0721) Acc D Fake: 0.000% 
Loss D: 1.409 
Loss G: 0.5141 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,759 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.5032 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 0.9168 (1.0714) Acc D Fake: 0.000% 
Loss D: 1.420 
Loss G: 0.5093 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,766 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4954 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 0.9243 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.420 
Loss G: 0.5044 (0.4254) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,773 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4919 (0.4088) Acc D Real: 100.000% 
Loss D Fake: 0.9321 (1.0700) Acc D Fake: 0.000% 
Loss D: 1.424 
Loss G: 0.4993 (0.4257) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,781 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4706 (0.4092) Acc D Real: 100.000% 
Loss D Fake: 0.9400 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.411 
Loss G: 0.4944 (0.4261) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,788 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4819 (0.4095) Acc D Real: 100.000% 
Loss D Fake: 0.9477 (1.0687) Acc D Fake: 0.000% 
Loss D: 1.430 
Loss G: 0.4896 (0.4264) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,795 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4743 (0.4098) Acc D Real: 100.000% 
Loss D Fake: 0.9554 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.430 
Loss G: 0.4849 (0.4267) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,803 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4714 (0.4101) Acc D Real: 100.000% 
Loss D Fake: 0.9630 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.434 
Loss G: 0.4804 (0.4269) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,810 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4680 (0.4104) Acc D Real: 100.000% 
Loss D Fake: 0.9704 (1.0672) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4760 (0.4272) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,817 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4641 (0.4106) Acc D Real: 100.000% 
Loss D Fake: 0.9776 (1.0668) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4718 (0.4274) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,824 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4601 (0.4109) Acc D Real: 100.000% 
Loss D Fake: 0.9845 (1.0664) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4679 (0.4276) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,831 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4518 (0.4111) Acc D Real: 100.000% 
Loss D Fake: 0.9910 (1.0660) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4642 (0.4277) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,839 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4502 (0.4113) Acc D Real: 100.000% 
Loss D Fake: 0.9973 (1.0657) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4607 (0.4279) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,847 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4450 (0.4114) Acc D Real: 100.000% 
Loss D Fake: 1.0032 (1.0654) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4574 (0.4280) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,854 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4434 (0.4116) Acc D Real: 100.000% 
Loss D Fake: 1.0087 (1.0652) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4544 (0.4282) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,861 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4372 (0.4117) Acc D Real: 100.000% 
Loss D Fake: 1.0138 (1.0649) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4517 (0.4283) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,869 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4318 (0.4118) Acc D Real: 100.000% 
Loss D Fake: 1.0186 (1.0647) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4491 (0.4284) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,876 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4372 (0.4119) Acc D Real: 100.000% 
Loss D Fake: 1.0230 (1.0645) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4468 (0.4284) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,884 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4310 (0.4120) Acc D Real: 100.000% 
Loss D Fake: 1.0271 (1.0643) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4446 (0.4285) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,891 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4291 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0308 (1.0642) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4427 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,898 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4183 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0342 (1.0641) Acc D Fake: 0.000% 
Loss D: 1.453 
Loss G: 0.4409 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,905 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4116 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0372 (1.0639) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4395 (0.4287) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,913 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4268 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0397 (1.0638) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4381 (0.4287) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,920 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4226 (0.4122) Acc D Real: 100.000% 
Loss D Fake: 1.0421 (1.0637) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4369 (0.4288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,929 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4217 (0.4122) Acc D Real: 100.000% 
Loss D Fake: 1.0442 (1.0636) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4358 (0.4288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,936 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4157 (0.4123) Acc D Real: 100.000% 
Loss D Fake: 1.0463 (1.0636) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4347 (0.4288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:28,948 -                train: [    INFO] - 
Epoch: 10/20
2023-03-01 13:45:29,162 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4151 (0.4107) Acc D Real: 100.000% 
Loss D Fake: 1.0494 (1.0487) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4332 (0.4335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,172 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4145 (0.4120) Acc D Real: 100.000% 
Loss D Fake: 1.0507 (1.0494) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4325 (0.4332) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,179 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4005 (0.4091) Acc D Real: 100.000% 
Loss D Fake: 1.0519 (1.0500) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4320 (0.4329) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,196 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4142 (0.4101) Acc D Real: 100.000% 
Loss D Fake: 1.0527 (1.0505) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4315 (0.4326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,203 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4159 (0.4111) Acc D Real: 100.000% 
Loss D Fake: 1.0535 (1.0510) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4311 (0.4324) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,210 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4154 (0.4117) Acc D Real: 100.000% 
Loss D Fake: 1.0543 (1.0515) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4306 (0.4321) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,218 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3918 (0.4092) Acc D Real: 100.000% 
Loss D Fake: 1.0550 (1.0519) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4304 (0.4319) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,226 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4083 (0.4091) Acc D Real: 100.000% 
Loss D Fake: 1.0553 (1.0523) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4302 (0.4317) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,234 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4109 (0.4093) Acc D Real: 100.000% 
Loss D Fake: 1.0556 (1.0526) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4300 (0.4315) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,241 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4131 (0.4096) Acc D Real: 100.000% 
Loss D Fake: 1.0560 (1.0530) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4297 (0.4314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,248 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4123 (0.4099) Acc D Real: 100.000% 
Loss D Fake: 1.0564 (1.0532) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4295 (0.4312) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,255 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3997 (0.4091) Acc D Real: 100.000% 
Loss D Fake: 1.0568 (1.0535) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4293 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,262 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4060 (0.4089) Acc D Real: 100.000% 
Loss D Fake: 1.0570 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4292 (0.4309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,269 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4014 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 1.0571 (1.0540) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4291 (0.4308) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,276 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4104 (0.4085) Acc D Real: 100.000% 
Loss D Fake: 1.0573 (1.0542) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4289 (0.4307) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,283 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4056 (0.4083) Acc D Real: 100.000% 
Loss D Fake: 1.0575 (1.0544) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4288 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,290 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4011 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.0577 (1.0546) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4287 (0.4305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,297 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4034 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.0578 (1.0547) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4286 (0.4304) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,304 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3909 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.0579 (1.0549) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4285 (0.4303) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,311 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3890 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.0579 (1.0550) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4286 (0.4302) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,318 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3628 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.0575 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.420 
Loss G: 0.4289 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,325 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3871 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.0566 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.444 
Loss G: 0.4293 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,331 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4070 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.0558 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4296 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,338 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3390 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0553 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.4298 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,345 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3420 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.0546 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.397 
Loss G: 0.4303 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,352 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3185 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.0531 (1.0551) Acc D Fake: 0.000% 
Loss D: 1.372 
Loss G: 0.4314 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,359 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2819 (0.3916) Acc D Real: 100.000% 
Loss D Fake: 1.0503 (1.0550) Acc D Fake: 0.000% 
Loss D: 1.332 
Loss G: 0.4333 (0.4302) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,366 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.2781 (0.3877) Acc D Real: 100.000% 
Loss D Fake: 1.0466 (1.0547) Acc D Fake: 0.000% 
Loss D: 1.325 
Loss G: 0.4355 (0.4304) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,374 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.2903 (0.3844) Acc D Real: 100.000% 
Loss D Fake: 1.0426 (1.0543) Acc D Fake: 0.000% 
Loss D: 1.333 
Loss G: 0.4380 (0.4307) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,381 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2423 (0.3798) Acc D Real: 100.000% 
Loss D Fake: 1.0381 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.280 
Loss G: 0.4411 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,388 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.2392 (0.3754) Acc D Real: 100.000% 
Loss D Fake: 1.0330 (1.0531) Acc D Fake: 0.000% 
Loss D: 1.272 
Loss G: 0.4447 (0.4314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,396 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.1613 (0.3689) Acc D Real: 100.000% 
Loss D Fake: 1.0268 (1.0523) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.4499 (0.4320) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,403 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1697 (0.3631) Acc D Real: 100.000% 
Loss D Fake: 1.0186 (1.0513) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.4554 (0.4327) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,412 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1677 (0.3575) Acc D Real: 100.000% 
Loss D Fake: 1.0136 (1.0502) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.4594 (0.4335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,420 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1412 (0.3515) Acc D Real: 100.000% 
Loss D Fake: 1.0166 (1.0493) Acc D Fake: 0.000% 
Loss D: 1.158 
Loss G: 0.4620 (0.4342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,427 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.1502 (0.3460) Acc D Real: 100.000% 
Loss D Fake: 1.0288 (1.0488) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.4625 (0.4350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,435 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1558 (0.3410) Acc D Real: 100.000% 
Loss D Fake: 1.0677 (1.0493) Acc D Fake: 0.000% 
Loss D: 1.223 
Loss G: 0.4654 (0.4358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,443 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1384 (0.3358) Acc D Real: 100.000% 
Loss D Fake: 1.0467 (1.0492) Acc D Fake: 0.000% 
Loss D: 1.185 
Loss G: 0.4926 (0.4373) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,450 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.1395 (0.3309) Acc D Real: 100.000% 
Loss D Fake: 0.9543 (1.0468) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.5225 (0.4394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,457 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1805 (0.3273) Acc D Real: 100.000% 
Loss D Fake: 0.9018 (1.0433) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.5476 (0.4420) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,465 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.2026 (0.3243) Acc D Real: 100.000% 
Loss D Fake: 0.8639 (1.0390) Acc D Fake: 0.000% 
Loss D: 1.066 
Loss G: 0.5706 (0.4451) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,473 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1740 (0.3208) Acc D Real: 100.000% 
Loss D Fake: 0.8323 (1.0342) Acc D Fake: 0.000% 
Loss D: 1.006 
Loss G: 0.5928 (0.4485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,480 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.1909 (0.3178) Acc D Real: 100.000% 
Loss D Fake: 0.8041 (1.0290) Acc D Fake: 0.000% 
Loss D: 0.995 
Loss G: 0.6145 (0.4523) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,488 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2447 (0.3162) Acc D Real: 99.998% 
Loss D Fake: 0.7790 (1.0234) Acc D Fake: 0.000% 
Loss D: 1.024 
Loss G: 0.6353 (0.4564) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:29,495 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3212 (0.3163) Acc D Real: 99.995% 
Loss D Fake: 0.7569 (1.0176) Acc D Fake: 0.000% 
Loss D: 1.078 
Loss G: 0.6539 (0.4607) Acc G: 98.659% 
LR: 2.000e-04 

2023-03-01 13:45:29,503 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3457 (0.3170) Acc D Real: 99.813% 
Loss D Fake: 0.7385 (1.0117) Acc D Fake: 1.702% 
Loss D: 1.084 
Loss G: 0.6697 (0.4651) Acc G: 96.986% 
LR: 2.000e-04 

2023-03-01 13:45:29,511 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.2986 (0.3166) Acc D Real: 99.243% 
Loss D Fake: 0.7234 (1.0057) Acc D Fake: 3.333% 
Loss D: 1.022 
Loss G: 0.6837 (0.4697) Acc G: 95.347% 
LR: 2.000e-04 

2023-03-01 13:45:29,518 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2114 (0.3144) Acc D Real: 98.909% 
Loss D Fake: 0.7096 (0.9996) Acc D Fake: 4.932% 
Loss D: 0.921 
Loss G: 0.6980 (0.4743) Acc G: 93.776% 
LR: 2.000e-04 

2023-03-01 13:45:29,525 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3841 (0.3158) Acc D Real: 98.135% 
Loss D Fake: 0.6967 (0.9936) Acc D Fake: 6.467% 
Loss D: 1.081 
Loss G: 0.7099 (0.4790) Acc G: 92.267% 
LR: 2.000e-04 

2023-03-01 13:45:29,533 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2934 (0.3154) Acc D Real: 97.630% 
Loss D Fake: 0.6867 (0.9876) Acc D Fake: 7.941% 
Loss D: 0.980 
Loss G: 0.7203 (0.4838) Acc G: 90.817% 
LR: 2.000e-04 

2023-03-01 13:45:29,540 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3510 (0.3161) Acc D Real: 97.013% 
Loss D Fake: 0.6780 (0.9816) Acc D Fake: 9.359% 
Loss D: 1.029 
Loss G: 0.7289 (0.4885) Acc G: 89.397% 
LR: 2.000e-04 

2023-03-01 13:45:29,548 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4032 (0.3177) Acc D Real: 96.305% 
Loss D Fake: 0.6718 (0.9758) Acc D Fake: 10.723% 
Loss D: 1.075 
Loss G: 0.7344 (0.4931) Acc G: 88.056% 
LR: 2.000e-04 

2023-03-01 13:45:29,556 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3866 (0.3190) Acc D Real: 95.669% 
Loss D Fake: 0.6731 (0.9702) Acc D Fake: 12.037% 
Loss D: 1.060 
Loss G: 0.7278 (0.4975) Acc G: 86.796% 
LR: 2.000e-04 

2023-03-01 13:45:29,563 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2906 (0.3185) Acc D Real: 95.277% 
Loss D Fake: 0.7089 (0.9654) Acc D Fake: 13.212% 
Loss D: 0.999 
Loss G: 0.0969 (0.4902) Acc G: 87.036% 
LR: 2.000e-04 

2023-03-01 13:45:29,571 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.1825 (0.3160) Acc D Real: 95.135% 
Loss D Fake: 2.9979 (1.0017) Acc D Fake: 12.976% 
Loss D: 3.180 
Loss G: 0.0908 (0.4831) Acc G: 87.267% 
LR: 2.000e-04 

2023-03-01 13:45:29,579 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.1053 (0.3123) Acc D Real: 95.174% 
Loss D Fake: 2.9340 (1.0356) Acc D Fake: 12.749% 
Loss D: 3.039 
Loss G: 0.0897 (0.4762) Acc G: 87.491% 
LR: 2.000e-04 

2023-03-01 13:45:29,586 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.1047 (0.3088) Acc D Real: 95.216% 
Loss D Fake: 2.8621 (1.0671) Acc D Fake: 12.529% 
Loss D: 2.967 
Loss G: 0.0909 (0.4695) Acc G: 87.707% 
LR: 2.000e-04 

2023-03-01 13:45:29,594 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.0865 (0.3050) Acc D Real: 95.297% 
Loss D Fake: 2.7916 (1.0963) Acc D Fake: 12.316% 
Loss D: 2.878 
Loss G: 0.0934 (0.4631) Acc G: 87.915% 
LR: 2.000e-04 

2023-03-01 13:45:29,601 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.0889 (0.3014) Acc D Real: 95.375% 
Loss D Fake: 2.7197 (1.1234) Acc D Fake: 12.111% 
Loss D: 2.809 
Loss G: 0.0968 (0.4570) Acc G: 88.116% 
LR: 2.000e-04 

2023-03-01 13:45:29,609 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.0930 (0.2980) Acc D Real: 95.451% 
Loss D Fake: 2.6466 (1.1483) Acc D Fake: 11.913% 
Loss D: 2.740 
Loss G: 0.1009 (0.4512) Acc G: 88.311% 
LR: 2.000e-04 

2023-03-01 13:45:29,616 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.0970 (0.2947) Acc D Real: 95.524% 
Loss D Fake: 2.5732 (1.1713) Acc D Fake: 11.720% 
Loss D: 2.670 
Loss G: 0.1056 (0.4456) Acc G: 88.500% 
LR: 2.000e-04 

2023-03-01 13:45:29,624 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.1028 (0.2917) Acc D Real: 95.595% 
Loss D Fake: 2.5004 (1.1924) Acc D Fake: 11.534% 
Loss D: 2.603 
Loss G: 0.1109 (0.4403) Acc G: 88.682% 
LR: 2.000e-04 

2023-03-01 13:45:29,632 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.1080 (0.2888) Acc D Real: 95.664% 
Loss D Fake: 2.4290 (1.2117) Acc D Fake: 11.354% 
Loss D: 2.537 
Loss G: 0.1165 (0.4352) Acc G: 88.859% 
LR: 2.000e-04 

2023-03-01 13:45:29,639 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.1131 (0.2861) Acc D Real: 95.731% 
Loss D Fake: 2.3599 (1.2294) Acc D Fake: 11.179% 
Loss D: 2.473 
Loss G: 0.1226 (0.4304) Acc G: 89.030% 
LR: 2.000e-04 

2023-03-01 13:45:29,647 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1203 (0.2836) Acc D Real: 95.795% 
Loss D Fake: 2.2935 (1.2455) Acc D Fake: 11.010% 
Loss D: 2.414 
Loss G: 0.1288 (0.4259) Acc G: 89.197% 
LR: 2.000e-04 

2023-03-01 13:45:29,654 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1262 (0.2813) Acc D Real: 95.858% 
Loss D Fake: 2.2304 (1.2602) Acc D Fake: 10.846% 
Loss D: 2.357 
Loss G: 0.1353 (0.4215) Acc G: 89.358% 
LR: 2.000e-04 

2023-03-01 13:45:29,661 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1332 (0.2791) Acc D Real: 95.919% 
Loss D Fake: 2.1706 (1.2736) Acc D Fake: 10.686% 
Loss D: 2.304 
Loss G: 0.1419 (0.4174) Acc G: 89.514% 
LR: 2.000e-04 

2023-03-01 13:45:29,669 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.1398 (0.2771) Acc D Real: 95.978% 
Loss D Fake: 2.1144 (1.2858) Acc D Fake: 10.531% 
Loss D: 2.254 
Loss G: 0.1485 (0.4135) Acc G: 89.666% 
LR: 2.000e-04 

2023-03-01 13:45:29,676 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1461 (0.2752) Acc D Real: 96.036% 
Loss D Fake: 2.0618 (1.2969) Acc D Fake: 10.381% 
Loss D: 2.208 
Loss G: 0.1552 (0.4098) Acc G: 89.814% 
LR: 2.000e-04 

2023-03-01 13:45:29,684 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.1529 (0.2735) Acc D Real: 96.092% 
Loss D Fake: 2.0127 (1.3070) Acc D Fake: 10.235% 
Loss D: 2.166 
Loss G: 0.1618 (0.4063) Acc G: 89.957% 
LR: 2.000e-04 

2023-03-01 13:45:29,691 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1599 (0.2719) Acc D Real: 96.146% 
Loss D Fake: 1.9670 (1.3161) Acc D Fake: 10.093% 
Loss D: 2.127 
Loss G: 0.1683 (0.4030) Acc G: 90.097% 
LR: 2.000e-04 

2023-03-01 13:45:29,699 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.1664 (0.2704) Acc D Real: 96.199% 
Loss D Fake: 1.9244 (1.3245) Acc D Fake: 9.954% 
Loss D: 2.091 
Loss G: 0.1747 (0.3999) Acc G: 90.233% 
LR: 2.000e-04 

2023-03-01 13:45:29,706 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1724 (0.2691) Acc D Real: 96.250% 
Loss D Fake: 1.8849 (1.3320) Acc D Fake: 9.820% 
Loss D: 2.057 
Loss G: 0.1809 (0.3969) Acc G: 90.365% 
LR: 2.000e-04 

2023-03-01 13:45:29,714 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.1787 (0.2679) Acc D Real: 96.300% 
Loss D Fake: 1.8483 (1.3389) Acc D Fake: 9.689% 
Loss D: 2.027 
Loss G: 0.1869 (0.3941) Acc G: 90.493% 
LR: 2.000e-04 

2023-03-01 13:45:29,721 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.1850 (0.2668) Acc D Real: 96.349% 
Loss D Fake: 1.8143 (1.3452) Acc D Fake: 9.561% 
Loss D: 1.999 
Loss G: 0.1928 (0.3915) Acc G: 90.618% 
LR: 2.000e-04 

2023-03-01 13:45:29,728 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.1910 (0.2658) Acc D Real: 96.396% 
Loss D Fake: 1.7827 (1.3509) Acc D Fake: 9.437% 
Loss D: 1.974 
Loss G: 0.1985 (0.3890) Acc G: 90.740% 
LR: 2.000e-04 

2023-03-01 13:45:29,736 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1968 (0.2650) Acc D Real: 96.442% 
Loss D Fake: 1.7534 (1.3560) Acc D Fake: 9.316% 
Loss D: 1.950 
Loss G: 0.2040 (0.3866) Acc G: 90.859% 
LR: 2.000e-04 

2023-03-01 13:45:29,743 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.2022 (0.2642) Acc D Real: 96.487% 
Loss D Fake: 1.7261 (1.3607) Acc D Fake: 9.198% 
Loss D: 1.928 
Loss G: 0.2093 (0.3844) Acc G: 90.974% 
LR: 2.000e-04 

2023-03-01 13:45:29,751 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.2075 (0.2635) Acc D Real: 96.531% 
Loss D Fake: 1.7008 (1.3650) Acc D Fake: 9.083% 
Loss D: 1.908 
Loss G: 0.2144 (0.3822) Acc G: 91.087% 
LR: 2.000e-04 

2023-03-01 13:45:29,758 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.2126 (0.2628) Acc D Real: 96.574% 
Loss D Fake: 1.6771 (1.3688) Acc D Fake: 8.971% 
Loss D: 1.890 
Loss G: 0.2193 (0.3802) Acc G: 91.197% 
LR: 2.000e-04 

2023-03-01 13:45:29,766 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2175 (0.2623) Acc D Real: 96.616% 
Loss D Fake: 1.6551 (1.3723) Acc D Fake: 8.862% 
Loss D: 1.873 
Loss G: 0.2240 (0.3783) Acc G: 91.305% 
LR: 2.000e-04 

2023-03-01 13:45:29,773 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.2221 (0.2618) Acc D Real: 96.657% 
Loss D Fake: 1.6344 (1.3755) Acc D Fake: 8.755% 
Loss D: 1.857 
Loss G: 0.2286 (0.3765) Acc G: 91.409% 
LR: 2.000e-04 

2023-03-01 13:45:29,781 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.2270 (0.2614) Acc D Real: 96.696% 
Loss D Fake: 1.6152 (1.3783) Acc D Fake: 8.651% 
Loss D: 1.842 
Loss G: 0.2329 (0.3748) Acc G: 91.512% 
LR: 2.000e-04 

2023-03-01 13:45:29,788 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.2311 (0.2610) Acc D Real: 96.735% 
Loss D Fake: 1.5971 (1.3809) Acc D Fake: 8.549% 
Loss D: 1.828 
Loss G: 0.2371 (0.3732) Acc G: 91.612% 
LR: 2.000e-04 

2023-03-01 13:45:29,795 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.2352 (0.2607) Acc D Real: 96.773% 
Loss D Fake: 1.5801 (1.3832) Acc D Fake: 8.450% 
Loss D: 1.815 
Loss G: 0.2411 (0.3717) Acc G: 91.709% 
LR: 2.000e-04 

2023-03-01 13:45:29,803 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.2393 (0.2605) Acc D Real: 96.810% 
Loss D Fake: 1.5641 (1.3853) Acc D Fake: 8.352% 
Loss D: 1.803 
Loss G: 0.2450 (0.3702) Acc G: 91.804% 
LR: 2.000e-04 

2023-03-01 13:45:29,810 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.2432 (0.2603) Acc D Real: 96.847% 
Loss D Fake: 1.5490 (1.3872) Acc D Fake: 8.258% 
Loss D: 1.792 
Loss G: 0.2488 (0.3688) Acc G: 91.897% 
LR: 2.000e-04 

2023-03-01 13:45:29,817 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.2468 (0.2601) Acc D Real: 96.882% 
Loss D Fake: 1.5348 (1.3888) Acc D Fake: 8.165% 
Loss D: 1.782 
Loss G: 0.2524 (0.3675) Acc G: 91.989% 
LR: 2.000e-04 

2023-03-01 13:45:29,825 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.2503 (0.2600) Acc D Real: 96.917% 
Loss D Fake: 1.5214 (1.3903) Acc D Fake: 8.074% 
Loss D: 1.772 
Loss G: 0.2558 (0.3663) Acc G: 92.078% 
LR: 2.000e-04 

2023-03-01 13:45:29,832 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.2540 (0.2599) Acc D Real: 96.951% 
Loss D Fake: 1.5087 (1.3916) Acc D Fake: 7.985% 
Loss D: 1.763 
Loss G: 0.2592 (0.3651) Acc G: 92.165% 
LR: 2.000e-04 

2023-03-01 13:45:29,839 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.2572 (0.2599) Acc D Real: 96.984% 
Loss D Fake: 1.4966 (1.3927) Acc D Fake: 7.899% 
Loss D: 1.754 
Loss G: 0.2624 (0.3640) Acc G: 92.250% 
LR: 2.000e-04 

2023-03-01 13:45:29,847 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2604 (0.2599) Acc D Real: 97.016% 
Loss D Fake: 1.4852 (1.3937) Acc D Fake: 7.814% 
Loss D: 1.746 
Loss G: 0.2655 (0.3629) Acc G: 92.333% 
LR: 2.000e-04 

2023-03-01 13:45:29,854 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.2634 (0.2600) Acc D Real: 97.048% 
Loss D Fake: 1.4743 (1.3946) Acc D Fake: 7.730% 
Loss D: 1.738 
Loss G: 0.2685 (0.3619) Acc G: 92.415% 
LR: 2.000e-04 

2023-03-01 13:45:29,861 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.2665 (0.2600) Acc D Real: 97.079% 
Loss D Fake: 1.4639 (1.3953) Acc D Fake: 7.649% 
Loss D: 1.730 
Loss G: 0.2714 (0.3610) Acc G: 92.495% 
LR: 2.000e-04 

2023-03-01 13:45:29,868 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2695 (0.2601) Acc D Real: 97.109% 
Loss D Fake: 1.4540 (1.3959) Acc D Fake: 7.569% 
Loss D: 1.724 
Loss G: 0.2742 (0.3601) Acc G: 92.573% 
LR: 2.000e-04 

2023-03-01 13:45:29,876 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.2721 (0.2603) Acc D Real: 97.139% 
Loss D Fake: 1.4446 (1.3964) Acc D Fake: 7.491% 
Loss D: 1.717 
Loss G: 0.2769 (0.3592) Acc G: 92.649% 
LR: 2.000e-04 

2023-03-01 13:45:29,883 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2747 (0.2604) Acc D Real: 97.168% 
Loss D Fake: 1.4356 (1.3968) Acc D Fake: 7.415% 
Loss D: 1.710 
Loss G: 0.2796 (0.3584) Acc G: 92.724% 
LR: 2.000e-04 

2023-03-01 13:45:29,890 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2773 (0.2606) Acc D Real: 97.197% 
Loss D Fake: 1.4269 (1.3971) Acc D Fake: 7.340% 
Loss D: 1.704 
Loss G: 0.2821 (0.3576) Acc G: 92.798% 
LR: 2.000e-04 

2023-03-01 13:45:29,897 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2798 (0.2608) Acc D Real: 97.225% 
Loss D Fake: 1.4187 (1.3973) Acc D Fake: 7.267% 
Loss D: 1.698 
Loss G: 0.2846 (0.3569) Acc G: 92.870% 
LR: 2.000e-04 

2023-03-01 13:45:29,905 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2824 (0.2610) Acc D Real: 97.252% 
Loss D Fake: 1.4107 (1.3975) Acc D Fake: 7.195% 
Loss D: 1.693 
Loss G: 0.2870 (0.3562) Acc G: 92.940% 
LR: 2.000e-04 

2023-03-01 13:45:29,912 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2848 (0.2612) Acc D Real: 97.279% 
Loss D Fake: 1.4031 (1.3975) Acc D Fake: 7.124% 
Loss D: 1.688 
Loss G: 0.2893 (0.3555) Acc G: 93.010% 
LR: 2.000e-04 

2023-03-01 13:45:29,920 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.2873 (0.2615) Acc D Real: 97.306% 
Loss D Fake: 1.3958 (1.3975) Acc D Fake: 7.055% 
Loss D: 1.683 
Loss G: 0.2916 (0.3549) Acc G: 93.077% 
LR: 2.000e-04 

2023-03-01 13:45:29,928 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2894 (0.2617) Acc D Real: 97.332% 
Loss D Fake: 1.3887 (1.3974) Acc D Fake: 6.987% 
Loss D: 1.678 
Loss G: 0.2938 (0.3543) Acc G: 93.144% 
LR: 2.000e-04 

2023-03-01 13:45:29,935 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2916 (0.2620) Acc D Real: 97.357% 
Loss D Fake: 1.3819 (1.3973) Acc D Fake: 6.921% 
Loss D: 1.674 
Loss G: 0.2960 (0.3538) Acc G: 93.209% 
LR: 2.000e-04 

2023-03-01 13:45:29,943 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.2935 (0.2623) Acc D Real: 97.382% 
Loss D Fake: 1.3754 (1.3971) Acc D Fake: 6.855% 
Loss D: 1.669 
Loss G: 0.2981 (0.3533) Acc G: 93.273% 
LR: 2.000e-04 

2023-03-01 13:45:29,950 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2956 (0.2626) Acc D Real: 97.407% 
Loss D Fake: 1.3691 (1.3968) Acc D Fake: 6.791% 
Loss D: 1.665 
Loss G: 0.3001 (0.3528) Acc G: 93.336% 
LR: 2.000e-04 

2023-03-01 13:45:29,958 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2975 (0.2629) Acc D Real: 97.431% 
Loss D Fake: 1.3630 (1.3965) Acc D Fake: 6.728% 
Loss D: 1.661 
Loss G: 0.3021 (0.3523) Acc G: 93.398% 
LR: 2.000e-04 

2023-03-01 13:45:29,965 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.2996 (0.2633) Acc D Real: 97.454% 
Loss D Fake: 1.3571 (1.3961) Acc D Fake: 6.667% 
Loss D: 1.657 
Loss G: 0.3040 (0.3518) Acc G: 93.459% 
LR: 2.000e-04 

2023-03-01 13:45:29,973 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3010 (0.2636) Acc D Real: 97.477% 
Loss D Fake: 1.3514 (1.3957) Acc D Fake: 6.606% 
Loss D: 1.652 
Loss G: 0.3059 (0.3514) Acc G: 93.518% 
LR: 2.000e-04 

2023-03-01 13:45:29,980 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3034 (0.2640) Acc D Real: 97.500% 
Loss D Fake: 1.3459 (1.3953) Acc D Fake: 6.547% 
Loss D: 1.649 
Loss G: 0.3077 (0.3510) Acc G: 93.576% 
LR: 2.000e-04 

2023-03-01 13:45:29,987 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3050 (0.2644) Acc D Real: 97.522% 
Loss D Fake: 1.3406 (1.3948) Acc D Fake: 6.488% 
Loss D: 1.646 
Loss G: 0.3095 (0.3507) Acc G: 93.634% 
LR: 2.000e-04 

2023-03-01 13:45:29,995 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3070 (0.2647) Acc D Real: 97.544% 
Loss D Fake: 1.3354 (1.3943) Acc D Fake: 6.431% 
Loss D: 1.642 
Loss G: 0.3113 (0.3503) Acc G: 93.690% 
LR: 2.000e-04 

2023-03-01 13:45:30,003 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3087 (0.2651) Acc D Real: 97.566% 
Loss D Fake: 1.3304 (1.3937) Acc D Fake: 6.374% 
Loss D: 1.639 
Loss G: 0.3130 (0.3500) Acc G: 93.745% 
LR: 2.000e-04 

2023-03-01 13:45:30,010 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3103 (0.2655) Acc D Real: 97.587% 
Loss D Fake: 1.3255 (1.3931) Acc D Fake: 6.319% 
Loss D: 1.636 
Loss G: 0.3147 (0.3497) Acc G: 93.800% 
LR: 2.000e-04 

2023-03-01 13:45:30,017 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3117 (0.2659) Acc D Real: 97.608% 
Loss D Fake: 1.3208 (1.3925) Acc D Fake: 6.264% 
Loss D: 1.633 
Loss G: 0.3163 (0.3494) Acc G: 93.853% 
LR: 2.000e-04 

2023-03-01 13:45:30,025 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3138 (0.2663) Acc D Real: 97.628% 
Loss D Fake: 1.3162 (1.3918) Acc D Fake: 6.211% 
Loss D: 1.630 
Loss G: 0.3179 (0.3491) Acc G: 93.906% 
LR: 2.000e-04 

2023-03-01 13:45:30,032 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3148 (0.2667) Acc D Real: 97.648% 
Loss D Fake: 1.3118 (1.3912) Acc D Fake: 6.158% 
Loss D: 1.627 
Loss G: 0.3195 (0.3489) Acc G: 93.957% 
LR: 2.000e-04 

2023-03-01 13:45:30,040 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3167 (0.2671) Acc D Real: 97.668% 
Loss D Fake: 1.3075 (1.3905) Acc D Fake: 6.106% 
Loss D: 1.624 
Loss G: 0.3210 (0.3486) Acc G: 94.008% 
LR: 2.000e-04 

2023-03-01 13:45:30,047 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3178 (0.2676) Acc D Real: 97.688% 
Loss D Fake: 1.3033 (1.3897) Acc D Fake: 6.056% 
Loss D: 1.621 
Loss G: 0.3225 (0.3484) Acc G: 94.058% 
LR: 2.000e-04 

2023-03-01 13:45:30,055 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3197 (0.2680) Acc D Real: 97.707% 
Loss D Fake: 1.2992 (1.3890) Acc D Fake: 6.006% 
Loss D: 1.619 
Loss G: 0.3240 (0.3482) Acc G: 94.107% 
LR: 2.000e-04 

2023-03-01 13:45:30,064 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3213 (0.2684) Acc D Real: 97.725% 
Loss D Fake: 1.2952 (1.3882) Acc D Fake: 5.956% 
Loss D: 1.616 
Loss G: 0.3254 (0.3480) Acc G: 94.156% 
LR: 2.000e-04 

2023-03-01 13:45:30,072 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3221 (0.2689) Acc D Real: 97.744% 
Loss D Fake: 1.2913 (1.3874) Acc D Fake: 5.908% 
Loss D: 1.613 
Loss G: 0.3268 (0.3479) Acc G: 94.203% 
LR: 2.000e-04 

2023-03-01 13:45:30,080 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3240 (0.2693) Acc D Real: 97.762% 
Loss D Fake: 1.2876 (1.3866) Acc D Fake: 5.860% 
Loss D: 1.612 
Loss G: 0.3282 (0.3477) Acc G: 94.250% 
LR: 2.000e-04 

2023-03-01 13:45:30,088 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3254 (0.2698) Acc D Real: 97.780% 
Loss D Fake: 1.2839 (1.3858) Acc D Fake: 5.813% 
Loss D: 1.609 
Loss G: 0.3295 (0.3475) Acc G: 94.296% 
LR: 2.000e-04 

2023-03-01 13:45:30,096 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3258 (0.2702) Acc D Real: 97.798% 
Loss D Fake: 1.2804 (1.3850) Acc D Fake: 5.767% 
Loss D: 1.606 
Loss G: 0.3308 (0.3474) Acc G: 94.341% 
LR: 2.000e-04 

2023-03-01 13:45:30,103 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3277 (0.2707) Acc D Real: 97.815% 
Loss D Fake: 1.2769 (1.3841) Acc D Fake: 5.722% 
Loss D: 1.605 
Loss G: 0.3321 (0.3473) Acc G: 94.386% 
LR: 2.000e-04 

2023-03-01 13:45:30,112 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3290 (0.2711) Acc D Real: 97.832% 
Loss D Fake: 1.2735 (1.3832) Acc D Fake: 5.677% 
Loss D: 1.603 
Loss G: 0.3334 (0.3472) Acc G: 94.430% 
LR: 2.000e-04 

2023-03-01 13:45:30,120 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3300 (0.2716) Acc D Real: 97.849% 
Loss D Fake: 1.2702 (1.3824) Acc D Fake: 5.633% 
Loss D: 1.600 
Loss G: 0.3346 (0.3471) Acc G: 94.473% 
LR: 2.000e-04 

2023-03-01 13:45:30,129 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3313 (0.2720) Acc D Real: 97.865% 
Loss D Fake: 1.2669 (1.3815) Acc D Fake: 5.590% 
Loss D: 1.598 
Loss G: 0.3358 (0.3470) Acc G: 94.515% 
LR: 2.000e-04 

2023-03-01 13:45:30,137 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3330 (0.2725) Acc D Real: 97.882% 
Loss D Fake: 1.2638 (1.3806) Acc D Fake: 5.547% 
Loss D: 1.597 
Loss G: 0.3370 (0.3469) Acc G: 94.557% 
LR: 2.000e-04 

2023-03-01 13:45:30,145 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3336 (0.2730) Acc D Real: 97.898% 
Loss D Fake: 1.2607 (1.3797) Acc D Fake: 5.505% 
Loss D: 1.594 
Loss G: 0.3382 (0.3469) Acc G: 94.598% 
LR: 2.000e-04 

2023-03-01 13:45:30,153 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3350 (0.2734) Acc D Real: 97.914% 
Loss D Fake: 1.2577 (1.3788) Acc D Fake: 5.464% 
Loss D: 1.593 
Loss G: 0.3393 (0.3468) Acc G: 94.639% 
LR: 2.000e-04 

2023-03-01 13:45:30,160 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3356 (0.2739) Acc D Real: 97.929% 
Loss D Fake: 1.2548 (1.3778) Acc D Fake: 5.423% 
Loss D: 1.590 
Loss G: 0.3405 (0.3468) Acc G: 94.679% 
LR: 2.000e-04 

2023-03-01 13:45:30,168 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3371 (0.2744) Acc D Real: 97.944% 
Loss D Fake: 1.2519 (1.3769) Acc D Fake: 5.383% 
Loss D: 1.589 
Loss G: 0.3416 (0.3467) Acc G: 94.718% 
LR: 2.000e-04 

2023-03-01 13:45:30,175 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3385 (0.2748) Acc D Real: 97.960% 
Loss D Fake: 1.2491 (1.3760) Acc D Fake: 5.343% 
Loss D: 1.588 
Loss G: 0.3426 (0.3467) Acc G: 94.757% 
LR: 2.000e-04 

2023-03-01 13:45:30,183 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3397 (0.2753) Acc D Real: 97.974% 
Loss D Fake: 1.2464 (1.3750) Acc D Fake: 5.304% 
Loss D: 1.586 
Loss G: 0.3437 (0.3467) Acc G: 94.795% 
LR: 2.000e-04 

2023-03-01 13:45:30,190 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3402 (0.2758) Acc D Real: 97.989% 
Loss D Fake: 1.2437 (1.3741) Acc D Fake: 5.266% 
Loss D: 1.584 
Loss G: 0.3447 (0.3467) Acc G: 94.833% 
LR: 2.000e-04 

2023-03-01 13:45:30,198 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3409 (0.2762) Acc D Real: 98.004% 
Loss D Fake: 1.2411 (1.3731) Acc D Fake: 5.228% 
Loss D: 1.582 
Loss G: 0.3458 (0.3466) Acc G: 94.870% 
LR: 2.000e-04 

2023-03-01 13:45:30,207 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3427 (0.2767) Acc D Real: 98.018% 
Loss D Fake: 1.2385 (1.3721) Acc D Fake: 5.190% 
Loss D: 1.581 
Loss G: 0.3468 (0.3466) Acc G: 94.907% 
LR: 2.000e-04 

2023-03-01 13:45:30,215 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3431 (0.2772) Acc D Real: 98.032% 
Loss D Fake: 1.2360 (1.3712) Acc D Fake: 5.154% 
Loss D: 1.579 
Loss G: 0.3478 (0.3467) Acc G: 94.943% 
LR: 2.000e-04 

2023-03-01 13:45:30,224 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3445 (0.2777) Acc D Real: 98.046% 
Loss D Fake: 1.2336 (1.3702) Acc D Fake: 5.117% 
Loss D: 1.578 
Loss G: 0.3487 (0.3467) Acc G: 94.979% 
LR: 2.000e-04 

2023-03-01 13:45:30,233 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3448 (0.2781) Acc D Real: 98.059% 
Loss D Fake: 1.2312 (1.3692) Acc D Fake: 5.082% 
Loss D: 1.576 
Loss G: 0.3497 (0.3467) Acc G: 95.014% 
LR: 2.000e-04 

2023-03-01 13:45:30,241 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3464 (0.2786) Acc D Real: 98.073% 
Loss D Fake: 1.2288 (1.3683) Acc D Fake: 5.046% 
Loss D: 1.575 
Loss G: 0.3506 (0.3467) Acc G: 95.048% 
LR: 2.000e-04 

2023-03-01 13:45:30,250 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3478 (0.2791) Acc D Real: 98.086% 
Loss D Fake: 1.2265 (1.3673) Acc D Fake: 5.011% 
Loss D: 1.574 
Loss G: 0.3515 (0.3468) Acc G: 95.083% 
LR: 2.000e-04 

2023-03-01 13:45:30,258 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3486 (0.2796) Acc D Real: 98.099% 
Loss D Fake: 1.2243 (1.3663) Acc D Fake: 4.977% 
Loss D: 1.573 
Loss G: 0.3524 (0.3468) Acc G: 95.116% 
LR: 2.000e-04 

2023-03-01 13:45:30,267 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3494 (0.2800) Acc D Real: 98.112% 
Loss D Fake: 1.2221 (1.3653) Acc D Fake: 4.943% 
Loss D: 1.572 
Loss G: 0.3533 (0.3468) Acc G: 95.150% 
LR: 2.000e-04 

2023-03-01 13:45:30,275 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3493 (0.2805) Acc D Real: 98.125% 
Loss D Fake: 1.2200 (1.3643) Acc D Fake: 4.910% 
Loss D: 1.569 
Loss G: 0.3542 (0.3469) Acc G: 95.182% 
LR: 2.000e-04 

2023-03-01 13:45:30,284 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3507 (0.2810) Acc D Real: 98.138% 
Loss D Fake: 1.2179 (1.3634) Acc D Fake: 4.877% 
Loss D: 1.569 
Loss G: 0.3550 (0.3469) Acc G: 95.215% 
LR: 2.000e-04 

2023-03-01 13:45:30,292 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3518 (0.2814) Acc D Real: 98.150% 
Loss D Fake: 1.2159 (1.3624) Acc D Fake: 4.844% 
Loss D: 1.568 
Loss G: 0.3558 (0.3470) Acc G: 95.247% 
LR: 2.000e-04 

2023-03-01 13:45:30,299 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3526 (0.2819) Acc D Real: 98.162% 
Loss D Fake: 1.2139 (1.3614) Acc D Fake: 4.812% 
Loss D: 1.566 
Loss G: 0.3567 (0.3471) Acc G: 95.278% 
LR: 2.000e-04 

2023-03-01 13:45:30,307 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3526 (0.2824) Acc D Real: 98.174% 
Loss D Fake: 1.2119 (1.3604) Acc D Fake: 4.781% 
Loss D: 1.565 
Loss G: 0.3575 (0.3471) Acc G: 95.309% 
LR: 2.000e-04 

2023-03-01 13:45:30,314 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3532 (0.2828) Acc D Real: 98.186% 
Loss D Fake: 1.2100 (1.3594) Acc D Fake: 4.749% 
Loss D: 1.563 
Loss G: 0.3583 (0.3472) Acc G: 95.340% 
LR: 2.000e-04 

2023-03-01 13:45:30,322 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3534 (0.2833) Acc D Real: 98.198% 
Loss D Fake: 1.2081 (1.3584) Acc D Fake: 4.719% 
Loss D: 1.561 
Loss G: 0.3591 (0.3473) Acc G: 95.370% 
LR: 2.000e-04 

2023-03-01 13:45:30,329 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3548 (0.2838) Acc D Real: 98.210% 
Loss D Fake: 1.2062 (1.3575) Acc D Fake: 4.688% 
Loss D: 1.561 
Loss G: 0.3598 (0.3474) Acc G: 95.400% 
LR: 2.000e-04 

2023-03-01 13:45:30,336 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3564 (0.2842) Acc D Real: 98.221% 
Loss D Fake: 1.2043 (1.3565) Acc D Fake: 4.658% 
Loss D: 1.561 
Loss G: 0.3606 (0.3474) Acc G: 95.429% 
LR: 2.000e-04 

2023-03-01 13:45:30,344 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3572 (0.2847) Acc D Real: 98.232% 
Loss D Fake: 1.2025 (1.3555) Acc D Fake: 4.628% 
Loss D: 1.560 
Loss G: 0.3614 (0.3475) Acc G: 95.458% 
LR: 2.000e-04 

2023-03-01 13:45:30,351 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3580 (0.2852) Acc D Real: 98.244% 
Loss D Fake: 1.2008 (1.3545) Acc D Fake: 4.599% 
Loss D: 1.559 
Loss G: 0.3621 (0.3476) Acc G: 95.487% 
LR: 2.000e-04 

2023-03-01 13:45:30,358 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3586 (0.2856) Acc D Real: 98.255% 
Loss D Fake: 1.1990 (1.3535) Acc D Fake: 4.570% 
Loss D: 1.558 
Loss G: 0.3628 (0.3477) Acc G: 95.516% 
LR: 2.000e-04 

2023-03-01 13:45:30,365 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3594 (0.2861) Acc D Real: 98.266% 
Loss D Fake: 1.1974 (1.3526) Acc D Fake: 4.542% 
Loss D: 1.557 
Loss G: 0.3635 (0.3478) Acc G: 95.544% 
LR: 2.000e-04 

2023-03-01 13:45:30,373 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3594 (0.2865) Acc D Real: 98.276% 
Loss D Fake: 1.1957 (1.3516) Acc D Fake: 4.513% 
Loss D: 1.555 
Loss G: 0.3642 (0.3479) Acc G: 95.571% 
LR: 2.000e-04 

2023-03-01 13:45:30,380 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3615 (0.2870) Acc D Real: 98.287% 
Loss D Fake: 1.1941 (1.3506) Acc D Fake: 4.486% 
Loss D: 1.556 
Loss G: 0.3649 (0.3480) Acc G: 95.599% 
LR: 2.000e-04 

2023-03-01 13:45:30,388 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3615 (0.2875) Acc D Real: 98.298% 
Loss D Fake: 1.1925 (1.3496) Acc D Fake: 4.458% 
Loss D: 1.554 
Loss G: 0.3655 (0.3481) Acc G: 95.626% 
LR: 2.000e-04 

2023-03-01 13:45:30,395 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3623 (0.2879) Acc D Real: 98.308% 
Loss D Fake: 1.1910 (1.3487) Acc D Fake: 4.431% 
Loss D: 1.553 
Loss G: 0.3662 (0.3482) Acc G: 95.652% 
LR: 2.000e-04 

2023-03-01 13:45:30,402 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3630 (0.2884) Acc D Real: 98.318% 
Loss D Fake: 1.1895 (1.3477) Acc D Fake: 4.404% 
Loss D: 1.552 
Loss G: 0.3668 (0.3484) Acc G: 95.679% 
LR: 2.000e-04 

2023-03-01 13:45:30,410 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3638 (0.2888) Acc D Real: 98.328% 
Loss D Fake: 1.1880 (1.3468) Acc D Fake: 4.378% 
Loss D: 1.552 
Loss G: 0.3675 (0.3485) Acc G: 95.705% 
LR: 2.000e-04 

2023-03-01 13:45:30,417 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3634 (0.2893) Acc D Real: 98.338% 
Loss D Fake: 1.1865 (1.3458) Acc D Fake: 4.351% 
Loss D: 1.550 
Loss G: 0.3681 (0.3486) Acc G: 95.730% 
LR: 2.000e-04 

2023-03-01 13:45:30,424 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3634 (0.2897) Acc D Real: 98.348% 
Loss D Fake: 1.1851 (1.3448) Acc D Fake: 4.325% 
Loss D: 1.548 
Loss G: 0.3687 (0.3487) Acc G: 95.756% 
LR: 2.000e-04 

2023-03-01 13:45:30,432 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3659 (0.2902) Acc D Real: 98.358% 
Loss D Fake: 1.1837 (1.3439) Acc D Fake: 4.300% 
Loss D: 1.550 
Loss G: 0.3693 (0.3488) Acc G: 95.781% 
LR: 2.000e-04 

2023-03-01 13:45:30,439 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3663 (0.2906) Acc D Real: 98.368% 
Loss D Fake: 1.1823 (1.3429) Acc D Fake: 4.275% 
Loss D: 1.549 
Loss G: 0.3699 (0.3490) Acc G: 95.806% 
LR: 2.000e-04 

2023-03-01 13:45:30,446 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3668 (0.2911) Acc D Real: 98.377% 
Loss D Fake: 1.1810 (1.3420) Acc D Fake: 4.250% 
Loss D: 1.548 
Loss G: 0.3705 (0.3491) Acc G: 95.830% 
LR: 2.000e-04 

2023-03-01 13:45:30,454 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3667 (0.2915) Acc D Real: 98.387% 
Loss D Fake: 1.1796 (1.3410) Acc D Fake: 4.225% 
Loss D: 1.546 
Loss G: 0.3710 (0.3492) Acc G: 95.855% 
LR: 2.000e-04 

2023-03-01 13:45:30,461 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3672 (0.2919) Acc D Real: 98.396% 
Loss D Fake: 1.1783 (1.3401) Acc D Fake: 4.200% 
Loss D: 1.546 
Loss G: 0.3716 (0.3493) Acc G: 95.878% 
LR: 2.000e-04 

2023-03-01 13:45:30,468 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3678 (0.2924) Acc D Real: 98.405% 
Loss D Fake: 1.1771 (1.3392) Acc D Fake: 4.176% 
Loss D: 1.545 
Loss G: 0.3721 (0.3495) Acc G: 95.902% 
LR: 2.000e-04 

2023-03-01 13:45:30,476 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3682 (0.2928) Acc D Real: 98.414% 
Loss D Fake: 1.1758 (1.3382) Acc D Fake: 4.152% 
Loss D: 1.544 
Loss G: 0.3727 (0.3496) Acc G: 95.926% 
LR: 2.000e-04 

2023-03-01 13:45:30,483 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3702 (0.2932) Acc D Real: 98.423% 
Loss D Fake: 1.1746 (1.3373) Acc D Fake: 4.129% 
Loss D: 1.545 
Loss G: 0.3732 (0.3497) Acc G: 95.949% 
LR: 2.000e-04 

2023-03-01 13:45:30,490 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3696 (0.2937) Acc D Real: 98.432% 
Loss D Fake: 1.1734 (1.3364) Acc D Fake: 4.105% 
Loss D: 1.543 
Loss G: 0.3737 (0.3499) Acc G: 95.972% 
LR: 2.000e-04 

2023-03-01 13:45:30,498 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3716 (0.2941) Acc D Real: 98.441% 
Loss D Fake: 1.1722 (1.3355) Acc D Fake: 4.082% 
Loss D: 1.544 
Loss G: 0.3743 (0.3500) Acc G: 95.994% 
LR: 2.000e-04 

2023-03-01 13:45:30,505 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3710 (0.2945) Acc D Real: 98.450% 
Loss D Fake: 1.1711 (1.3345) Acc D Fake: 4.060% 
Loss D: 1.542 
Loss G: 0.3748 (0.3501) Acc G: 96.017% 
LR: 2.000e-04 

2023-03-01 13:45:30,512 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3704 (0.2950) Acc D Real: 98.458% 
Loss D Fake: 1.1699 (1.3336) Acc D Fake: 4.037% 
Loss D: 1.540 
Loss G: 0.3753 (0.3503) Acc G: 96.039% 
LR: 2.000e-04 

2023-03-01 13:45:30,520 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3712 (0.2954) Acc D Real: 98.467% 
Loss D Fake: 1.1688 (1.3327) Acc D Fake: 4.015% 
Loss D: 1.540 
Loss G: 0.3757 (0.3504) Acc G: 96.061% 
LR: 2.000e-04 

2023-03-01 13:45:30,528 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3723 (0.2958) Acc D Real: 98.475% 
Loss D Fake: 1.1677 (1.3318) Acc D Fake: 3.993% 
Loss D: 1.540 
Loss G: 0.3762 (0.3506) Acc G: 96.082% 
LR: 2.000e-04 

2023-03-01 13:45:30,535 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3726 (0.2962) Acc D Real: 98.484% 
Loss D Fake: 1.1666 (1.3309) Acc D Fake: 3.971% 
Loss D: 1.539 
Loss G: 0.3767 (0.3507) Acc G: 96.104% 
LR: 2.000e-04 

2023-03-01 13:45:30,542 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3745 (0.2967) Acc D Real: 98.492% 
Loss D Fake: 1.1656 (1.3300) Acc D Fake: 3.949% 
Loss D: 1.540 
Loss G: 0.3772 (0.3509) Acc G: 96.125% 
LR: 2.000e-04 

2023-03-01 13:45:30,550 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3744 (0.2971) Acc D Real: 98.500% 
Loss D Fake: 1.1645 (1.3291) Acc D Fake: 3.928% 
Loss D: 1.539 
Loss G: 0.3776 (0.3510) Acc G: 96.146% 
LR: 2.000e-04 

2023-03-01 13:45:30,557 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3739 (0.2975) Acc D Real: 98.508% 
Loss D Fake: 1.1635 (1.3282) Acc D Fake: 3.907% 
Loss D: 1.537 
Loss G: 0.3781 (0.3511) Acc G: 96.167% 
LR: 2.000e-04 

2023-03-01 13:45:30,565 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3729 (0.2979) Acc D Real: 98.516% 
Loss D Fake: 1.1625 (1.3273) Acc D Fake: 3.886% 
Loss D: 1.535 
Loss G: 0.3785 (0.3513) Acc G: 96.187% 
LR: 2.000e-04 

2023-03-01 13:45:30,572 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3753 (0.2983) Acc D Real: 98.524% 
Loss D Fake: 1.1615 (1.3264) Acc D Fake: 3.865% 
Loss D: 1.537 
Loss G: 0.3790 (0.3514) Acc G: 96.207% 
LR: 2.000e-04 

2023-03-01 13:45:30,580 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3761 (0.2987) Acc D Real: 98.532% 
Loss D Fake: 1.1605 (1.3256) Acc D Fake: 3.845% 
Loss D: 1.537 
Loss G: 0.3794 (0.3516) Acc G: 96.227% 
LR: 2.000e-04 

2023-03-01 13:45:30,587 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3773 (0.2991) Acc D Real: 98.539% 
Loss D Fake: 1.1596 (1.3247) Acc D Fake: 3.825% 
Loss D: 1.537 
Loss G: 0.3798 (0.3517) Acc G: 96.247% 
LR: 2.000e-04 

2023-03-01 13:45:30,595 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3768 (0.2995) Acc D Real: 98.547% 
Loss D Fake: 1.1587 (1.3238) Acc D Fake: 3.805% 
Loss D: 1.535 
Loss G: 0.3802 (0.3519) Acc G: 96.267% 
LR: 2.000e-04 

2023-03-01 13:45:30,602 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3778 (0.2999) Acc D Real: 98.555% 
Loss D Fake: 1.1578 (1.3230) Acc D Fake: 3.785% 
Loss D: 1.536 
Loss G: 0.3806 (0.3520) Acc G: 96.286% 
LR: 2.000e-04 

2023-03-01 13:45:30,611 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3783 (0.3003) Acc D Real: 98.562% 
Loss D Fake: 1.1569 (1.3221) Acc D Fake: 3.765% 
Loss D: 1.535 
Loss G: 0.3810 (0.3522) Acc G: 96.306% 
LR: 2.000e-04 

2023-03-01 13:45:30,620 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3780 (0.3007) Acc D Real: 98.570% 
Loss D Fake: 1.1560 (1.3212) Acc D Fake: 3.746% 
Loss D: 1.534 
Loss G: 0.3814 (0.3523) Acc G: 96.325% 
LR: 2.000e-04 

2023-03-01 13:45:30,629 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3784 (0.3011) Acc D Real: 98.577% 
Loss D Fake: 1.1552 (1.3204) Acc D Fake: 3.726% 
Loss D: 1.534 
Loss G: 0.3818 (0.3525) Acc G: 96.343% 
LR: 2.000e-04 

2023-03-01 13:45:30,637 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3789 (0.3015) Acc D Real: 98.584% 
Loss D Fake: 1.1544 (1.3195) Acc D Fake: 3.707% 
Loss D: 1.533 
Loss G: 0.3821 (0.3526) Acc G: 96.362% 
LR: 2.000e-04 

2023-03-01 13:45:30,644 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3794 (0.3019) Acc D Real: 98.591% 
Loss D Fake: 1.1536 (1.3187) Acc D Fake: 3.689% 
Loss D: 1.533 
Loss G: 0.3825 (0.3528) Acc G: 96.381% 
LR: 2.000e-04 

2023-03-01 13:45:30,653 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3795 (0.3023) Acc D Real: 98.598% 
Loss D Fake: 1.1528 (1.3179) Acc D Fake: 3.670% 
Loss D: 1.532 
Loss G: 0.3829 (0.3529) Acc G: 96.399% 
LR: 2.000e-04 

2023-03-01 13:45:30,661 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3793 (0.3027) Acc D Real: 98.606% 
Loss D Fake: 1.1520 (1.3170) Acc D Fake: 3.652% 
Loss D: 1.531 
Loss G: 0.3832 (0.3531) Acc G: 96.417% 
LR: 2.000e-04 

2023-03-01 13:45:30,669 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3804 (0.3031) Acc D Real: 98.612% 
Loss D Fake: 1.1512 (1.3162) Acc D Fake: 3.633% 
Loss D: 1.532 
Loss G: 0.3836 (0.3532) Acc G: 96.435% 
LR: 2.000e-04 

2023-03-01 13:45:30,678 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3800 (0.3035) Acc D Real: 98.619% 
Loss D Fake: 1.1504 (1.3154) Acc D Fake: 3.615% 
Loss D: 1.530 
Loss G: 0.3839 (0.3534) Acc G: 96.453% 
LR: 2.000e-04 

2023-03-01 13:45:30,685 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3812 (0.3039) Acc D Real: 98.626% 
Loss D Fake: 1.1497 (1.3146) Acc D Fake: 3.597% 
Loss D: 1.531 
Loss G: 0.3843 (0.3536) Acc G: 96.470% 
LR: 2.000e-04 

2023-03-01 13:45:30,693 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3820 (0.3043) Acc D Real: 98.633% 
Loss D Fake: 1.1489 (1.3137) Acc D Fake: 3.580% 
Loss D: 1.531 
Loss G: 0.3846 (0.3537) Acc G: 96.488% 
LR: 2.000e-04 

2023-03-01 13:45:30,700 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3811 (0.3046) Acc D Real: 98.640% 
Loss D Fake: 1.1482 (1.3129) Acc D Fake: 3.562% 
Loss D: 1.529 
Loss G: 0.3849 (0.3539) Acc G: 96.505% 
LR: 2.000e-04 

2023-03-01 13:45:30,707 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3819 (0.3050) Acc D Real: 98.646% 
Loss D Fake: 1.1475 (1.3121) Acc D Fake: 3.545% 
Loss D: 1.529 
Loss G: 0.3852 (0.3540) Acc G: 96.522% 
LR: 2.000e-04 

2023-03-01 13:45:30,714 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3842 (0.3054) Acc D Real: 98.653% 
Loss D Fake: 1.1468 (1.3113) Acc D Fake: 3.528% 
Loss D: 1.531 
Loss G: 0.3856 (0.3542) Acc G: 96.539% 
LR: 2.000e-04 

2023-03-01 13:45:30,721 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3829 (0.3058) Acc D Real: 98.659% 
Loss D Fake: 1.1461 (1.3105) Acc D Fake: 3.510% 
Loss D: 1.529 
Loss G: 0.3859 (0.3543) Acc G: 96.555% 
LR: 2.000e-04 

2023-03-01 13:45:30,729 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3835 (0.3061) Acc D Real: 98.666% 
Loss D Fake: 1.1455 (1.3097) Acc D Fake: 3.494% 
Loss D: 1.529 
Loss G: 0.3862 (0.3545) Acc G: 96.572% 
LR: 2.000e-04 

2023-03-01 13:45:30,736 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3837 (0.3065) Acc D Real: 98.672% 
Loss D Fake: 1.1448 (1.3089) Acc D Fake: 3.477% 
Loss D: 1.528 
Loss G: 0.3864 (0.3546) Acc G: 96.588% 
LR: 2.000e-04 

2023-03-01 13:45:30,743 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3844 (0.3069) Acc D Real: 98.679% 
Loss D Fake: 1.1442 (1.3082) Acc D Fake: 3.460% 
Loss D: 1.529 
Loss G: 0.3867 (0.3548) Acc G: 96.605% 
LR: 2.000e-04 

2023-03-01 13:45:30,751 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3848 (0.3073) Acc D Real: 98.685% 
Loss D Fake: 1.1435 (1.3074) Acc D Fake: 3.444% 
Loss D: 1.528 
Loss G: 0.3870 (0.3549) Acc G: 96.621% 
LR: 2.000e-04 

2023-03-01 13:45:30,758 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3852 (0.3076) Acc D Real: 98.691% 
Loss D Fake: 1.1429 (1.3066) Acc D Fake: 3.428% 
Loss D: 1.528 
Loss G: 0.3873 (0.3551) Acc G: 96.637% 
LR: 2.000e-04 

2023-03-01 13:45:30,766 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3847 (0.3080) Acc D Real: 98.697% 
Loss D Fake: 1.1423 (1.3058) Acc D Fake: 3.412% 
Loss D: 1.527 
Loss G: 0.3876 (0.3552) Acc G: 96.652% 
LR: 2.000e-04 

2023-03-01 13:45:30,773 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3854 (0.3083) Acc D Real: 98.703% 
Loss D Fake: 1.1418 (1.3051) Acc D Fake: 3.396% 
Loss D: 1.527 
Loss G: 0.3878 (0.3554) Acc G: 96.668% 
LR: 2.000e-04 

2023-03-01 13:45:30,780 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3848 (0.3087) Acc D Real: 98.709% 
Loss D Fake: 1.1412 (1.3043) Acc D Fake: 3.380% 
Loss D: 1.526 
Loss G: 0.3881 (0.3555) Acc G: 96.684% 
LR: 2.000e-04 

2023-03-01 13:45:30,788 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3854 (0.3091) Acc D Real: 98.715% 
Loss D Fake: 1.1406 (1.3035) Acc D Fake: 3.364% 
Loss D: 1.526 
Loss G: 0.3884 (0.3557) Acc G: 96.699% 
LR: 2.000e-04 

2023-03-01 13:45:30,795 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3863 (0.3094) Acc D Real: 98.721% 
Loss D Fake: 1.1400 (1.3028) Acc D Fake: 3.349% 
Loss D: 1.526 
Loss G: 0.3886 (0.3558) Acc G: 96.714% 
LR: 2.000e-04 

2023-03-01 13:45:30,803 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3859 (0.3098) Acc D Real: 98.727% 
Loss D Fake: 1.1395 (1.3020) Acc D Fake: 3.333% 
Loss D: 1.525 
Loss G: 0.3889 (0.3560) Acc G: 96.729% 
LR: 2.000e-04 

2023-03-01 13:45:30,810 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3882 (0.3101) Acc D Real: 98.733% 
Loss D Fake: 1.1389 (1.3013) Acc D Fake: 3.318% 
Loss D: 1.527 
Loss G: 0.3891 (0.3561) Acc G: 96.744% 
LR: 2.000e-04 

2023-03-01 13:45:30,817 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3862 (0.3105) Acc D Real: 98.739% 
Loss D Fake: 1.1384 (1.3006) Acc D Fake: 3.303% 
Loss D: 1.525 
Loss G: 0.3894 (0.3563) Acc G: 96.759% 
LR: 2.000e-04 

2023-03-01 13:45:30,825 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3874 (0.3108) Acc D Real: 98.744% 
Loss D Fake: 1.1379 (1.2998) Acc D Fake: 3.288% 
Loss D: 1.525 
Loss G: 0.3896 (0.3564) Acc G: 96.774% 
LR: 2.000e-04 

2023-03-01 13:45:30,832 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3891 (0.3112) Acc D Real: 98.750% 
Loss D Fake: 1.1374 (1.2991) Acc D Fake: 3.273% 
Loss D: 1.526 
Loss G: 0.3898 (0.3566) Acc G: 96.788% 
LR: 2.000e-04 

2023-03-01 13:45:30,839 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3884 (0.3115) Acc D Real: 98.756% 
Loss D Fake: 1.1369 (1.2984) Acc D Fake: 3.259% 
Loss D: 1.525 
Loss G: 0.3901 (0.3567) Acc G: 96.803% 
LR: 2.000e-04 

2023-03-01 13:45:30,847 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3879 (0.3119) Acc D Real: 98.761% 
Loss D Fake: 1.1364 (1.2976) Acc D Fake: 3.244% 
Loss D: 1.524 
Loss G: 0.3903 (0.3569) Acc G: 96.817% 
LR: 2.000e-04 

2023-03-01 13:45:30,854 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3880 (0.3122) Acc D Real: 98.767% 
Loss D Fake: 1.1359 (1.2969) Acc D Fake: 3.230% 
Loss D: 1.524 
Loss G: 0.3905 (0.3570) Acc G: 96.831% 
LR: 2.000e-04 

2023-03-01 13:45:30,861 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.3905 (0.3125) Acc D Real: 98.768% 
Loss D Fake: 1.1355 (1.2962) Acc D Fake: 3.226% 
Loss D: 1.526 
Loss G: 0.3907 (0.3572) Acc G: 96.835% 
LR: 2.000e-04 

2023-03-01 13:45:30,873 -                train: [    INFO] - 
Epoch: 11/20
2023-03-01 13:45:31,104 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3896 (0.3887) Acc D Real: 100.000% 
Loss D Fake: 1.1346 (1.1348) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3911 (0.3910) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,112 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3884 (0.3886) Acc D Real: 100.000% 
Loss D Fake: 1.1341 (1.1346) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3913 (0.3911) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,119 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3898 (0.3889) Acc D Real: 100.000% 
Loss D Fake: 1.1337 (1.1343) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3915 (0.3912) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,136 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3904 (0.3892) Acc D Real: 100.000% 
Loss D Fake: 1.1332 (1.1341) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3917 (0.3913) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,143 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3903 (0.3894) Acc D Real: 100.000% 
Loss D Fake: 1.1328 (1.1339) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3919 (0.3914) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,150 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3898 (0.3894) Acc D Real: 100.000% 
Loss D Fake: 1.1324 (1.1337) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3921 (0.3915) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,157 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3920 (0.3898) Acc D Real: 100.000% 
Loss D Fake: 1.1320 (1.1335) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3923 (0.3916) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,164 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3933 (0.3902) Acc D Real: 100.000% 
Loss D Fake: 1.1316 (1.1333) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3925 (0.3917) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,171 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3925 (0.3904) Acc D Real: 100.000% 
Loss D Fake: 1.1313 (1.1331) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3926 (0.3918) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,178 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3903 (0.3904) Acc D Real: 100.000% 
Loss D Fake: 1.1309 (1.1329) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3928 (0.3919) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,185 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3929 (0.3906) Acc D Real: 100.000% 
Loss D Fake: 1.1306 (1.1327) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3930 (0.3920) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,192 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3917 (0.3907) Acc D Real: 100.000% 
Loss D Fake: 1.1302 (1.1325) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3931 (0.3921) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,199 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3916 (0.3907) Acc D Real: 100.000% 
Loss D Fake: 1.1299 (1.1323) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3933 (0.3922) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,206 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3918 (0.3908) Acc D Real: 100.000% 
Loss D Fake: 1.1295 (1.1321) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3935 (0.3923) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,213 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3929 (0.3909) Acc D Real: 100.000% 
Loss D Fake: 1.1292 (1.1319) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3936 (0.3923) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,220 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3926 (0.3910) Acc D Real: 100.000% 
Loss D Fake: 1.1288 (1.1318) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3938 (0.3924) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,227 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3927 (0.3911) Acc D Real: 100.000% 
Loss D Fake: 1.1285 (1.1316) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3939 (0.3925) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,234 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3929 (0.3912) Acc D Real: 100.000% 
Loss D Fake: 1.1282 (1.1314) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3941 (0.3926) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,241 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3934 (0.3913) Acc D Real: 100.000% 
Loss D Fake: 1.1279 (1.1312) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3942 (0.3927) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,248 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3941 (0.3915) Acc D Real: 100.000% 
Loss D Fake: 1.1276 (1.1310) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3944 (0.3928) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,255 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3930 (0.3915) Acc D Real: 100.000% 
Loss D Fake: 1.1273 (1.1309) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3945 (0.3928) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,262 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3953 (0.3917) Acc D Real: 100.000% 
Loss D Fake: 1.1270 (1.1307) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3947 (0.3929) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,269 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3939 (0.3918) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.1305) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3948 (0.3930) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,276 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3933 (0.3919) Acc D Real: 100.000% 
Loss D Fake: 1.1264 (1.1304) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3949 (0.3931) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,284 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3947 (0.3920) Acc D Real: 100.000% 
Loss D Fake: 1.1261 (1.1302) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3951 (0.3931) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,291 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3965 (0.3921) Acc D Real: 100.000% 
Loss D Fake: 1.1258 (1.1300) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3952 (0.3932) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,298 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3943 (0.3922) Acc D Real: 100.000% 
Loss D Fake: 1.1256 (1.1299) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3953 (0.3933) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,305 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3959 (0.3923) Acc D Real: 100.000% 
Loss D Fake: 1.1253 (1.1297) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3954 (0.3934) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,312 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3959 (0.3925) Acc D Real: 100.000% 
Loss D Fake: 1.1251 (1.1296) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3955 (0.3934) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,320 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3946 (0.3925) Acc D Real: 100.000% 
Loss D Fake: 1.1248 (1.1294) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3956 (0.3935) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,327 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3947 (0.3926) Acc D Real: 100.000% 
Loss D Fake: 1.1246 (1.1293) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3957 (0.3936) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,334 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3954 (0.3927) Acc D Real: 100.000% 
Loss D Fake: 1.1244 (1.1291) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3959 (0.3937) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,342 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3943 (0.3927) Acc D Real: 100.000% 
Loss D Fake: 1.1241 (1.1290) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3960 (0.3937) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,349 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3969 (0.3928) Acc D Real: 100.000% 
Loss D Fake: 1.1239 (1.1288) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3961 (0.3938) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,357 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3959 (0.3929) Acc D Real: 100.000% 
Loss D Fake: 1.1236 (1.1287) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3962 (0.3939) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,364 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3959 (0.3930) Acc D Real: 100.000% 
Loss D Fake: 1.1234 (1.1285) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3963 (0.3939) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,371 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3967 (0.3931) Acc D Real: 100.000% 
Loss D Fake: 1.1232 (1.1284) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3964 (0.3940) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,379 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3980 (0.3932) Acc D Real: 100.000% 
Loss D Fake: 1.1230 (1.1283) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3965 (0.3941) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,386 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3974 (0.3933) Acc D Real: 100.000% 
Loss D Fake: 1.1228 (1.1281) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3966 (0.3941) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,393 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3977 (0.3934) Acc D Real: 100.000% 
Loss D Fake: 1.1226 (1.1280) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3967 (0.3942) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,401 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3973 (0.3935) Acc D Real: 100.000% 
Loss D Fake: 1.1224 (1.1278) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3968 (0.3942) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,408 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3982 (0.3936) Acc D Real: 100.000% 
Loss D Fake: 1.1222 (1.1277) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3969 (0.3943) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,415 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3965 (0.3937) Acc D Real: 100.000% 
Loss D Fake: 1.1220 (1.1276) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3969 (0.3944) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,423 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3995 (0.3938) Acc D Real: 100.000% 
Loss D Fake: 1.1218 (1.1275) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3970 (0.3944) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,430 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3972 (0.3939) Acc D Real: 100.000% 
Loss D Fake: 1.1217 (1.1273) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3971 (0.3945) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,437 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3979 (0.3940) Acc D Real: 100.000% 
Loss D Fake: 1.1215 (1.1272) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3972 (0.3945) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,445 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3989 (0.3941) Acc D Real: 100.000% 
Loss D Fake: 1.1214 (1.1271) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3973 (0.3946) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,452 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3983 (0.3942) Acc D Real: 100.000% 
Loss D Fake: 1.1212 (1.1270) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3973 (0.3946) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,460 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3995 (0.3943) Acc D Real: 100.000% 
Loss D Fake: 1.1211 (1.1269) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3974 (0.3947) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,467 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3986 (0.3944) Acc D Real: 100.000% 
Loss D Fake: 1.1209 (1.1267) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3975 (0.3948) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,475 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3982 (0.3944) Acc D Real: 100.000% 
Loss D Fake: 1.1208 (1.1266) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3975 (0.3948) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,482 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4001 (0.3946) Acc D Real: 100.000% 
Loss D Fake: 1.1206 (1.1265) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3976 (0.3949) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,490 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3986 (0.3946) Acc D Real: 100.000% 
Loss D Fake: 1.1205 (1.1264) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3976 (0.3949) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,497 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3984 (0.3947) Acc D Real: 100.000% 
Loss D Fake: 1.1204 (1.1263) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3977 (0.3950) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,504 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3998 (0.3948) Acc D Real: 100.000% 
Loss D Fake: 1.1203 (1.1262) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3978 (0.3950) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,512 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3991 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1201 (1.1261) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3978 (0.3951) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,519 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3993 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1200 (1.1260) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3979 (0.3951) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,526 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4004 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1199 (1.1259) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3979 (0.3952) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,534 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4002 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1198 (1.1258) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3980 (0.3952) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,541 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4007 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1197 (1.1257) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3980 (0.3953) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,548 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3986 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1196 (1.1256) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3981 (0.3953) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,556 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3994 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1195 (1.1255) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3981 (0.3953) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,563 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4015 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1194 (1.1254) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3982 (0.3954) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,570 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4004 (0.3955) Acc D Real: 100.000% 
Loss D Fake: 1.1193 (1.1253) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3982 (0.3954) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,577 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3995 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1192 (1.1252) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3983 (0.3955) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,585 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4013 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1191 (1.1251) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3983 (0.3955) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,592 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4003 (0.3957) Acc D Real: 100.000% 
Loss D Fake: 1.1190 (1.1250) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3983 (0.3956) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,600 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3991 (0.3958) Acc D Real: 100.000% 
Loss D Fake: 1.1189 (1.1249) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3984 (0.3956) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,607 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4015 (0.3958) Acc D Real: 100.000% 
Loss D Fake: 1.1188 (1.1248) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3984 (0.3956) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,614 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4012 (0.3959) Acc D Real: 100.000% 
Loss D Fake: 1.1187 (1.1247) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3985 (0.3957) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,622 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4014 (0.3960) Acc D Real: 100.000% 
Loss D Fake: 1.1186 (1.1247) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3985 (0.3957) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,629 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4021 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.1186 (1.1246) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3985 (0.3958) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,636 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4020 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.1185 (1.1245) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3986 (0.3958) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,644 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4010 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.1185 (1.1244) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3986 (0.3958) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,651 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4009 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1184 (1.1243) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3986 (0.3959) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,658 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4016 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1183 (1.1243) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3986 (0.3959) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,665 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4015 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1183 (1.1242) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3987 (0.3959) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,673 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4019 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1182 (1.1241) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3987 (0.3960) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,680 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4017 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1182 (1.1240) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3987 (0.3960) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,688 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4010 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1181 (1.1240) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3987 (0.3960) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,695 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4013 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1181 (1.1239) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3988 (0.3961) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,702 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4016 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1180 (1.1238) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3961) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,709 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4018 (0.3968) Acc D Real: 100.000% 
Loss D Fake: 1.1180 (1.1237) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3961) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,717 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4022 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1179 (1.1237) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3962) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,724 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4029 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1179 (1.1236) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3962) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,731 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4023 (0.3970) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1235) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3989 (0.3962) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,739 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4013 (0.3970) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1235) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3963) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,746 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4032 (0.3971) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1234) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3963) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,753 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4029 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1233) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3963) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,761 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4016 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1233) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,768 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4026 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1232) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3989 (0.3964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,775 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4038 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1232) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,783 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4014 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1231) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,790 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4020 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1230) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3990 (0.3965) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,798 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4015 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1230) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3965) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,805 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4031 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1229) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3965) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,813 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4034 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1229) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3965) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,820 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4019 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1228) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3966) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,827 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4019 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1228) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3966) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,835 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4029 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1227) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3990 (0.3966) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,842 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4019 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1227) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3966) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,849 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4033 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1226) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,856 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4029 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1226) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,864 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4026 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1225) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,871 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4032 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1225) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,878 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4028 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1224) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3967) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,885 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4036 (0.3981) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1224) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,892 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4033 (0.3981) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1223) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,900 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4020 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1223) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3991 (0.3968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,907 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4019 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1222) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3991 (0.3968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,914 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4038 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1222) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3969) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,921 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4033 (0.3983) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1221) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3969) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,928 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4029 (0.3983) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1221) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3969) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,935 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4035 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1221) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3969) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,943 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4036 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1220) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3969) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,950 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4026 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1220) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,957 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4020 (0.3985) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1219) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,964 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4031 (0.3985) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1219) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,972 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4037 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1218) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,979 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4030 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1218) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,986 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4038 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1218) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3970) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:31,993 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4041 (0.3987) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1217) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,000 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4033 (0.3987) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1217) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,007 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4031 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1217) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,015 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4041 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1216) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,022 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4022 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1216) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,029 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4030 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1216) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,036 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4044 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1215) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,044 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4043 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1215) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3992 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,051 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4029 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1214) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3992 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,058 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4045 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1214) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,065 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4036 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1214) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,072 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4041 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1213) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,080 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4031 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1213) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,087 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4031 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1213) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,094 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4041 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1213) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,101 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4025 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1212) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,109 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4039 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1212) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,116 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4038 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1212) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,123 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4031 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1211) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,131 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4037 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1211) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3973) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,138 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4037 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1211) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,145 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4034 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1211) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,152 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4034 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1210) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,159 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4041 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1210) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,167 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4044 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1210) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,174 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4042 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1210) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,181 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4039 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1209) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,188 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4037 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1209) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3991 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,195 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4034 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1209) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,203 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4042 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1209) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,210 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4046 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1208) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,217 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4031 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1208) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,225 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4040 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1208) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,232 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4047 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1208) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,240 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4038 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1207) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,247 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4039 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1207) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3990 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,254 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4037 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1207) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,261 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4034 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1207) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,269 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4028 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1207) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3989 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,278 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4035 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1206) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3975) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,286 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4040 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1206) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,294 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4036 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1206) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,302 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4037 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1206) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,310 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4039 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1206) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,318 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4041 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,327 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4037 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3989 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,335 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4031 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,343 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4037 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,352 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4038 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,360 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4036 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1205) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,368 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4040 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,376 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4030 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,384 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4030 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3976) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,392 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4033 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,399 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4028 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,406 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4028 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,414 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4030 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1204) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,421 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4027 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,429 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4029 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,436 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4026 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,443 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4025 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1178 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,451 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4025 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,458 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4034 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,466 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4035 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1203) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,473 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4025 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,481 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4022 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,488 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4028 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,495 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4025 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,503 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4025 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,511 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4031 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,518 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4031 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,525 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4024 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1177 (1.1202) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,532 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4022 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,540 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4027 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,547 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4027 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,555 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4030 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,562 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4030 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,570 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4023 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,578 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4023 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,586 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4027 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1201) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,593 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4023 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,601 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4028 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,610 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4024 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,618 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4022 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,626 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4016 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,635 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4025 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,643 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4024 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,651 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4024 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1200) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,660 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4018 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,669 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4022 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3988 (0.3978) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,676 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4019 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3988 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,684 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4021 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,692 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4018 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,699 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4014 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,707 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4012 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,714 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4012 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,721 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4016 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1174 (1.1199) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,729 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4015 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,736 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4014 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3989 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,743 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4014 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1173 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,751 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4017 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,758 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4015 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,766 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4013 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3990 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,773 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4014 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1198) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.3979) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:32,785 -                train: [    INFO] - 
Epoch: 12/20
2023-03-01 13:45:33,035 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4012 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1171) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3990 (0.3990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,043 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4011 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.1171) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,052 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4008 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1170 (1.1171) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,060 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4008 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1170 (1.1171) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,067 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4007 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1169 (1.1170) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,078 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4008 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1169 (1.1170) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,085 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4008 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1169 (1.1170) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3991 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,092 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4007 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1168 (1.1170) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,100 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4007 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1168 (1.1170) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,109 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4005 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1167 (1.1169) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,116 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4005 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1167 (1.1169) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,123 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4004 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1167 (1.1169) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,130 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4003 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1166 (1.1169) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,137 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4001 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1166 (1.1169) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3993 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,144 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4001 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1165 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3993 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,151 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4000 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1165 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3993 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,158 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3998 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1165 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3993 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,164 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3999 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3994 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,171 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3997 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3994 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,178 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3996 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1167) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3994 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,184 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3996 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1167) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3994 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,192 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3995 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1167) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3995 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,199 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3994 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1167) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3995 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,206 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3994 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1166) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3995 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,213 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3992 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1166) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3995 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,220 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3991 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1159 (1.1166) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3996 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,228 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3990 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1159 (1.1166) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3996 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,235 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3990 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.1165) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3996 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,244 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3989 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1157 (1.1165) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3997 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,252 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3988 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.1165) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3997 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,260 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3989 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.1165) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3998 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,268 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3986 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1155 (1.1164) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3998 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,275 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3985 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1164) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3998 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,282 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3982 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1164) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3999 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,289 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3985 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1163) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.3999 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,297 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3983 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1163) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.3999 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,305 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3982 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1163) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4000 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,312 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3982 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1162) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4000 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,320 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3982 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1149 (1.1162) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4000 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,327 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3984 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.1162) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4001 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,334 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3979 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.1161) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4001 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,342 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3980 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1147 (1.1161) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4002 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,350 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3978 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.1161) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4002 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,357 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3976 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.1160) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4003 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,365 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3978 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1144 (1.1160) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4003 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,372 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3973 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1143 (1.1160) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4003 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,380 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3977 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1142 (1.1159) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4004 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,387 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3974 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.1159) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4004 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,394 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3973 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.1159) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4005 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,402 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3974 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1140 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4005 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,409 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3969 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1139 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4005 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,416 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3968 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1138 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4006 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,424 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3969 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1137 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4006 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,431 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3970 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1136 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4007 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,438 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3970 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4007 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,446 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3965 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4007 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,453 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3968 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1134 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4008 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,461 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3960 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1133 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4008 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,468 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3963 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1132 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4009 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,475 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3960 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4009 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,483 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3957 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4009 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,490 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3960 (0.3987) Acc D Real: 100.000% 
Loss D Fake: 1.1130 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4010 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,497 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3958 (0.3987) Acc D Real: 100.000% 
Loss D Fake: 1.1129 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4010 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,504 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3964 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1128 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4010 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,511 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3956 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1128 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4011 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,518 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3958 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1127 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4011 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,526 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.3949 (0.3985) Acc D Real: 100.000% 
Loss D Fake: 1.1126 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4011 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,533 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3952 (0.3985) Acc D Real: 100.000% 
Loss D Fake: 1.1126 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4012 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,540 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3950 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1125 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4012 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,548 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.3951 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4012 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,555 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3952 (0.3983) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4013 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,563 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3950 (0.3983) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4013 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,570 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3956 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4013 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,577 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3948 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4013 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,584 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3958 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4014 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,591 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3943 (0.3981) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4014 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,599 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3949 (0.3981) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4014 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,606 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3946 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4014 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,613 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3948 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4015 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,620 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3940 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4015 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,628 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3941 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4015 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,635 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3943 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4015 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,642 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3939 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,650 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3942 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,657 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3933 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4016 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,664 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3941 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,671 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3937 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4016 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,679 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3936 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,686 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3931 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,693 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3937 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,700 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3932 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,709 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3934 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,716 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3938 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,723 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3941 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1142) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,731 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.3939 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1142) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,738 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3934 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1142) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,745 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3944 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4018 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,753 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3934 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,760 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3924 (0.3971) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,767 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3919 (0.3971) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4018 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,775 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3923 (0.3970) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4018 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,782 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3929 (0.3970) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,789 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3925 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,796 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3932 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,804 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3932 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1139) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,811 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.3934 (0.3968) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1139) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,818 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3924 (0.3968) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1139) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,825 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3930 (0.3968) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,833 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3930 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,840 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3928 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,847 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3937 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,855 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3928 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,862 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3917 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,869 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3931 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,876 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3918 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,884 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3936 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,891 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3925 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,898 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3911 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,905 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3913 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,913 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3903 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,920 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3929 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,927 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3931 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,934 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3924 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,941 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3920 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,949 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3923 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,956 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3910 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,963 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3907 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,970 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3946 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,977 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3919 (0.3960) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,984 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3927 (0.3960) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,991 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3935 (0.3960) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:33,998 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3937 (0.3960) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,005 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3914 (0.3959) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,013 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3933 (0.3959) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,020 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3904 (0.3959) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,027 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3920 (0.3959) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,035 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3908 (0.3958) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,042 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3908 (0.3958) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,049 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3917 (0.3958) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,056 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3921 (0.3957) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,063 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3916 (0.3957) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,070 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3915 (0.3957) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,078 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3916 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,085 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3926 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,092 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3934 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,100 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3925 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,107 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3913 (0.3956) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,115 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3939 (0.3955) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,122 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3918 (0.3955) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,129 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3923 (0.3955) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,136 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3909 (0.3955) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4019 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,144 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3920 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,151 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3922 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4019 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,158 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3899 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,165 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3944 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,173 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3931 (0.3954) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,180 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3902 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,187 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3926 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,195 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3935 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,202 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3934 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,209 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3912 (0.3953) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,216 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3918 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,223 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3926 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,230 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3937 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,237 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3915 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,244 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3943 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,251 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3924 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,258 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3941 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,266 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3925 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,273 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3931 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4018 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,280 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3904 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4017 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,289 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3923 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,297 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3940 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,304 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3920 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,311 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3923 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,318 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3924 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,326 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3957 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,333 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3927 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,340 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3943 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,348 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3927 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,355 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3926 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,362 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3938 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,369 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3940 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,376 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3922 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,384 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3946 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,391 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3924 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,398 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3940 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,405 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3960 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,412 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3952 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,420 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3947 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,427 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3941 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,434 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3970 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,441 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3953 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,449 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3930 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,456 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3925 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4017 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,465 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3931 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,472 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3954 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,479 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3932 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,487 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3939 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,494 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3959 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,501 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3940 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,509 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3961 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,516 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3944 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,523 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3977 (0.3949) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,531 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3964 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,538 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3986 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,545 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3983 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,552 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3980 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,560 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3971 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,567 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3983 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,574 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3964 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,581 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3954 (0.3950) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,589 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3996 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,596 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3975 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,603 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3967 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,610 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3977 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,617 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3968 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,624 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3966 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,632 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3963 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,639 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3980 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,646 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3962 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,653 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3946 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,660 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3969 (0.3951) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,668 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4013 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4016 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,675 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.3993 (0.3952) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4015 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,686 -                train: [    INFO] - 
Epoch: 13/20
2023-03-01 13:45:34,898 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3983 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,905 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3961 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,915 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4016 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,931 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3992 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,938 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4001 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,946 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3991 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,953 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4005 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,961 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3999 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,968 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3982 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,975 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4014 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,982 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3987 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4015 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,989 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4003 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:34,995 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3997 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,002 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3988 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,009 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3989 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,017 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4010 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,024 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4017 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,031 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4016 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,038 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4017 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,044 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3996 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4014 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,051 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4009 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4013 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,058 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4024 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,065 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4037 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,072 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4003 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,079 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4020 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,086 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4023 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,093 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4027 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,100 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4043 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,108 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4028 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,115 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4037 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,123 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4022 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,132 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4041 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,139 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4049 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,146 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4039 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,153 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4040 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,160 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4057 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,168 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4059 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,176 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4061 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,184 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4034 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,191 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4044 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,199 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4044 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,207 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4043 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,214 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4054 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,221 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4070 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,229 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4071 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,237 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4069 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4013 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,244 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4074 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4014 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,253 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4078 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4014 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,261 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4067 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4014 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,270 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4086 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4014 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,278 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4103 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4015 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,285 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4085 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4015 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,293 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4076 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4016 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,300 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4058 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4016 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,308 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4082 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4016 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,315 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4081 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4017 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,323 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4084 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4017 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,330 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4102 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4017 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,337 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4113 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4018 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,345 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4123 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4019 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,352 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4109 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4019 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,361 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4106 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4020 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,368 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4113 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1107 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4021 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,376 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4109 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4022 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,383 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4116 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1103 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4022 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,390 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4112 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1102 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4023 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,398 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4103 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1100 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4024 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,405 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4113 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4025 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,413 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4126 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1096 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4026 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,421 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4112 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.1094 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4027 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,429 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4124 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4028 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,437 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4132 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.1089 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4029 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,444 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4113 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.1087 (1.1117) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4030 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,451 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4125 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.1085 (1.1117) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4031 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,458 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4122 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.1082 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4032 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,465 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4132 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4034 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,473 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4162 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.1077 (1.1115) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4035 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,480 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4149 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.1074 (1.1115) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4036 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,487 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4128 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.1071 (1.1114) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4038 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,494 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4139 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.1068 (1.1114) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4039 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,502 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4176 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.1065 (1.1113) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4041 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,509 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4150 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.1112) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4042 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,516 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4152 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.1112) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4044 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,524 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4168 (0.4063) Acc D Real: 100.000% 
Loss D Fake: 1.1054 (1.1111) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4046 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,532 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4156 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.1110) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4048 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,540 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4165 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.1047 (1.1110) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4050 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,547 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4142 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.1043 (1.1109) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4052 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,555 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4171 (0.4067) Acc D Real: 100.000% 
Loss D Fake: 1.1039 (1.1108) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4054 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,562 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4165 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.1107) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4055 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,571 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4179 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1030 (1.1106) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4058 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,579 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4166 (0.4070) Acc D Real: 100.000% 
Loss D Fake: 1.1026 (1.1106) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4060 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,586 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4174 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.1105) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4062 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,595 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4166 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.1017 (1.1104) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4064 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,602 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4173 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.1013 (1.1103) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4066 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,609 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4183 (0.4075) Acc D Real: 100.000% 
Loss D Fake: 1.1008 (1.1102) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4068 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,616 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4171 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1004 (1.1101) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4070 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,623 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4183 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.0999 (1.1100) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4073 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,631 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4210 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.0995 (1.1099) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4075 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,639 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4196 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.1098) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4077 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,646 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4208 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.1097) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4080 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,653 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4187 (0.4081) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.1095) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4083 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,661 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4209 (0.4083) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1094) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4085 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,668 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4194 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 1.0968 (1.1093) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4088 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,676 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4201 (0.4085) Acc D Real: 100.000% 
Loss D Fake: 1.0963 (1.1092) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4090 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,683 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4198 (0.4086) Acc D Real: 100.000% 
Loss D Fake: 1.0958 (1.1090) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4093 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,690 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4209 (0.4087) Acc D Real: 100.000% 
Loss D Fake: 1.0952 (1.1089) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4096 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,698 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4201 (0.4088) Acc D Real: 100.000% 
Loss D Fake: 1.0947 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4098 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,705 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4218 (0.4089) Acc D Real: 100.000% 
Loss D Fake: 1.0941 (1.1087) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4101 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,712 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4201 (0.4090) Acc D Real: 100.000% 
Loss D Fake: 1.0936 (1.1085) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4104 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,720 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4213 (0.4091) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.1084) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4106 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,727 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4217 (0.4093) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.1082) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4109 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,734 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4214 (0.4094) Acc D Real: 100.000% 
Loss D Fake: 1.0920 (1.1081) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4112 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,742 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4218 (0.4095) Acc D Real: 100.000% 
Loss D Fake: 1.0914 (1.1079) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4114 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,749 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4209 (0.4096) Acc D Real: 100.000% 
Loss D Fake: 1.0909 (1.1078) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4117 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,757 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4229 (0.4097) Acc D Real: 100.000% 
Loss D Fake: 1.0903 (1.1076) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4120 (0.4036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,764 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4218 (0.4098) Acc D Real: 100.000% 
Loss D Fake: 1.0898 (1.1075) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4122 (0.4036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,771 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4234 (0.4099) Acc D Real: 100.000% 
Loss D Fake: 1.0892 (1.1073) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4125 (0.4037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,779 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4232 (0.4100) Acc D Real: 100.000% 
Loss D Fake: 1.0887 (1.1072) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4128 (0.4038) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,786 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4228 (0.4101) Acc D Real: 100.000% 
Loss D Fake: 1.0881 (1.1070) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4131 (0.4039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,794 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4238 (0.4102) Acc D Real: 100.000% 
Loss D Fake: 1.0875 (1.1069) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4134 (0.4039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,801 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4237 (0.4103) Acc D Real: 100.000% 
Loss D Fake: 1.0870 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4136 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,808 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4239 (0.4105) Acc D Real: 100.000% 
Loss D Fake: 1.0864 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4139 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,816 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4223 (0.4106) Acc D Real: 100.000% 
Loss D Fake: 1.0858 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4142 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,823 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4238 (0.4107) Acc D Real: 100.000% 
Loss D Fake: 1.0853 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4145 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,831 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4244 (0.4108) Acc D Real: 100.000% 
Loss D Fake: 1.0847 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4148 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,838 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4253 (0.4109) Acc D Real: 100.000% 
Loss D Fake: 1.0841 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4150 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,846 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4253 (0.4110) Acc D Real: 100.000% 
Loss D Fake: 1.0836 (1.1057) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4153 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,853 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4249 (0.4111) Acc D Real: 100.000% 
Loss D Fake: 1.0830 (1.1055) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4156 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,860 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4237 (0.4112) Acc D Real: 100.000% 
Loss D Fake: 1.0824 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4159 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,868 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4246 (0.4113) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.1051) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4162 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,875 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4251 (0.4114) Acc D Real: 100.000% 
Loss D Fake: 1.0812 (1.1050) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4165 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,883 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4255 (0.4115) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.1048) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4168 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,890 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4255 (0.4116) Acc D Real: 100.000% 
Loss D Fake: 1.0801 (1.1046) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4171 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,898 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4267 (0.4117) Acc D Real: 100.000% 
Loss D Fake: 1.0796 (1.1044) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4174 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,905 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4252 (0.4118) Acc D Real: 100.000% 
Loss D Fake: 1.0790 (1.1042) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4176 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,912 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4250 (0.4119) Acc D Real: 100.000% 
Loss D Fake: 1.0784 (1.1040) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4179 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,920 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4257 (0.4120) Acc D Real: 100.000% 
Loss D Fake: 1.0779 (1.1038) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4182 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,927 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4268 (0.4121) Acc D Real: 100.000% 
Loss D Fake: 1.0773 (1.1037) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4185 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,935 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4269 (0.4122) Acc D Real: 100.000% 
Loss D Fake: 1.0768 (1.1035) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4188 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,942 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4259 (0.4123) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.1033) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4190 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,950 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4260 (0.4124) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.1031) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4193 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,957 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4269 (0.4125) Acc D Real: 100.000% 
Loss D Fake: 1.0751 (1.1029) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4196 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,964 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4265 (0.4126) Acc D Real: 100.000% 
Loss D Fake: 1.0746 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4199 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,972 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4262 (0.4127) Acc D Real: 100.000% 
Loss D Fake: 1.0740 (1.1025) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4201 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,979 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4271 (0.4128) Acc D Real: 100.000% 
Loss D Fake: 1.0735 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4204 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,987 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4262 (0.4129) Acc D Real: 100.000% 
Loss D Fake: 1.0730 (1.1021) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4207 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:35,994 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4268 (0.4130) Acc D Real: 100.000% 
Loss D Fake: 1.0725 (1.1019) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4209 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,002 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4271 (0.4131) Acc D Real: 100.000% 
Loss D Fake: 1.0720 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4212 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,009 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4274 (0.4132) Acc D Real: 100.000% 
Loss D Fake: 1.0715 (1.1015) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4214 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,016 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4274 (0.4133) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.1013) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4217 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,024 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4271 (0.4134) Acc D Real: 100.000% 
Loss D Fake: 1.0705 (1.1011) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4219 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,032 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4279 (0.4135) Acc D Real: 100.000% 
Loss D Fake: 1.0700 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4222 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,039 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4271 (0.4136) Acc D Real: 100.000% 
Loss D Fake: 1.0695 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4224 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,047 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4276 (0.4137) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4227 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,054 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4277 (0.4137) Acc D Real: 100.000% 
Loss D Fake: 1.0685 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4229 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,062 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4280 (0.4138) Acc D Real: 100.000% 
Loss D Fake: 1.0681 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4232 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,069 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4280 (0.4139) Acc D Real: 100.000% 
Loss D Fake: 1.0676 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4234 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,077 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4282 (0.4140) Acc D Real: 100.000% 
Loss D Fake: 1.0671 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4237 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,084 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4286 (0.4141) Acc D Real: 100.000% 
Loss D Fake: 1.0666 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4239 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,092 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4283 (0.4142) Acc D Real: 100.000% 
Loss D Fake: 1.0661 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4242 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,099 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4283 (0.4143) Acc D Real: 100.000% 
Loss D Fake: 1.0656 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4244 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,107 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4286 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.0652 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4247 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,116 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4284 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.0647 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4249 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,124 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4284 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.0642 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4252 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,133 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4286 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.0638 (1.0982) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4254 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,141 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4288 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.0633 (1.0980) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4256 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,150 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4286 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.0629 (1.0978) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4259 (0.4084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,158 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4292 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0976) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4261 (0.4085) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,166 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4293 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0620 (1.0974) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4263 (0.4086) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,175 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4294 (0.4150) Acc D Real: 100.000% 
Loss D Fake: 1.0615 (1.0971) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4266 (0.4087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,183 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4293 (0.4151) Acc D Real: 100.000% 
Loss D Fake: 1.0610 (1.0969) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4268 (0.4088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,192 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4290 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.0605 (1.0967) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4271 (0.4089) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,200 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4291 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.0601 (1.0965) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4273 (0.4091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,207 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4295 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0597 (1.0963) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4275 (0.4092) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,215 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4295 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.0592 (1.0961) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4278 (0.4093) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,223 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4297 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.0588 (1.0959) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4280 (0.4094) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,231 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4295 (0.4156) Acc D Real: 100.000% 
Loss D Fake: 1.0583 (1.0957) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4283 (0.4095) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,239 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4295 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.0578 (1.0955) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4285 (0.4096) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,247 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4297 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0574 (1.0953) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4287 (0.4097) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,254 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4299 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.0569 (1.0950) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4290 (0.4098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,262 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4297 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.0565 (1.0948) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4292 (0.4099) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,269 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4298 (0.4160) Acc D Real: 100.000% 
Loss D Fake: 1.0560 (1.0946) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4294 (0.4100) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,277 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4299 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0556 (1.0944) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4297 (0.4101) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,284 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4299 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.0551 (1.0942) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4299 (0.4102) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,291 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4301 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.0547 (1.0940) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4302 (0.4103) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,299 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4301 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.0542 (1.0938) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4304 (0.4104) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,306 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4300 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0538 (1.0936) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4306 (0.4105) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,314 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4301 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.0534 (1.0933) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4309 (0.4107) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,321 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4301 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.0529 (1.0931) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4311 (0.4108) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,328 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4303 (0.4166) Acc D Real: 100.000% 
Loss D Fake: 1.0525 (1.0929) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4313 (0.4109) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,336 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4304 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0521 (1.0927) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4315 (0.4110) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,343 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4304 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0517 (1.0925) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4317 (0.4111) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,350 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4303 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0513 (1.0923) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4319 (0.4112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,357 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4303 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0509 (1.0921) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4321 (0.4113) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,365 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4302 (0.4169) Acc D Real: 100.000% 
Loss D Fake: 1.0505 (1.0919) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4324 (0.4114) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,372 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4303 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0501 (1.0916) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4326 (0.4115) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,379 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4303 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0497 (1.0914) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4328 (0.4116) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,387 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4306 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0492 (1.0912) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4330 (0.4117) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,394 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4304 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0488 (1.0910) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4333 (0.4118) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,402 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4307 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0484 (1.0908) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4335 (0.4119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,409 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4305 (0.4173) Acc D Real: 100.000% 
Loss D Fake: 1.0480 (1.0906) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4337 (0.4121) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,416 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4308 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0476 (1.0904) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4339 (0.4122) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,424 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4306 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0472 (1.0902) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4341 (0.4123) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,431 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4310 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0468 (1.0899) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4343 (0.4124) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,438 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4304 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0465 (1.0897) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4345 (0.4125) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,445 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4309 (0.4177) Acc D Real: 100.000% 
Loss D Fake: 1.0461 (1.0895) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4347 (0.4126) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,452 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4304 (0.4177) Acc D Real: 100.000% 
Loss D Fake: 1.0457 (1.0893) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4349 (0.4127) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,460 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4312 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0453 (1.0891) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4351 (0.4128) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,467 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4306 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0450 (1.0889) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4353 (0.4129) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,474 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4306 (0.4179) Acc D Real: 100.000% 
Loss D Fake: 1.0446 (1.0887) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4355 (0.4130) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,481 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4310 (0.4180) Acc D Real: 100.000% 
Loss D Fake: 1.0443 (1.0885) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4357 (0.4131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,490 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4311 (0.4180) Acc D Real: 100.000% 
Loss D Fake: 1.0439 (1.0883) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4359 (0.4132) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,497 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4307 (0.4181) Acc D Real: 100.000% 
Loss D Fake: 1.0436 (1.0881) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4361 (0.4133) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,505 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4309 (0.4181) Acc D Real: 100.000% 
Loss D Fake: 1.0432 (1.0878) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4363 (0.4134) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,512 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4302 (0.4182) Acc D Real: 100.000% 
Loss D Fake: 1.0429 (1.0876) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4365 (0.4136) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,519 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4313 (0.4183) Acc D Real: 100.000% 
Loss D Fake: 1.0425 (1.0874) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4366 (0.4137) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,527 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4312 (0.4183) Acc D Real: 100.000% 
Loss D Fake: 1.0422 (1.0872) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4368 (0.4138) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,534 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4316 (0.4184) Acc D Real: 100.000% 
Loss D Fake: 1.0419 (1.0870) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4370 (0.4139) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,541 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4312 (0.4184) Acc D Real: 100.000% 
Loss D Fake: 1.0416 (1.0868) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4372 (0.4140) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,549 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4312 (0.4185) Acc D Real: 100.000% 
Loss D Fake: 1.0413 (1.0866) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4373 (0.4141) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,556 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4309 (0.4186) Acc D Real: 100.000% 
Loss D Fake: 1.0410 (1.0864) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4375 (0.4142) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,563 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4308 (0.4186) Acc D Real: 100.000% 
Loss D Fake: 1.0407 (1.0862) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4376 (0.4143) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,571 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4317 (0.4187) Acc D Real: 100.000% 
Loss D Fake: 1.0404 (1.0860) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4378 (0.4144) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,578 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4304 (0.4187) Acc D Real: 100.000% 
Loss D Fake: 1.0401 (1.0858) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4379 (0.4145) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,585 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4316 (0.4188) Acc D Real: 100.000% 
Loss D Fake: 1.0398 (1.0856) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4381 (0.4146) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,597 -                train: [    INFO] - 
Epoch: 14/20
2023-03-01 13:45:36,793 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4306 (0.4309) Acc D Real: 100.000% 
Loss D Fake: 1.0393 (1.0394) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4384 (0.4383) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,801 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4305 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0390 (1.0393) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4385 (0.4384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,809 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4307 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0387 (1.0391) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4387 (0.4385) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,829 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4303 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0384 (1.0390) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4389 (0.4386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,836 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4310 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0381 (1.0389) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4390 (0.4386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,843 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4304 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0378 (1.0387) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4392 (0.4387) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,850 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4300 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0375 (1.0386) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4394 (0.4388) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,857 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4306 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0372 (1.0384) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4395 (0.4389) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,864 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4305 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0369 (1.0383) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4397 (0.4390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,871 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4303 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0366 (1.0381) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4399 (0.4390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,878 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4305 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0363 (1.0380) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4400 (0.4391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,885 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4309 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0360 (1.0378) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4402 (0.4392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,892 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4307 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0357 (1.0377) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4404 (0.4393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,899 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4310 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0354 (1.0375) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4405 (0.4394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,906 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4301 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0352 (1.0374) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4407 (0.4395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,913 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4303 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0349 (1.0372) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4408 (0.4395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,920 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4300 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0346 (1.0371) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4410 (0.4396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,927 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4305 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0343 (1.0369) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4411 (0.4397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,934 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4311 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0341 (1.0368) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4413 (0.4398) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,941 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4302 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0338 (1.0366) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4414 (0.4399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,948 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4313 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0335 (1.0365) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4416 (0.4399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,955 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4302 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0333 (1.0364) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4417 (0.4400) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,962 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4301 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0331 (1.0362) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4418 (0.4401) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,969 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4293 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0328 (1.0361) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4420 (0.4402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,976 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4297 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0326 (1.0360) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4421 (0.4402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,983 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4308 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0323 (1.0358) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4423 (0.4403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,990 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4315 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0321 (1.0357) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4424 (0.4404) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:36,997 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4314 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0319 (1.0356) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4425 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,005 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4295 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0317 (1.0354) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4426 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,012 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4300 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0315 (1.0353) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4427 (0.4406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,020 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4292 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0312 (1.0352) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4429 (0.4407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,028 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4301 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0310 (1.0350) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4430 (0.4407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,035 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4300 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0308 (1.0349) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4431 (0.4408) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,043 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4299 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0306 (1.0348) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4432 (0.4409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,050 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4303 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0304 (1.0347) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4433 (0.4409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,058 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4321 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0302 (1.0346) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4434 (0.4410) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,065 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4317 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0301 (1.0344) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4435 (0.4411) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,073 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4299 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0300 (1.0343) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4436 (0.4411) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,080 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4308 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0298 (1.0342) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4436 (0.4412) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,088 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4287 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0297 (1.0341) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4437 (0.4413) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,095 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4277 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0296 (1.0340) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4438 (0.4413) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,103 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4299 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0294 (1.0339) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4439 (0.4414) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,110 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4302 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0293 (1.0338) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4440 (0.4414) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,118 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4292 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0292 (1.0337) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4440 (0.4415) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,125 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4310 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0290 (1.0336) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4441 (0.4416) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,132 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4298 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0289 (1.0335) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4442 (0.4416) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,140 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4285 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0288 (1.0334) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4442 (0.4417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,147 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4288 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0287 (1.0333) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4443 (0.4417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,155 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4285 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0286 (1.0332) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4444 (0.4418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,162 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4292 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0285 (1.0331) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4444 (0.4418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,170 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4290 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0284 (1.0330) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4445 (0.4419) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,177 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4284 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0283 (1.0329) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4445 (0.4419) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,185 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4288 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0282 (1.0328) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4446 (0.4420) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,192 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4278 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0281 (1.0328) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4447 (0.4420) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,200 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4292 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0280 (1.0327) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4447 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,207 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4291 (0.4300) Acc D Real: 100.000% 
Loss D Fake: 1.0279 (1.0326) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4447 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,215 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4295 (0.4300) Acc D Real: 100.000% 
Loss D Fake: 1.0279 (1.0325) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4448 (0.4422) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,222 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4278 (0.4300) Acc D Real: 100.000% 
Loss D Fake: 1.0279 (1.0324) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4448 (0.4422) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,230 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4264 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.0278 (1.0323) Acc D Fake: 0.000% 
Loss D: 1.454 
Loss G: 0.4448 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,237 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4297 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0323) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4449 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,245 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4277 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0322) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4449 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,252 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4295 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0321) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4449 (0.4424) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,260 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4291 (0.4299) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0321) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4449 (0.4424) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,268 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4269 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0320) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4449 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,275 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4277 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.0277 (1.0319) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4449 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,283 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4280 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.0278 (1.0319) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4449 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,292 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4287 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.0278 (1.0318) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4449 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,300 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4280 (0.4297) Acc D Real: 100.000% 
Loss D Fake: 1.0279 (1.0317) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4448 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,307 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4289 (0.4297) Acc D Real: 100.000% 
Loss D Fake: 1.0280 (1.0317) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4448 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,315 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4279 (0.4297) Acc D Real: 100.000% 
Loss D Fake: 1.0281 (1.0316) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4447 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,322 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4277 (0.4297) Acc D Real: 100.000% 
Loss D Fake: 1.0282 (1.0316) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4447 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,330 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4281 (0.4296) Acc D Real: 100.000% 
Loss D Fake: 1.0283 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4446 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,338 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4273 (0.4296) Acc D Real: 100.000% 
Loss D Fake: 1.0285 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4445 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,346 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4287 (0.4296) Acc D Real: 100.000% 
Loss D Fake: 1.0286 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4444 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,355 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4264 (0.4296) Acc D Real: 100.000% 
Loss D Fake: 1.0288 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4443 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,363 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4280 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.0290 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4442 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,371 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4286 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.0292 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4441 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,378 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4283 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.0295 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4439 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,385 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4258 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.0297 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4438 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,393 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4288 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.0300 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4437 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,400 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4253 (0.4294) Acc D Real: 100.000% 
Loss D Fake: 1.0303 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4435 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,407 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4285 (0.4294) Acc D Real: 100.000% 
Loss D Fake: 1.0306 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4433 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,415 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4266 (0.4294) Acc D Real: 100.000% 
Loss D Fake: 1.0310 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4431 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,423 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4268 (0.4293) Acc D Real: 100.000% 
Loss D Fake: 1.0313 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4430 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,430 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4251 (0.4293) Acc D Real: 100.000% 
Loss D Fake: 1.0317 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4428 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,437 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4260 (0.4292) Acc D Real: 100.000% 
Loss D Fake: 1.0320 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4426 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,445 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4257 (0.4292) Acc D Real: 100.000% 
Loss D Fake: 1.0324 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4424 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,453 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4271 (0.4292) Acc D Real: 100.000% 
Loss D Fake: 1.0328 (1.0313) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4421 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,460 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4255 (0.4291) Acc D Real: 100.000% 
Loss D Fake: 1.0332 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4419 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,467 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4260 (0.4291) Acc D Real: 100.000% 
Loss D Fake: 1.0337 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4417 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,475 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4252 (0.4291) Acc D Real: 100.000% 
Loss D Fake: 1.0341 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4414 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,482 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4264 (0.4290) Acc D Real: 100.000% 
Loss D Fake: 1.0346 (1.0314) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4412 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,489 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4257 (0.4290) Acc D Real: 100.000% 
Loss D Fake: 1.0351 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4409 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,497 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4246 (0.4290) Acc D Real: 100.000% 
Loss D Fake: 1.0356 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4406 (0.4428) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,504 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4252 (0.4289) Acc D Real: 100.000% 
Loss D Fake: 1.0362 (1.0316) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4403 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,511 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4267 (0.4289) Acc D Real: 100.000% 
Loss D Fake: 1.0367 (1.0316) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4400 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,519 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4255 (0.4289) Acc D Real: 100.000% 
Loss D Fake: 1.0373 (1.0317) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4397 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,526 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4250 (0.4288) Acc D Real: 100.000% 
Loss D Fake: 1.0379 (1.0317) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4393 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,533 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4244 (0.4288) Acc D Real: 100.000% 
Loss D Fake: 1.0386 (1.0318) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4390 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,542 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4257 (0.4287) Acc D Real: 100.000% 
Loss D Fake: 1.0392 (1.0319) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4386 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,549 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4245 (0.4287) Acc D Real: 100.000% 
Loss D Fake: 1.0399 (1.0320) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4383 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,557 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4269 (0.4287) Acc D Real: 100.000% 
Loss D Fake: 1.0406 (1.0320) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4379 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,564 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4248 (0.4286) Acc D Real: 100.000% 
Loss D Fake: 1.0413 (1.0321) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4375 (0.4424) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,572 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4263 (0.4286) Acc D Real: 100.000% 
Loss D Fake: 1.0421 (1.0322) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4371 (0.4424) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,580 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4226 (0.4286) Acc D Real: 100.000% 
Loss D Fake: 1.0429 (1.0323) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4367 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,587 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4234 (0.4285) Acc D Real: 100.000% 
Loss D Fake: 1.0436 (1.0324) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4362 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,594 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4246 (0.4285) Acc D Real: 100.000% 
Loss D Fake: 1.0444 (1.0325) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4358 (0.4422) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,602 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4251 (0.4285) Acc D Real: 100.000% 
Loss D Fake: 1.0452 (1.0327) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4354 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,610 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4255 (0.4284) Acc D Real: 100.000% 
Loss D Fake: 1.0461 (1.0328) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4349 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,617 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4234 (0.4284) Acc D Real: 100.000% 
Loss D Fake: 1.0469 (1.0329) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4344 (0.4420) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,624 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4263 (0.4284) Acc D Real: 100.000% 
Loss D Fake: 1.0478 (1.0330) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4340 (0.4419) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,631 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4222 (0.4283) Acc D Real: 100.000% 
Loss D Fake: 1.0487 (1.0332) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4335 (0.4419) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,639 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4231 (0.4283) Acc D Real: 100.000% 
Loss D Fake: 1.0496 (1.0333) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4330 (0.4418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,646 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4203 (0.4282) Acc D Real: 100.000% 
Loss D Fake: 1.0505 (1.0335) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4326 (0.4417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,653 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4225 (0.4281) Acc D Real: 100.000% 
Loss D Fake: 1.0513 (1.0336) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4321 (0.4416) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,661 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4211 (0.4281) Acc D Real: 100.000% 
Loss D Fake: 1.0522 (1.0338) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4316 (0.4415) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,668 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4217 (0.4280) Acc D Real: 100.000% 
Loss D Fake: 1.0531 (1.0340) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4312 (0.4414) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,676 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4235 (0.4280) Acc D Real: 100.000% 
Loss D Fake: 1.0540 (1.0341) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4307 (0.4414) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,684 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4202 (0.4279) Acc D Real: 100.000% 
Loss D Fake: 1.0549 (1.0343) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4302 (0.4413) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,692 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4228 (0.4279) Acc D Real: 100.000% 
Loss D Fake: 1.0557 (1.0345) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4298 (0.4412) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,700 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4218 (0.4278) Acc D Real: 100.000% 
Loss D Fake: 1.0566 (1.0347) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4293 (0.4411) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,707 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4221 (0.4278) Acc D Real: 100.000% 
Loss D Fake: 1.0575 (1.0348) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4288 (0.4410) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,715 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4218 (0.4277) Acc D Real: 100.000% 
Loss D Fake: 1.0585 (1.0350) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4283 (0.4409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,722 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4219 (0.4277) Acc D Real: 100.000% 
Loss D Fake: 1.0594 (1.0352) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4278 (0.4408) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,730 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4198 (0.4276) Acc D Real: 100.000% 
Loss D Fake: 1.0603 (1.0354) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4273 (0.4407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,737 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4225 (0.4276) Acc D Real: 100.000% 
Loss D Fake: 1.0612 (1.0356) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4269 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,745 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4213 (0.4275) Acc D Real: 100.000% 
Loss D Fake: 1.0621 (1.0358) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4264 (0.4404) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,752 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4206 (0.4275) Acc D Real: 100.000% 
Loss D Fake: 1.0631 (1.0360) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4259 (0.4403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,760 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4203 (0.4274) Acc D Real: 100.000% 
Loss D Fake: 1.0640 (1.0363) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4254 (0.4402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,767 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4217 (0.4274) Acc D Real: 100.000% 
Loss D Fake: 1.0649 (1.0365) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4249 (0.4401) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,775 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4176 (0.4273) Acc D Real: 100.000% 
Loss D Fake: 1.0658 (1.0367) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4245 (0.4400) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,782 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4199 (0.4273) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0369) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4240 (0.4399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,790 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4204 (0.4272) Acc D Real: 100.000% 
Loss D Fake: 1.0676 (1.0372) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4235 (0.4397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,797 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4238 (0.4272) Acc D Real: 100.000% 
Loss D Fake: 1.0685 (1.0374) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4230 (0.4396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,805 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4190 (0.4271) Acc D Real: 100.000% 
Loss D Fake: 1.0694 (1.0376) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4226 (0.4395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,812 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4203 (0.4271) Acc D Real: 100.000% 
Loss D Fake: 1.0703 (1.0379) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4221 (0.4394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,820 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4184 (0.4270) Acc D Real: 100.000% 
Loss D Fake: 1.0712 (1.0381) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4216 (0.4392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,827 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4202 (0.4270) Acc D Real: 100.000% 
Loss D Fake: 1.0721 (1.0383) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4212 (0.4391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,835 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4189 (0.4269) Acc D Real: 100.000% 
Loss D Fake: 1.0730 (1.0386) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4207 (0.4390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,842 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4197 (0.4268) Acc D Real: 100.000% 
Loss D Fake: 1.0739 (1.0388) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4203 (0.4388) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,849 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4204 (0.4268) Acc D Real: 100.000% 
Loss D Fake: 1.0747 (1.0391) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4198 (0.4387) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,858 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4178 (0.4267) Acc D Real: 100.000% 
Loss D Fake: 1.0756 (1.0394) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4194 (0.4386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,865 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4186 (0.4267) Acc D Real: 100.000% 
Loss D Fake: 1.0764 (1.0396) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4190 (0.4384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,873 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4169 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0773 (1.0399) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4185 (0.4383) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,880 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4208 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0781 (1.0401) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4181 (0.4382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,887 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4176 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0789 (1.0404) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4177 (0.4380) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,895 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4202 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0798 (1.0407) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4173 (0.4379) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,903 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4161 (0.4264) Acc D Real: 100.000% 
Loss D Fake: 1.0806 (1.0409) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4168 (0.4377) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,910 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4159 (0.4263) Acc D Real: 100.000% 
Loss D Fake: 1.0814 (1.0412) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4164 (0.4376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,917 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4182 (0.4263) Acc D Real: 100.000% 
Loss D Fake: 1.0821 (1.0415) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4161 (0.4374) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,924 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4183 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.0829 (1.0417) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4157 (0.4373) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,932 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4191 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.0837 (1.0420) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4153 (0.4372) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,940 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4198 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.0844 (1.0423) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4149 (0.4370) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,947 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4182 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.0852 (1.0426) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4145 (0.4369) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,954 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4182 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.0860 (1.0429) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4141 (0.4367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,962 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4167 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.0868 (1.0431) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4137 (0.4366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,969 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4152 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.0875 (1.0434) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4133 (0.4364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,976 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4182 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.0882 (1.0437) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4130 (0.4363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,984 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4169 (0.4258) Acc D Real: 100.000% 
Loss D Fake: 1.0889 (1.0440) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4126 (0.4361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,991 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4154 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.0896 (1.0443) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4123 (0.4360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:37,999 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4185 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.0903 (1.0445) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4119 (0.4358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,006 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4152 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.0910 (1.0448) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4116 (0.4357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,013 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4172 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.0451) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4112 (0.4355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,021 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4166 (0.4255) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.0454) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4109 (0.4354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,028 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4149 (0.4255) Acc D Real: 100.000% 
Loss D Fake: 1.0930 (1.0457) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4106 (0.4352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,035 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4151 (0.4254) Acc D Real: 100.000% 
Loss D Fake: 1.0937 (1.0460) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4103 (0.4351) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,043 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4163 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.0943 (1.0463) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4099 (0.4349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,051 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4135 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.0949 (1.0466) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4096 (0.4348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,058 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4145 (0.4252) Acc D Real: 100.000% 
Loss D Fake: 1.0955 (1.0468) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4093 (0.4346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,066 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4136 (0.4251) Acc D Real: 100.000% 
Loss D Fake: 1.0961 (1.0471) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4091 (0.4345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,073 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4136 (0.4251) Acc D Real: 100.000% 
Loss D Fake: 1.0966 (1.0474) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4088 (0.4343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,080 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4130 (0.4250) Acc D Real: 100.000% 
Loss D Fake: 1.0972 (1.0477) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4085 (0.4342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,087 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4125 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.0480) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4083 (0.4340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,095 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4136 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.0982 (1.0483) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4080 (0.4339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,102 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4121 (0.4248) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.0486) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4078 (0.4337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,109 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4129 (0.4247) Acc D Real: 100.000% 
Loss D Fake: 1.0991 (1.0488) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4076 (0.4336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,117 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4163 (0.4247) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.0491) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4073 (0.4335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,124 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4114 (0.4246) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.0494) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4071 (0.4333) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,132 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4147 (0.4246) Acc D Real: 100.000% 
Loss D Fake: 1.1005 (1.0497) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4069 (0.4332) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,139 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4131 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.1010 (1.0500) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4066 (0.4330) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,148 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4116 (0.4244) Acc D Real: 100.000% 
Loss D Fake: 1.1014 (1.0503) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4064 (0.4329) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,156 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4133 (0.4244) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.0505) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4062 (0.4327) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,163 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4123 (0.4243) Acc D Real: 100.000% 
Loss D Fake: 1.1023 (1.0508) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4060 (0.4326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,171 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4125 (0.4242) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.0511) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4058 (0.4324) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,178 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4127 (0.4242) Acc D Real: 100.000% 
Loss D Fake: 1.1031 (1.0514) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4056 (0.4323) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,185 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4145 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.0517) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4054 (0.4321) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,193 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4123 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.1039 (1.0519) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4052 (0.4320) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,201 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4107 (0.4240) Acc D Real: 100.000% 
Loss D Fake: 1.1043 (1.0522) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4050 (0.4319) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,209 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4136 (0.4239) Acc D Real: 100.000% 
Loss D Fake: 1.1047 (1.0525) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4048 (0.4317) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,217 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4140 (0.4239) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.0528) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4046 (0.4316) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,225 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4109 (0.4238) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.0531) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4044 (0.4314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,233 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4122 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.1059 (1.0533) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4042 (0.4313) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,242 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4131 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.0536) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4041 (0.4312) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,250 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4126 (0.4236) Acc D Real: 100.000% 
Loss D Fake: 1.1066 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4039 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,258 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4109 (0.4236) Acc D Real: 100.000% 
Loss D Fake: 1.1070 (1.0541) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4037 (0.4309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,266 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4099 (0.4235) Acc D Real: 100.000% 
Loss D Fake: 1.1073 (1.0544) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4035 (0.4307) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,274 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4138 (0.4235) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.0547) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4034 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,281 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4125 (0.4234) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.0549) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4032 (0.4305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,290 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4101 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.1083 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4031 (0.4303) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,298 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4128 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.1086 (1.0555) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4029 (0.4302) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,306 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4089 (0.4232) Acc D Real: 100.000% 
Loss D Fake: 1.1089 (1.0557) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4028 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,313 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4114 (0.4232) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.0560) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4026 (0.4299) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,321 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4138 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.1095 (1.0563) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4025 (0.4298) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,328 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4123 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.0565) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4023 (0.4296) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,336 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4108 (0.4230) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.0568) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4022 (0.4295) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,343 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4081 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.1104 (1.0570) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4020 (0.4294) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,350 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4090 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.1106 (1.0573) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4019 (0.4292) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,357 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4114 (0.4228) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.0576) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4018 (0.4291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,365 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4093 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.0578) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4017 (0.4290) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,372 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4096 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.0581) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4016 (0.4289) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,380 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4093 (0.4226) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.0583) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4015 (0.4287) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,387 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4077 (0.4225) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.0586) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4014 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,395 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4078 (0.4225) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.0588) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4013 (0.4285) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,402 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4096 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.0591) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4012 (0.4283) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,409 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4133 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.0593) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4011 (0.4282) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,416 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4106 (0.4223) Acc D Real: 100.000% 
Loss D Fake: 1.1125 (1.0596) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4010 (0.4281) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,423 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4072 (0.4222) Acc D Real: 100.000% 
Loss D Fake: 1.1127 (1.0598) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4009 (0.4280) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,430 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4090 (0.4222) Acc D Real: 100.000% 
Loss D Fake: 1.1129 (1.0601) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4008 (0.4278) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,437 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4091 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.0603) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4008 (0.4277) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,445 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4099 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.1132 (1.0605) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4007 (0.4276) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,452 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4104 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.1134 (1.0608) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4006 (0.4275) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,459 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4081 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.1136 (1.0610) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4005 (0.4274) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,466 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4114 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.1137 (1.0612) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4004 (0.4272) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,473 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4103 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.1139 (1.0615) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4003 (0.4271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,480 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4094 (0.4218) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.0617) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4003 (0.4270) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,492 -                train: [    INFO] - 
Epoch: 15/20
2023-03-01 13:45:38,759 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4073 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.1144 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4001 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,767 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4091 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4001 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,774 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4082 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4000 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,782 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4085 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1147 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3999 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,789 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4089 (0.4082) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3999 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,800 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4091 (0.4083) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3998 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,808 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4087 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3998 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,815 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4092 (0.4085) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3997 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,823 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4078 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3997 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,832 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4070 (0.4083) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3996 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,841 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4059 (0.4081) Acc D Real: 100.000% 
Loss D Fake: 1.1155 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3996 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,848 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4075 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3996 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,855 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4084 (0.4081) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3995 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,862 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4075 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1157 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3995 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,869 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4061 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1157 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3995 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,877 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4081 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3994 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,884 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4088 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3994 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,891 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4079 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1159 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3994 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,898 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4090 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3993 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,905 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4075 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3993 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,912 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4068 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3993 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,919 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4076 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3993 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,926 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4059 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,934 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4069 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,942 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4075 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,949 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4079 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,957 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4079 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,964 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4073 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3992 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,972 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4060 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,979 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4060 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,987 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4076 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3992 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:38,994 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4059 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,002 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4071 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,010 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4073 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,018 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4065 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,025 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4069 (0.4075) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,032 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4057 (0.4075) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,040 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4057 (0.4074) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3991 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,047 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4053 (0.4074) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,054 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4058 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,061 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4055 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.1163 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,068 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4057 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,076 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4054 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.1162 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3992 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,084 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4061 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3993 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,091 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4056 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.1161 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3993 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,099 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4065 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3993 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,107 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4055 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3993 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,114 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4057 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.1159 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3994 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,122 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4057 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.1159 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3994 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,129 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4054 (0.4070) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3994 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,137 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4044 (0.4070) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3994 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,145 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4055 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1157 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3995 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,152 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4058 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3995 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,159 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4060 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1155 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3995 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,167 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4045 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.1155 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3996 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,175 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4055 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3996 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,182 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4061 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3996 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,189 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4048 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3997 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,197 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4053 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3997 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,204 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4046 (0.4067) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3997 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,212 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4039 (0.4067) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3998 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,220 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4058 (0.4067) Acc D Real: 100.000% 
Loss D Fake: 1.1149 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3998 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,227 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4052 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.1149 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3999 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,235 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4046 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3999 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,242 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4040 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.1147 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4000 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,250 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4048 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4000 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,257 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4043 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4001 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,265 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4053 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.1144 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4001 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,273 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4046 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.1143 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4002 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,280 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4046 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.1142 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4002 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,288 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4042 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4003 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,296 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4046 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.1140 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4003 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,303 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4038 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.1138 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4004 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,311 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4041 (0.4063) Acc D Real: 100.000% 
Loss D Fake: 1.1137 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4004 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,318 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4042 (0.4063) Acc D Real: 100.000% 
Loss D Fake: 1.1136 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4005 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,326 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4045 (0.4063) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4005 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,333 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4040 (0.4062) Acc D Real: 100.000% 
Loss D Fake: 1.1134 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4006 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,341 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4038 (0.4062) Acc D Real: 100.000% 
Loss D Fake: 1.1133 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4006 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,349 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4036 (0.4062) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4007 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,357 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4040 (0.4062) Acc D Real: 100.000% 
Loss D Fake: 1.1130 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4008 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,364 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4037 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.1129 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4008 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,371 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4039 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.1127 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4009 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,379 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4036 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.1126 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4010 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,386 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4036 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.1125 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4010 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,394 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4036 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4011 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,402 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4034 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.1122 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4012 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,410 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4028 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4012 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,419 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4034 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1151) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4013 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,427 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4030 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4014 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,435 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4037 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4014 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,443 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4037 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4015 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,451 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4034 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4016 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,459 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4029 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4017 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,468 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4033 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1149) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4017 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,475 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4028 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4018 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,482 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4032 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.1107 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4019 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,489 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4029 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4019 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,497 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4031 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.1104 (1.1147) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4020 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,505 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4030 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.1102 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4021 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,512 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4029 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4022 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,520 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4028 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.1099 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4022 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,527 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4026 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4023 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,535 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4026 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.1096 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4024 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,542 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4026 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.1095 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4025 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,550 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4024 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.1093 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4025 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,557 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4024 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4026 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,564 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4024 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.1090 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4027 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,573 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4023 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.1089 (1.1142) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4028 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,581 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4023 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.1087 (1.1142) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4028 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,588 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4023 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.1085 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4029 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,596 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4022 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.1084 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4030 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,603 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4021 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.1082 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4031 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,610 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4021 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4032 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,618 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4021 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.1079 (1.1139) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4032 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,625 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4020 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.1077 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4033 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,632 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4017 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.1075 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4034 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,640 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4016 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.1074 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4035 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,648 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4017 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.1072 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4036 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,655 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4016 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.1070 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4036 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,662 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4015 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.1069 (1.1136) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4037 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,670 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4015 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.1067 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4038 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,677 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4014 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.1066 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4039 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,685 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4016 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.1064 (1.1134) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4040 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,692 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4012 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4040 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,700 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4015 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.1061 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4041 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,707 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4016 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.1059 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4042 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,714 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4010 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.1132) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4043 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,722 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4011 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.1056 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4044 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,729 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4013 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.1054 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4044 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,736 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4009 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.1053 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4045 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,744 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4010 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4046 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,751 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4008 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.1049 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4047 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,758 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4006 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4048 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,765 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4008 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1046 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4048 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,773 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4007 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.1127) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4049 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,781 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4005 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1043 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4050 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,789 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4012 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1041 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4051 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,796 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4009 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1040 (1.1125) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4051 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,804 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4005 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1038 (1.1124) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4052 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,811 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4009 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1037 (1.1124) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4053 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,819 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3996 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.1123) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4054 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,827 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4007 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1033 (1.1123) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4055 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,834 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4004 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1032 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4055 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,841 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3999 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1030 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4056 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,849 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4005 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1029 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4057 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,856 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3997 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1028 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4057 (0.4013) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,864 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4005 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1026 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4058 (0.4013) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,871 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4001 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1025 (1.1119) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4059 (0.4013) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,879 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4005 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1023 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4060 (0.4013) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,886 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3995 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4060 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,894 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3997 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1020 (1.1117) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4061 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,901 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3997 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4062 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,908 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4000 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1017 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4062 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,916 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3988 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1016 (1.1115) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4063 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,923 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3994 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1015 (1.1114) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4064 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,931 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4005 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1013 (1.1114) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4064 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,938 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4000 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1012 (1.1113) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4065 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,946 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4007 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1011 (1.1112) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4066 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,953 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4001 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1009 (1.1112) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4066 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,960 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3998 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1008 (1.1111) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4067 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,968 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4009 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1007 (1.1110) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4068 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,975 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3997 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1006 (1.1110) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4068 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,983 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3985 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1004 (1.1109) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4069 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,990 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3990 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.1109) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4069 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:39,998 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3992 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1002 (1.1108) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4070 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,005 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3990 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.1107) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4071 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,013 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3988 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.1000 (1.1107) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4071 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,020 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3998 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.0999 (1.1106) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4072 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,029 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3996 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.1105) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4072 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,036 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3987 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.0997 (1.1105) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4073 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,043 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3986 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.1104) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4073 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,050 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3997 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.0995 (1.1103) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4074 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,057 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3990 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.0994 (1.1103) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4074 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,065 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3988 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.1102) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4075 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,072 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3986 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.0992 (1.1102) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4075 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,079 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3996 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.0991 (1.1101) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4076 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,086 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3994 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.1100) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4076 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,093 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3982 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.0989 (1.1100) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4076 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,100 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3973 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.0988 (1.1099) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4077 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,107 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3991 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.1098) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4077 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,114 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3977 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.1098) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4078 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,122 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3981 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.0986 (1.1097) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4078 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,129 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3997 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.1097) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4078 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,136 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3980 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.1096) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4079 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,144 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3984 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.1095) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4079 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,152 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3981 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.1095) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4079 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,160 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3968 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.0982 (1.1094) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4080 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,167 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3985 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.0982 (1.1094) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4080 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,177 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3978 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.1093) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4080 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,185 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3974 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.1092) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4081 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,192 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3986 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.1092) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4081 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,199 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3961 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.1091) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4081 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,207 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3971 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.1091) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4081 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,214 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3975 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1090) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4082 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,221 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3977 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1090) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4082 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,228 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3984 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1089) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4082 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,236 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3966 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4082 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,243 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3979 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4082 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,250 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3976 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.1087) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,257 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3957 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.1087) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4083 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,265 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3979 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.1086) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,272 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3972 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.1086) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,280 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3980 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1085) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4083 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,287 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3962 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1085) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4083 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,294 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3979 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1084) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,302 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3967 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1083) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4083 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,309 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3971 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1083) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4084 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,316 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3962 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1082) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4084 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,323 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3969 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1082) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4084 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,331 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3963 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1081) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4084 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,338 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3982 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1081) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4084 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,345 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3972 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1080) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4084 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,352 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3992 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1080) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4084 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,359 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3976 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1079) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4084 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,366 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3976 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1079) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4084 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,373 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3958 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1078) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4084 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,380 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3998 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1078) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4084 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,387 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3964 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1077) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4084 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,394 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3964 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1077) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4084 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,402 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3955 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.0974 (1.1077) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4084 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,409 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3987 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1076) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4084 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,416 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3956 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1076) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4084 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,424 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3954 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1075) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4083 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,431 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3976 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1075) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,438 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.3974 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1074) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,450 -                train: [    INFO] - 
Epoch: 16/20
2023-03-01 13:45:40,643 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3956 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.0976) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4083 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,653 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3977 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.0976) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4083 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,660 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3965 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.0976) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4083 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,677 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3949 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.0976) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4082 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,684 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3977 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4082 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,691 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3983 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4082 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,698 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3937 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4082 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,705 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3959 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4082 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,712 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3990 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4081 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,719 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3955 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.0978) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4081 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,726 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3978 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.0978) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4081 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,734 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3950 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.0978) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4081 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,741 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3947 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.0978) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4080 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,748 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3960 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0982 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4080 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,756 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3951 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4080 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,763 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3956 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4080 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,770 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3949 (0.3961) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4079 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,777 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3977 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.0980) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4079 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,783 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3976 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.0980) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4079 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,790 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3971 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0986 (1.0980) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4078 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,797 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3981 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.0981) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4078 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,804 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3962 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0988 (1.0981) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4077 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,811 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3933 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0989 (1.0981) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4077 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,818 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3967 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.0982) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4076 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,825 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3954 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.0991 (1.0982) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4076 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,832 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3961 (0.3962) Acc D Real: 100.000% 
Loss D Fake: 1.0992 (1.0982) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4075 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,839 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3980 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4075 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,846 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3981 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0994 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4074 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,853 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3980 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0995 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4074 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,860 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3970 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4073 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,870 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3950 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0997 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4073 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,877 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3949 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4072 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,885 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3991 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1000 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4072 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,892 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3997 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4071 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,899 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3988 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1002 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4071 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,906 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3947 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4070 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,914 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3941 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1004 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4069 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,921 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3928 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1005 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4069 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,928 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3959 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1007 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4068 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,936 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3973 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1008 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4067 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,943 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3965 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1010 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4067 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,950 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3942 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1011 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4066 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,957 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3957 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1012 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4065 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,964 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3982 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1014 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4065 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,972 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3970 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1015 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4064 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,979 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3955 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1017 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4063 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,987 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3941 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4062 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:40,994 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3966 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1020 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4062 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,001 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3963 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4061 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,009 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3969 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1023 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4060 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,016 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3953 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1025 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4059 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,023 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3958 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4059 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,031 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3939 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1029 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4058 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,038 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3963 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1030 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4057 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,045 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3983 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1032 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4056 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,053 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3986 (0.3963) Acc D Real: 100.000% 
Loss D Fake: 1.1034 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4055 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,060 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3982 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1036 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4054 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,067 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3966 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1038 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4053 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,075 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3962 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1040 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4052 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,083 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3988 (0.3964) Acc D Real: 100.000% 
Loss D Fake: 1.1042 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4051 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,091 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4002 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4050 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,099 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3999 (0.3965) Acc D Real: 100.000% 
Loss D Fake: 1.1046 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4049 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,106 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4000 (0.3966) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4049 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,113 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4023 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1049 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4048 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,121 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4016 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.1004) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4047 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,128 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3964 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1053 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4046 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,135 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.3967 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4045 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,143 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3946 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4044 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,150 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3987 (0.3967) Acc D Real: 100.000% 
Loss D Fake: 1.1059 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4043 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,157 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4026 (0.3968) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4042 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,164 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4025 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1064 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4041 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,171 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3997 (0.3969) Acc D Real: 100.000% 
Loss D Fake: 1.1066 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4040 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,179 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4005 (0.3970) Acc D Real: 100.000% 
Loss D Fake: 1.1068 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4039 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,187 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4042 (0.3971) Acc D Real: 100.000% 
Loss D Fake: 1.1070 (1.1011) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4038 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,195 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3999 (0.3971) Acc D Real: 100.000% 
Loss D Fake: 1.1072 (1.1012) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4037 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,202 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4000 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1074 (1.1012) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4036 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,210 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3976 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.1013) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4035 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,217 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4008 (0.3972) Acc D Real: 100.000% 
Loss D Fake: 1.1078 (1.1014) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4034 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,224 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4018 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.1015) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4033 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,232 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3981 (0.3973) Acc D Real: 100.000% 
Loss D Fake: 1.1083 (1.1016) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4032 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,239 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4050 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1085 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4031 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,247 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4010 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1087 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4030 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,255 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4001 (0.3974) Acc D Real: 100.000% 
Loss D Fake: 1.1089 (1.1018) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4029 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,263 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4019 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.1019) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4027 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,270 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3983 (0.3975) Acc D Real: 100.000% 
Loss D Fake: 1.1094 (1.1020) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4026 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,278 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4022 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1096 (1.1021) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4025 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,286 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4032 (0.3976) Acc D Real: 100.000% 
Loss D Fake: 1.1099 (1.1022) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4024 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,293 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4010 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4023 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,300 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4014 (0.3977) Acc D Real: 100.000% 
Loss D Fake: 1.1104 (1.1024) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4022 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,308 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4029 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1106 (1.1024) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4020 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,315 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4012 (0.3978) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.1025) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4019 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,323 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4033 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1111 (1.1026) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4018 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,330 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4010 (0.3979) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4017 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,337 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4038 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1028) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4016 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,345 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4028 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1029) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4015 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,353 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4021 (0.3980) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.1030) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4013 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,360 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4051 (0.3981) Acc D Real: 100.000% 
Loss D Fake: 1.1123 (1.1031) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4012 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,368 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4039 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1126 (1.1032) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4011 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,375 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4052 (0.3982) Acc D Real: 100.000% 
Loss D Fake: 1.1128 (1.1033) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4010 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,383 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4083 (0.3983) Acc D Real: 100.000% 
Loss D Fake: 1.1130 (1.1034) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4009 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,391 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4059 (0.3984) Acc D Real: 100.000% 
Loss D Fake: 1.1132 (1.1035) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4008 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,398 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4047 (0.3985) Acc D Real: 100.000% 
Loss D Fake: 1.1134 (1.1036) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4007 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,405 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4071 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1136 (1.1037) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4006 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,413 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4068 (0.3986) Acc D Real: 100.000% 
Loss D Fake: 1.1138 (1.1038) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4005 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,420 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4068 (0.3987) Acc D Real: 100.000% 
Loss D Fake: 1.1140 (1.1039) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4005 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,427 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4069 (0.3988) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.1040) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4004 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,435 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4060 (0.3989) Acc D Real: 100.000% 
Loss D Fake: 1.1143 (1.1041) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4003 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,442 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4085 (0.3990) Acc D Real: 100.000% 
Loss D Fake: 1.1144 (1.1042) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4002 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,449 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4104 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.1042) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4002 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,458 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4075 (0.3991) Acc D Real: 100.000% 
Loss D Fake: 1.1147 (1.1043) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4001 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,466 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4081 (0.3992) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.1044) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4001 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,474 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4112 (0.3993) Acc D Real: 100.000% 
Loss D Fake: 1.1149 (1.1045) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4000 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,481 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4075 (0.3994) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1046) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4000 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,489 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4105 (0.3995) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1047) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3999 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,496 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4076 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1048) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3999 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,504 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4094 (0.3996) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1049) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3999 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,511 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4061 (0.3997) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1050) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3998 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,519 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4117 (0.3998) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1051) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3998 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,526 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4091 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1052) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3998 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,534 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4081 (0.3999) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1052) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3998 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,541 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4121 (0.4000) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3998 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,548 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4071 (0.4001) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1054) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3998 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,556 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4105 (0.4002) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1055) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3998 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,563 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4119 (0.4003) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3998 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,571 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4112 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1153 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3998 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,579 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4107 (0.4004) Acc D Real: 100.000% 
Loss D Fake: 1.1152 (1.1057) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3999 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,587 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4098 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1151 (1.1058) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3999 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,594 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4136 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3999 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,602 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4145 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1149 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4000 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,611 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4140 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1147 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4001 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,619 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4129 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.1061) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4002 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,628 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4149 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1143 (1.1061) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4003 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,636 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4151 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1141 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4004 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,645 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4136 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1138 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4005 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,653 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4174 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1063) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.4007 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,660 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4131 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.1063) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4009 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,668 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4165 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1128 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4010 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,675 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4154 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4012 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,682 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4134 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4014 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,689 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4126 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4016 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,697 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4205 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.4018 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,705 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4142 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.1107 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4020 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,713 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4185 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.1102 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4022 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,720 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4187 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1097 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4025 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,728 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4179 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4027 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,736 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4190 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1086 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.4030 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,744 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4184 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4033 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,751 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4200 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1074 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4036 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,759 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4180 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.1068 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4039 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,766 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4207 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.1061 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4042 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,774 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4199 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4045 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,781 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4210 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4048 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,789 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4180 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.1041 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4052 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,796 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4205 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.1034 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4055 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,804 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4191 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4058 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,811 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4188 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.1020 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4062 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,819 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4193 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.1013 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4065 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,827 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4204 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.1006 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4068 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,835 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4202 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.0999 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4072 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,842 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4205 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.0992 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4075 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,850 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4205 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.1063) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4078 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,857 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4223 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1063) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4082 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,865 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4233 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.0971 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4085 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,872 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4228 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.0963 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4089 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,880 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4234 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.0956 (1.1061) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4093 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,888 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4244 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4097 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,895 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4229 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.0940 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4101 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,903 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4240 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.0932 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4105 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,911 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4233 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1058) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4109 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,918 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4233 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.0915 (1.1057) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4113 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,925 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4217 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.0907 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4117 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,933 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4251 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.0899 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4121 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,940 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4267 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.0891 (1.1055) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4125 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,947 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4245 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.0883 (1.1054) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4129 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,954 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4241 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.0874 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4133 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,963 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4245 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.0866 (1.1052) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4137 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,970 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4239 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.0858 (1.1050) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4141 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,977 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4239 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.0850 (1.1049) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4145 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,984 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4254 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.0842 (1.1048) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4149 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,991 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4256 (0.4061) Acc D Real: 100.000% 
Loss D Fake: 1.0834 (1.1047) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4153 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:41,998 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4259 (0.4063) Acc D Real: 100.000% 
Loss D Fake: 1.0826 (1.1046) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4157 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,006 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4250 (0.4064) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.1045) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4161 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,013 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4261 (0.4065) Acc D Real: 100.000% 
Loss D Fake: 1.0810 (1.1043) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4165 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,020 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4259 (0.4066) Acc D Real: 100.000% 
Loss D Fake: 1.0802 (1.1042) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4169 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,027 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4259 (0.4067) Acc D Real: 100.000% 
Loss D Fake: 1.0795 (1.1041) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4173 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,035 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4273 (0.4068) Acc D Real: 100.000% 
Loss D Fake: 1.0787 (1.1039) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4177 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,042 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4273 (0.4069) Acc D Real: 100.000% 
Loss D Fake: 1.0778 (1.1038) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4181 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,050 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4265 (0.4070) Acc D Real: 100.000% 
Loss D Fake: 1.0770 (1.1036) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4185 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,057 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4268 (0.4071) Acc D Real: 100.000% 
Loss D Fake: 1.0762 (1.1035) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4189 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,064 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4272 (0.4072) Acc D Real: 100.000% 
Loss D Fake: 1.0754 (1.1034) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4193 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,072 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4268 (0.4073) Acc D Real: 100.000% 
Loss D Fake: 1.0746 (1.1032) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4197 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,079 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4269 (0.4074) Acc D Real: 100.000% 
Loss D Fake: 1.0738 (1.1031) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4201 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,086 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4277 (0.4075) Acc D Real: 100.000% 
Loss D Fake: 1.0730 (1.1029) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4206 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,094 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4280 (0.4076) Acc D Real: 100.000% 
Loss D Fake: 1.0721 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4210 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,101 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4266 (0.4077) Acc D Real: 100.000% 
Loss D Fake: 1.0713 (1.1026) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4214 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,108 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4260 (0.4078) Acc D Real: 100.000% 
Loss D Fake: 1.0705 (1.1024) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4218 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,115 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4282 (0.4079) Acc D Real: 100.000% 
Loss D Fake: 1.0698 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4222 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,123 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4284 (0.4080) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.1021) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4226 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,130 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4278 (0.4081) Acc D Real: 100.000% 
Loss D Fake: 1.0682 (1.1019) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4230 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,137 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4282 (0.4082) Acc D Real: 100.000% 
Loss D Fake: 1.0674 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4234 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,145 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4282 (0.4083) Acc D Real: 100.000% 
Loss D Fake: 1.0666 (1.1016) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4238 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,152 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4277 (0.4084) Acc D Real: 100.000% 
Loss D Fake: 1.0659 (1.1014) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4242 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,159 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4289 (0.4085) Acc D Real: 100.000% 
Loss D Fake: 1.0651 (1.1012) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4246 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,166 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4290 (0.4086) Acc D Real: 100.000% 
Loss D Fake: 1.0643 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4250 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,173 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4281 (0.4087) Acc D Real: 100.000% 
Loss D Fake: 1.0635 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4254 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,181 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4285 (0.4088) Acc D Real: 100.000% 
Loss D Fake: 1.0627 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4258 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,188 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4282 (0.4089) Acc D Real: 100.000% 
Loss D Fake: 1.0619 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4262 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,195 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4286 (0.4090) Acc D Real: 100.000% 
Loss D Fake: 1.0611 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4266 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,202 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4290 (0.4091) Acc D Real: 100.000% 
Loss D Fake: 1.0604 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4270 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,209 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4284 (0.4092) Acc D Real: 100.000% 
Loss D Fake: 1.0596 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4274 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,216 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4284 (0.4093) Acc D Real: 100.000% 
Loss D Fake: 1.0589 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4278 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,224 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4294 (0.4094) Acc D Real: 100.000% 
Loss D Fake: 1.0582 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4282 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,231 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4289 (0.4094) Acc D Real: 100.000% 
Loss D Fake: 1.0574 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4286 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,238 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4291 (0.4095) Acc D Real: 100.000% 
Loss D Fake: 1.0567 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4289 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,245 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4291 (0.4096) Acc D Real: 100.000% 
Loss D Fake: 1.0559 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4293 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,252 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4294 (0.4097) Acc D Real: 100.000% 
Loss D Fake: 1.0552 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4297 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,260 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4292 (0.4098) Acc D Real: 100.000% 
Loss D Fake: 1.0545 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.484 
Loss G: 0.4301 (0.4080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,267 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4293 (0.4099) Acc D Real: 100.000% 
Loss D Fake: 1.0537 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4305 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,274 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4295 (0.4100) Acc D Real: 100.000% 
Loss D Fake: 1.0530 (1.0981) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4309 (0.4082) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,281 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4295 (0.4101) Acc D Real: 100.000% 
Loss D Fake: 1.0522 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4313 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,288 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4295 (0.4102) Acc D Real: 100.000% 
Loss D Fake: 1.0515 (1.0977) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4317 (0.4084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,296 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4297 (0.4103) Acc D Real: 100.000% 
Loss D Fake: 1.0508 (1.0975) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4321 (0.4085) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,303 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4297 (0.4103) Acc D Real: 100.000% 
Loss D Fake: 1.0500 (1.0973) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4325 (0.4086) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,310 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4297 (0.4104) Acc D Real: 100.000% 
Loss D Fake: 1.0493 (1.0971) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4329 (0.4087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,317 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4298 (0.4105) Acc D Real: 100.000% 
Loss D Fake: 1.0485 (1.0968) Acc D Fake: 0.000% 
Loss D: 1.478 
Loss G: 0.4333 (0.4088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,332 -                train: [    INFO] - 
Epoch: 17/20
2023-03-01 13:45:42,515 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4301 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0471 (1.0474) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4340 (0.4338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,524 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4300 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0463 (1.0471) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4344 (0.4340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,532 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4301 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0456 (1.0467) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4348 (0.4342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,548 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4302 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0449 (1.0463) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4352 (0.4344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,555 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4302 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0442 (1.0460) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4356 (0.4346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,563 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4301 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0435 (1.0456) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4359 (0.4348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,569 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4300 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0428 (1.0453) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4363 (0.4350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,576 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4300 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0420 (1.0449) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4367 (0.4352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,583 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4305 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0413 (1.0446) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4371 (0.4354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,591 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4303 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0405 (1.0442) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4376 (0.4356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,598 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4305 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0398 (1.0438) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4380 (0.4358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,604 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4306 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0390 (1.0434) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4384 (0.4360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,611 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4307 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0382 (1.0431) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4388 (0.4362) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,618 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4306 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0375 (1.0427) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4392 (0.4364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,625 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4309 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0367 (1.0423) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4396 (0.4366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,632 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4308 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0360 (1.0420) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4400 (0.4368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,639 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4307 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0352 (1.0416) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4404 (0.4370) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,646 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4312 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0345 (1.0412) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4409 (0.4372) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,653 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4309 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0337 (1.0408) Acc D Fake: 0.000% 
Loss D: 1.465 
Loss G: 0.4413 (0.4374) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,660 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4312 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0330 (1.0405) Acc D Fake: 0.000% 
Loss D: 1.464 
Loss G: 0.4417 (0.4376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,667 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4312 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0323 (1.0401) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4421 (0.4378) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,674 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4301 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0315 (1.0397) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4425 (0.4380) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,681 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4306 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0307 (1.0393) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4429 (0.4382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,688 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4316 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0299 (1.0390) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4434 (0.4384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,695 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4305 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0292 (1.0386) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4438 (0.4386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,702 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4314 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0284 (1.0382) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4442 (0.4388) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,709 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4318 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0276 (1.0378) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4447 (0.4390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,716 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4323 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0269 (1.0374) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4451 (0.4393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,723 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4321 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0261 (1.0371) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4455 (0.4395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,731 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4323 (0.4308) Acc D Real: 100.000% 
Loss D Fake: 1.0254 (1.0367) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4459 (0.4397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,738 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4321 (0.4308) Acc D Real: 100.000% 
Loss D Fake: 1.0248 (1.0363) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4462 (0.4399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,745 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4310 (0.4308) Acc D Real: 100.000% 
Loss D Fake: 1.0241 (1.0360) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4466 (0.4401) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,753 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4322 (0.4308) Acc D Real: 100.000% 
Loss D Fake: 1.0234 (1.0356) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4470 (0.4403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,760 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4328 (0.4309) Acc D Real: 100.000% 
Loss D Fake: 1.0227 (1.0352) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4474 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,768 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4323 (0.4309) Acc D Real: 100.000% 
Loss D Fake: 1.0220 (1.0348) Acc D Fake: 0.000% 
Loss D: 1.454 
Loss G: 0.4478 (0.4407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,775 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4318 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0214 (1.0345) Acc D Fake: 0.000% 
Loss D: 1.453 
Loss G: 0.4482 (0.4409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,784 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4303 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0207 (1.0341) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4486 (0.4411) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,793 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4320 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0200 (1.0338) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4490 (0.4413) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,801 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4314 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0192 (1.0334) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4494 (0.4415) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,808 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4329 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0185 (1.0330) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4498 (0.4417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,816 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4324 (0.4311) Acc D Real: 100.000% 
Loss D Fake: 1.0178 (1.0327) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4502 (0.4419) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,823 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4316 (0.4311) Acc D Real: 100.000% 
Loss D Fake: 1.0172 (1.0323) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4506 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,830 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4315 (0.4311) Acc D Real: 100.000% 
Loss D Fake: 1.0165 (1.0319) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4510 (0.4423) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,837 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4319 (0.4311) Acc D Real: 100.000% 
Loss D Fake: 1.0157 (1.0316) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4514 (0.4425) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,845 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4344 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0151 (1.0312) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4518 (0.4427) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,852 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4308 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0144 (1.0309) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4522 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,859 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4319 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0137 (1.0305) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4526 (0.4431) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,866 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4315 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0130 (1.0302) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4530 (0.4433) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,873 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4337 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0123 (1.0298) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4533 (0.4435) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,881 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4337 (0.4313) Acc D Real: 100.000% 
Loss D Fake: 1.0117 (1.0294) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4537 (0.4437) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,888 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4324 (0.4313) Acc D Real: 100.000% 
Loss D Fake: 1.0111 (1.0291) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4541 (0.4439) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,895 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4323 (0.4313) Acc D Real: 100.000% 
Loss D Fake: 1.0105 (1.0287) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4544 (0.4441) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,902 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4325 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.0099 (1.0284) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4548 (0.4443) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,909 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4330 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.0093 (1.0280) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4551 (0.4445) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,917 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4348 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.0088 (1.0277) Acc D Fake: 0.000% 
Loss D: 1.444 
Loss G: 0.4554 (0.4447) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,924 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4334 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0083 (1.0274) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4557 (0.4449) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,931 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4311 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0078 (1.0270) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4560 (0.4451) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,939 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4341 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0072 (1.0267) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4563 (0.4453) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,946 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4317 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0068 (1.0264) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4566 (0.4455) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,953 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4339 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0063 (1.0260) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4569 (0.4456) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,961 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4328 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0058 (1.0257) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4572 (0.4458) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,968 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4316 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0054 (1.0254) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4574 (0.4460) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,975 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4320 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0050 (1.0251) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4577 (0.4462) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,984 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4333 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0045 (1.0247) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4580 (0.4464) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,992 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4334 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0041 (1.0244) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4582 (0.4466) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:42,999 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4342 (0.4317) Acc D Real: 100.000% 
Loss D Fake: 1.0038 (1.0241) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4584 (0.4467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,007 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4353 (0.4317) Acc D Real: 100.000% 
Loss D Fake: 1.0035 (1.0238) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4585 (0.4469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,014 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4342 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.0033 (1.0235) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4586 (0.4471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,021 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4324 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.0032 (1.0232) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4587 (0.4472) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,029 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4353 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.0031 (1.0229) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4588 (0.4474) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,036 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4351 (0.4319) Acc D Real: 100.000% 
Loss D Fake: 1.0030 (1.0227) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4588 (0.4476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,044 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4359 (0.4319) Acc D Real: 100.000% 
Loss D Fake: 1.0031 (1.0224) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4588 (0.4477) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,051 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4345 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0032 (1.0221) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4587 (0.4479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,058 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4341 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0034 (1.0219) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4586 (0.4480) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,065 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4332 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0037 (1.0217) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4585 (0.4481) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,073 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4373 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0039 (1.0214) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4583 (0.4483) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,080 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4343 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0044 (1.0212) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4581 (0.4484) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,087 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4350 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0048 (1.0210) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4578 (0.4485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,094 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4308 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0053 (1.0208) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4575 (0.4486) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,101 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4302 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0058 (1.0206) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4573 (0.4487) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,108 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4331 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0063 (1.0204) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4570 (0.4488) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,117 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4342 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0069 (1.0203) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4567 (0.4489) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,124 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4337 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0076 (1.0201) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4563 (0.4490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,131 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4324 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0083 (1.0200) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4558 (0.4491) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,139 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4317 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0091 (1.0199) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4554 (0.4492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,146 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4350 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0100 (1.0198) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4549 (0.4492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,153 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4360 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0110 (1.0197) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4543 (0.4493) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,160 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4306 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0121 (1.0196) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4537 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,167 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4316 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0132 (1.0195) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4530 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,175 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4306 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0144 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4524 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,182 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4321 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0156 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4517 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,189 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4311 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0169 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4510 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,196 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4299 (0.4322) Acc D Real: 100.000% 
Loss D Fake: 1.0182 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.448 
Loss G: 0.4502 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,204 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4296 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0195 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4495 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,211 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4304 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0209 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4487 (0.4495) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,218 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4315 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0224 (1.0194) Acc D Fake: 0.000% 
Loss D: 1.454 
Loss G: 0.4479 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,225 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4318 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0239 (1.0195) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4470 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,233 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4291 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0255 (1.0195) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4461 (0.4494) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,240 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4315 (0.4321) Acc D Real: 100.000% 
Loss D Fake: 1.0272 (1.0196) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4451 (0.4493) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,248 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4309 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0289 (1.0197) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4441 (0.4493) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,255 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4297 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0308 (1.0198) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4431 (0.4492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,262 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4340 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0326 (1.0199) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4420 (0.4492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,270 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4283 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0346 (1.0201) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4409 (0.4491) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,277 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4297 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0367 (1.0202) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4398 (0.4490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,285 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4336 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0387 (1.0204) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4387 (0.4489) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,292 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4277 (0.4320) Acc D Real: 100.000% 
Loss D Fake: 1.0409 (1.0206) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4375 (0.4488) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,300 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4297 (0.4319) Acc D Real: 100.000% 
Loss D Fake: 1.0430 (1.0208) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4363 (0.4487) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,308 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4247 (0.4319) Acc D Real: 100.000% 
Loss D Fake: 1.0452 (1.0210) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4351 (0.4486) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,315 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4298 (0.4319) Acc D Real: 100.000% 
Loss D Fake: 1.0473 (1.0213) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4340 (0.4484) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,322 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4251 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.0495 (1.0215) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4328 (0.4483) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,330 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4291 (0.4318) Acc D Real: 100.000% 
Loss D Fake: 1.0516 (1.0218) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4317 (0.4481) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,337 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4273 (0.4317) Acc D Real: 100.000% 
Loss D Fake: 1.0538 (1.0221) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4305 (0.4480) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,344 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4260 (0.4317) Acc D Real: 100.000% 
Loss D Fake: 1.0560 (1.0224) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4294 (0.4478) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,351 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4272 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0581 (1.0227) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4282 (0.4476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,358 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4245 (0.4316) Acc D Real: 100.000% 
Loss D Fake: 1.0603 (1.0230) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4270 (0.4475) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,366 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4260 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0624 (1.0233) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4259 (0.4473) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,373 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4249 (0.4315) Acc D Real: 100.000% 
Loss D Fake: 1.0646 (1.0237) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4248 (0.4471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,380 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4218 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.0667 (1.0240) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4237 (0.4469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,388 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4261 (0.4314) Acc D Real: 100.000% 
Loss D Fake: 1.0688 (1.0244) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4226 (0.4467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,395 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4250 (0.4313) Acc D Real: 100.000% 
Loss D Fake: 1.0709 (1.0248) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4215 (0.4465) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,402 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4233 (0.4312) Acc D Real: 100.000% 
Loss D Fake: 1.0730 (1.0252) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4204 (0.4463) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,410 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4177 (0.4311) Acc D Real: 100.000% 
Loss D Fake: 1.0750 (1.0256) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4194 (0.4461) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,417 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4210 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0769 (1.0260) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4184 (0.4458) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,425 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4216 (0.4310) Acc D Real: 100.000% 
Loss D Fake: 1.0788 (1.0264) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4174 (0.4456) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,434 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4238 (0.4309) Acc D Real: 100.000% 
Loss D Fake: 1.0807 (1.0269) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4165 (0.4454) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,442 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4200 (0.4308) Acc D Real: 100.000% 
Loss D Fake: 1.0826 (1.0273) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4155 (0.4451) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,449 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4174 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0844 (1.0277) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4146 (0.4449) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,456 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4221 (0.4307) Acc D Real: 100.000% 
Loss D Fake: 1.0862 (1.0282) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4137 (0.4447) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,463 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4201 (0.4306) Acc D Real: 100.000% 
Loss D Fake: 1.0880 (1.0287) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4128 (0.4444) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,471 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4172 (0.4305) Acc D Real: 100.000% 
Loss D Fake: 1.0897 (1.0291) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4119 (0.4442) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,478 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4203 (0.4304) Acc D Real: 100.000% 
Loss D Fake: 1.0914 (1.0296) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4110 (0.4439) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,485 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4169 (0.4303) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.0301) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4102 (0.4437) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,492 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4151 (0.4302) Acc D Real: 100.000% 
Loss D Fake: 1.0947 (1.0306) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4094 (0.4434) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,500 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4179 (0.4301) Acc D Real: 100.000% 
Loss D Fake: 1.0963 (1.0310) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4086 (0.4431) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,507 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4153 (0.4300) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.0315) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4079 (0.4429) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,514 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4122 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.0992 (1.0320) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4072 (0.4426) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,522 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4172 (0.4298) Acc D Real: 100.000% 
Loss D Fake: 1.1006 (1.0325) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4065 (0.4424) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,529 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4134 (0.4296) Acc D Real: 100.000% 
Loss D Fake: 1.1020 (1.0330) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4059 (0.4421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,536 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4155 (0.4295) Acc D Real: 100.000% 
Loss D Fake: 1.1032 (1.0335) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4052 (0.4418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,544 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4119 (0.4294) Acc D Real: 100.000% 
Loss D Fake: 1.1045 (1.0340) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4046 (0.4416) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,551 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4161 (0.4293) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0345) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4040 (0.4413) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,558 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4123 (0.4292) Acc D Real: 100.000% 
Loss D Fake: 1.1069 (1.0350) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4035 (0.4410) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,565 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4130 (0.4291) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.0356) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4029 (0.4408) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,573 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4123 (0.4290) Acc D Real: 100.000% 
Loss D Fake: 1.1091 (1.0361) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4024 (0.4405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,580 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4120 (0.4289) Acc D Real: 100.000% 
Loss D Fake: 1.1102 (1.0366) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4019 (0.4402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,587 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4094 (0.4287) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.0371) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4014 (0.4400) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,595 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4118 (0.4286) Acc D Real: 100.000% 
Loss D Fake: 1.1121 (1.0376) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4009 (0.4397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,602 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4097 (0.4285) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.0381) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4005 (0.4395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,609 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4117 (0.4284) Acc D Real: 100.000% 
Loss D Fake: 1.1139 (1.0386) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4001 (0.4392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,616 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4090 (0.4282) Acc D Real: 100.000% 
Loss D Fake: 1.1148 (1.0391) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3997 (0.4389) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,624 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4121 (0.4281) Acc D Real: 100.000% 
Loss D Fake: 1.1156 (1.0396) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3993 (0.4387) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,631 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4100 (0.4280) Acc D Real: 100.000% 
Loss D Fake: 1.1164 (1.0401) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3989 (0.4384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,638 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4089 (0.4279) Acc D Real: 100.000% 
Loss D Fake: 1.1171 (1.0406) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3985 (0.4382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,645 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4097 (0.4278) Acc D Real: 100.000% 
Loss D Fake: 1.1179 (1.0411) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3982 (0.4379) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,653 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4093 (0.4277) Acc D Real: 100.000% 
Loss D Fake: 1.1186 (1.0416) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3979 (0.4376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,660 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4105 (0.4275) Acc D Real: 100.000% 
Loss D Fake: 1.1192 (1.0421) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3976 (0.4374) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,667 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4035 (0.4274) Acc D Real: 100.000% 
Loss D Fake: 1.1199 (1.0426) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3973 (0.4371) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,674 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4060 (0.4273) Acc D Real: 100.000% 
Loss D Fake: 1.1204 (1.0431) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3970 (0.4369) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,682 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4094 (0.4272) Acc D Real: 100.000% 
Loss D Fake: 1.1209 (1.0436) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3968 (0.4366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,689 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4108 (0.4270) Acc D Real: 100.000% 
Loss D Fake: 1.1214 (1.0440) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3965 (0.4364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,696 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4049 (0.4269) Acc D Real: 100.000% 
Loss D Fake: 1.1219 (1.0445) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3963 (0.4361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,704 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4071 (0.4268) Acc D Real: 100.000% 
Loss D Fake: 1.1223 (1.0450) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3961 (0.4359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,711 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4087 (0.4267) Acc D Real: 100.000% 
Loss D Fake: 1.1228 (1.0455) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3959 (0.4356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,719 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4087 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.1232 (1.0459) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3957 (0.4354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,726 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4083 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.1236 (1.0464) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3955 (0.4352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,733 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4078 (0.4263) Acc D Real: 100.000% 
Loss D Fake: 1.1240 (1.0469) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3953 (0.4349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,741 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4060 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.1244 (1.0473) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3951 (0.4347) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,748 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4042 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.1248 (1.0478) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3950 (0.4344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,755 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4068 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.1251 (1.0483) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3948 (0.4342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,763 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4048 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.1253 (1.0487) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3947 (0.4340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,770 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4045 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.1256 (1.0492) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3946 (0.4338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,777 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4052 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.1258 (1.0496) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3945 (0.4335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,784 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4058 (0.4255) Acc D Real: 100.000% 
Loss D Fake: 1.1260 (1.0500) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3944 (0.4333) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,792 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4043 (0.4254) Acc D Real: 100.000% 
Loss D Fake: 1.1261 (1.0505) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3943 (0.4331) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,799 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4048 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.1263 (1.0509) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3943 (0.4329) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,806 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4036 (0.4251) Acc D Real: 100.000% 
Loss D Fake: 1.1264 (1.0513) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3942 (0.4326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,814 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4046 (0.4250) Acc D Real: 100.000% 
Loss D Fake: 1.1265 (1.0517) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3942 (0.4324) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,821 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4052 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.1266 (1.0522) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3941 (0.4322) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,828 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4048 (0.4248) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0526) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3941 (0.4320) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,837 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4024 (0.4247) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0530) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3941 (0.4318) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,844 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4042 (0.4246) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0534) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3941 (0.4316) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,852 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4024 (0.4244) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3941 (0.4314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,859 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4042 (0.4243) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0542) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3941 (0.4312) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,867 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4027 (0.4242) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0546) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3941 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,874 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4050 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0550) Acc D Fake: 0.000% 
Loss D: 1.532 
Loss G: 0.3941 (0.4308) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,882 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4028 (0.4240) Acc D Real: 100.000% 
Loss D Fake: 1.1266 (1.0554) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3942 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,889 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4026 (0.4239) Acc D Real: 100.000% 
Loss D Fake: 1.1265 (1.0557) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3942 (0.4304) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,897 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4025 (0.4238) Acc D Real: 100.000% 
Loss D Fake: 1.1264 (1.0561) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3942 (0.4302) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,904 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4027 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.1263 (1.0565) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3943 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,912 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4028 (0.4236) Acc D Real: 100.000% 
Loss D Fake: 1.1262 (1.0568) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3944 (0.4298) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,919 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4029 (0.4234) Acc D Real: 100.000% 
Loss D Fake: 1.1261 (1.0572) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3944 (0.4296) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,927 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4018 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.1259 (1.0576) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3945 (0.4295) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,934 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4023 (0.4232) Acc D Real: 100.000% 
Loss D Fake: 1.1258 (1.0579) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3946 (0.4293) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,942 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4035 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.1256 (1.0583) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3947 (0.4291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,949 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4019 (0.4230) Acc D Real: 100.000% 
Loss D Fake: 1.1254 (1.0586) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3947 (0.4289) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,957 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4025 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.1252 (1.0589) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3948 (0.4287) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,965 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4008 (0.4228) Acc D Real: 100.000% 
Loss D Fake: 1.1250 (1.0593) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3949 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,972 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4005 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.1248 (1.0596) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3951 (0.4284) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,980 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4027 (0.4226) Acc D Real: 100.000% 
Loss D Fake: 1.1246 (1.0599) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3952 (0.4282) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,987 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4020 (0.4225) Acc D Real: 100.000% 
Loss D Fake: 1.1243 (1.0602) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3953 (0.4281) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:43,995 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4028 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.1241 (1.0606) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3954 (0.4279) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,003 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4022 (0.4223) Acc D Real: 100.000% 
Loss D Fake: 1.1238 (1.0609) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3955 (0.4278) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,011 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4016 (0.4222) Acc D Real: 100.000% 
Loss D Fake: 1.1236 (1.0612) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3956 (0.4276) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,019 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4014 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.1233 (1.0615) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3958 (0.4274) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,026 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4026 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.1230 (1.0618) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3959 (0.4273) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,033 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4014 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.1228 (1.0621) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3960 (0.4271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,041 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4014 (0.4218) Acc D Real: 100.000% 
Loss D Fake: 1.1225 (1.0624) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3962 (0.4270) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,048 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4022 (0.4217) Acc D Real: 100.000% 
Loss D Fake: 1.1222 (1.0627) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3963 (0.4268) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,056 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4023 (0.4216) Acc D Real: 100.000% 
Loss D Fake: 1.1219 (1.0629) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3965 (0.4267) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,063 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4015 (0.4215) Acc D Real: 100.000% 
Loss D Fake: 1.1216 (1.0632) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3966 (0.4266) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,070 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4016 (0.4214) Acc D Real: 100.000% 
Loss D Fake: 1.1212 (1.0635) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3968 (0.4264) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,078 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4018 (0.4213) Acc D Real: 100.000% 
Loss D Fake: 1.1209 (1.0638) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3969 (0.4263) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,085 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4034 (0.4212) Acc D Real: 100.000% 
Loss D Fake: 1.1206 (1.0640) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.3971 (0.4261) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,092 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4015 (0.4212) Acc D Real: 100.000% 
Loss D Fake: 1.1203 (1.0643) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3972 (0.4260) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,100 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4018 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.1200 (1.0645) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3974 (0.4259) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,107 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4022 (0.4210) Acc D Real: 100.000% 
Loss D Fake: 1.1197 (1.0648) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3975 (0.4257) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,115 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4017 (0.4209) Acc D Real: 100.000% 
Loss D Fake: 1.1193 (1.0650) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3977 (0.4256) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,122 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4015 (0.4208) Acc D Real: 100.000% 
Loss D Fake: 1.1190 (1.0653) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3978 (0.4255) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,129 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4010 (0.4207) Acc D Real: 100.000% 
Loss D Fake: 1.1186 (1.0655) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3980 (0.4254) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,137 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4013 (0.4206) Acc D Real: 100.000% 
Loss D Fake: 1.1183 (1.0658) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3982 (0.4252) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,144 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4021 (0.4205) Acc D Real: 100.000% 
Loss D Fake: 1.1179 (1.0660) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3983 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,151 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4013 (0.4205) Acc D Real: 100.000% 
Loss D Fake: 1.1176 (1.0662) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3985 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,159 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4008 (0.4204) Acc D Real: 100.000% 
Loss D Fake: 1.1172 (1.0665) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3987 (0.4249) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,166 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4014 (0.4203) Acc D Real: 100.000% 
Loss D Fake: 1.1168 (1.0667) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3989 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,173 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4021 (0.4202) Acc D Real: 100.000% 
Loss D Fake: 1.1165 (1.0669) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3990 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,185 -                train: [    INFO] - 
Epoch: 18/20
2023-03-01 13:45:44,405 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4015 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1157 (1.1159) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3994 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,412 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4018 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1154 (1.1157) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3996 (0.3994) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,420 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4018 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1156) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3998 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,427 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4012 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1146 (1.1154) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3999 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,437 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4017 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1143 (1.1152) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4001 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,444 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4011 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1139 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4003 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,456 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4005 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,463 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.1146) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4007 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,470 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1128 (1.1144) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4008 (0.4000) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,478 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1124 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4010 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,486 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1141) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4012 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,493 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4014 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1139) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4014 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,500 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4017 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1137) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4016 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,507 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4017 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,514 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4019 (0.4006) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,523 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.1131) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4021 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,530 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4014 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1097 (1.1129) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4023 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,537 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1094 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4025 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,544 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1090 (1.1126) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4026 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,551 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1086 (1.1124) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4028 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,558 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1083 (1.1122) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4030 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,564 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4017 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1079 (1.1120) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4032 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,571 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4019 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4033 (0.4013) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,579 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4018 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1072 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4035 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,586 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4018 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1069 (1.1115) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4037 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,593 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1065 (1.1113) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4039 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,601 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4017 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.1111) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4040 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,608 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4020 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.1109) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4042 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,616 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.1107) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4044 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,623 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.1105) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4045 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,630 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4020 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.1104) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4047 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,638 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4014 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.1102) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4049 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,645 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4018 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1041 (1.1100) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4050 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,652 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4019 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1038 (1.1098) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4052 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,660 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4012 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.1096) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4053 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,667 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4021 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1031 (1.1095) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4055 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,674 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4019 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1028 (1.1093) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4057 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,682 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4020 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1025 (1.1091) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4058 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,689 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.1090) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4060 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,697 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4021 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4061 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,704 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4015 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1016 (1.1086) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4063 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,712 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4012 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1013 (1.1084) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4064 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,719 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4018 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1010 (1.1083) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4065 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,727 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1007 (1.1081) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4067 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,734 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4013 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1004 (1.1079) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4068 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,741 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.1078) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4070 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,749 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4007 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.1076) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4071 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,757 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4019 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.1074) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4072 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,764 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.1073) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4074 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,772 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4017 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.1071) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4075 (0.4036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,779 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4014 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0988 (1.1070) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4076 (0.4037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,787 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4011 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.1068) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4078 (0.4037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,794 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4006 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4079 (0.4038) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,802 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4012 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4080 (0.4039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,809 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1063) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4081 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,817 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4014 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4083 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,824 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4008 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0973 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4084 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,832 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0970 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4085 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,839 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0968 (1.1057) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4086 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,847 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4012 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0966 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4087 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,854 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4007 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0964 (1.1054) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4088 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,862 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4010 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0962 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4089 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,869 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4024 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0960 (1.1051) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4090 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,877 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4011 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0958 (1.1050) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4091 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,884 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3996 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0956 (1.1048) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4092 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,892 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4003 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0954 (1.1047) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4093 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,900 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4018 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0952 (1.1046) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4094 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,907 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4008 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0950 (1.1044) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4095 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,915 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4011 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.1043) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4096 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,922 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4013 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0946 (1.1041) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4097 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,929 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4025 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0944 (1.1040) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4098 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,937 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4019 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0943 (1.1039) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4099 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,944 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4005 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0941 (1.1037) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4099 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,952 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4005 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0939 (1.1036) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4100 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,959 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4008 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0938 (1.1035) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4101 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,967 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3997 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0937 (1.1034) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4102 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,975 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3987 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0935 (1.1032) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4102 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,985 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4017 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0934 (1.1031) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4103 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:44,993 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4001 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0932 (1.1030) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4104 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,000 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4000 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.1029) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4104 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,008 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4016 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0930 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4105 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,015 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4014 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0929 (1.1026) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4105 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,024 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3997 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0928 (1.1025) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4106 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,031 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4016 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0927 (1.1024) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4106 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,039 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4007 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0926 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4107 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,046 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3993 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.1022) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4107 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,055 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4003 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1020) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4108 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,063 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4014 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1019) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4108 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,070 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4008 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1018) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4109 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,078 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3998 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4109 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,085 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4012 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0921 (1.1016) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4109 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,092 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3993 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0921 (1.1015) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4109 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,100 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3997 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0920 (1.1014) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4110 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,107 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3997 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0920 (1.1013) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4110 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,115 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4009 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0919 (1.1012) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4110 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,123 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3977 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0919 (1.1011) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4110 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,131 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4000 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,139 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3999 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,147 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4009 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4111 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,154 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4018 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4111 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,161 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4009 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4111 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,169 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3980 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4111 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,177 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3991 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4111 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,184 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3994 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1004) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4111 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,192 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4002 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,200 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4008 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4111 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,208 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3991 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0917 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4111 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,215 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3980 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4111 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,225 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4006 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,234 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4004 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0918 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,242 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3997 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0919 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4111 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,251 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4005 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0919 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4110 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,259 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4011 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0919 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4110 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,268 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3985 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0920 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4110 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,277 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3994 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0921 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4110 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,285 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3996 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0921 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4109 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,294 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4003 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4109 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,302 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3988 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4109 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,309 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3976 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4108 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,317 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4000 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4108 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,324 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3995 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4107 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,332 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4021 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0926 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4107 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,339 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3986 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0927 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4106 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,346 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4012 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0928 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4106 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,354 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3992 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0929 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4105 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,361 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3986 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4105 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,368 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4004 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0932 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4104 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,377 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4006 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0933 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4103 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,384 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3994 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0934 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4103 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,392 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3983 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0936 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4102 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,399 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4025 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0937 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4101 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,406 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4017 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0939 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4101 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,414 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4015 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0940 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4100 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,422 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4012 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0942 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4099 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,429 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3993 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0943 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4098 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,437 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3981 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0945 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4098 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,444 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3988 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0946 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4097 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,452 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4010 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4096 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,460 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4000 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0950 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4095 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,468 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3983 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0952 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4094 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,475 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4000 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0954 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4093 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,482 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4000 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0956 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4092 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,490 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3981 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0958 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4091 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,497 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4010 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0960 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4090 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,504 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4005 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0962 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4089 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,512 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4008 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0964 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4088 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,519 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3996 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0966 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4087 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,529 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4027 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0969 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4086 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,537 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4001 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0971 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4085 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,544 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3981 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0973 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4084 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,552 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4014 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0976 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4083 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,559 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3996 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4081 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,567 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3988 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0980 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4080 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,574 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4010 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4079 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,581 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3997 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4078 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,589 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4001 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0988 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4077 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,596 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4009 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4075 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,604 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3998 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4074 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,611 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4059 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.0995 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4073 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,619 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4015 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4072 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,626 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4010 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4070 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,634 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4031 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4069 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,641 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4021 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1006 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4068 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,649 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3997 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1009 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4067 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,656 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4029 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1011 (1.0984) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4065 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,664 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4021 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1014 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4064 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,671 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4028 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1017 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4063 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,679 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4002 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1020 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4061 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,686 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4012 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1023 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4060 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,694 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4033 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1025 (1.0985) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4058 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,701 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4034 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1028 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4057 (0.4078) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,709 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4041 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1031 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4056 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,716 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4020 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1034 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4054 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,724 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4062 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1037 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4053 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,731 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4010 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1040 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4051 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,738 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4054 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1043 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4050 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,746 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4055 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1046 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4049 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,753 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4051 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4047 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,761 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4049 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4046 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,768 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4011 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1054 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4045 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,776 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4050 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4043 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,784 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4060 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.1059 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4042 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,791 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4048 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4041 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,799 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4082 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1065 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4039 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,806 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4025 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1068 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4038 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,814 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4050 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1070 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4037 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,823 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4053 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.1073 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4035 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,830 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4063 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4034 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,837 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4064 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1078 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4033 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,845 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4076 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.1081 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4032 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,852 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4079 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1083 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4030 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,860 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4045 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1086 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4029 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,867 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4059 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1088 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4028 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,874 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4047 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1090 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4027 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,882 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4052 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.1093 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4026 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,889 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4102 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.1095 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4025 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,897 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4052 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4023 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,905 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4077 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.1100 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4022 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,913 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4088 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1102 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4021 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,920 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4104 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.1104 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4021 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,927 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4082 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4020 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,934 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4041 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1107 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4019 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,942 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4083 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4018 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,949 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4113 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4017 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,956 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4081 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4017 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,963 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4064 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1113 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4016 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,970 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4095 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4016 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,977 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4095 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4015 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,984 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4124 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4015 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,991 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4065 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4014 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:45,998 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4100 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1004) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4014 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,006 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4135 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1004) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4014 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,013 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4062 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4014 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,020 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4090 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4013 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,027 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4130 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.1119 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4013 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,034 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4124 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4013 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,041 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4101 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4014 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,048 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4070 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1118 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4014 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,055 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4122 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1117 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4014 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,062 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4130 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4014 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,070 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4154 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.4015 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,077 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4106 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1114 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4016 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,084 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4130 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1112 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4016 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,091 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4115 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4017 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,098 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4120 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1109 (1.1011) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4018 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,110 -                train: [    INFO] - 
Epoch: 19/20
2023-03-01 13:45:46,324 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4188 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.1104 (1.1105) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.4020 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,332 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4133 (0.4146) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.1104) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4022 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,340 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4137 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.1098 (1.1103) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4023 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,357 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4162 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.1095 (1.1101) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4025 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,363 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4130 (0.4144) Acc D Real: 100.000% 
Loss D Fake: 1.1092 (1.1099) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4026 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,370 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4152 (0.4145) Acc D Real: 100.000% 
Loss D Fake: 1.1088 (1.1098) Acc D Fake: 0.000% 
Loss D: 1.524 
Loss G: 0.4028 (0.4023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,377 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4171 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.1084 (1.1096) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4030 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,383 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4150 (0.4149) Acc D Real: 100.000% 
Loss D Fake: 1.1080 (1.1094) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4032 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,390 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4132 (0.4147) Acc D Real: 100.000% 
Loss D Fake: 1.1076 (1.1092) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4034 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,396 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4161 (0.4148) Acc D Real: 100.000% 
Loss D Fake: 1.1071 (1.1090) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4036 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,403 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4192 (0.4152) Acc D Real: 100.000% 
Loss D Fake: 1.1067 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.4038 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,410 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4165 (0.4153) Acc D Real: 100.000% 
Loss D Fake: 1.1062 (1.1086) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4040 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,417 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4165 (0.4154) Acc D Real: 100.000% 
Loss D Fake: 1.1056 (1.1084) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4043 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,424 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4165 (0.4155) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.1082) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4046 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,430 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4187 (0.4157) Acc D Real: 100.000% 
Loss D Fake: 1.1045 (1.1080) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4048 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,438 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4176 (0.4158) Acc D Real: 100.000% 
Loss D Fake: 1.1040 (1.1077) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4051 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,445 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4177 (0.4159) Acc D Real: 100.000% 
Loss D Fake: 1.1034 (1.1075) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4054 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,452 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4193 (0.4161) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.1072) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4057 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,459 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4187 (0.4162) Acc D Real: 100.000% 
Loss D Fake: 1.1021 (1.1070) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4060 (0.4037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,466 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4176 (0.4163) Acc D Real: 100.000% 
Loss D Fake: 1.1014 (1.1067) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4063 (0.4038) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,473 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4189 (0.4164) Acc D Real: 100.000% 
Loss D Fake: 1.1008 (1.1065) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4066 (0.4039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,480 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4192 (0.4165) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4070 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,487 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4202 (0.4167) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.1059) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4073 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,494 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4205 (0.4168) Acc D Real: 100.000% 
Loss D Fake: 1.0986 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4077 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,501 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4207 (0.4170) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4081 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,508 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4201 (0.4171) Acc D Real: 100.000% 
Loss D Fake: 1.0971 (1.1050) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4084 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,515 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4195 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0963 (1.1047) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4088 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,523 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4196 (0.4172) Acc D Real: 100.000% 
Loss D Fake: 1.0955 (1.1044) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4092 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,531 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4208 (0.4174) Acc D Real: 100.000% 
Loss D Fake: 1.0947 (1.1040) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4096 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,538 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4219 (0.4175) Acc D Real: 100.000% 
Loss D Fake: 1.0939 (1.1037) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4100 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,545 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4202 (0.4176) Acc D Real: 100.000% 
Loss D Fake: 1.0930 (1.1034) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4104 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,553 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4209 (0.4177) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1030) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4108 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,560 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4203 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0913 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4112 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,567 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4197 (0.4178) Acc D Real: 100.000% 
Loss D Fake: 1.0905 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4117 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,575 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4205 (0.4179) Acc D Real: 100.000% 
Loss D Fake: 1.0897 (1.1020) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4121 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,582 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4213 (0.4180) Acc D Real: 100.000% 
Loss D Fake: 1.0888 (1.1016) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4125 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,590 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4222 (0.4181) Acc D Real: 100.000% 
Loss D Fake: 1.0880 (1.1013) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4129 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,597 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4199 (0.4181) Acc D Real: 100.000% 
Loss D Fake: 1.0871 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4133 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,605 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4198 (0.4182) Acc D Real: 100.000% 
Loss D Fake: 1.0863 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4137 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,612 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4226 (0.4183) Acc D Real: 100.000% 
Loss D Fake: 1.0854 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4142 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,619 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4221 (0.4184) Acc D Real: 100.000% 
Loss D Fake: 1.0846 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4146 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,627 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4229 (0.4185) Acc D Real: 100.000% 
Loss D Fake: 1.0837 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4150 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,634 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4237 (0.4186) Acc D Real: 100.000% 
Loss D Fake: 1.0828 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4155 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,641 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4228 (0.4187) Acc D Real: 100.000% 
Loss D Fake: 1.0818 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4160 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,648 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4245 (0.4188) Acc D Real: 100.000% 
Loss D Fake: 1.0809 (1.0983) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4165 (0.4079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,656 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4218 (0.4189) Acc D Real: 100.000% 
Loss D Fake: 1.0799 (1.0979) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4170 (0.4081) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,663 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4227 (0.4190) Acc D Real: 100.000% 
Loss D Fake: 1.0789 (1.0975) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4175 (0.4083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,670 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4232 (0.4191) Acc D Real: 100.000% 
Loss D Fake: 1.0779 (1.0971) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4180 (0.4085) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,677 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4235 (0.4191) Acc D Real: 100.000% 
Loss D Fake: 1.0769 (1.0967) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4185 (0.4087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,684 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4226 (0.4192) Acc D Real: 100.000% 
Loss D Fake: 1.0759 (1.0963) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4190 (0.4089) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,692 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4222 (0.4193) Acc D Real: 100.000% 
Loss D Fake: 1.0749 (1.0959) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4195 (0.4091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,699 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4232 (0.4193) Acc D Real: 100.000% 
Loss D Fake: 1.0740 (1.0955) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4199 (0.4093) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,707 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4237 (0.4194) Acc D Real: 100.000% 
Loss D Fake: 1.0730 (1.0950) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4204 (0.4095) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,714 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4239 (0.4195) Acc D Real: 100.000% 
Loss D Fake: 1.0720 (1.0946) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4209 (0.4097) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,721 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4231 (0.4196) Acc D Real: 100.000% 
Loss D Fake: 1.0710 (1.0942) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4214 (0.4099) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,728 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4237 (0.4196) Acc D Real: 100.000% 
Loss D Fake: 1.0700 (1.0938) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4220 (0.4101) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,736 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4248 (0.4197) Acc D Real: 100.000% 
Loss D Fake: 1.0690 (1.0934) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4225 (0.4103) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,743 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4244 (0.4198) Acc D Real: 100.000% 
Loss D Fake: 1.0680 (1.0929) Acc D Fake: 0.000% 
Loss D: 1.492 
Loss G: 0.4230 (0.4105) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,750 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4240 (0.4199) Acc D Real: 100.000% 
Loss D Fake: 1.0669 (1.0925) Acc D Fake: 0.000% 
Loss D: 1.491 
Loss G: 0.4236 (0.4108) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,758 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4244 (0.4200) Acc D Real: 100.000% 
Loss D Fake: 1.0659 (1.0921) Acc D Fake: 0.000% 
Loss D: 1.490 
Loss G: 0.4241 (0.4110) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,765 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4242 (0.4200) Acc D Real: 100.000% 
Loss D Fake: 1.0648 (1.0916) Acc D Fake: 0.000% 
Loss D: 1.489 
Loss G: 0.4247 (0.4112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,772 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4236 (0.4201) Acc D Real: 100.000% 
Loss D Fake: 1.0637 (1.0912) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4252 (0.4114) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,780 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4242 (0.4201) Acc D Real: 100.000% 
Loss D Fake: 1.0627 (1.0907) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4257 (0.4116) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,787 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4251 (0.4202) Acc D Real: 100.000% 
Loss D Fake: 1.0617 (1.0903) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4263 (0.4119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,795 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4246 (0.4203) Acc D Real: 100.000% 
Loss D Fake: 1.0607 (1.0898) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4268 (0.4121) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,802 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4249 (0.4204) Acc D Real: 100.000% 
Loss D Fake: 1.0596 (1.0894) Acc D Fake: 0.000% 
Loss D: 1.485 
Loss G: 0.4273 (0.4123) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,809 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4248 (0.4204) Acc D Real: 100.000% 
Loss D Fake: 1.0586 (1.0889) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4279 (0.4126) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,817 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4251 (0.4205) Acc D Real: 100.000% 
Loss D Fake: 1.0575 (1.0885) Acc D Fake: 0.000% 
Loss D: 1.483 
Loss G: 0.4285 (0.4128) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,824 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4249 (0.4206) Acc D Real: 100.000% 
Loss D Fake: 1.0564 (1.0880) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4290 (0.4130) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,831 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4251 (0.4206) Acc D Real: 100.000% 
Loss D Fake: 1.0554 (1.0876) Acc D Fake: 0.000% 
Loss D: 1.481 
Loss G: 0.4296 (0.4133) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,838 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4254 (0.4207) Acc D Real: 100.000% 
Loss D Fake: 1.0543 (1.0871) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4302 (0.4135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,846 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4255 (0.4208) Acc D Real: 100.000% 
Loss D Fake: 1.0532 (1.0866) Acc D Fake: 0.000% 
Loss D: 1.479 
Loss G: 0.4308 (0.4137) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,853 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4254 (0.4208) Acc D Real: 100.000% 
Loss D Fake: 1.0520 (1.0862) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4314 (0.4140) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,860 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4256 (0.4209) Acc D Real: 100.000% 
Loss D Fake: 1.0509 (1.0857) Acc D Fake: 0.000% 
Loss D: 1.477 
Loss G: 0.4320 (0.4142) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,867 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4255 (0.4209) Acc D Real: 100.000% 
Loss D Fake: 1.0498 (1.0852) Acc D Fake: 0.000% 
Loss D: 1.475 
Loss G: 0.4326 (0.4144) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,875 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4256 (0.4210) Acc D Real: 100.000% 
Loss D Fake: 1.0486 (1.0847) Acc D Fake: 0.000% 
Loss D: 1.474 
Loss G: 0.4332 (0.4147) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,882 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4259 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.0475 (1.0843) Acc D Fake: 0.000% 
Loss D: 1.473 
Loss G: 0.4338 (0.4149) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,890 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4261 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.0463 (1.0838) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4344 (0.4152) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,898 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4261 (0.4212) Acc D Real: 100.000% 
Loss D Fake: 1.0451 (1.0833) Acc D Fake: 0.000% 
Loss D: 1.471 
Loss G: 0.4351 (0.4154) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,905 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4264 (0.4213) Acc D Real: 100.000% 
Loss D Fake: 1.0439 (1.0828) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4357 (0.4157) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,912 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4262 (0.4213) Acc D Real: 100.000% 
Loss D Fake: 1.0428 (1.0823) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4364 (0.4159) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,920 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4268 (0.4214) Acc D Real: 100.000% 
Loss D Fake: 1.0416 (1.0818) Acc D Fake: 0.000% 
Loss D: 1.468 
Loss G: 0.4370 (0.4162) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,928 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4268 (0.4214) Acc D Real: 100.000% 
Loss D Fake: 1.0404 (1.0813) Acc D Fake: 0.000% 
Loss D: 1.467 
Loss G: 0.4376 (0.4164) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,935 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4272 (0.4215) Acc D Real: 100.000% 
Loss D Fake: 1.0392 (1.0808) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4383 (0.4167) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,942 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4278 (0.4216) Acc D Real: 100.000% 
Loss D Fake: 1.0381 (1.0804) Acc D Fake: 0.000% 
Loss D: 1.466 
Loss G: 0.4389 (0.4170) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,949 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4264 (0.4216) Acc D Real: 100.000% 
Loss D Fake: 1.0369 (1.0799) Acc D Fake: 0.000% 
Loss D: 1.463 
Loss G: 0.4395 (0.4172) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,956 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4259 (0.4217) Acc D Real: 100.000% 
Loss D Fake: 1.0357 (1.0794) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4402 (0.4175) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,964 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4272 (0.4217) Acc D Real: 100.000% 
Loss D Fake: 1.0344 (1.0788) Acc D Fake: 0.000% 
Loss D: 1.462 
Loss G: 0.4409 (0.4177) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,973 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4278 (0.4218) Acc D Real: 100.000% 
Loss D Fake: 1.0332 (1.0783) Acc D Fake: 0.000% 
Loss D: 1.461 
Loss G: 0.4416 (0.4180) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,982 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4279 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.0319 (1.0778) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4423 (0.4183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:46,991 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4278 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.0307 (1.0773) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4430 (0.4185) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,000 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4282 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.0294 (1.0768) Acc D Fake: 0.000% 
Loss D: 1.458 
Loss G: 0.4437 (0.4188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,007 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4270 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.0282 (1.0763) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4444 (0.4191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,014 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4285 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.0269 (1.0758) Acc D Fake: 0.000% 
Loss D: 1.455 
Loss G: 0.4451 (0.4194) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,022 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4306 (0.4222) Acc D Real: 100.000% 
Loss D Fake: 1.0256 (1.0752) Acc D Fake: 0.000% 
Loss D: 1.456 
Loss G: 0.4458 (0.4196) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,029 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4285 (0.4223) Acc D Real: 100.000% 
Loss D Fake: 1.0244 (1.0747) Acc D Fake: 0.000% 
Loss D: 1.453 
Loss G: 0.4465 (0.4199) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,037 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4280 (0.4223) Acc D Real: 100.000% 
Loss D Fake: 1.0232 (1.0742) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4472 (0.4202) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,045 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4300 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.0219 (1.0737) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4479 (0.4205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,052 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4299 (0.4225) Acc D Real: 100.000% 
Loss D Fake: 1.0207 (1.0731) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4486 (0.4207) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,059 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4309 (0.4226) Acc D Real: 100.000% 
Loss D Fake: 1.0196 (1.0726) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4492 (0.4210) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,066 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4307 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.0184 (1.0721) Acc D Fake: 0.000% 
Loss D: 1.449 
Loss G: 0.4499 (0.4213) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,074 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4292 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.0173 (1.0715) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4505 (0.4216) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,081 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4312 (0.4228) Acc D Real: 100.000% 
Loss D Fake: 1.0162 (1.0710) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4512 (0.4219) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,088 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4308 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.0151 (1.0705) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4518 (0.4222) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,096 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4323 (0.4230) Acc D Real: 100.000% 
Loss D Fake: 1.0141 (1.0699) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4524 (0.4225) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,103 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4317 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.0131 (1.0694) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4530 (0.4227) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,111 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4305 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.0121 (1.0689) Acc D Fake: 0.000% 
Loss D: 1.443 
Loss G: 0.4536 (0.4230) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,119 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4311 (0.4232) Acc D Real: 100.000% 
Loss D Fake: 1.0111 (1.0684) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4542 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,126 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4304 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.0101 (1.0678) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4548 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,134 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4307 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.0090 (1.0673) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4554 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,141 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4340 (0.4234) Acc D Real: 100.000% 
Loss D Fake: 1.0080 (1.0668) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4560 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,149 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4323 (0.4235) Acc D Real: 100.000% 
Loss D Fake: 1.0071 (1.0662) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4565 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,156 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4329 (0.4236) Acc D Real: 100.000% 
Loss D Fake: 1.0062 (1.0657) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4571 (0.4247) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,163 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4362 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.0054 (1.0652) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4575 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,171 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4322 (0.4238) Acc D Real: 100.000% 
Loss D Fake: 1.0047 (1.0647) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4579 (0.4253) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,178 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4376 (0.4239) Acc D Real: 100.000% 
Loss D Fake: 1.0041 (1.0641) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4582 (0.4256) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,185 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4357 (0.4240) Acc D Real: 100.000% 
Loss D Fake: 1.0037 (1.0636) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4585 (0.4259) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,193 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4372 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.0034 (1.0631) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4586 (0.4261) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,200 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4356 (0.4242) Acc D Real: 100.000% 
Loss D Fake: 1.0033 (1.0626) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4587 (0.4264) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,208 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4340 (0.4243) Acc D Real: 100.000% 
Loss D Fake: 1.0032 (1.0621) Acc D Fake: 0.000% 
Loss D: 1.437 
Loss G: 0.4588 (0.4267) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,215 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4357 (0.4244) Acc D Real: 100.000% 
Loss D Fake: 1.0031 (1.0617) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4588 (0.4269) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,222 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4373 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.0032 (1.0612) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4588 (0.4272) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,230 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4327 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.0033 (1.0607) Acc D Fake: 0.000% 
Loss D: 1.436 
Loss G: 0.4587 (0.4275) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,237 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4355 (0.4246) Acc D Real: 100.000% 
Loss D Fake: 1.0035 (1.0603) Acc D Fake: 0.000% 
Loss D: 1.439 
Loss G: 0.4587 (0.4277) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,244 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4370 (0.4247) Acc D Real: 100.000% 
Loss D Fake: 1.0037 (1.0598) Acc D Fake: 0.000% 
Loss D: 1.441 
Loss G: 0.4585 (0.4280) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,252 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4382 (0.4248) Acc D Real: 100.000% 
Loss D Fake: 1.0041 (1.0594) Acc D Fake: 0.000% 
Loss D: 1.442 
Loss G: 0.4582 (0.4282) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,260 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4390 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.0047 (1.0589) Acc D Fake: 0.000% 
Loss D: 1.444 
Loss G: 0.4578 (0.4284) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,267 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4390 (0.4250) Acc D Real: 100.000% 
Loss D Fake: 1.0056 (1.0585) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4573 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,274 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4383 (0.4252) Acc D Real: 100.000% 
Loss D Fake: 1.0066 (1.0581) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4567 (0.4289) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,282 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4376 (0.4252) Acc D Real: 100.000% 
Loss D Fake: 1.0077 (1.0577) Acc D Fake: 0.000% 
Loss D: 1.445 
Loss G: 0.4560 (0.4291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,289 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4366 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.0090 (1.0574) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4553 (0.4293) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,296 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4362 (0.4254) Acc D Real: 100.000% 
Loss D Fake: 1.0104 (1.0570) Acc D Fake: 0.000% 
Loss D: 1.447 
Loss G: 0.4545 (0.4295) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,304 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4346 (0.4255) Acc D Real: 100.000% 
Loss D Fake: 1.0118 (1.0567) Acc D Fake: 0.000% 
Loss D: 1.446 
Loss G: 0.4537 (0.4296) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,312 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4371 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.0132 (1.0564) Acc D Fake: 0.000% 
Loss D: 1.450 
Loss G: 0.4529 (0.4298) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,319 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4369 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.0148 (1.0561) Acc D Fake: 0.000% 
Loss D: 1.452 
Loss G: 0.4520 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,327 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4404 (0.4258) Acc D Real: 100.000% 
Loss D Fake: 1.0166 (1.0558) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4509 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,334 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4351 (0.4258) Acc D Real: 100.000% 
Loss D Fake: 1.0186 (1.0555) Acc D Fake: 0.000% 
Loss D: 1.454 
Loss G: 0.4497 (0.4303) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,341 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4308 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.0207 (1.0552) Acc D Fake: 0.000% 
Loss D: 1.451 
Loss G: 0.4486 (0.4304) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,348 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4375 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.0226 (1.0550) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4475 (0.4305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,355 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4323 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.0247 (1.0548) Acc D Fake: 0.000% 
Loss D: 1.457 
Loss G: 0.4463 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,363 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4328 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.0268 (1.0546) Acc D Fake: 0.000% 
Loss D: 1.460 
Loss G: 0.4451 (0.4307) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,370 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4407 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.0291 (1.0544) Acc D Fake: 0.000% 
Loss D: 1.470 
Loss G: 0.4438 (0.4308) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,377 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4378 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.0316 (1.0543) Acc D Fake: 0.000% 
Loss D: 1.469 
Loss G: 0.4423 (0.4309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,385 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4380 (0.4263) Acc D Real: 100.000% 
Loss D Fake: 1.0344 (1.0541) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4407 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,392 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4342 (0.4264) Acc D Real: 100.000% 
Loss D Fake: 1.0374 (1.0540) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4390 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,399 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4317 (0.4264) Acc D Real: 100.000% 
Loss D Fake: 1.0404 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.472 
Loss G: 0.4374 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,407 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4363 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0434 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4357 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,414 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4295 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0465 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.476 
Loss G: 0.4341 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,422 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4302 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0495 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.480 
Loss G: 0.4325 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,429 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4300 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0524 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.482 
Loss G: 0.4309 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,436 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4317 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0554 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.487 
Loss G: 0.4293 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,443 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4272 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0585 (1.0538) Acc D Fake: 0.000% 
Loss D: 1.486 
Loss G: 0.4277 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,451 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4269 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0614 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.488 
Loss G: 0.4261 (0.4311) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,458 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4300 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0643 (1.0539) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4246 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,465 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4292 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0673 (1.0540) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4230 (0.4310) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,472 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4273 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0703 (1.0541) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4214 (0.4309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,480 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4278 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0733 (1.0542) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4198 (0.4308) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,488 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4241 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0763 (1.0544) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4183 (0.4308) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,496 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4240 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0792 (1.0545) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4168 (0.4307) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,504 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4245 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0821 (1.0547) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4153 (0.4306) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,513 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4249 (0.4266) Acc D Real: 100.000% 
Loss D Fake: 1.0849 (1.0549) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4139 (0.4305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,521 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4198 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0878 (1.0551) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4125 (0.4304) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,529 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4224 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0904 (1.0553) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4111 (0.4303) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,537 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4271 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.0555) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4098 (0.4301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,546 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4183 (0.4265) Acc D Real: 100.000% 
Loss D Fake: 1.0958 (1.0558) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4084 (0.4300) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,554 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4132 (0.4264) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.0560) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4073 (0.4299) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,562 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4166 (0.4263) Acc D Real: 100.000% 
Loss D Fake: 1.1006 (1.0563) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4062 (0.4297) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,571 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4140 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.1028 (1.0566) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4051 (0.4296) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,579 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4138 (0.4262) Acc D Real: 100.000% 
Loss D Fake: 1.1047 (1.0568) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4042 (0.4294) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,588 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4141 (0.4261) Acc D Real: 100.000% 
Loss D Fake: 1.1066 (1.0571) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.4033 (0.4293) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,596 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4141 (0.4260) Acc D Real: 100.000% 
Loss D Fake: 1.1084 (1.0574) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4025 (0.4291) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,603 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4126 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.1100 (1.0577) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.4017 (0.4290) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,611 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4129 (0.4259) Acc D Real: 100.000% 
Loss D Fake: 1.1116 (1.0581) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.4009 (0.4288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,618 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4088 (0.4258) Acc D Real: 100.000% 
Loss D Fake: 1.1131 (1.0584) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.4002 (0.4286) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,626 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4107 (0.4257) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.0587) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3996 (0.4285) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,633 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4100 (0.4256) Acc D Real: 100.000% 
Loss D Fake: 1.1158 (1.0590) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3990 (0.4283) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,641 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4113 (0.4255) Acc D Real: 100.000% 
Loss D Fake: 1.1170 (1.0593) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3984 (0.4281) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,648 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4075 (0.4254) Acc D Real: 100.000% 
Loss D Fake: 1.1182 (1.0597) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3978 (0.4280) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,655 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4106 (0.4253) Acc D Real: 100.000% 
Loss D Fake: 1.1192 (1.0600) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3973 (0.4278) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,663 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4062 (0.4252) Acc D Real: 100.000% 
Loss D Fake: 1.1202 (1.0603) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3969 (0.4276) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,671 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4070 (0.4251) Acc D Real: 100.000% 
Loss D Fake: 1.1211 (1.0607) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3965 (0.4275) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,678 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4075 (0.4250) Acc D Real: 100.000% 
Loss D Fake: 1.1219 (1.0610) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3961 (0.4273) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,686 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4062 (0.4249) Acc D Real: 100.000% 
Loss D Fake: 1.1227 (1.0613) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3957 (0.4271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,694 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4055 (0.4248) Acc D Real: 100.000% 
Loss D Fake: 1.1234 (1.0617) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3954 (0.4269) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,701 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4050 (0.4247) Acc D Real: 100.000% 
Loss D Fake: 1.1240 (1.0620) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3952 (0.4268) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,709 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4042 (0.4246) Acc D Real: 100.000% 
Loss D Fake: 1.1245 (1.0623) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3949 (0.4266) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,716 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4047 (0.4245) Acc D Real: 100.000% 
Loss D Fake: 1.1250 (1.0627) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3947 (0.4264) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,724 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4032 (0.4244) Acc D Real: 100.000% 
Loss D Fake: 1.1254 (1.0630) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3945 (0.4263) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,731 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4006 (0.4243) Acc D Real: 100.000% 
Loss D Fake: 1.1257 (1.0633) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3944 (0.4261) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,739 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4035 (0.4242) Acc D Real: 100.000% 
Loss D Fake: 1.1260 (1.0637) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3943 (0.4259) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,747 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4040 (0.4241) Acc D Real: 100.000% 
Loss D Fake: 1.1262 (1.0640) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3942 (0.4258) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,754 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4044 (0.4240) Acc D Real: 100.000% 
Loss D Fake: 1.1264 (1.0643) Acc D Fake: 0.000% 
Loss D: 1.531 
Loss G: 0.3941 (0.4256) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,762 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4030 (0.4238) Acc D Real: 100.000% 
Loss D Fake: 1.1265 (1.0646) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3940 (0.4254) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,769 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4034 (0.4237) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0649) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3940 (0.4253) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,777 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4036 (0.4236) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0653) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3940 (0.4251) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,784 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4017 (0.4235) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0656) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3939 (0.4250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,792 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4030 (0.4234) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0659) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3939 (0.4248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,799 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4028 (0.4233) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0662) Acc D Fake: 0.000% 
Loss D: 1.530 
Loss G: 0.3939 (0.4246) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,807 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4007 (0.4232) Acc D Real: 100.000% 
Loss D Fake: 1.1268 (1.0665) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3940 (0.4245) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,814 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4008 (0.4231) Acc D Real: 100.000% 
Loss D Fake: 1.1267 (1.0668) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3940 (0.4243) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,822 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4012 (0.4230) Acc D Real: 100.000% 
Loss D Fake: 1.1265 (1.0671) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3941 (0.4242) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,829 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4004 (0.4229) Acc D Real: 100.000% 
Loss D Fake: 1.1264 (1.0674) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3942 (0.4240) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,836 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4024 (0.4228) Acc D Real: 100.000% 
Loss D Fake: 1.1262 (1.0677) Acc D Fake: 0.000% 
Loss D: 1.529 
Loss G: 0.3943 (0.4239) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,844 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4007 (0.4227) Acc D Real: 100.000% 
Loss D Fake: 1.1260 (1.0680) Acc D Fake: 0.000% 
Loss D: 1.527 
Loss G: 0.3944 (0.4237) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,851 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4021 (0.4226) Acc D Real: 100.000% 
Loss D Fake: 1.1257 (1.0682) Acc D Fake: 0.000% 
Loss D: 1.528 
Loss G: 0.3945 (0.4236) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,859 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3997 (0.4225) Acc D Real: 100.000% 
Loss D Fake: 1.1255 (1.0685) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3946 (0.4235) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,866 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4005 (0.4224) Acc D Real: 100.000% 
Loss D Fake: 1.1252 (1.0688) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3948 (0.4233) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,873 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4007 (0.4222) Acc D Real: 100.000% 
Loss D Fake: 1.1249 (1.0691) Acc D Fake: 0.000% 
Loss D: 1.526 
Loss G: 0.3949 (0.4232) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,880 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4008 (0.4221) Acc D Real: 100.000% 
Loss D Fake: 1.1246 (1.0693) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3951 (0.4231) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,888 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4006 (0.4220) Acc D Real: 100.000% 
Loss D Fake: 1.1242 (1.0696) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3953 (0.4229) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,895 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3995 (0.4219) Acc D Real: 100.000% 
Loss D Fake: 1.1239 (1.0698) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3954 (0.4228) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,902 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4011 (0.4218) Acc D Real: 100.000% 
Loss D Fake: 1.1235 (1.0701) Acc D Fake: 0.000% 
Loss D: 1.525 
Loss G: 0.3956 (0.4227) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,910 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3996 (0.4217) Acc D Real: 100.000% 
Loss D Fake: 1.1231 (1.0703) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3958 (0.4225) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,917 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4006 (0.4216) Acc D Real: 100.000% 
Loss D Fake: 1.1227 (1.0706) Acc D Fake: 0.000% 
Loss D: 1.523 
Loss G: 0.3960 (0.4224) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,924 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4001 (0.4215) Acc D Real: 100.000% 
Loss D Fake: 1.1222 (1.0708) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3962 (0.4223) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,932 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4000 (0.4214) Acc D Real: 100.000% 
Loss D Fake: 1.1218 (1.0710) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3964 (0.4222) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,939 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4001 (0.4213) Acc D Real: 100.000% 
Loss D Fake: 1.1213 (1.0713) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3967 (0.4221) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,947 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4000 (0.4212) Acc D Real: 100.000% 
Loss D Fake: 1.1209 (1.0715) Acc D Fake: 0.000% 
Loss D: 1.521 
Loss G: 0.3969 (0.4219) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,954 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4013 (0.4212) Acc D Real: 100.000% 
Loss D Fake: 1.1204 (1.0717) Acc D Fake: 0.000% 
Loss D: 1.522 
Loss G: 0.3971 (0.4218) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,961 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3999 (0.4211) Acc D Real: 100.000% 
Loss D Fake: 1.1200 (1.0719) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3973 (0.4217) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,969 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4003 (0.4210) Acc D Real: 100.000% 
Loss D Fake: 1.1195 (1.0722) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.3975 (0.4216) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,976 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4000 (0.4209) Acc D Real: 100.000% 
Loss D Fake: 1.1190 (1.0724) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3978 (0.4215) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,983 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3998 (0.4208) Acc D Real: 100.000% 
Loss D Fake: 1.1185 (1.0726) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3980 (0.4214) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,991 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3997 (0.4207) Acc D Real: 100.000% 
Loss D Fake: 1.1180 (1.0728) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.3983 (0.4213) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:47,998 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4011 (0.4206) Acc D Real: 100.000% 
Loss D Fake: 1.1175 (1.0730) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.3985 (0.4212) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,010 -                train: [    INFO] - 
Epoch: 20/20
2023-03-01 13:45:48,214 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4002 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1165 (1.1168) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3990 (0.3989) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,221 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4010 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1160 (1.1165) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.3992 (0.3990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,230 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4008 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1155 (1.1163) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.3995 (0.3991) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,240 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4003 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1150 (1.1160) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3997 (0.3992) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,247 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4004 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1145 (1.1158) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.3999 (0.3993) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,254 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4003 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1140 (1.1155) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4002 (0.3995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,261 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4007 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1135 (1.1153) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4004 (0.3996) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,268 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4003 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1130 (1.1150) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4007 (0.3997) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,275 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.4004 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1125 (1.1148) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4009 (0.3998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,282 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4004 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1120 (1.1145) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4011 (0.3999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,289 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4005 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1115 (1.1143) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4014 (0.4001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,296 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4005 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1110 (1.1140) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4016 (0.4002) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,303 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4007 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1105 (1.1138) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4018 (0.4003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,309 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4006 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1101 (1.1135) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4021 (0.4004) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,316 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4008 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1096 (1.1133) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4023 (0.4005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,324 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4006 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1091 (1.1130) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4025 (0.4007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,330 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4007 (0.4005) Acc D Real: 100.000% 
Loss D Fake: 1.1086 (1.1128) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4028 (0.4008) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,338 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4009 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1082 (1.1125) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4030 (0.4009) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,345 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4008 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1077 (1.1123) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4032 (0.4010) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,352 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4008 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1072 (1.1121) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4034 (0.4011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,359 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4007 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1068 (1.1118) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4037 (0.4012) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,366 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4012 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1063 (1.1116) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4039 (0.4014) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,373 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4010 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1059 (1.1113) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4041 (0.4015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,380 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4010 (0.4006) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.1111) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4043 (0.4016) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,388 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4009 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1050 (1.1109) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4045 (0.4017) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,395 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4012 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1046 (1.1106) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4047 (0.4018) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,403 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4016 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1042 (1.1104) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4049 (0.4019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,410 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4015 (0.4007) Acc D Real: 100.000% 
Loss D Fake: 1.1038 (1.1102) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4051 (0.4020) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,417 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4016 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1034 (1.1100) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4053 (0.4021) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,425 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4008 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1030 (1.1097) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4055 (0.4022) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,432 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4002 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1026 (1.1095) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4057 (0.4024) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,440 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4013 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.1093) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4059 (0.4025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,447 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4016 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1018 (1.1091) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4061 (0.4026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,454 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4013 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1014 (1.1088) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4063 (0.4027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,462 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4013 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1011 (1.1086) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4065 (0.4028) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,469 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4013 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1007 (1.1084) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4066 (0.4029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,476 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4011 (0.4008) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.1082) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4068 (0.4030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,484 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4021 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.1000 (1.1080) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4070 (0.4031) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,491 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4017 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.1078) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4071 (0.4032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,499 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4025 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0993 (1.1076) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4073 (0.4033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,506 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4007 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0990 (1.1074) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4075 (0.4034) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,514 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4010 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.1072) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4076 (0.4035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,521 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4014 (0.4009) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.1070) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4078 (0.4036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,529 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4020 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.1068) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4079 (0.4037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,536 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4023 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0978 (1.1066) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4081 (0.4038) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,544 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4023 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.1064) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4082 (0.4039) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,551 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4010 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0972 (1.1062) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4083 (0.4040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,559 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4013 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0969 (1.1060) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4085 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,567 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4008 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0967 (1.1058) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4086 (0.4041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,575 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4008 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0964 (1.1056) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4087 (0.4042) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,583 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4015 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0962 (1.1054) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4088 (0.4043) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,592 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4005 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0959 (1.1053) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4090 (0.4044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,600 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4020 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0957 (1.1051) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4091 (0.4045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,608 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4013 (0.4010) Acc D Real: 100.000% 
Loss D Fake: 1.0954 (1.1049) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4092 (0.4046) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,617 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4030 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0952 (1.1047) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4093 (0.4047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,625 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4024 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0950 (1.1046) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4094 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,633 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4005 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.1044) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4095 (0.4048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,640 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4017 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0946 (1.1042) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4096 (0.4049) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,647 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4013 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0944 (1.1041) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4097 (0.4050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,655 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4027 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0942 (1.1039) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4098 (0.4051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,662 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4025 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0940 (1.1038) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4099 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,669 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4010 (0.4011) Acc D Real: 100.000% 
Loss D Fake: 1.0939 (1.1036) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4100 (0.4052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,676 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4019 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0937 (1.1034) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4100 (0.4053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,683 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4015 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0936 (1.1033) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4101 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,690 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4027 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0934 (1.1031) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4102 (0.4054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,698 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4008 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0933 (1.1030) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4102 (0.4055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,705 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4024 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0932 (1.1028) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4103 (0.4056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,712 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4036 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.1027) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4104 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,720 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4005 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0930 (1.1026) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4104 (0.4057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,727 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4004 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0929 (1.1024) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4105 (0.4058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,734 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4025 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0928 (1.1023) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4105 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,741 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4021 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0927 (1.1022) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4106 (0.4059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,748 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4018 (0.4012) Acc D Real: 100.000% 
Loss D Fake: 1.0926 (1.1020) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4106 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,755 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4033 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.1019) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4106 (0.4060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,763 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4050 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1018) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4107 (0.4061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,770 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4025 (0.4013) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1017) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4107 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,777 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4051 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1015) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4107 (0.4062) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,784 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4031 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1014) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4107 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,792 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4021 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1013) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4107 (0.4063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,799 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4020 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1012) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4107 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,806 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4017 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1011) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4108 (0.4064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,813 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4014 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1010) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4108 (0.4065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,820 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4007 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1009) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4108 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,828 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4036 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1008) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4108 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,835 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4005 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1007) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4108 (0.4066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,842 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4023 (0.4014) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1006) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4108 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,849 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4036 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1005) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4107 (0.4067) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,856 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4022 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1004) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4107 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,864 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4034 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0923 (1.1003) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4107 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,871 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4036 (0.4015) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4107 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,878 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4050 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0924 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4107 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,885 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4023 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0925 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4106 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,892 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4015 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0926 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4106 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,900 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4027 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0926 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4106 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,907 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4013 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0927 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.494 
Loss G: 0.4105 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,914 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4029 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0928 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4105 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,921 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4024 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0929 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4104 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,928 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4040 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0930 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4104 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,936 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4016 (0.4016) Acc D Real: 100.000% 
Loss D Fake: 1.0931 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.495 
Loss G: 0.4103 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,943 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4057 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0932 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4103 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,950 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4038 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0933 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4102 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,957 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4058 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0934 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4102 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,964 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3990 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0935 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.493 
Loss G: 0.4101 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,972 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4032 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0936 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4101 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,979 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4023 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0938 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4100 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,987 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4023 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0939 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4099 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:48,995 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4028 (0.4017) Acc D Real: 100.000% 
Loss D Fake: 1.0940 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4099 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,003 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4046 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0942 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4098 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,010 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4023 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0943 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4097 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,018 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4046 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0945 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4096 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,025 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4036 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0946 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4096 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,032 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4047 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0948 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4095 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,039 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4017 (0.4018) Acc D Real: 100.000% 
Loss D Fake: 1.0949 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4094 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,047 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4084 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0951 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4093 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,054 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4009 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0953 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4093 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,061 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4016 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0954 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4092 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,068 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4029 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0956 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4091 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,075 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4050 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0958 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4090 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,083 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4019 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0960 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.498 
Loss G: 0.4089 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,090 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4010 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0961 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.497 
Loss G: 0.4088 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,097 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4058 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0963 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.502 
Loss G: 0.4087 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,104 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4030 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0965 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4086 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,111 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3995 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0967 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.496 
Loss G: 0.4086 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,119 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4038 (0.4019) Acc D Real: 100.000% 
Loss D Fake: 1.0969 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4085 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,126 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4101 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.0971 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4084 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,133 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4017 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.0973 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.499 
Loss G: 0.4083 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,141 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4039 (0.4020) Acc D Real: 100.000% 
Loss D Fake: 1.0975 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4082 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,149 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4070 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0977 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.505 
Loss G: 0.4081 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,156 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4053 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4080 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,164 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4026 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0981 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.501 
Loss G: 0.4079 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,172 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4020 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0983 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4078 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,179 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4049 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0985 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4077 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,186 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4039 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0987 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4076 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,193 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4014 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0989 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4074 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,201 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4047 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0992 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4073 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,208 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4007 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0994 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.500 
Loss G: 0.4072 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,215 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4039 (0.4021) Acc D Real: 100.000% 
Loss D Fake: 1.0996 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4071 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,223 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4076 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4070 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,230 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4043 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1001 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4069 (0.4077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,238 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4063 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4068 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,245 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4029 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1005 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4067 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,253 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4030 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1008 (1.0986) Acc D Fake: 0.000% 
Loss D: 1.504 
Loss G: 0.4066 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,260 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4021 (0.4022) Acc D Real: 100.000% 
Loss D Fake: 1.1010 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4064 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,268 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4068 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1012 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4063 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,275 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4055 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1015 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4062 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,283 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4069 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1017 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4061 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,290 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4061 (0.4023) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.0987) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4060 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,298 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4080 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1021 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4059 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,305 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4089 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1023 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4058 (0.4076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,312 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4065 (0.4024) Acc D Real: 100.000% 
Loss D Fake: 1.1025 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4057 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,320 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4065 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1027 (1.0988) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4056 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,327 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4030 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1029 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4055 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,335 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4064 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1031 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4054 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,342 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4060 (0.4025) Acc D Real: 100.000% 
Loss D Fake: 1.1033 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4053 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,350 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4099 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.0989) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4052 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,357 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4091 (0.4026) Acc D Real: 100.000% 
Loss D Fake: 1.1037 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4051 (0.4075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,364 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4081 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1039 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4050 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,372 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4077 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1041 (1.0990) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4049 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,379 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4090 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1042 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4049 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,387 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4060 (0.4027) Acc D Real: 100.000% 
Loss D Fake: 1.1044 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4048 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,394 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4099 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.1045 (1.0991) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4047 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,401 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4040 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.1047 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4046 (0.4074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,408 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4083 (0.4028) Acc D Real: 100.000% 
Loss D Fake: 1.1048 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4046 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,416 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4078 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.1050 (1.0992) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4045 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,423 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4079 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.1051 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4044 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,431 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4095 (0.4029) Acc D Real: 100.000% 
Loss D Fake: 1.1052 (1.0993) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4044 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,439 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4099 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.1053 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4043 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,446 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4087 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.1054 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4043 (0.4073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,453 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4097 (0.4030) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.0994) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4042 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,461 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4099 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.1056 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4042 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,468 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4084 (0.4031) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4041 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,476 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4108 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4041 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,483 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4115 (0.4032) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4041 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,490 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4134 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4041 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,498 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4111 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4041 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,505 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4102 (0.4033) Acc D Real: 100.000% 
Loss D Fake: 1.1058 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4041 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,512 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4095 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4041 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,520 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4136 (0.4034) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4041 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,528 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4091 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.1057 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4041 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,536 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4109 (0.4035) Acc D Real: 100.000% 
Loss D Fake: 1.1056 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4042 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,544 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4113 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.1056 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4042 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,552 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4106 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.1055 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4042 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,559 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4110 (0.4036) Acc D Real: 100.000% 
Loss D Fake: 1.1054 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4043 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,566 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4118 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.1053 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4043 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,574 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4121 (0.4037) Acc D Real: 100.000% 
Loss D Fake: 1.1052 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4044 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,581 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4125 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1050 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4044 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,589 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4125 (0.4038) Acc D Real: 100.000% 
Loss D Fake: 1.1049 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4045 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,596 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4135 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1047 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4046 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,603 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4150 (0.4039) Acc D Real: 100.000% 
Loss D Fake: 1.1045 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.520 
Loss G: 0.4047 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,611 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4127 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1043 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4048 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,619 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4143 (0.4040) Acc D Real: 100.000% 
Loss D Fake: 1.1041 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4049 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,626 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4154 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1038 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.519 
Loss G: 0.4050 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,634 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4149 (0.4041) Acc D Real: 100.000% 
Loss D Fake: 1.1035 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4052 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,641 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4137 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1032 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4053 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,649 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4135 (0.4042) Acc D Real: 100.000% 
Loss D Fake: 1.1029 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4055 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,656 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4147 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1026 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4056 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,664 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4146 (0.4043) Acc D Real: 100.000% 
Loss D Fake: 1.1022 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4058 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,671 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4153 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1019 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4060 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,679 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4138 (0.4044) Acc D Real: 100.000% 
Loss D Fake: 1.1015 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4061 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,686 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4139 (0.4045) Acc D Real: 100.000% 
Loss D Fake: 1.1011 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4063 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,694 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4178 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1007 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.518 
Loss G: 0.4065 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,701 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4141 (0.4046) Acc D Real: 100.000% 
Loss D Fake: 1.1003 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4067 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,709 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4152 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.0998 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4069 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,716 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4160 (0.4047) Acc D Real: 100.000% 
Loss D Fake: 1.0994 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4072 (0.4068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,724 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4178 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.0989 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.517 
Loss G: 0.4074 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,731 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4173 (0.4048) Acc D Real: 100.000% 
Loss D Fake: 1.0984 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4076 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,739 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4167 (0.4049) Acc D Real: 100.000% 
Loss D Fake: 1.0979 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4079 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,746 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4180 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.0973 (1.1002) Acc D Fake: 0.000% 
Loss D: 1.515 
Loss G: 0.4082 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,754 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4195 (0.4050) Acc D Real: 100.000% 
Loss D Fake: 1.0968 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.516 
Loss G: 0.4084 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,761 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4179 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.0962 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.514 
Loss G: 0.4087 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,768 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4177 (0.4051) Acc D Real: 100.000% 
Loss D Fake: 1.0955 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4091 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,776 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4180 (0.4052) Acc D Real: 100.000% 
Loss D Fake: 1.0949 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.513 
Loss G: 0.4094 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,783 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4182 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.0942 (1.1001) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4097 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,791 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4162 (0.4053) Acc D Real: 100.000% 
Loss D Fake: 1.0935 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.510 
Loss G: 0.4100 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,798 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4193 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.0929 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.512 
Loss G: 0.4103 (0.4069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,806 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4170 (0.4054) Acc D Real: 100.000% 
Loss D Fake: 1.0922 (1.1000) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4107 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,813 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4191 (0.4055) Acc D Real: 100.000% 
Loss D Fake: 1.0915 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.511 
Loss G: 0.4110 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,820 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4186 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.0908 (1.0999) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4114 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,828 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4182 (0.4056) Acc D Real: 100.000% 
Loss D Fake: 1.0901 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4117 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,835 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4199 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.0893 (1.0998) Acc D Fake: 0.000% 
Loss D: 1.509 
Loss G: 0.4121 (0.4070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,842 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4192 (0.4057) Acc D Real: 100.000% 
Loss D Fake: 1.0886 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.508 
Loss G: 0.4125 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,850 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4191 (0.4058) Acc D Real: 100.000% 
Loss D Fake: 1.0878 (1.0997) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4129 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,857 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4198 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.0870 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.507 
Loss G: 0.4133 (0.4071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,864 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4197 (0.4059) Acc D Real: 100.000% 
Loss D Fake: 1.0862 (1.0996) Acc D Fake: 0.000% 
Loss D: 1.506 
Loss G: 0.4137 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,871 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4173 (0.4060) Acc D Real: 100.000% 
Loss D Fake: 1.0854 (1.0995) Acc D Fake: 0.000% 
Loss D: 1.503 
Loss G: 0.4141 (0.4072) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-01 13:45:49,886 -                train: [    INFO] - Best Metric: At 5 Epoch Gen 0.677 Dis
2023-03-01 13:45:49,886 -                train: [    INFO] - MODEL TRAINING COMPLETED. 
 BEST RESULT SAVED
2023-03-01 13:45:50,304 -         Optimization: [    INFO] - 
 Batch: 1/121
2023-03-01 13:45:52,470 -         Optimization: [    INFO] - Batch [1/121]: Anomaly Score: 388.667 label: 0.0
2023-03-01 13:45:52,472 -         Optimization: [    INFO] - 
 Batch: 2/121
2023-03-01 13:45:54,586 -         Optimization: [    INFO] - Batch [2/121]: Anomaly Score: 400.877 label: 0.0
2023-03-01 13:45:54,589 -         Optimization: [    INFO] - 
 Batch: 3/121
2023-03-01 13:45:56,663 -         Optimization: [    INFO] - Batch [3/121]: Anomaly Score: 396.713 label: 0.0
2023-03-01 13:45:56,665 -         Optimization: [    INFO] - 
 Batch: 4/121
2023-03-01 13:45:58,738 -         Optimization: [    INFO] - Batch [4/121]: Anomaly Score: 385.431 label: 0.0
2023-03-01 13:45:58,748 -         Optimization: [    INFO] - 
 Batch: 5/121
2023-03-01 13:46:00,816 -         Optimization: [    INFO] - Batch [5/121]: Anomaly Score: 396.282 label: 0.0
2023-03-01 13:46:00,817 -         Optimization: [    INFO] - 
 Batch: 6/121
2023-03-01 13:46:02,886 -         Optimization: [    INFO] - Batch [6/121]: Anomaly Score: 379.090 label: 0.0
2023-03-01 13:46:02,887 -         Optimization: [    INFO] - 
 Batch: 7/121
2023-03-01 13:46:04,952 -         Optimization: [    INFO] - Batch [7/121]: Anomaly Score: 381.965 label: 0.0
2023-03-01 13:46:04,952 -         Optimization: [    INFO] - 
 Batch: 8/121
2023-03-01 13:46:07,047 -         Optimization: [    INFO] - Batch [8/121]: Anomaly Score: 398.052 label: 0.0
2023-03-01 13:46:07,047 -         Optimization: [    INFO] - 
 Batch: 9/121
2023-03-01 13:46:09,215 -         Optimization: [    INFO] - Batch [9/121]: Anomaly Score: 388.418 label: 0.0
2023-03-01 13:46:09,216 -         Optimization: [    INFO] - 
 Batch: 10/121
2023-03-01 13:46:11,385 -         Optimization: [    INFO] - Batch [10/121]: Anomaly Score: 377.248 label: 0.0
2023-03-01 13:46:11,386 -         Optimization: [    INFO] - 
 Batch: 11/121
2023-03-01 13:46:13,558 -         Optimization: [    INFO] - Batch [11/121]: Anomaly Score: 378.082 label: 0.0
2023-03-01 13:46:13,559 -         Optimization: [    INFO] - 
 Batch: 12/121
2023-03-01 13:46:15,781 -         Optimization: [    INFO] - Batch [12/121]: Anomaly Score: 374.707 label: 0.0
2023-03-01 13:46:15,781 -         Optimization: [    INFO] - 
 Batch: 13/121
2023-03-01 13:46:17,946 -         Optimization: [    INFO] - Batch [13/121]: Anomaly Score: 396.279 label: 0.0
2023-03-01 13:46:17,947 -         Optimization: [    INFO] - 
 Batch: 14/121
2023-03-01 13:46:20,108 -         Optimization: [    INFO] - Batch [14/121]: Anomaly Score: 381.354 label: 0.0
2023-03-01 13:46:20,109 -         Optimization: [    INFO] - 
 Batch: 15/121
2023-03-01 13:46:22,297 -         Optimization: [    INFO] - Batch [15/121]: Anomaly Score: 389.618 label: 0.0
2023-03-01 13:46:22,297 -         Optimization: [    INFO] - 
 Batch: 16/121
2023-03-01 13:46:24,458 -         Optimization: [    INFO] - Batch [16/121]: Anomaly Score: 400.964 label: 0.0
2023-03-01 13:46:24,460 -         Optimization: [    INFO] - 
 Batch: 17/121
2023-03-01 13:46:26,618 -         Optimization: [    INFO] - Batch [17/121]: Anomaly Score: 388.848 label: 0.0
2023-03-01 13:46:26,618 -         Optimization: [    INFO] - 
 Batch: 18/121
2023-03-01 13:46:28,782 -         Optimization: [    INFO] - Batch [18/121]: Anomaly Score: 389.620 label: 0.0
2023-03-01 13:46:28,783 -         Optimization: [    INFO] - 
 Batch: 19/121
2023-03-01 13:46:30,937 -         Optimization: [    INFO] - Batch [19/121]: Anomaly Score: 395.936 label: 0.0
2023-03-01 13:46:30,938 -         Optimization: [    INFO] - 
 Batch: 20/121
2023-03-01 13:46:33,057 -         Optimization: [    INFO] - Batch [20/121]: Anomaly Score: 393.399 label: 0.0
2023-03-01 13:46:33,058 -         Optimization: [    INFO] - 
 Batch: 21/121
2023-03-01 13:46:35,165 -         Optimization: [    INFO] - Batch [21/121]: Anomaly Score: 402.836 label: 0.0
2023-03-01 13:46:35,166 -         Optimization: [    INFO] - 
 Batch: 22/121
2023-03-01 13:46:37,299 -         Optimization: [    INFO] - Batch [22/121]: Anomaly Score: 396.799 label: 0.0
2023-03-01 13:46:37,300 -         Optimization: [    INFO] - 
 Batch: 23/121
2023-03-01 13:46:39,400 -         Optimization: [    INFO] - Batch [23/121]: Anomaly Score: 397.740 label: 0.0
2023-03-01 13:46:39,401 -         Optimization: [    INFO] - 
 Batch: 24/121
2023-03-01 13:46:41,487 -         Optimization: [    INFO] - Batch [24/121]: Anomaly Score: 391.733 label: 0.0
2023-03-01 13:46:41,487 -         Optimization: [    INFO] - 
 Batch: 25/121
2023-03-01 13:46:43,575 -         Optimization: [    INFO] - Batch [25/121]: Anomaly Score: 383.337 label: 0.0
2023-03-01 13:46:43,577 -         Optimization: [    INFO] - 
 Batch: 26/121
2023-03-01 13:46:45,660 -         Optimization: [    INFO] - Batch [26/121]: Anomaly Score: 386.870 label: 0.0
2023-03-01 13:46:45,662 -         Optimization: [    INFO] - 
 Batch: 27/121
2023-03-01 13:46:47,746 -         Optimization: [    INFO] - Batch [27/121]: Anomaly Score: 379.714 label: 0.0
2023-03-01 13:46:47,748 -         Optimization: [    INFO] - 
 Batch: 28/121
2023-03-01 13:46:49,818 -         Optimization: [    INFO] - Batch [28/121]: Anomaly Score: 394.784 label: 0.0
2023-03-01 13:46:49,819 -         Optimization: [    INFO] - 
 Batch: 29/121
2023-03-01 13:46:51,928 -         Optimization: [    INFO] - Batch [29/121]: Anomaly Score: 389.525 label: 0.0
2023-03-01 13:46:51,930 -         Optimization: [    INFO] - 
 Batch: 30/121
2023-03-01 13:46:54,037 -         Optimization: [    INFO] - Batch [30/121]: Anomaly Score: 378.772 label: 0.0
2023-03-01 13:46:54,039 -         Optimization: [    INFO] - 
 Batch: 31/121
2023-03-01 13:46:56,131 -         Optimization: [    INFO] - Batch [31/121]: Anomaly Score: 359.174 label: 0.0
2023-03-01 13:46:56,132 -         Optimization: [    INFO] - 
 Batch: 32/121
2023-03-01 13:46:58,252 -         Optimization: [    INFO] - Batch [32/121]: Anomaly Score: 365.216 label: 0.0
2023-03-01 13:46:58,254 -         Optimization: [    INFO] - 
 Batch: 33/121
2023-03-01 13:47:00,382 -         Optimization: [    INFO] - Batch [33/121]: Anomaly Score: 358.041 label: 0.0
2023-03-01 13:47:00,384 -         Optimization: [    INFO] - 
 Batch: 34/121
2023-03-01 13:47:02,503 -         Optimization: [    INFO] - Batch [34/121]: Anomaly Score: 376.802 label: 0.0
2023-03-01 13:47:02,505 -         Optimization: [    INFO] - 
 Batch: 35/121
2023-03-01 13:47:04,615 -         Optimization: [    INFO] - Batch [35/121]: Anomaly Score: 373.927 label: 0.0
2023-03-01 13:47:04,617 -         Optimization: [    INFO] - 
 Batch: 36/121
2023-03-01 13:47:06,728 -         Optimization: [    INFO] - Batch [36/121]: Anomaly Score: 380.419 label: 0.0
2023-03-01 13:47:06,730 -         Optimization: [    INFO] - 
 Batch: 37/121
2023-03-01 13:47:08,844 -         Optimization: [    INFO] - Batch [37/121]: Anomaly Score: 379.265 label: 0.0
2023-03-01 13:47:08,846 -         Optimization: [    INFO] - 
 Batch: 38/121
2023-03-01 13:47:10,951 -         Optimization: [    INFO] - Batch [38/121]: Anomaly Score: 378.408 label: 0.0
2023-03-01 13:47:10,953 -         Optimization: [    INFO] - 
 Batch: 39/121
2023-03-01 13:47:13,080 -         Optimization: [    INFO] - Batch [39/121]: Anomaly Score: 367.943 label: 0.0
2023-03-01 13:47:13,082 -         Optimization: [    INFO] - 
 Batch: 40/121
2023-03-01 13:47:15,231 -         Optimization: [    INFO] - Batch [40/121]: Anomaly Score: 381.466 label: 0.0
2023-03-01 13:47:15,233 -         Optimization: [    INFO] - 
 Batch: 41/121
2023-03-01 13:47:17,373 -         Optimization: [    INFO] - Batch [41/121]: Anomaly Score: 374.863 label: 0.0
2023-03-01 13:47:17,374 -         Optimization: [    INFO] - 
 Batch: 42/121
2023-03-01 13:47:19,511 -         Optimization: [    INFO] - Batch [42/121]: Anomaly Score: 366.563 label: 0.0
2023-03-01 13:47:19,512 -         Optimization: [    INFO] - 
 Batch: 43/121
2023-03-01 13:47:21,630 -         Optimization: [    INFO] - Batch [43/121]: Anomaly Score: 364.983 label: 0.0
2023-03-01 13:47:21,632 -         Optimization: [    INFO] - 
 Batch: 44/121
2023-03-01 13:47:23,774 -         Optimization: [    INFO] - Batch [44/121]: Anomaly Score: 365.576 label: 0.0
2023-03-01 13:47:23,775 -         Optimization: [    INFO] - 
 Batch: 45/121
2023-03-01 13:47:25,899 -         Optimization: [    INFO] - Batch [45/121]: Anomaly Score: 366.813 label: 0.0
2023-03-01 13:47:25,901 -         Optimization: [    INFO] - 
 Batch: 46/121
2023-03-01 13:47:28,023 -         Optimization: [    INFO] - Batch [46/121]: Anomaly Score: 377.453 label: 0.0
2023-03-01 13:47:28,025 -         Optimization: [    INFO] - 
 Batch: 47/121
2023-03-01 13:47:30,119 -         Optimization: [    INFO] - Batch [47/121]: Anomaly Score: 370.067 label: 0.0
2023-03-01 13:47:30,121 -         Optimization: [    INFO] - 
 Batch: 48/121
2023-03-01 13:47:32,217 -         Optimization: [    INFO] - Batch [48/121]: Anomaly Score: 380.927 label: 0.0
2023-03-01 13:47:32,219 -         Optimization: [    INFO] - 
 Batch: 49/121
2023-03-01 13:47:34,413 -         Optimization: [    INFO] - Batch [49/121]: Anomaly Score: 380.440 label: 0.0
2023-03-01 13:47:34,415 -         Optimization: [    INFO] - 
 Batch: 50/121
2023-03-01 13:47:36,610 -         Optimization: [    INFO] - Batch [50/121]: Anomaly Score: 371.385 label: 0.0
2023-03-01 13:47:36,612 -         Optimization: [    INFO] - 
 Batch: 51/121
2023-03-01 13:47:38,817 -         Optimization: [    INFO] - Batch [51/121]: Anomaly Score: 369.314 label: 0.0
2023-03-01 13:47:38,819 -         Optimization: [    INFO] - 
 Batch: 52/121
2023-03-01 13:47:41,026 -         Optimization: [    INFO] - Batch [52/121]: Anomaly Score: 369.765 label: 0.0
2023-03-01 13:47:41,028 -         Optimization: [    INFO] - 
 Batch: 53/121
2023-03-01 13:47:43,230 -         Optimization: [    INFO] - Batch [53/121]: Anomaly Score: 354.884 label: 0.0
2023-03-01 13:47:43,231 -         Optimization: [    INFO] - 
 Batch: 54/121
2023-03-01 13:47:45,423 -         Optimization: [    INFO] - Batch [54/121]: Anomaly Score: 354.934 label: 0.0
2023-03-01 13:47:45,424 -         Optimization: [    INFO] - 
 Batch: 55/121
2023-03-01 13:47:47,628 -         Optimization: [    INFO] - Batch [55/121]: Anomaly Score: 368.114 label: 0.0
2023-03-01 13:47:47,630 -         Optimization: [    INFO] - 
 Batch: 56/121
2023-03-01 13:47:49,806 -         Optimization: [    INFO] - Batch [56/121]: Anomaly Score: 367.346 label: 0.0
2023-03-01 13:47:49,808 -         Optimization: [    INFO] - 
 Batch: 57/121
2023-03-01 13:47:51,949 -         Optimization: [    INFO] - Batch [57/121]: Anomaly Score: 365.519 label: 0.0
2023-03-01 13:47:51,951 -         Optimization: [    INFO] - 
 Batch: 58/121
2023-03-01 13:47:54,064 -         Optimization: [    INFO] - Batch [58/121]: Anomaly Score: 376.468 label: 0.0
2023-03-01 13:47:54,066 -         Optimization: [    INFO] - 
 Batch: 59/121
2023-03-01 13:47:56,179 -         Optimization: [    INFO] - Batch [59/121]: Anomaly Score: 364.806 label: 0.0
2023-03-01 13:47:56,181 -         Optimization: [    INFO] - 
 Batch: 60/121
2023-03-01 13:47:58,304 -         Optimization: [    INFO] - Batch [60/121]: Anomaly Score: 357.900 label: 1.0
2023-03-01 13:47:58,306 -         Optimization: [    INFO] - 
 Batch: 61/121
2023-03-01 13:48:00,428 -         Optimization: [    INFO] - Batch [61/121]: Anomaly Score: 359.582 label: 1.0
2023-03-01 13:48:00,429 -         Optimization: [    INFO] - 
 Batch: 62/121
2023-03-01 13:48:02,550 -         Optimization: [    INFO] - Batch [62/121]: Anomaly Score: 357.994 label: 1.0
2023-03-01 13:48:02,552 -         Optimization: [    INFO] - 
 Batch: 63/121
2023-03-01 13:48:04,693 -         Optimization: [    INFO] - Batch [63/121]: Anomaly Score: 28.346 label: 1.0
2023-03-01 13:48:04,695 -         Optimization: [    INFO] - 
 Batch: 64/121
2023-03-01 13:48:06,880 -         Optimization: [    INFO] - Batch [64/121]: Anomaly Score: 352.504 label: 1.0
2023-03-01 13:48:06,882 -         Optimization: [    INFO] - 
 Batch: 65/121
2023-03-01 13:48:09,063 -         Optimization: [    INFO] - Batch [65/121]: Anomaly Score: 364.573 label: 1.0
2023-03-01 13:48:09,065 -         Optimization: [    INFO] - 
 Batch: 66/121
2023-03-01 13:48:11,241 -         Optimization: [    INFO] - Batch [66/121]: Anomaly Score: 363.729 label: 1.0
2023-03-01 13:48:11,243 -         Optimization: [    INFO] - 
 Batch: 67/121
2023-03-01 13:48:13,429 -         Optimization: [    INFO] - Batch [67/121]: Anomaly Score: 358.867 label: 0.0
2023-03-01 13:48:13,431 -         Optimization: [    INFO] - 
 Batch: 68/121
2023-03-01 13:48:15,632 -         Optimization: [    INFO] - Batch [68/121]: Anomaly Score: 376.380 label: 0.0
2023-03-01 13:48:15,634 -         Optimization: [    INFO] - 
 Batch: 69/121
2023-03-01 13:48:17,804 -         Optimization: [    INFO] - Batch [69/121]: Anomaly Score: 370.170 label: 0.0
2023-03-01 13:48:17,805 -         Optimization: [    INFO] - 
 Batch: 70/121
2023-03-01 13:48:19,951 -         Optimization: [    INFO] - Batch [70/121]: Anomaly Score: 372.747 label: 0.0
2023-03-01 13:48:19,952 -         Optimization: [    INFO] - 
 Batch: 71/121
2023-03-01 13:48:22,099 -         Optimization: [    INFO] - Batch [71/121]: Anomaly Score: 353.838 label: 0.0
2023-03-01 13:48:22,101 -         Optimization: [    INFO] - 
 Batch: 72/121
2023-03-01 13:48:24,352 -         Optimization: [    INFO] - Batch [72/121]: Anomaly Score: 361.429 label: 0.0
2023-03-01 13:48:24,355 -         Optimization: [    INFO] - 
 Batch: 73/121
2023-03-01 13:48:26,540 -         Optimization: [    INFO] - Batch [73/121]: Anomaly Score: 371.973 label: 0.0
2023-03-01 13:48:26,542 -         Optimization: [    INFO] - 
 Batch: 74/121
2023-03-01 13:48:28,713 -         Optimization: [    INFO] - Batch [74/121]: Anomaly Score: 376.414 label: 0.0
2023-03-01 13:48:28,714 -         Optimization: [    INFO] - 
 Batch: 75/121
2023-03-01 13:48:30,876 -         Optimization: [    INFO] - Batch [75/121]: Anomaly Score: 372.977 label: 0.0
2023-03-01 13:48:30,878 -         Optimization: [    INFO] - 
 Batch: 76/121
2023-03-01 13:48:33,054 -         Optimization: [    INFO] - Batch [76/121]: Anomaly Score: 379.558 label: 0.0
2023-03-01 13:48:33,056 -         Optimization: [    INFO] - 
 Batch: 77/121
2023-03-01 13:48:35,266 -         Optimization: [    INFO] - Batch [77/121]: Anomaly Score: 381.431 label: 0.0
2023-03-01 13:48:35,268 -         Optimization: [    INFO] - 
 Batch: 78/121
2023-03-01 13:48:37,442 -         Optimization: [    INFO] - Batch [78/121]: Anomaly Score: 387.505 label: 0.0
2023-03-01 13:48:37,444 -         Optimization: [    INFO] - 
 Batch: 79/121
2023-03-01 13:48:39,629 -         Optimization: [    INFO] - Batch [79/121]: Anomaly Score: 378.472 label: 0.0
2023-03-01 13:48:39,630 -         Optimization: [    INFO] - 
 Batch: 80/121
2023-03-01 13:48:41,804 -         Optimization: [    INFO] - Batch [80/121]: Anomaly Score: 380.481 label: 0.0
2023-03-01 13:48:41,806 -         Optimization: [    INFO] - 
 Batch: 81/121
2023-03-01 13:48:43,955 -         Optimization: [    INFO] - Batch [81/121]: Anomaly Score: 378.576 label: 0.0
2023-03-01 13:48:43,957 -         Optimization: [    INFO] - 
 Batch: 82/121
2023-03-01 13:48:46,104 -         Optimization: [    INFO] - Batch [82/121]: Anomaly Score: 382.456 label: 0.0
2023-03-01 13:48:46,106 -         Optimization: [    INFO] - 
 Batch: 83/121
2023-03-01 13:48:48,251 -         Optimization: [    INFO] - Batch [83/121]: Anomaly Score: 378.782 label: 0.0
2023-03-01 13:48:48,253 -         Optimization: [    INFO] - 
 Batch: 84/121
2023-03-01 13:48:50,407 -         Optimization: [    INFO] - Batch [84/121]: Anomaly Score: 381.667 label: 0.0
2023-03-01 13:48:50,409 -         Optimization: [    INFO] - 
 Batch: 85/121
2023-03-01 13:48:52,564 -         Optimization: [    INFO] - Batch [85/121]: Anomaly Score: 397.662 label: 0.0
2023-03-01 13:48:52,566 -         Optimization: [    INFO] - 
 Batch: 86/121
2023-03-01 13:48:54,714 -         Optimization: [    INFO] - Batch [86/121]: Anomaly Score: 389.958 label: 0.0
2023-03-01 13:48:54,717 -         Optimization: [    INFO] - 
 Batch: 87/121
2023-03-01 13:48:56,865 -         Optimization: [    INFO] - Batch [87/121]: Anomaly Score: 381.424 label: 0.0
2023-03-01 13:48:56,867 -         Optimization: [    INFO] - 
 Batch: 88/121
2023-03-01 13:48:59,020 -         Optimization: [    INFO] - Batch [88/121]: Anomaly Score: 386.662 label: 0.0
2023-03-01 13:48:59,021 -         Optimization: [    INFO] - 
 Batch: 89/121
2023-03-01 13:49:01,169 -         Optimization: [    INFO] - Batch [89/121]: Anomaly Score: 376.319 label: 0.0
2023-03-01 13:49:01,170 -         Optimization: [    INFO] - 
 Batch: 90/121
2023-03-01 13:49:03,321 -         Optimization: [    INFO] - Batch [90/121]: Anomaly Score: 389.130 label: 0.0
2023-03-01 13:49:03,323 -         Optimization: [    INFO] - 
 Batch: 91/121
2023-03-01 13:49:05,501 -         Optimization: [    INFO] - Batch [91/121]: Anomaly Score: 396.279 label: 0.0
2023-03-01 13:49:05,503 -         Optimization: [    INFO] - 
 Batch: 92/121
2023-03-01 13:49:07,682 -         Optimization: [    INFO] - Batch [92/121]: Anomaly Score: 402.073 label: 0.0
2023-03-01 13:49:07,684 -         Optimization: [    INFO] - 
 Batch: 93/121
2023-03-01 13:49:09,849 -         Optimization: [    INFO] - Batch [93/121]: Anomaly Score: 405.135 label: 0.0
2023-03-01 13:49:09,851 -         Optimization: [    INFO] - 
 Batch: 94/121
2023-03-01 13:49:12,016 -         Optimization: [    INFO] - Batch [94/121]: Anomaly Score: 386.969 label: 0.0
2023-03-01 13:49:12,018 -         Optimization: [    INFO] - 
 Batch: 95/121
2023-03-01 13:49:14,193 -         Optimization: [    INFO] - Batch [95/121]: Anomaly Score: 395.516 label: 0.0
2023-03-01 13:49:14,195 -         Optimization: [    INFO] - 
 Batch: 96/121
2023-03-01 13:49:16,358 -         Optimization: [    INFO] - Batch [96/121]: Anomaly Score: 402.827 label: 0.0
2023-03-01 13:49:16,359 -         Optimization: [    INFO] - 
 Batch: 97/121
2023-03-01 13:49:18,506 -         Optimization: [    INFO] - Batch [97/121]: Anomaly Score: 394.921 label: 0.0
2023-03-01 13:49:18,508 -         Optimization: [    INFO] - 
 Batch: 98/121
2023-03-01 13:49:20,658 -         Optimization: [    INFO] - Batch [98/121]: Anomaly Score: 395.703 label: 0.0
2023-03-01 13:49:20,660 -         Optimization: [    INFO] - 
 Batch: 99/121
2023-03-01 13:49:22,801 -         Optimization: [    INFO] - Batch [99/121]: Anomaly Score: 395.790 label: 0.0
2023-03-01 13:49:22,803 -         Optimization: [    INFO] - 
 Batch: 100/121
2023-03-01 13:49:24,929 -         Optimization: [    INFO] - Batch [100/121]: Anomaly Score: 394.383 label: 1.0
2023-03-01 13:49:24,931 -         Optimization: [    INFO] - 
 Batch: 101/121
2023-03-01 13:49:27,028 -         Optimization: [    INFO] - Batch [101/121]: Anomaly Score: 402.796 label: 1.0
2023-03-01 13:49:27,030 -         Optimization: [    INFO] - 
 Batch: 102/121
2023-03-01 13:49:29,139 -         Optimization: [    INFO] - Batch [102/121]: Anomaly Score: 396.948 label: 1.0
2023-03-01 13:49:29,140 -         Optimization: [    INFO] - 
 Batch: 103/121
2023-03-01 13:49:31,243 -         Optimization: [    INFO] - Batch [103/121]: Anomaly Score: 404.478 label: 1.0
2023-03-01 13:49:31,244 -         Optimization: [    INFO] - 
 Batch: 104/121
2023-03-01 13:49:33,397 -         Optimization: [    INFO] - Batch [104/121]: Anomaly Score: 412.443 label: 1.0
2023-03-01 13:49:33,399 -         Optimization: [    INFO] - 
 Batch: 105/121
2023-03-01 13:49:35,565 -         Optimization: [    INFO] - Batch [105/121]: Anomaly Score: 400.065 label: 1.0
2023-03-01 13:49:35,567 -         Optimization: [    INFO] - 
 Batch: 106/121
2023-03-01 13:49:37,733 -         Optimization: [    INFO] - Batch [106/121]: Anomaly Score: 400.684 label: 1.0
2023-03-01 13:49:37,735 -         Optimization: [    INFO] - 
 Batch: 107/121
2023-03-01 13:49:39,902 -         Optimization: [    INFO] - Batch [107/121]: Anomaly Score: 407.361 label: 1.0
2023-03-01 13:49:39,904 -         Optimization: [    INFO] - 
 Batch: 108/121
2023-03-01 13:49:42,079 -         Optimization: [    INFO] - Batch [108/121]: Anomaly Score: 393.058 label: 0.0
2023-03-01 13:49:42,081 -         Optimization: [    INFO] - 
 Batch: 109/121
2023-03-01 13:49:44,261 -         Optimization: [    INFO] - Batch [109/121]: Anomaly Score: 400.934 label: 0.0
2023-03-01 13:49:44,263 -         Optimization: [    INFO] - 
 Batch: 110/121
2023-03-01 13:49:46,408 -         Optimization: [    INFO] - Batch [110/121]: Anomaly Score: 401.528 label: 0.0
2023-03-01 13:49:46,409 -         Optimization: [    INFO] - 
 Batch: 111/121
2023-03-01 13:49:48,551 -         Optimization: [    INFO] - Batch [111/121]: Anomaly Score: 402.821 label: 0.0
2023-03-01 13:49:48,552 -         Optimization: [    INFO] - 
 Batch: 112/121
2023-03-01 13:49:50,688 -         Optimization: [    INFO] - Batch [112/121]: Anomaly Score: 407.781 label: 0.0
2023-03-01 13:49:50,689 -         Optimization: [    INFO] - 
 Batch: 113/121
2023-03-01 13:49:52,842 -         Optimization: [    INFO] - Batch [113/121]: Anomaly Score: 404.220 label: 0.0
2023-03-01 13:49:52,844 -         Optimization: [    INFO] - 
 Batch: 114/121
2023-03-01 13:49:55,035 -         Optimization: [    INFO] - Batch [114/121]: Anomaly Score: 391.742 label: 0.0
2023-03-01 13:49:55,037 -         Optimization: [    INFO] - 
 Batch: 115/121
2023-03-01 13:49:57,277 -         Optimization: [    INFO] - Batch [115/121]: Anomaly Score: 405.971 label: 0.0
2023-03-01 13:49:57,279 -         Optimization: [    INFO] - 
 Batch: 116/121
2023-03-01 13:49:59,488 -         Optimization: [    INFO] - Batch [116/121]: Anomaly Score: 393.439 label: 0.0
2023-03-01 13:49:59,491 -         Optimization: [    INFO] - 
 Batch: 117/121
2023-03-01 13:50:01,639 -         Optimization: [    INFO] - Batch [117/121]: Anomaly Score: 397.403 label: 0.0
2023-03-01 13:50:01,641 -         Optimization: [    INFO] - 
 Batch: 118/121
2023-03-01 13:50:03,752 -         Optimization: [    INFO] - Batch [118/121]: Anomaly Score: 411.412 label: 0.0
2023-03-01 13:50:03,754 -         Optimization: [    INFO] - 
 Batch: 119/121
2023-03-01 13:50:05,826 -         Optimization: [    INFO] - Batch [119/121]: Anomaly Score: 397.374 label: 0.0
2023-03-01 13:50:05,827 -         Optimization: [    INFO] - 
 Batch: 120/121
2023-03-01 13:50:07,910 -         Optimization: [    INFO] - Batch [120/121]: Anomaly Score: 395.226 label: 0.0
2023-03-01 13:50:07,911 -         Optimization: [    INFO] - 
 Batch: 121/121
2023-03-01 13:50:10,012 -         Optimization: [    INFO] - Batch [121/121]: Anomaly Score: 403.745 label: 0.0
2023-03-02 01:45:46,811 -                train: [    INFO] - Device: cuda:0
2023-03-02 01:45:49,295 -                train: [    INFO] - 
Epoch: 1/20
2023-03-02 01:45:49,587 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.6834 (0.6845) Acc D Real: 72.734% 
Loss D Fake: 0.7040 (0.7026) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6813 (0.6826) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,596 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.6771 (0.6820) Acc D Real: 77.240% 
Loss D Fake: 0.7066 (0.7039) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.6788 (0.6814) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,606 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.6741 (0.6800) Acc D Real: 79.505% 
Loss D Fake: 0.7092 (0.7053) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.6763 (0.6801) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,614 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.6717 (0.6784) Acc D Real: 82.240% 
Loss D Fake: 0.7118 (0.7066) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.6738 (0.6788) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,621 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.6789 (0.6784) Acc D Real: 81.502% 
Loss D Fake: 0.7143 (0.7079) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6713 (0.6776) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,628 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.6672 (0.6768) Acc D Real: 82.805% 
Loss D Fake: 0.7169 (0.7091) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.6689 (0.6763) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,638 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.6680 (0.6757) Acc D Real: 84.316% 
Loss D Fake: 0.7195 (0.7104) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6664 (0.6751) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,647 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.6687 (0.6749) Acc D Real: 85.556% 
Loss D Fake: 0.7221 (0.7117) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6640 (0.6739) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,655 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.6642 (0.6739) Acc D Real: 86.115% 
Loss D Fake: 0.7247 (0.7130) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.6615 (0.6726) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,662 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.6637 (0.6729) Acc D Real: 87.131% 
Loss D Fake: 0.7273 (0.7143) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6591 (0.6714) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,669 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.6582 (0.6717) Acc D Real: 88.147% 
Loss D Fake: 0.7299 (0.7156) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6566 (0.6702) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,676 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.6530 (0.6703) Acc D Real: 89.058% 
Loss D Fake: 0.7326 (0.7169) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.6541 (0.6689) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,684 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.6559 (0.6693) Acc D Real: 89.833% 
Loss D Fake: 0.7353 (0.7182) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6516 (0.6677) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,691 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.6528 (0.6682) Acc D Real: 90.503% 
Loss D Fake: 0.7380 (0.7196) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6491 (0.6665) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,698 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.6522 (0.6672) Acc D Real: 91.097% 
Loss D Fake: 0.7408 (0.7209) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6466 (0.6652) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,709 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.6484 (0.6661) Acc D Real: 91.621% 
Loss D Fake: 0.7435 (0.7222) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6441 (0.6640) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,716 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.6440 (0.6648) Acc D Real: 92.086% 
Loss D Fake: 0.7463 (0.7236) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6416 (0.6627) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,723 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.6441 (0.6637) Acc D Real: 92.503% 
Loss D Fake: 0.7492 (0.7249) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6390 (0.6615) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,731 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.6372 (0.6624) Acc D Real: 92.878% 
Loss D Fake: 0.7521 (0.7263) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.6364 (0.6602) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,738 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.6373 (0.6612) Acc D Real: 93.217% 
Loss D Fake: 0.7550 (0.7276) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6338 (0.6590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,745 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.6340 (0.6600) Acc D Real: 93.525% 
Loss D Fake: 0.7580 (0.7290) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6311 (0.6577) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,753 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.6343 (0.6589) Acc D Real: 93.807% 
Loss D Fake: 0.7610 (0.7304) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.6285 (0.6564) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,760 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.6282 (0.6576) Acc D Real: 94.065% 
Loss D Fake: 0.7640 (0.7318) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6259 (0.6552) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,767 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.6236 (0.6562) Acc D Real: 94.302% 
Loss D Fake: 0.7671 (0.7332) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6231 (0.6539) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,775 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.6238 (0.6550) Acc D Real: 94.521% 
Loss D Fake: 0.7704 (0.7346) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.6203 (0.6526) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,783 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.6175 (0.6536) Acc D Real: 94.724% 
Loss D Fake: 0.7737 (0.7361) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6174 (0.6513) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,791 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.6157 (0.6522) Acc D Real: 94.913% 
Loss D Fake: 0.7771 (0.7376) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6145 (0.6500) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,799 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.6122 (0.6509) Acc D Real: 95.088% 
Loss D Fake: 0.7806 (0.7390) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6115 (0.6486) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,807 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.6081 (0.6494) Acc D Real: 95.252% 
Loss D Fake: 0.7843 (0.7406) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6083 (0.6473) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,814 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.6034 (0.6479) Acc D Real: 95.405% 
Loss D Fake: 0.7882 (0.7421) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6050 (0.6459) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,822 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.5998 (0.6464) Acc D Real: 95.549% 
Loss D Fake: 0.7923 (0.7437) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6016 (0.6445) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,829 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.5968 (0.6449) Acc D Real: 95.683% 
Loss D Fake: 0.7965 (0.7453) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5980 (0.6431) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,837 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.5908 (0.6433) Acc D Real: 95.810% 
Loss D Fake: 0.8011 (0.7469) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5942 (0.6417) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,844 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.5866 (0.6417) Acc D Real: 95.930% 
Loss D Fake: 0.8059 (0.7486) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5902 (0.6402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,852 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.5827 (0.6401) Acc D Real: 96.043% 
Loss D Fake: 0.8110 (0.7503) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5860 (0.6387) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,859 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.5768 (0.6384) Acc D Real: 96.150% 
Loss D Fake: 0.8165 (0.7521) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5817 (0.6372) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,867 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.5727 (0.6366) Acc D Real: 96.251% 
Loss D Fake: 0.8222 (0.7540) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.5770 (0.6356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,874 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.5631 (0.6348) Acc D Real: 96.347% 
Loss D Fake: 0.8284 (0.7559) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5721 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,881 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.5597 (0.6329) Acc D Real: 96.439% 
Loss D Fake: 0.8353 (0.7578) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.5667 (0.6323) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,889 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.5434 (0.6307) Acc D Real: 96.526% 
Loss D Fake: 0.8428 (0.7599) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.5608 (0.6305) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,896 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.5333 (0.6284) Acc D Real: 96.608% 
Loss D Fake: 0.8512 (0.7621) Acc D Fake: 0.000% 
Loss D: 1.385 
Loss G: 0.5543 (0.6287) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,904 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.5300 (0.6261) Acc D Real: 96.687% 
Loss D Fake: 0.8608 (0.7644) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.5470 (0.6268) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,912 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.5200 (0.6237) Acc D Real: 96.763% 
Loss D Fake: 0.8720 (0.7668) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5386 (0.6248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,919 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.5076 (0.6211) Acc D Real: 96.834% 
Loss D Fake: 0.8851 (0.7695) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5289 (0.6227) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,927 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.5038 (0.6185) Acc D Real: 96.903% 
Loss D Fake: 0.9009 (0.7723) Acc D Fake: 0.000% 
Loss D: 1.405 
Loss G: 0.5176 (0.6204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,935 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4583 (0.6151) Acc D Real: 96.969% 
Loss D Fake: 0.9198 (0.7755) Acc D Fake: 0.000% 
Loss D: 1.378 
Loss G: 0.5043 (0.6179) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,942 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4580 (0.6119) Acc D Real: 97.032% 
Loss D Fake: 0.9438 (0.7790) Acc D Fake: 0.000% 
Loss D: 1.402 
Loss G: 0.4886 (0.6152) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,950 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4260 (0.6081) Acc D Real: 97.093% 
Loss D Fake: 0.9732 (0.7829) Acc D Fake: 0.000% 
Loss D: 1.399 
Loss G: 0.4715 (0.6123) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,957 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4241 (0.6044) Acc D Real: 97.151% 
Loss D Fake: 1.0056 (0.7874) Acc D Fake: 0.000% 
Loss D: 1.430 
Loss G: 0.4556 (0.6092) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,965 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3987 (0.6004) Acc D Real: 97.207% 
Loss D Fake: 1.0331 (0.7922) Acc D Fake: 0.000% 
Loss D: 1.432 
Loss G: 0.4459 (0.6060) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,972 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4169 (0.5968) Acc D Real: 97.261% 
Loss D Fake: 1.0420 (0.7970) Acc D Fake: 0.000% 
Loss D: 1.459 
Loss G: 0.4469 (0.6029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,980 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3664 (0.5925) Acc D Real: 97.312% 
Loss D Fake: 1.0317 (0.8014) Acc D Fake: 0.000% 
Loss D: 1.398 
Loss G: 0.4523 (0.6001) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,987 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3761 (0.5885) Acc D Real: 97.362% 
Loss D Fake: 1.0190 (0.8055) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.4585 (0.5974) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:49,995 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3991 (0.5850) Acc D Real: 97.410% 
Loss D Fake: 1.0057 (0.8091) Acc D Fake: 0.000% 
Loss D: 1.405 
Loss G: 0.4657 (0.5951) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,002 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3715 (0.5812) Acc D Real: 97.456% 
Loss D Fake: 0.9923 (0.8124) Acc D Fake: 0.000% 
Loss D: 1.364 
Loss G: 0.4718 (0.5929) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,010 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4232 (0.5784) Acc D Real: 97.501% 
Loss D Fake: 0.9822 (0.8154) Acc D Fake: 0.000% 
Loss D: 1.405 
Loss G: 0.4768 (0.5908) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,017 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3921 (0.5752) Acc D Real: 97.544% 
Loss D Fake: 0.9739 (0.8181) Acc D Fake: 0.000% 
Loss D: 1.366 
Loss G: 0.4807 (0.5889) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,025 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3988 (0.5722) Acc D Real: 97.586% 
Loss D Fake: 0.9681 (0.8206) Acc D Fake: 0.000% 
Loss D: 1.367 
Loss G: 0.4835 (0.5871) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,032 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4198 (0.5697) Acc D Real: 97.626% 
Loss D Fake: 0.9638 (0.8230) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.4858 (0.5854) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,040 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4475 (0.5677) Acc D Real: 97.665% 
Loss D Fake: 0.9601 (0.8253) Acc D Fake: 0.000% 
Loss D: 1.408 
Loss G: 0.4876 (0.5838) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,047 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4519 (0.5658) Acc D Real: 97.702% 
Loss D Fake: 0.9570 (0.8274) Acc D Fake: 0.000% 
Loss D: 1.409 
Loss G: 0.4898 (0.5823) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,055 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4347 (0.5637) Acc D Real: 97.739% 
Loss D Fake: 0.9530 (0.8294) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.4919 (0.5809) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,062 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3710 (0.5607) Acc D Real: 97.774% 
Loss D Fake: 0.9496 (0.8313) Acc D Fake: 0.000% 
Loss D: 1.321 
Loss G: 0.4936 (0.5795) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,070 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3920 (0.5581) Acc D Real: 97.808% 
Loss D Fake: 0.9471 (0.8330) Acc D Fake: 0.000% 
Loss D: 1.339 
Loss G: 0.4950 (0.5782) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,078 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3633 (0.5552) Acc D Real: 97.842% 
Loss D Fake: 0.9457 (0.8348) Acc D Fake: 0.000% 
Loss D: 1.309 
Loss G: 0.4948 (0.5770) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,085 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4542 (0.5537) Acc D Real: 97.874% 
Loss D Fake: 0.9471 (0.8364) Acc D Fake: 0.000% 
Loss D: 1.401 
Loss G: 0.4940 (0.5757) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,093 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4074 (0.5515) Acc D Real: 97.905% 
Loss D Fake: 0.9477 (0.8381) Acc D Fake: 0.000% 
Loss D: 1.355 
Loss G: 0.4944 (0.5745) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,100 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3775 (0.5490) Acc D Real: 97.936% 
Loss D Fake: 0.9481 (0.8397) Acc D Fake: 0.000% 
Loss D: 1.326 
Loss G: 0.4925 (0.5733) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,108 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3235 (0.5458) Acc D Real: 97.965% 
Loss D Fake: 0.9527 (0.8413) Acc D Fake: 0.000% 
Loss D: 1.276 
Loss G: 0.4903 (0.5721) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,116 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4567 (0.5445) Acc D Real: 97.994% 
Loss D Fake: 0.9550 (0.8429) Acc D Fake: 0.000% 
Loss D: 1.412 
Loss G: 0.4906 (0.5710) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,123 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3839 (0.5423) Acc D Real: 98.022% 
Loss D Fake: 0.9530 (0.8444) Acc D Fake: 0.000% 
Loss D: 1.337 
Loss G: 0.4923 (0.5699) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,131 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3520 (0.5397) Acc D Real: 98.049% 
Loss D Fake: 0.9499 (0.8459) Acc D Fake: 0.000% 
Loss D: 1.302 
Loss G: 0.4939 (0.5689) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,138 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3912 (0.5377) Acc D Real: 98.075% 
Loss D Fake: 0.9467 (0.8472) Acc D Fake: 0.000% 
Loss D: 1.338 
Loss G: 0.4965 (0.5679) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,146 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3975 (0.5358) Acc D Real: 98.101% 
Loss D Fake: 0.9419 (0.8485) Acc D Fake: 0.000% 
Loss D: 1.339 
Loss G: 0.4991 (0.5670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,153 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4274 (0.5344) Acc D Real: 98.126% 
Loss D Fake: 0.9375 (0.8496) Acc D Fake: 0.000% 
Loss D: 1.365 
Loss G: 0.5017 (0.5661) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,161 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4029 (0.5327) Acc D Real: 98.150% 
Loss D Fake: 0.9333 (0.8507) Acc D Fake: 0.000% 
Loss D: 1.336 
Loss G: 0.5043 (0.5653) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,168 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3829 (0.5308) Acc D Real: 98.174% 
Loss D Fake: 0.9295 (0.8517) Acc D Fake: 0.000% 
Loss D: 1.312 
Loss G: 0.5062 (0.5646) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,176 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4286 (0.5295) Acc D Real: 98.197% 
Loss D Fake: 0.9267 (0.8527) Acc D Fake: 0.000% 
Loss D: 1.355 
Loss G: 0.5078 (0.5638) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,184 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3303 (0.5270) Acc D Real: 98.219% 
Loss D Fake: 0.9246 (0.8536) Acc D Fake: 0.000% 
Loss D: 1.255 
Loss G: 0.5087 (0.5631) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,191 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3777 (0.5251) Acc D Real: 98.241% 
Loss D Fake: 0.9236 (0.8545) Acc D Fake: 0.000% 
Loss D: 1.301 
Loss G: 0.5091 (0.5625) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,199 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4028 (0.5237) Acc D Real: 98.263% 
Loss D Fake: 0.9230 (0.8553) Acc D Fake: 0.000% 
Loss D: 1.326 
Loss G: 0.5096 (0.5618) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,206 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4031 (0.5222) Acc D Real: 98.284% 
Loss D Fake: 0.9222 (0.8561) Acc D Fake: 0.000% 
Loss D: 1.325 
Loss G: 0.5100 (0.5612) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,214 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4177 (0.5210) Acc D Real: 98.304% 
Loss D Fake: 0.9214 (0.8569) Acc D Fake: 0.000% 
Loss D: 1.339 
Loss G: 0.5106 (0.5606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,223 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3845 (0.5194) Acc D Real: 98.324% 
Loss D Fake: 0.9208 (0.8576) Acc D Fake: 0.000% 
Loss D: 1.305 
Loss G: 0.5105 (0.5600) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,231 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3701 (0.5176) Acc D Real: 98.344% 
Loss D Fake: 0.9217 (0.8584) Acc D Fake: 0.000% 
Loss D: 1.292 
Loss G: 0.5091 (0.5594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,240 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3263 (0.5154) Acc D Real: 98.363% 
Loss D Fake: 0.9245 (0.8591) Acc D Fake: 0.000% 
Loss D: 1.251 
Loss G: 0.5070 (0.5588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,249 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3495 (0.5135) Acc D Real: 98.381% 
Loss D Fake: 0.9281 (0.8599) Acc D Fake: 0.000% 
Loss D: 1.278 
Loss G: 0.5048 (0.5582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,257 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3143 (0.5113) Acc D Real: 98.399% 
Loss D Fake: 0.9318 (0.8607) Acc D Fake: 0.000% 
Loss D: 1.246 
Loss G: 0.5022 (0.5576) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,266 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3416 (0.5094) Acc D Real: 98.417% 
Loss D Fake: 0.9362 (0.8616) Acc D Fake: 0.000% 
Loss D: 1.278 
Loss G: 0.4988 (0.5569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,274 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.2795 (0.5069) Acc D Real: 98.435% 
Loss D Fake: 0.9420 (0.8624) Acc D Fake: 0.000% 
Loss D: 1.222 
Loss G: 0.4947 (0.5562) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,283 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3113 (0.5048) Acc D Real: 98.452% 
Loss D Fake: 0.9491 (0.8634) Acc D Fake: 0.000% 
Loss D: 1.260 
Loss G: 0.4896 (0.5555) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,290 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2631 (0.5022) Acc D Real: 98.468% 
Loss D Fake: 0.9569 (0.8644) Acc D Fake: 0.000% 
Loss D: 1.220 
Loss G: 0.4861 (0.5548) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,297 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.2399 (0.4994) Acc D Real: 98.485% 
Loss D Fake: 0.9596 (0.8654) Acc D Fake: 0.000% 
Loss D: 1.199 
Loss G: 0.4863 (0.5540) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,305 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.2254 (0.4965) Acc D Real: 98.501% 
Loss D Fake: 0.9572 (0.8664) Acc D Fake: 0.000% 
Loss D: 1.183 
Loss G: 0.4886 (0.5534) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,313 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2204 (0.4936) Acc D Real: 98.516% 
Loss D Fake: 0.9573 (0.8673) Acc D Fake: 0.000% 
Loss D: 1.178 
Loss G: 0.4881 (0.5527) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,321 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1917 (0.4905) Acc D Real: 98.531% 
Loss D Fake: 1.0054 (0.8687) Acc D Fake: 0.000% 
Loss D: 1.197 
Loss G: 0.4937 (0.5521) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,328 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2132 (0.4877) Acc D Real: 98.546% 
Loss D Fake: 0.9441 (0.8695) Acc D Fake: 0.000% 
Loss D: 1.157 
Loss G: 0.4902 (0.5514) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,335 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.1767 (0.4845) Acc D Real: 98.561% 
Loss D Fake: 0.9867 (0.8707) Acc D Fake: 0.000% 
Loss D: 1.163 
Loss G: 0.4897 (0.5508) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,343 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.1921 (0.4816) Acc D Real: 98.576% 
Loss D Fake: 0.9478 (0.8715) Acc D Fake: 0.000% 
Loss D: 1.140 
Loss G: 0.5107 (0.5504) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,351 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.1985 (0.4788) Acc D Real: 98.590% 
Loss D Fake: 0.9107 (0.8719) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.5250 (0.5502) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,358 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2101 (0.4762) Acc D Real: 98.603% 
Loss D Fake: 0.8919 (0.8720) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.5343 (0.5500) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,366 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.1774 (0.4733) Acc D Real: 98.617% 
Loss D Fake: 0.8799 (0.8721) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.5413 (0.5499) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,373 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2398 (0.4710) Acc D Real: 98.630% 
Loss D Fake: 0.8710 (0.8721) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.5466 (0.5499) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,380 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2274 (0.4687) Acc D Real: 98.643% 
Loss D Fake: 0.8644 (0.8720) Acc D Fake: 0.000% 
Loss D: 1.092 
Loss G: 0.5506 (0.5499) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,387 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.2023 (0.4662) Acc D Real: 98.656% 
Loss D Fake: 0.8595 (0.8719) Acc D Fake: 0.000% 
Loss D: 1.062 
Loss G: 0.5535 (0.5499) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,395 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2510 (0.4642) Acc D Real: 98.669% 
Loss D Fake: 0.8562 (0.8718) Acc D Fake: 0.000% 
Loss D: 1.107 
Loss G: 0.5555 (0.5500) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,402 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2253 (0.4620) Acc D Real: 98.681% 
Loss D Fake: 0.8541 (0.8716) Acc D Fake: 0.000% 
Loss D: 1.079 
Loss G: 0.5568 (0.5500) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,409 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.2036 (0.4596) Acc D Real: 98.693% 
Loss D Fake: 0.8527 (0.8714) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.5576 (0.5501) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,416 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.2181 (0.4574) Acc D Real: 98.705% 
Loss D Fake: 0.8520 (0.8713) Acc D Fake: 0.000% 
Loss D: 1.070 
Loss G: 0.5579 (0.5502) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,424 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.1910 (0.4550) Acc D Real: 98.717% 
Loss D Fake: 0.8521 (0.8711) Acc D Fake: 0.000% 
Loss D: 1.043 
Loss G: 0.5578 (0.5502) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,431 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.2227 (0.4529) Acc D Real: 98.728% 
Loss D Fake: 0.8525 (0.8709) Acc D Fake: 0.000% 
Loss D: 1.075 
Loss G: 0.5576 (0.5503) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,438 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.1896 (0.4506) Acc D Real: 98.739% 
Loss D Fake: 0.8531 (0.8708) Acc D Fake: 0.000% 
Loss D: 1.043 
Loss G: 0.5572 (0.5504) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,446 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2167 (0.4485) Acc D Real: 98.750% 
Loss D Fake: 0.8540 (0.8706) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.5568 (0.5504) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,453 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.2025 (0.4464) Acc D Real: 98.761% 
Loss D Fake: 0.8545 (0.8705) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.5569 (0.5505) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,460 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.1508 (0.4438) Acc D Real: 98.772% 
Loss D Fake: 0.8540 (0.8703) Acc D Fake: 0.000% 
Loss D: 1.005 
Loss G: 0.5576 (0.5506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,468 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.1599 (0.4414) Acc D Real: 98.782% 
Loss D Fake: 0.8526 (0.8702) Acc D Fake: 0.000% 
Loss D: 1.012 
Loss G: 0.5589 (0.5506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,475 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.1911 (0.4393) Acc D Real: 98.793% 
Loss D Fake: 0.8504 (0.8700) Acc D Fake: 0.000% 
Loss D: 1.042 
Loss G: 0.5606 (0.5507) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,482 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.1779 (0.4371) Acc D Real: 98.803% 
Loss D Fake: 0.8478 (0.8698) Acc D Fake: 0.000% 
Loss D: 1.026 
Loss G: 0.5622 (0.5508) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,490 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.1703 (0.4349) Acc D Real: 98.813% 
Loss D Fake: 0.8455 (0.8696) Acc D Fake: 0.000% 
Loss D: 1.016 
Loss G: 0.5637 (0.5509) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,497 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.1990 (0.4329) Acc D Real: 98.823% 
Loss D Fake: 0.8435 (0.8694) Acc D Fake: 0.000% 
Loss D: 1.042 
Loss G: 0.5650 (0.5510) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,504 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.1543 (0.4306) Acc D Real: 98.832% 
Loss D Fake: 0.8416 (0.8692) Acc D Fake: 0.000% 
Loss D: 0.996 
Loss G: 0.5663 (0.5512) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,511 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.1693 (0.4285) Acc D Real: 98.842% 
Loss D Fake: 0.8397 (0.8689) Acc D Fake: 0.000% 
Loss D: 1.009 
Loss G: 0.5676 (0.5513) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,519 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.1473 (0.4263) Acc D Real: 98.851% 
Loss D Fake: 0.8378 (0.8687) Acc D Fake: 0.000% 
Loss D: 0.985 
Loss G: 0.5689 (0.5514) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,526 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.1338 (0.4239) Acc D Real: 98.860% 
Loss D Fake: 0.8359 (0.8684) Acc D Fake: 0.000% 
Loss D: 0.970 
Loss G: 0.5703 (0.5516) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,533 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.1793 (0.4220) Acc D Real: 98.869% 
Loss D Fake: 0.8341 (0.8682) Acc D Fake: 0.000% 
Loss D: 1.013 
Loss G: 0.5715 (0.5517) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,541 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.1430 (0.4198) Acc D Real: 98.878% 
Loss D Fake: 0.8324 (0.8679) Acc D Fake: 0.000% 
Loss D: 0.975 
Loss G: 0.5729 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,548 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.1737 (0.4179) Acc D Real: 98.887% 
Loss D Fake: 0.8307 (0.8676) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.5740 (0.5521) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,555 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.1369 (0.4157) Acc D Real: 98.896% 
Loss D Fake: 0.8291 (0.8673) Acc D Fake: 0.000% 
Loss D: 0.966 
Loss G: 0.5752 (0.5523) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,563 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.1803 (0.4139) Acc D Real: 98.904% 
Loss D Fake: 0.8276 (0.8670) Acc D Fake: 0.000% 
Loss D: 1.008 
Loss G: 0.5763 (0.5524) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,570 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1845 (0.4121) Acc D Real: 98.913% 
Loss D Fake: 0.8263 (0.8667) Acc D Fake: 0.000% 
Loss D: 1.011 
Loss G: 0.5773 (0.5526) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,577 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1876 (0.4104) Acc D Real: 98.921% 
Loss D Fake: 0.8251 (0.8664) Acc D Fake: 0.000% 
Loss D: 1.013 
Loss G: 0.5780 (0.5528) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,584 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.1658 (0.4086) Acc D Real: 98.929% 
Loss D Fake: 0.8242 (0.8660) Acc D Fake: 0.000% 
Loss D: 0.990 
Loss G: 0.5789 (0.5530) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,592 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.1310 (0.4065) Acc D Real: 98.937% 
Loss D Fake: 0.8229 (0.8657) Acc D Fake: 0.000% 
Loss D: 0.954 
Loss G: 0.5800 (0.5532) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,599 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.1282 (0.4044) Acc D Real: 98.945% 
Loss D Fake: 0.8215 (0.8654) Acc D Fake: 0.000% 
Loss D: 0.950 
Loss G: 0.5812 (0.5534) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,606 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.1794 (0.4028) Acc D Real: 98.953% 
Loss D Fake: 0.8199 (0.8651) Acc D Fake: 0.000% 
Loss D: 0.999 
Loss G: 0.5823 (0.5536) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,613 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1582 (0.4010) Acc D Real: 98.960% 
Loss D Fake: 0.8186 (0.8647) Acc D Fake: 0.000% 
Loss D: 0.977 
Loss G: 0.5833 (0.5539) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,621 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1451 (0.3991) Acc D Real: 98.968% 
Loss D Fake: 0.8174 (0.8644) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5841 (0.5541) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,628 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.1427 (0.3973) Acc D Real: 98.975% 
Loss D Fake: 0.8164 (0.8640) Acc D Fake: 0.000% 
Loss D: 0.959 
Loss G: 0.5851 (0.5543) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,635 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.1370 (0.3954) Acc D Real: 98.982% 
Loss D Fake: 0.8150 (0.8637) Acc D Fake: 0.000% 
Loss D: 0.952 
Loss G: 0.5864 (0.5545) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,643 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.1346 (0.3936) Acc D Real: 98.989% 
Loss D Fake: 0.8133 (0.8633) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.5878 (0.5548) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,650 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1661 (0.3920) Acc D Real: 98.996% 
Loss D Fake: 0.8115 (0.8630) Acc D Fake: 0.000% 
Loss D: 0.978 
Loss G: 0.5892 (0.5550) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,657 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.1695 (0.3904) Acc D Real: 99.003% 
Loss D Fake: 0.8098 (0.8626) Acc D Fake: 0.000% 
Loss D: 0.979 
Loss G: 0.5906 (0.5553) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,664 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1417 (0.3887) Acc D Real: 99.010% 
Loss D Fake: 0.8078 (0.8622) Acc D Fake: 0.000% 
Loss D: 0.949 
Loss G: 0.5922 (0.5555) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,671 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2106 (0.3875) Acc D Real: 99.017% 
Loss D Fake: 0.8059 (0.8618) Acc D Fake: 0.000% 
Loss D: 1.017 
Loss G: 0.5934 (0.5558) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,679 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.1769 (0.3860) Acc D Real: 99.023% 
Loss D Fake: 0.8051 (0.8614) Acc D Fake: 0.000% 
Loss D: 0.982 
Loss G: 0.5936 (0.5560) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,686 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1577 (0.3845) Acc D Real: 99.029% 
Loss D Fake: 0.8055 (0.8610) Acc D Fake: 0.000% 
Loss D: 0.963 
Loss G: 0.5935 (0.5563) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,693 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.1374 (0.3828) Acc D Real: 99.035% 
Loss D Fake: 0.8057 (0.8607) Acc D Fake: 0.000% 
Loss D: 0.943 
Loss G: 0.5938 (0.5565) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,701 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.1525 (0.3813) Acc D Real: 99.041% 
Loss D Fake: 0.8051 (0.8603) Acc D Fake: 0.000% 
Loss D: 0.958 
Loss G: 0.5949 (0.5568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,708 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.1339 (0.3796) Acc D Real: 99.047% 
Loss D Fake: 0.8041 (0.8599) Acc D Fake: 0.000% 
Loss D: 0.938 
Loss G: 0.5952 (0.5571) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,716 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.1374 (0.3780) Acc D Real: 99.053% 
Loss D Fake: 0.8042 (0.8596) Acc D Fake: 0.000% 
Loss D: 0.942 
Loss G: 0.5959 (0.5573) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,724 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.1316 (0.3764) Acc D Real: 99.059% 
Loss D Fake: 0.8029 (0.8592) Acc D Fake: 0.000% 
Loss D: 0.935 
Loss G: 0.5974 (0.5576) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,731 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.1661 (0.3750) Acc D Real: 99.065% 
Loss D Fake: 0.8008 (0.8588) Acc D Fake: 0.000% 
Loss D: 0.967 
Loss G: 0.5993 (0.5578) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,738 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.1280 (0.3734) Acc D Real: 99.071% 
Loss D Fake: 0.7980 (0.8584) Acc D Fake: 0.000% 
Loss D: 0.926 
Loss G: 0.6018 (0.5581) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,746 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.1362 (0.3719) Acc D Real: 99.076% 
Loss D Fake: 0.7946 (0.8580) Acc D Fake: 0.000% 
Loss D: 0.931 
Loss G: 0.6044 (0.5584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,753 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.1484 (0.3705) Acc D Real: 99.082% 
Loss D Fake: 0.7913 (0.8576) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6068 (0.5587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,761 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.1253 (0.3689) Acc D Real: 99.087% 
Loss D Fake: 0.7883 (0.8571) Acc D Fake: 0.000% 
Loss D: 0.914 
Loss G: 0.6090 (0.5591) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,768 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.1600 (0.3676) Acc D Real: 99.092% 
Loss D Fake: 0.7856 (0.8567) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6112 (0.5594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,775 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.1795 (0.3664) Acc D Real: 99.096% 
Loss D Fake: 0.7831 (0.8562) Acc D Fake: 0.000% 
Loss D: 0.963 
Loss G: 0.6131 (0.5597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,783 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.1381 (0.3650) Acc D Real: 99.101% 
Loss D Fake: 0.7808 (0.8557) Acc D Fake: 0.000% 
Loss D: 0.919 
Loss G: 0.6150 (0.5601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,790 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.1536 (0.3636) Acc D Real: 99.105% 
Loss D Fake: 0.7791 (0.8553) Acc D Fake: 0.000% 
Loss D: 0.933 
Loss G: 0.6155 (0.5604) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,797 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.1613 (0.3624) Acc D Real: 99.109% 
Loss D Fake: 0.7794 (0.8548) Acc D Fake: 0.000% 
Loss D: 0.941 
Loss G: 0.6158 (0.5608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,805 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.1665 (0.3612) Acc D Real: 99.113% 
Loss D Fake: 0.7793 (0.8543) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6159 (0.5611) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,812 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.1523 (0.3599) Acc D Real: 99.117% 
Loss D Fake: 0.7799 (0.8539) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.6159 (0.5614) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,819 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1647 (0.3587) Acc D Real: 99.122% 
Loss D Fake: 0.7810 (0.8534) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6153 (0.5618) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,827 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.1434 (0.3574) Acc D Real: 99.126% 
Loss D Fake: 0.7827 (0.8530) Acc D Fake: 0.000% 
Loss D: 0.926 
Loss G: 0.6157 (0.5621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,834 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.1624 (0.3563) Acc D Real: 99.131% 
Loss D Fake: 0.7821 (0.8526) Acc D Fake: 0.000% 
Loss D: 0.944 
Loss G: 0.6171 (0.5624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,841 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.1386 (0.3550) Acc D Real: 99.134% 
Loss D Fake: 0.7803 (0.8522) Acc D Fake: 0.000% 
Loss D: 0.919 
Loss G: 0.6194 (0.5628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,849 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.1555 (0.3538) Acc D Real: 99.138% 
Loss D Fake: 0.7768 (0.8517) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.6228 (0.5631) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,856 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.1565 (0.3526) Acc D Real: 99.142% 
Loss D Fake: 0.7723 (0.8512) Acc D Fake: 0.000% 
Loss D: 0.929 
Loss G: 0.6259 (0.5635) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,863 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.1579 (0.3515) Acc D Real: 99.146% 
Loss D Fake: 0.7686 (0.8508) Acc D Fake: 0.000% 
Loss D: 0.927 
Loss G: 0.6284 (0.5639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,871 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.1620 (0.3504) Acc D Real: 99.150% 
Loss D Fake: 0.7659 (0.8503) Acc D Fake: 0.000% 
Loss D: 0.928 
Loss G: 0.6307 (0.5642) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,878 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.1347 (0.3492) Acc D Real: 99.153% 
Loss D Fake: 0.7633 (0.8498) Acc D Fake: 0.000% 
Loss D: 0.898 
Loss G: 0.6329 (0.5646) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,886 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1212 (0.3478) Acc D Real: 99.155% 
Loss D Fake: 0.7611 (0.8493) Acc D Fake: 0.000% 
Loss D: 0.882 
Loss G: 0.6345 (0.5650) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,893 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.1427 (0.3467) Acc D Real: 99.159% 
Loss D Fake: 0.7598 (0.8487) Acc D Fake: 0.000% 
Loss D: 0.903 
Loss G: 0.6360 (0.5655) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,901 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.1808 (0.3457) Acc D Real: 99.163% 
Loss D Fake: 0.7591 (0.8482) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6363 (0.5659) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,908 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.1562 (0.3447) Acc D Real: 99.165% 
Loss D Fake: 0.7608 (0.8477) Acc D Fake: 0.000% 
Loss D: 0.917 
Loss G: 0.6365 (0.5663) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,916 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.1308 (0.3435) Acc D Real: 99.169% 
Loss D Fake: 0.7624 (0.8473) Acc D Fake: 0.000% 
Loss D: 0.893 
Loss G: 0.6369 (0.5666) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,924 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.1847 (0.3426) Acc D Real: 99.172% 
Loss D Fake: 0.7667 (0.8468) Acc D Fake: 0.000% 
Loss D: 0.951 
Loss G: 0.6350 (0.5670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,931 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.1383 (0.3414) Acc D Real: 99.175% 
Loss D Fake: 0.7812 (0.8464) Acc D Fake: 0.000% 
Loss D: 0.919 
Loss G: 0.6346 (0.5674) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,938 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.1579 (0.3404) Acc D Real: 99.178% 
Loss D Fake: 0.7829 (0.8461) Acc D Fake: 0.000% 
Loss D: 0.941 
Loss G: 0.6390 (0.5678) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,945 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.1651 (0.3395) Acc D Real: 99.182% 
Loss D Fake: 0.7674 (0.8457) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.6453 (0.5682) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,953 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.1351 (0.3383) Acc D Real: 99.184% 
Loss D Fake: 0.7552 (0.8452) Acc D Fake: 0.000% 
Loss D: 0.890 
Loss G: 0.6503 (0.5687) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,960 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.1167 (0.3371) Acc D Real: 99.185% 
Loss D Fake: 0.7486 (0.8446) Acc D Fake: 0.000% 
Loss D: 0.865 
Loss G: 0.6536 (0.5691) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,967 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.1944 (0.3364) Acc D Real: 99.189% 
Loss D Fake: 0.7464 (0.8441) Acc D Fake: 0.000% 
Loss D: 0.941 
Loss G: 0.6556 (0.5696) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,975 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1416 (0.3353) Acc D Real: 99.192% 
Loss D Fake: 0.7483 (0.8436) Acc D Fake: 0.000% 
Loss D: 0.890 
Loss G: 0.6560 (0.5701) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,982 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.1693 (0.3344) Acc D Real: 99.195% 
Loss D Fake: 0.7632 (0.8432) Acc D Fake: 0.000% 
Loss D: 0.933 
Loss G: 0.6507 (0.5705) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,989 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.1964 (0.3337) Acc D Real: 99.197% 
Loss D Fake: 0.8211 (0.8431) Acc D Fake: 0.000% 
Loss D: 1.018 
Loss G: 0.6529 (0.5709) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:50,998 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.1268 (0.3326) Acc D Real: 99.200% 
Loss D Fake: 0.8370 (0.8430) Acc D Fake: 0.000% 
Loss D: 0.964 
Loss G: 0.6407 (0.5713) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,005 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.1490 (0.3316) Acc D Real: 99.201% 
Loss D Fake: 0.8552 (0.8431) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.6579 (0.5718) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,013 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.1400 (0.3306) Acc D Real: 99.203% 
Loss D Fake: 0.7254 (0.8425) Acc D Fake: 0.000% 
Loss D: 0.865 
Loss G: 0.6614 (0.5722) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,021 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.1741 (0.3298) Acc D Real: 99.202% 
Loss D Fake: 1.5518 (0.8462) Acc D Fake: 0.000% 
Loss D: 1.726 
Loss G: 0.6729 (0.5728) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,028 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.1667 (0.3290) Acc D Real: 99.200% 
Loss D Fake: 0.7213 (0.8455) Acc D Fake: 0.000% 
Loss D: 0.888 
Loss G: 0.6554 (0.5732) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,036 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.1691 (0.3281) Acc D Real: 99.193% 
Loss D Fake: 0.7846 (0.8452) Acc D Fake: 0.000% 
Loss D: 0.954 
Loss G: 0.6003 (0.5733) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,044 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.1676 (0.3273) Acc D Real: 99.191% 
Loss D Fake: 1.0120 (0.8461) Acc D Fake: 0.000% 
Loss D: 1.180 
Loss G: 0.6165 (0.5735) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,051 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.1707 (0.3265) Acc D Real: 99.184% 
Loss D Fake: 0.7509 (0.8456) Acc D Fake: 0.000% 
Loss D: 0.922 
Loss G: 0.6612 (0.5740) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:45:51,059 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.1642 (0.3257) Acc D Real: 99.174% 
Loss D Fake: 0.7165 (0.8449) Acc D Fake: 0.000% 
Loss D: 0.881 
Loss G: 0.6796 (0.5745) Acc G: 99.958% 
LR: 2.000e-04 

2023-03-02 01:45:51,068 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.1789 (0.3250) Acc D Real: 99.156% 
Loss D Fake: 0.7013 (0.8442) Acc D Fake: 0.084% 
Loss D: 0.880 
Loss G: 0.6910 (0.5751) Acc G: 99.747% 
LR: 2.000e-04 

2023-03-02 01:45:51,077 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3253 (0.3250) Acc D Real: 99.070% 
Loss D Fake: 0.6914 (0.8434) Acc D Fake: 0.494% 
Loss D: 1.017 
Loss G: 0.6991 (0.5757) Acc G: 99.330% 
LR: 2.000e-04 

2023-03-02 01:45:51,085 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2187 (0.3244) Acc D Real: 99.033% 
Loss D Fake: 0.6845 (0.8426) Acc D Fake: 0.908% 
Loss D: 0.903 
Loss G: 0.7051 (0.5764) Acc G: 98.917% 
LR: 2.000e-04 

2023-03-02 01:45:51,093 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2044 (0.3238) Acc D Real: 98.995% 
Loss D Fake: 0.6793 (0.8418) Acc D Fake: 1.318% 
Loss D: 0.884 
Loss G: 0.7099 (0.5771) Acc G: 98.499% 
LR: 2.000e-04 

2023-03-02 01:45:51,101 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.2441 (0.3234) Acc D Real: 98.933% 
Loss D Fake: 0.6751 (0.8410) Acc D Fake: 1.733% 
Loss D: 0.919 
Loss G: 0.7138 (0.5777) Acc G: 98.086% 
LR: 2.000e-04 

2023-03-02 01:45:51,108 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2033 (0.3228) Acc D Real: 98.896% 
Loss D Fake: 0.6716 (0.8402) Acc D Fake: 2.143% 
Loss D: 0.875 
Loss G: 0.7172 (0.5784) Acc G: 97.677% 
LR: 2.000e-04 

2023-03-02 01:45:51,116 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.2208 (0.3223) Acc D Real: 98.862% 
Loss D Fake: 0.6686 (0.8393) Acc D Fake: 2.549% 
Loss D: 0.889 
Loss G: 0.7202 (0.5791) Acc G: 97.263% 
LR: 2.000e-04 

2023-03-02 01:45:51,123 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2125 (0.3218) Acc D Real: 98.813% 
Loss D Fake: 0.6660 (0.8385) Acc D Fake: 2.959% 
Loss D: 0.879 
Loss G: 0.7227 (0.5798) Acc G: 96.854% 
LR: 2.000e-04 

2023-03-02 01:45:51,130 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3122 (0.3218) Acc D Real: 98.715% 
Loss D Fake: 0.6638 (0.8376) Acc D Fake: 3.366% 
Loss D: 0.976 
Loss G: 0.7249 (0.5805) Acc G: 96.448% 
LR: 2.000e-04 

2023-03-02 01:45:51,138 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3068 (0.3217) Acc D Real: 98.619% 
Loss D Fake: 0.6620 (0.8368) Acc D Fake: 3.768% 
Loss D: 0.969 
Loss G: 0.7267 (0.5812) Acc G: 96.047% 
LR: 2.000e-04 

2023-03-02 01:45:51,145 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2742 (0.3215) Acc D Real: 98.539% 
Loss D Fake: 0.6605 (0.8359) Acc D Fake: 4.167% 
Loss D: 0.935 
Loss G: 0.7281 (0.5819) Acc G: 95.649% 
LR: 2.000e-04 

2023-03-02 01:45:51,152 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3452 (0.3216) Acc D Real: 98.433% 
Loss D Fake: 0.6593 (0.8351) Acc D Fake: 4.561% 
Loss D: 1.005 
Loss G: 0.7292 (0.5826) Acc G: 95.255% 
LR: 2.000e-04 

2023-03-02 01:45:51,160 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3192 (0.3216) Acc D Real: 98.335% 
Loss D Fake: 0.6584 (0.8342) Acc D Fake: 4.952% 
Loss D: 0.978 
Loss G: 0.7302 (0.5833) Acc G: 94.865% 
LR: 2.000e-04 

2023-03-02 01:45:51,167 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3003 (0.3215) Acc D Real: 98.244% 
Loss D Fake: 0.6575 (0.8334) Acc D Fake: 5.340% 
Loss D: 0.958 
Loss G: 0.7311 (0.5840) Acc G: 94.479% 
LR: 2.000e-04 

2023-03-02 01:45:51,175 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3113 (0.3214) Acc D Real: 98.162% 
Loss D Fake: 0.6568 (0.8326) Acc D Fake: 5.723% 
Loss D: 0.968 
Loss G: 0.7318 (0.5847) Acc G: 94.096% 
LR: 2.000e-04 

2023-03-02 01:45:51,182 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2714 (0.3212) Acc D Real: 98.089% 
Loss D Fake: 0.6562 (0.8317) Acc D Fake: 6.103% 
Loss D: 0.928 
Loss G: 0.7323 (0.5854) Acc G: 93.717% 
LR: 2.000e-04 

2023-03-02 01:45:51,190 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3413 (0.3213) Acc D Real: 97.986% 
Loss D Fake: 0.6557 (0.8309) Acc D Fake: 6.480% 
Loss D: 0.997 
Loss G: 0.7328 (0.5861) Acc G: 93.341% 
LR: 2.000e-04 

2023-03-02 01:45:51,197 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3206 (0.3213) Acc D Real: 97.896% 
Loss D Fake: 0.6554 (0.8301) Acc D Fake: 6.853% 
Loss D: 0.976 
Loss G: 0.7331 (0.5868) Acc G: 92.969% 
LR: 2.000e-04 

2023-03-02 01:45:51,204 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2204 (0.3208) Acc D Real: 97.846% 
Loss D Fake: 0.6552 (0.8293) Acc D Fake: 7.222% 
Loss D: 0.876 
Loss G: 0.7333 (0.5875) Acc G: 92.600% 
LR: 2.000e-04 

2023-03-02 01:45:51,212 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3598 (0.3210) Acc D Real: 97.737% 
Loss D Fake: 0.6550 (0.8285) Acc D Fake: 7.588% 
Loss D: 1.015 
Loss G: 0.7335 (0.5881) Acc G: 92.235% 
LR: 2.000e-04 

2023-03-02 01:45:51,219 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2751 (0.3208) Acc D Real: 97.671% 
Loss D Fake: 0.6549 (0.8277) Acc D Fake: 7.951% 
Loss D: 0.930 
Loss G: 0.7336 (0.5888) Acc G: 91.873% 
LR: 2.000e-04 

2023-03-02 01:45:51,226 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2724 (0.3206) Acc D Real: 97.607% 
Loss D Fake: 0.6547 (0.8269) Acc D Fake: 8.311% 
Loss D: 0.927 
Loss G: 0.7339 (0.5895) Acc G: 91.514% 
LR: 2.000e-04 

2023-03-02 01:45:51,234 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2603 (0.3203) Acc D Real: 97.544% 
Loss D Fake: 0.6544 (0.8261) Acc D Fake: 8.667% 
Loss D: 0.915 
Loss G: 0.7343 (0.5901) Acc G: 91.159% 
LR: 2.000e-04 

2023-03-02 01:45:51,241 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2045 (0.3198) Acc D Real: 97.512% 
Loss D Fake: 0.6539 (0.8253) Acc D Fake: 9.020% 
Loss D: 0.858 
Loss G: 0.7348 (0.5908) Acc G: 90.807% 
LR: 2.000e-04 

2023-03-02 01:45:51,248 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.1949 (0.3192) Acc D Real: 97.478% 
Loss D Fake: 0.6534 (0.8246) Acc D Fake: 9.369% 
Loss D: 0.848 
Loss G: 0.7355 (0.5914) Acc G: 90.450% 
LR: 2.000e-04 

2023-03-02 01:45:51,256 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.1978 (0.3186) Acc D Real: 97.447% 
Loss D Fake: 0.6528 (0.8238) Acc D Fake: 9.723% 
Loss D: 0.851 
Loss G: 0.7363 (0.5921) Acc G: 90.097% 
LR: 2.000e-04 

2023-03-02 01:45:51,263 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2491 (0.3183) Acc D Real: 97.395% 
Loss D Fake: 0.6520 (0.8230) Acc D Fake: 10.074% 
Loss D: 0.901 
Loss G: 0.7370 (0.5927) Acc G: 89.747% 
LR: 2.000e-04 

2023-03-02 01:45:51,271 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.2723 (0.3181) Acc D Real: 97.318% 
Loss D Fake: 0.6514 (0.8223) Acc D Fake: 10.422% 
Loss D: 0.924 
Loss G: 0.7377 (0.5934) Acc G: 89.400% 
LR: 2.000e-04 

2023-03-02 01:45:51,278 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.1748 (0.3175) Acc D Real: 97.314% 
Loss D Fake: 0.6508 (0.8215) Acc D Fake: 10.509% 
Loss D: 0.826 
Loss G: 0.7384 (0.5940) Acc G: 89.314% 
LR: 2.000e-04 

2023-03-02 01:45:51,289 -                train: [    INFO] - Best Loss 100000000000.000 to 0.867
2023-03-02 01:45:51,289 -                train: [    INFO] - 
Epoch: 2/20
2023-03-02 01:45:51,482 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2926 (0.2538) Acc D Real: 85.651% 
Loss D Fake: 0.6511 (0.6508) Acc D Fake: 88.333% 
Loss D: 0.944 
Loss G: 0.7374 (0.7377) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,489 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.2771 (0.2616) Acc D Real: 84.705% 
Loss D Fake: 0.6516 (0.6511) Acc D Fake: 88.333% 
Loss D: 0.929 
Loss G: 0.7369 (0.7374) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,497 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2535 (0.2595) Acc D Real: 84.961% 
Loss D Fake: 0.6520 (0.6513) Acc D Fake: 88.333% 
Loss D: 0.906 
Loss G: 0.7366 (0.7372) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,507 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2350 (0.2546) Acc D Real: 85.438% 
Loss D Fake: 0.6522 (0.6515) Acc D Fake: 88.333% 
Loss D: 0.887 
Loss G: 0.7364 (0.7370) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,516 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2772 (0.2584) Acc D Real: 84.557% 
Loss D Fake: 0.6523 (0.6516) Acc D Fake: 88.333% 
Loss D: 0.930 
Loss G: 0.7363 (0.7369) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,525 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.1999 (0.2500) Acc D Real: 85.952% 
Loss D Fake: 0.6524 (0.6517) Acc D Fake: 88.333% 
Loss D: 0.852 
Loss G: 0.7364 (0.7369) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,534 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2225 (0.2466) Acc D Real: 85.983% 
Loss D Fake: 0.6522 (0.6518) Acc D Fake: 88.333% 
Loss D: 0.875 
Loss G: 0.7367 (0.7368) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,542 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2235 (0.2440) Acc D Real: 85.943% 
Loss D Fake: 0.6519 (0.6518) Acc D Fake: 88.333% 
Loss D: 0.875 
Loss G: 0.7371 (0.7369) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,549 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2122 (0.2409) Acc D Real: 86.458% 
Loss D Fake: 0.6514 (0.6518) Acc D Fake: 88.333% 
Loss D: 0.864 
Loss G: 0.7378 (0.7370) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,558 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.2122 (0.2383) Acc D Real: 86.591% 
Loss D Fake: 0.6506 (0.6517) Acc D Fake: 88.333% 
Loss D: 0.863 
Loss G: 0.7387 (0.7371) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,566 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2442 (0.2388) Acc D Real: 86.458% 
Loss D Fake: 0.6497 (0.6515) Acc D Fake: 88.333% 
Loss D: 0.894 
Loss G: 0.7398 (0.7373) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,573 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2120 (0.2367) Acc D Real: 86.514% 
Loss D Fake: 0.6487 (0.6513) Acc D Fake: 88.333% 
Loss D: 0.861 
Loss G: 0.7410 (0.7376) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,580 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2442 (0.2372) Acc D Real: 86.555% 
Loss D Fake: 0.6476 (0.6510) Acc D Fake: 88.333% 
Loss D: 0.892 
Loss G: 0.7421 (0.7379) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,587 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2349 (0.2371) Acc D Real: 86.611% 
Loss D Fake: 0.6467 (0.6507) Acc D Fake: 88.333% 
Loss D: 0.882 
Loss G: 0.7430 (0.7383) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,594 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2349 (0.2369) Acc D Real: 86.725% 
Loss D Fake: 0.6460 (0.6505) Acc D Fake: 88.333% 
Loss D: 0.881 
Loss G: 0.7437 (0.7386) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,605 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.2146 (0.2356) Acc D Real: 86.777% 
Loss D Fake: 0.6455 (0.6502) Acc D Fake: 88.333% 
Loss D: 0.860 
Loss G: 0.7443 (0.7390) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,612 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2191 (0.2347) Acc D Real: 86.863% 
Loss D Fake: 0.6449 (0.6499) Acc D Fake: 88.333% 
Loss D: 0.864 
Loss G: 0.7450 (0.7393) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,619 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.2193 (0.2339) Acc D Real: 87.050% 
Loss D Fake: 0.6445 (0.6496) Acc D Fake: 88.333% 
Loss D: 0.864 
Loss G: 0.7451 (0.7396) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,626 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3084 (0.2376) Acc D Real: 86.578% 
Loss D Fake: 0.6447 (0.6493) Acc D Fake: 88.333% 
Loss D: 0.953 
Loss G: 0.7448 (0.7399) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,633 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.2135 (0.2365) Acc D Real: 86.709% 
Loss D Fake: 0.6450 (0.6491) Acc D Fake: 88.333% 
Loss D: 0.858 
Loss G: 0.7444 (0.7401) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,640 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1803 (0.2339) Acc D Real: 86.927% 
Loss D Fake: 0.6452 (0.6490) Acc D Fake: 88.333% 
Loss D: 0.826 
Loss G: 0.7443 (0.7403) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,647 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1791 (0.2315) Acc D Real: 87.144% 
Loss D Fake: 0.6451 (0.6488) Acc D Fake: 88.333% 
Loss D: 0.824 
Loss G: 0.7446 (0.7405) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,654 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.2599 (0.2327) Acc D Real: 86.973% 
Loss D Fake: 0.6447 (0.6486) Acc D Fake: 88.333% 
Loss D: 0.905 
Loss G: 0.7452 (0.7406) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,661 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2050 (0.2316) Acc D Real: 87.102% 
Loss D Fake: 0.6441 (0.6484) Acc D Fake: 88.333% 
Loss D: 0.849 
Loss G: 0.7460 (0.7409) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,668 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2363 (0.2318) Acc D Real: 87.057% 
Loss D Fake: 0.6434 (0.6482) Acc D Fake: 88.333% 
Loss D: 0.880 
Loss G: 0.7468 (0.7411) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,674 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.2998 (0.2343) Acc D Real: 86.763% 
Loss D Fake: 0.6426 (0.6480) Acc D Fake: 88.333% 
Loss D: 0.942 
Loss G: 0.7477 (0.7413) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,681 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2198 (0.2338) Acc D Real: 86.760% 
Loss D Fake: 0.6418 (0.6478) Acc D Fake: 88.333% 
Loss D: 0.862 
Loss G: 0.7487 (0.7416) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,689 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.2541 (0.2345) Acc D Real: 86.618% 
Loss D Fake: 0.6408 (0.6476) Acc D Fake: 88.333% 
Loss D: 0.895 
Loss G: 0.7498 (0.7419) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,696 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.2256 (0.2342) Acc D Real: 86.547% 
Loss D Fake: 0.6400 (0.6473) Acc D Fake: 88.333% 
Loss D: 0.866 
Loss G: 0.7505 (0.7422) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,704 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2065 (0.2333) Acc D Real: 86.662% 
Loss D Fake: 0.6395 (0.6471) Acc D Fake: 88.333% 
Loss D: 0.846 
Loss G: 0.7512 (0.7425) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,711 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.2132 (0.2327) Acc D Real: 86.681% 
Loss D Fake: 0.6388 (0.6468) Acc D Fake: 88.333% 
Loss D: 0.852 
Loss G: 0.7520 (0.7428) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,720 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2639 (0.2336) Acc D Real: 86.679% 
Loss D Fake: 0.6389 (0.6466) Acc D Fake: 88.333% 
Loss D: 0.903 
Loss G: 0.7498 (0.7430) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,729 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.2048 (0.2328) Acc D Real: 86.711% 
Loss D Fake: 0.6417 (0.6464) Acc D Fake: 88.333% 
Loss D: 0.846 
Loss G: 0.7470 (0.7431) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,737 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1315 (0.2299) Acc D Real: 86.933% 
Loss D Fake: 0.6440 (0.6464) Acc D Fake: 88.333% 
Loss D: 0.775 
Loss G: 0.7448 (0.7431) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,745 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.2388 (0.2301) Acc D Real: 86.889% 
Loss D Fake: 0.6461 (0.6463) Acc D Fake: 88.333% 
Loss D: 0.885 
Loss G: 0.7416 (0.7431) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,752 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2885 (0.2317) Acc D Real: 86.700% 
Loss D Fake: 0.6492 (0.6464) Acc D Fake: 88.333% 
Loss D: 0.938 
Loss G: 0.7385 (0.7430) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,759 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1693 (0.2301) Acc D Real: 86.852% 
Loss D Fake: 0.6516 (0.6466) Acc D Fake: 88.333% 
Loss D: 0.821 
Loss G: 0.7361 (0.7428) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,767 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1853 (0.2289) Acc D Real: 86.981% 
Loss D Fake: 0.6538 (0.6467) Acc D Fake: 88.332% 
Loss D: 0.839 
Loss G: 0.7337 (0.7426) Acc G: 11.709% 
LR: 2.000e-04 

2023-03-02 01:45:51,774 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2015 (0.2282) Acc D Real: 87.049% 
Loss D Fake: 0.6557 (0.6470) Acc D Fake: 88.290% 
Loss D: 0.857 
Loss G: 0.7323 (0.7423) Acc G: 11.750% 
LR: 2.000e-04 

2023-03-02 01:45:51,782 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1785 (0.2270) Acc D Real: 87.139% 
Loss D Fake: 0.6563 (0.6472) Acc D Fake: 88.251% 
Loss D: 0.835 
Loss G: 0.7323 (0.7421) Acc G: 11.789% 
LR: 2.000e-04 

2023-03-02 01:45:51,789 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1920 (0.2262) Acc D Real: 87.250% 
Loss D Fake: 0.6559 (0.6474) Acc D Fake: 88.213% 
Loss D: 0.848 
Loss G: 0.7332 (0.7418) Acc G: 11.786% 
LR: 2.000e-04 

2023-03-02 01:45:51,796 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2417 (0.2265) Acc D Real: 87.146% 
Loss D Fake: 0.6546 (0.6476) Acc D Fake: 88.216% 
Loss D: 0.896 
Loss G: 0.7352 (0.7417) Acc G: 11.783% 
LR: 2.000e-04 

2023-03-02 01:45:51,804 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.1784 (0.2254) Acc D Real: 87.251% 
Loss D Fake: 0.6522 (0.6477) Acc D Fake: 88.219% 
Loss D: 0.831 
Loss G: 0.7385 (0.7416) Acc G: 11.780% 
LR: 2.000e-04 

2023-03-02 01:45:51,811 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2381 (0.2257) Acc D Real: 87.194% 
Loss D Fake: 0.6490 (0.6477) Acc D Fake: 88.221% 
Loss D: 0.887 
Loss G: 0.7420 (0.7416) Acc G: 11.778% 
LR: 2.000e-04 

2023-03-02 01:45:51,818 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2121 (0.2254) Acc D Real: 87.198% 
Loss D Fake: 0.6458 (0.6477) Acc D Fake: 88.224% 
Loss D: 0.858 
Loss G: 0.7455 (0.7417) Acc G: 11.775% 
LR: 2.000e-04 

2023-03-02 01:45:51,825 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1389 (0.2236) Acc D Real: 87.394% 
Loss D Fake: 0.6428 (0.6476) Acc D Fake: 88.226% 
Loss D: 0.782 
Loss G: 0.7486 (0.7419) Acc G: 11.773% 
LR: 2.000e-04 

2023-03-02 01:45:51,833 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.1766 (0.2226) Acc D Real: 87.472% 
Loss D Fake: 0.6403 (0.6474) Acc D Fake: 88.228% 
Loss D: 0.817 
Loss G: 0.7512 (0.7421) Acc G: 11.771% 
LR: 2.000e-04 

2023-03-02 01:45:51,840 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.1698 (0.2215) Acc D Real: 87.549% 
Loss D Fake: 0.6381 (0.6472) Acc D Fake: 88.230% 
Loss D: 0.808 
Loss G: 0.7539 (0.7423) Acc G: 11.769% 
LR: 2.000e-04 

2023-03-02 01:45:51,847 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.1407 (0.2199) Acc D Real: 87.710% 
Loss D Fake: 0.6357 (0.6470) Acc D Fake: 88.232% 
Loss D: 0.776 
Loss G: 0.7566 (0.7426) Acc G: 11.767% 
LR: 2.000e-04 

2023-03-02 01:45:51,855 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2039 (0.2196) Acc D Real: 87.750% 
Loss D Fake: 0.6333 (0.6467) Acc D Fake: 88.234% 
Loss D: 0.837 
Loss G: 0.7594 (0.7429) Acc G: 11.765% 
LR: 2.000e-04 

2023-03-02 01:45:51,862 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.1567 (0.2184) Acc D Real: 87.883% 
Loss D Fake: 0.6308 (0.6464) Acc D Fake: 88.236% 
Loss D: 0.788 
Loss G: 0.7624 (0.7433) Acc G: 11.763% 
LR: 2.000e-04 

2023-03-02 01:45:51,871 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.1540 (0.2172) Acc D Real: 87.962% 
Loss D Fake: 0.6283 (0.6461) Acc D Fake: 88.238% 
Loss D: 0.782 
Loss G: 0.7652 (0.7437) Acc G: 11.761% 
LR: 2.000e-04 

2023-03-02 01:45:51,879 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.1711 (0.2163) Acc D Real: 88.035% 
Loss D Fake: 0.6259 (0.6457) Acc D Fake: 88.240% 
Loss D: 0.797 
Loss G: 0.7679 (0.7441) Acc G: 11.728% 
LR: 2.000e-04 

2023-03-02 01:45:51,887 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.1849 (0.2158) Acc D Real: 88.079% 
Loss D Fake: 0.6238 (0.6453) Acc D Fake: 88.272% 
Loss D: 0.809 
Loss G: 0.7701 (0.7446) Acc G: 11.697% 
LR: 2.000e-04 

2023-03-02 01:45:51,896 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.1539 (0.2147) Acc D Real: 88.186% 
Loss D Fake: 0.6220 (0.6449) Acc D Fake: 88.303% 
Loss D: 0.776 
Loss G: 0.7722 (0.7451) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:45:51,903 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2146 (0.2147) Acc D Real: 88.151% 
Loss D Fake: 0.6203 (0.6445) Acc D Fake: 88.332% 
Loss D: 0.835 
Loss G: 0.7742 (0.7456) Acc G: 11.637% 
LR: 2.000e-04 

2023-03-02 01:45:51,912 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.1437 (0.2134) Acc D Real: 88.270% 
Loss D Fake: 0.6186 (0.6440) Acc D Fake: 88.361% 
Loss D: 0.762 
Loss G: 0.7763 (0.7462) Acc G: 11.609% 
LR: 2.000e-04 

2023-03-02 01:45:51,919 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.1755 (0.2128) Acc D Real: 88.290% 
Loss D Fake: 0.6169 (0.6436) Acc D Fake: 88.389% 
Loss D: 0.792 
Loss G: 0.7782 (0.7467) Acc G: 11.582% 
LR: 2.000e-04 

2023-03-02 01:45:51,927 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.1427 (0.2116) Acc D Real: 88.362% 
Loss D Fake: 0.6153 (0.6431) Acc D Fake: 88.416% 
Loss D: 0.758 
Loss G: 0.7801 (0.7473) Acc G: 11.556% 
LR: 2.000e-04 

2023-03-02 01:45:51,934 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.2078 (0.2116) Acc D Real: 88.353% 
Loss D Fake: 0.6143 (0.6426) Acc D Fake: 88.442% 
Loss D: 0.822 
Loss G: 0.7802 (0.7478) Acc G: 11.530% 
LR: 2.000e-04 

2023-03-02 01:45:51,941 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.1566 (0.2107) Acc D Real: 88.438% 
Loss D Fake: 0.6149 (0.6422) Acc D Fake: 88.467% 
Loss D: 0.772 
Loss G: 0.7791 (0.7483) Acc G: 11.505% 
LR: 2.000e-04 

2023-03-02 01:45:51,948 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.1563 (0.2098) Acc D Real: 88.508% 
Loss D Fake: 0.6158 (0.6417) Acc D Fake: 88.491% 
Loss D: 0.772 
Loss G: 0.7784 (0.7488) Acc G: 11.481% 
LR: 2.000e-04 

2023-03-02 01:45:51,956 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.1983 (0.2096) Acc D Real: 88.512% 
Loss D Fake: 0.6162 (0.6413) Acc D Fake: 88.515% 
Loss D: 0.814 
Loss G: 0.7781 (0.7492) Acc G: 11.458% 
LR: 2.000e-04 

2023-03-02 01:45:51,965 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.1470 (0.2087) Acc D Real: 88.588% 
Loss D Fake: 0.6161 (0.6410) Acc D Fake: 88.538% 
Loss D: 0.763 
Loss G: 0.7786 (0.7497) Acc G: 11.436% 
LR: 2.000e-04 

2023-03-02 01:45:51,974 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1453 (0.2077) Acc D Real: 88.678% 
Loss D Fake: 0.6155 (0.6406) Acc D Fake: 88.560% 
Loss D: 0.761 
Loss G: 0.7795 (0.7501) Acc G: 11.414% 
LR: 2.000e-04 

2023-03-02 01:45:51,982 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.2069 (0.2077) Acc D Real: 88.657% 
Loss D Fake: 0.6147 (0.6402) Acc D Fake: 88.581% 
Loss D: 0.822 
Loss G: 0.7807 (0.7506) Acc G: 11.393% 
LR: 2.000e-04 

2023-03-02 01:45:51,990 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1568 (0.2069) Acc D Real: 88.729% 
Loss D Fake: 0.6136 (0.6398) Acc D Fake: 88.602% 
Loss D: 0.770 
Loss G: 0.7817 (0.7511) Acc G: 11.373% 
LR: 2.000e-04 

2023-03-02 01:45:51,998 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.1667 (0.2064) Acc D Real: 88.764% 
Loss D Fake: 0.6128 (0.6394) Acc D Fake: 88.622% 
Loss D: 0.779 
Loss G: 0.7830 (0.7515) Acc G: 11.353% 
LR: 2.000e-04 

2023-03-02 01:45:52,005 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.2074 (0.2064) Acc D Real: 88.766% 
Loss D Fake: 0.6116 (0.6390) Acc D Fake: 88.642% 
Loss D: 0.819 
Loss G: 0.7847 (0.7520) Acc G: 11.333% 
LR: 2.000e-04 

2023-03-02 01:45:52,013 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2097 (0.2064) Acc D Real: 88.758% 
Loss D Fake: 0.6106 (0.6386) Acc D Fake: 88.661% 
Loss D: 0.820 
Loss G: 0.7850 (0.7525) Acc G: 11.315% 
LR: 2.000e-04 

2023-03-02 01:45:52,020 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1812 (0.2061) Acc D Real: 88.787% 
Loss D Fake: 0.6105 (0.6382) Acc D Fake: 88.680% 
Loss D: 0.792 
Loss G: 0.7854 (0.7529) Acc G: 11.296% 
LR: 2.000e-04 

2023-03-02 01:45:52,028 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.1713 (0.2056) Acc D Real: 88.834% 
Loss D Fake: 0.6099 (0.6378) Acc D Fake: 88.698% 
Loss D: 0.781 
Loss G: 0.7865 (0.7534) Acc G: 11.279% 
LR: 2.000e-04 

2023-03-02 01:45:52,035 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1726 (0.2051) Acc D Real: 88.880% 
Loss D Fake: 0.6088 (0.6374) Acc D Fake: 88.716% 
Loss D: 0.781 
Loss G: 0.7881 (0.7538) Acc G: 11.261% 
LR: 2.000e-04 

2023-03-02 01:45:52,043 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.1746 (0.2047) Acc D Real: 88.900% 
Loss D Fake: 0.6074 (0.6370) Acc D Fake: 88.733% 
Loss D: 0.782 
Loss G: 0.7902 (0.7543) Acc G: 11.244% 
LR: 2.000e-04 

2023-03-02 01:45:52,050 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.2005 (0.2047) Acc D Real: 88.884% 
Loss D Fake: 0.6055 (0.6366) Acc D Fake: 88.749% 
Loss D: 0.806 
Loss G: 0.7925 (0.7548) Acc G: 11.228% 
LR: 2.000e-04 

2023-03-02 01:45:52,058 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2111 (0.2048) Acc D Real: 88.872% 
Loss D Fake: 0.6055 (0.6362) Acc D Fake: 88.766% 
Loss D: 0.817 
Loss G: 0.7877 (0.7553) Acc G: 11.212% 
LR: 2.000e-04 

2023-03-02 01:45:52,065 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1281 (0.2038) Acc D Real: 88.974% 
Loss D Fake: 0.6119 (0.6359) Acc D Fake: 88.781% 
Loss D: 0.740 
Loss G: 0.7794 (0.7556) Acc G: 11.197% 
LR: 2.000e-04 

2023-03-02 01:45:52,073 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.1116 (0.2026) Acc D Real: 89.074% 
Loss D Fake: 0.6200 (0.6357) Acc D Fake: 88.797% 
Loss D: 0.732 
Loss G: 0.7705 (0.7558) Acc G: 11.181% 
LR: 2.000e-04 

2023-03-02 01:45:52,080 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1031 (0.2014) Acc D Real: 89.186% 
Loss D Fake: 0.6273 (0.6356) Acc D Fake: 88.812% 
Loss D: 0.730 
Loss G: 0.7740 (0.7560) Acc G: 11.167% 
LR: 2.000e-04 

2023-03-02 01:45:52,094 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.1210 (0.2004) Acc D Real: 89.278% 
Loss D Fake: 0.6153 (0.6353) Acc D Fake: 88.827% 
Loss D: 0.736 
Loss G: 0.7867 (0.7564) Acc G: 11.152% 
LR: 2.000e-04 

2023-03-02 01:45:52,101 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2026 (0.2004) Acc D Real: 89.279% 
Loss D Fake: 0.6059 (0.6350) Acc D Fake: 88.841% 
Loss D: 0.809 
Loss G: 0.7954 (0.7568) Acc G: 11.138% 
LR: 2.000e-04 

2023-03-02 01:45:52,109 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.1438 (0.1997) Acc D Real: 89.337% 
Loss D Fake: 0.5997 (0.6346) Acc D Fake: 88.855% 
Loss D: 0.743 
Loss G: 0.8024 (0.7574) Acc G: 11.124% 
LR: 2.000e-04 

2023-03-02 01:45:52,117 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.1482 (0.1991) Acc D Real: 89.391% 
Loss D Fake: 0.5945 (0.6341) Acc D Fake: 88.868% 
Loss D: 0.743 
Loss G: 0.8083 (0.7580) Acc G: 11.111% 
LR: 2.000e-04 

2023-03-02 01:45:52,125 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.1655 (0.1987) Acc D Real: 89.408% 
Loss D Fake: 0.5903 (0.6336) Acc D Fake: 88.882% 
Loss D: 0.756 
Loss G: 0.8131 (0.7586) Acc G: 11.098% 
LR: 2.000e-04 

2023-03-02 01:45:52,132 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1763 (0.1985) Acc D Real: 89.428% 
Loss D Fake: 0.5870 (0.6330) Acc D Fake: 88.895% 
Loss D: 0.763 
Loss G: 0.8170 (0.7593) Acc G: 11.085% 
LR: 2.000e-04 

2023-03-02 01:45:52,140 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.1507 (0.1979) Acc D Real: 89.460% 
Loss D Fake: 0.5841 (0.6325) Acc D Fake: 88.907% 
Loss D: 0.735 
Loss G: 0.8205 (0.7600) Acc G: 11.073% 
LR: 2.000e-04 

2023-03-02 01:45:52,147 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.1734 (0.1976) Acc D Real: 89.480% 
Loss D Fake: 0.5815 (0.6319) Acc D Fake: 88.939% 
Loss D: 0.755 
Loss G: 0.8238 (0.7607) Acc G: 11.042% 
LR: 2.000e-04 

2023-03-02 01:45:52,155 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.1370 (0.1969) Acc D Real: 89.535% 
Loss D Fake: 0.5791 (0.6313) Acc D Fake: 88.969% 
Loss D: 0.716 
Loss G: 0.8267 (0.7615) Acc G: 11.011% 
LR: 2.000e-04 

2023-03-02 01:45:52,162 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.1270 (0.1962) Acc D Real: 89.600% 
Loss D Fake: 0.5770 (0.6307) Acc D Fake: 88.999% 
Loss D: 0.704 
Loss G: 0.8294 (0.7622) Acc G: 10.981% 
LR: 2.000e-04 

2023-03-02 01:45:52,169 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.1117 (0.1952) Acc D Real: 89.690% 
Loss D Fake: 0.5751 (0.6301) Acc D Fake: 89.029% 
Loss D: 0.687 
Loss G: 0.8319 (0.7630) Acc G: 10.952% 
LR: 2.000e-04 

2023-03-02 01:45:52,178 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.1083 (0.1943) Acc D Real: 89.772% 
Loss D Fake: 0.5731 (0.6295) Acc D Fake: 89.057% 
Loss D: 0.681 
Loss G: 0.8346 (0.7638) Acc G: 10.924% 
LR: 2.000e-04 

2023-03-02 01:45:52,186 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.1274 (0.1936) Acc D Real: 89.834% 
Loss D Fake: 0.5711 (0.6288) Acc D Fake: 89.085% 
Loss D: 0.698 
Loss G: 0.8373 (0.7646) Acc G: 10.896% 
LR: 2.000e-04 

2023-03-02 01:45:52,193 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.1504 (0.1931) Acc D Real: 89.880% 
Loss D Fake: 0.5692 (0.6282) Acc D Fake: 89.113% 
Loss D: 0.720 
Loss G: 0.8396 (0.7654) Acc G: 10.869% 
LR: 2.000e-04 

2023-03-02 01:45:52,200 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.1217 (0.1924) Acc D Real: 89.935% 
Loss D Fake: 0.5676 (0.6276) Acc D Fake: 89.140% 
Loss D: 0.689 
Loss G: 0.8419 (0.7662) Acc G: 10.842% 
LR: 2.000e-04 

2023-03-02 01:45:52,207 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.1805 (0.1922) Acc D Real: 89.929% 
Loss D Fake: 0.5658 (0.6269) Acc D Fake: 89.166% 
Loss D: 0.746 
Loss G: 0.8444 (0.7670) Acc G: 10.816% 
LR: 2.000e-04 

2023-03-02 01:45:52,215 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1700 (0.1920) Acc D Real: 89.945% 
Loss D Fake: 0.5640 (0.6263) Acc D Fake: 89.192% 
Loss D: 0.734 
Loss G: 0.8468 (0.7678) Acc G: 10.790% 
LR: 2.000e-04 

2023-03-02 01:45:52,224 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2078 (0.1922) Acc D Real: 89.908% 
Loss D Fake: 0.5623 (0.6256) Acc D Fake: 89.217% 
Loss D: 0.770 
Loss G: 0.8492 (0.7686) Acc G: 10.765% 
LR: 2.000e-04 

2023-03-02 01:45:52,232 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.1589 (0.1918) Acc D Real: 89.922% 
Loss D Fake: 0.5606 (0.6250) Acc D Fake: 89.242% 
Loss D: 0.719 
Loss G: 0.8515 (0.7695) Acc G: 10.741% 
LR: 2.000e-04 

2023-03-02 01:45:52,239 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.1373 (0.1913) Acc D Real: 89.955% 
Loss D Fake: 0.5588 (0.6243) Acc D Fake: 89.283% 
Loss D: 0.696 
Loss G: 0.8541 (0.7703) Acc G: 10.700% 
LR: 2.000e-04 

2023-03-02 01:45:52,247 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2630 (0.1920) Acc D Real: 89.838% 
Loss D Fake: 0.5569 (0.6236) Acc D Fake: 89.323% 
Loss D: 0.820 
Loss G: 0.8566 (0.7712) Acc G: 10.660% 
LR: 2.000e-04 

2023-03-02 01:45:52,255 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.1614 (0.1917) Acc D Real: 89.854% 
Loss D Fake: 0.5552 (0.6230) Acc D Fake: 89.362% 
Loss D: 0.717 
Loss G: 0.8589 (0.7720) Acc G: 10.621% 
LR: 2.000e-04 

2023-03-02 01:45:52,262 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.1527 (0.1913) Acc D Real: 89.859% 
Loss D Fake: 0.5536 (0.6223) Acc D Fake: 89.401% 
Loss D: 0.706 
Loss G: 0.8612 (0.7729) Acc G: 10.566% 
LR: 2.000e-04 

2023-03-02 01:45:52,270 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.1528 (0.1910) Acc D Real: 89.874% 
Loss D Fake: 0.5519 (0.6216) Acc D Fake: 89.455% 
Loss D: 0.705 
Loss G: 0.8636 (0.7738) Acc G: 10.513% 
LR: 2.000e-04 

2023-03-02 01:45:52,277 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.1342 (0.1904) Acc D Real: 89.911% 
Loss D Fake: 0.5502 (0.6209) Acc D Fake: 89.507% 
Loss D: 0.684 
Loss G: 0.8660 (0.7747) Acc G: 10.460% 
LR: 2.000e-04 

2023-03-02 01:45:52,285 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.1816 (0.1903) Acc D Real: 89.886% 
Loss D Fake: 0.5485 (0.6202) Acc D Fake: 89.559% 
Loss D: 0.730 
Loss G: 0.8683 (0.7755) Acc G: 10.409% 
LR: 2.000e-04 

2023-03-02 01:45:52,293 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.0947 (0.1894) Acc D Real: 89.958% 
Loss D Fake: 0.5469 (0.6196) Acc D Fake: 89.610% 
Loss D: 0.642 
Loss G: 0.8707 (0.7764) Acc G: 10.358% 
LR: 2.000e-04 

2023-03-02 01:45:52,301 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.1471 (0.1890) Acc D Real: 89.964% 
Loss D Fake: 0.5451 (0.6189) Acc D Fake: 89.660% 
Loss D: 0.692 
Loss G: 0.8734 (0.7773) Acc G: 10.309% 
LR: 2.000e-04 

2023-03-02 01:45:52,308 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.1954 (0.1891) Acc D Real: 89.931% 
Loss D Fake: 0.5434 (0.6182) Acc D Fake: 89.709% 
Loss D: 0.739 
Loss G: 0.8755 (0.7782) Acc G: 10.260% 
LR: 2.000e-04 

2023-03-02 01:45:52,316 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.1317 (0.1886) Acc D Real: 89.954% 
Loss D Fake: 0.5420 (0.6175) Acc D Fake: 89.757% 
Loss D: 0.674 
Loss G: 0.8775 (0.7791) Acc G: 10.212% 
LR: 2.000e-04 

2023-03-02 01:45:52,324 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.1602 (0.1883) Acc D Real: 89.960% 
Loss D Fake: 0.5405 (0.6168) Acc D Fake: 89.804% 
Loss D: 0.701 
Loss G: 0.8797 (0.7800) Acc G: 10.165% 
LR: 2.000e-04 

2023-03-02 01:45:52,331 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.1297 (0.1878) Acc D Real: 89.989% 
Loss D Fake: 0.5390 (0.6161) Acc D Fake: 89.851% 
Loss D: 0.669 
Loss G: 0.8820 (0.7810) Acc G: 10.119% 
LR: 2.000e-04 

2023-03-02 01:45:52,339 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.1914 (0.1878) Acc D Real: 89.960% 
Loss D Fake: 0.5374 (0.6154) Acc D Fake: 89.896% 
Loss D: 0.729 
Loss G: 0.8844 (0.7819) Acc G: 10.074% 
LR: 2.000e-04 

2023-03-02 01:45:52,347 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.1741 (0.1877) Acc D Real: 89.950% 
Loss D Fake: 0.5358 (0.6147) Acc D Fake: 89.941% 
Loss D: 0.710 
Loss G: 0.8866 (0.7828) Acc G: 10.029% 
LR: 2.000e-04 

2023-03-02 01:45:52,354 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.1121 (0.1871) Acc D Real: 90.002% 
Loss D Fake: 0.5343 (0.6140) Acc D Fake: 89.985% 
Loss D: 0.646 
Loss G: 0.8891 (0.7837) Acc G: 9.986% 
LR: 2.000e-04 

2023-03-02 01:45:52,362 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.1179 (0.1865) Acc D Real: 90.035% 
Loss D Fake: 0.5325 (0.6133) Acc D Fake: 90.028% 
Loss D: 0.650 
Loss G: 0.8917 (0.7846) Acc G: 9.943% 
LR: 2.000e-04 

2023-03-02 01:45:52,370 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.1572 (0.1862) Acc D Real: 90.036% 
Loss D Fake: 0.5307 (0.6126) Acc D Fake: 90.071% 
Loss D: 0.688 
Loss G: 0.8943 (0.7856) Acc G: 9.900% 
LR: 2.000e-04 

2023-03-02 01:45:52,377 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.1894 (0.1862) Acc D Real: 90.004% 
Loss D Fake: 0.5291 (0.6119) Acc D Fake: 90.113% 
Loss D: 0.719 
Loss G: 0.8967 (0.7865) Acc G: 9.845% 
LR: 2.000e-04 

2023-03-02 01:45:52,385 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.1768 (0.1862) Acc D Real: 89.979% 
Loss D Fake: 0.5275 (0.6112) Acc D Fake: 90.168% 
Loss D: 0.704 
Loss G: 0.8990 (0.7875) Acc G: 9.790% 
LR: 2.000e-04 

2023-03-02 01:45:52,393 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.1380 (0.1858) Acc D Real: 89.997% 
Loss D Fake: 0.5260 (0.6105) Acc D Fake: 90.222% 
Loss D: 0.664 
Loss G: 0.9012 (0.7884) Acc G: 9.736% 
LR: 2.000e-04 

2023-03-02 01:45:52,401 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.1421 (0.1854) Acc D Real: 90.012% 
Loss D Fake: 0.5248 (0.6098) Acc D Fake: 90.275% 
Loss D: 0.667 
Loss G: 0.9023 (0.7894) Acc G: 9.683% 
LR: 2.000e-04 

2023-03-02 01:45:52,408 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.1356 (0.1850) Acc D Real: 90.018% 
Loss D Fake: 0.5243 (0.6091) Acc D Fake: 90.327% 
Loss D: 0.660 
Loss G: 0.9032 (0.7903) Acc G: 9.631% 
LR: 2.000e-04 

2023-03-02 01:45:52,416 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.1805 (0.1850) Acc D Real: 89.993% 
Loss D Fake: 0.5239 (0.6084) Acc D Fake: 90.379% 
Loss D: 0.704 
Loss G: 0.9036 (0.7912) Acc G: 9.580% 
LR: 2.000e-04 

2023-03-02 01:45:52,423 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.2231 (0.1853) Acc D Real: 89.931% 
Loss D Fake: 0.5237 (0.6077) Acc D Fake: 90.430% 
Loss D: 0.747 
Loss G: 0.9040 (0.7921) Acc G: 9.530% 
LR: 2.000e-04 

2023-03-02 01:45:52,431 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.1272 (0.1848) Acc D Real: 89.948% 
Loss D Fake: 0.5234 (0.6070) Acc D Fake: 90.480% 
Loss D: 0.651 
Loss G: 0.9051 (0.7930) Acc G: 9.480% 
LR: 2.000e-04 

2023-03-02 01:45:52,438 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.1342 (0.1844) Acc D Real: 89.962% 
Loss D Fake: 0.5223 (0.6063) Acc D Fake: 90.529% 
Loss D: 0.656 
Loss G: 0.9076 (0.7939) Acc G: 9.431% 
LR: 2.000e-04 

2023-03-02 01:45:52,445 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.1807 (0.1844) Acc D Real: 89.927% 
Loss D Fake: 0.5203 (0.6057) Acc D Fake: 90.577% 
Loss D: 0.701 
Loss G: 0.9110 (0.7949) Acc G: 9.383% 
LR: 2.000e-04 

2023-03-02 01:45:52,453 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.1771 (0.1843) Acc D Real: 89.908% 
Loss D Fake: 0.5179 (0.6050) Acc D Fake: 90.625% 
Loss D: 0.695 
Loss G: 0.9148 (0.7958) Acc G: 9.336% 
LR: 2.000e-04 

2023-03-02 01:45:52,460 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.1838 (0.1843) Acc D Real: 89.876% 
Loss D Fake: 0.5154 (0.6043) Acc D Fake: 90.671% 
Loss D: 0.699 
Loss G: 0.9190 (0.7967) Acc G: 9.289% 
LR: 2.000e-04 

2023-03-02 01:45:52,468 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.1976 (0.1844) Acc D Real: 89.825% 
Loss D Fake: 0.5128 (0.6036) Acc D Fake: 90.718% 
Loss D: 0.710 
Loss G: 0.9230 (0.7977) Acc G: 9.244% 
LR: 2.000e-04 

2023-03-02 01:45:52,475 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1441 (0.1841) Acc D Real: 89.822% 
Loss D Fake: 0.5103 (0.6029) Acc D Fake: 90.763% 
Loss D: 0.654 
Loss G: 0.9271 (0.7987) Acc G: 9.198% 
LR: 2.000e-04 

2023-03-02 01:45:52,483 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1752 (0.1840) Acc D Real: 89.804% 
Loss D Fake: 0.5078 (0.6021) Acc D Fake: 90.808% 
Loss D: 0.683 
Loss G: 0.9309 (0.7997) Acc G: 9.154% 
LR: 2.000e-04 

2023-03-02 01:45:52,490 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.1246 (0.1836) Acc D Real: 89.818% 
Loss D Fake: 0.5054 (0.6014) Acc D Fake: 90.852% 
Loss D: 0.630 
Loss G: 0.9350 (0.8007) Acc G: 9.110% 
LR: 2.000e-04 

2023-03-02 01:45:52,498 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.1724 (0.1835) Acc D Real: 89.808% 
Loss D Fake: 0.5029 (0.6007) Acc D Fake: 90.895% 
Loss D: 0.675 
Loss G: 0.9390 (0.8018) Acc G: 9.067% 
LR: 2.000e-04 

2023-03-02 01:45:52,506 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.1375 (0.1832) Acc D Real: 89.814% 
Loss D Fake: 0.5005 (0.5999) Acc D Fake: 90.938% 
Loss D: 0.638 
Loss G: 0.9428 (0.8028) Acc G: 9.025% 
LR: 2.000e-04 

2023-03-02 01:45:52,513 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.1208 (0.1827) Acc D Real: 89.835% 
Loss D Fake: 0.4982 (0.5992) Acc D Fake: 90.980% 
Loss D: 0.619 
Loss G: 0.9469 (0.8039) Acc G: 8.983% 
LR: 2.000e-04 

2023-03-02 01:45:52,520 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1365 (0.1824) Acc D Real: 89.839% 
Loss D Fake: 0.4958 (0.5984) Acc D Fake: 91.022% 
Loss D: 0.632 
Loss G: 0.9509 (0.8049) Acc G: 8.942% 
LR: 2.000e-04 

2023-03-02 01:45:52,528 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1747 (0.1823) Acc D Real: 89.816% 
Loss D Fake: 0.4935 (0.5977) Acc D Fake: 91.062% 
Loss D: 0.668 
Loss G: 0.9547 (0.8060) Acc G: 8.901% 
LR: 2.000e-04 

2023-03-02 01:45:52,535 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.1531 (0.1821) Acc D Real: 89.818% 
Loss D Fake: 0.4912 (0.5969) Acc D Fake: 91.103% 
Loss D: 0.644 
Loss G: 0.9588 (0.8071) Acc G: 8.861% 
LR: 2.000e-04 

2023-03-02 01:45:52,542 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2096 (0.1823) Acc D Real: 89.766% 
Loss D Fake: 0.4889 (0.5961) Acc D Fake: 91.142% 
Loss D: 0.699 
Loss G: 0.9623 (0.8082) Acc G: 8.821% 
LR: 2.000e-04 

2023-03-02 01:45:52,551 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.1296 (0.1819) Acc D Real: 89.774% 
Loss D Fake: 0.4870 (0.5954) Acc D Fake: 91.182% 
Loss D: 0.617 
Loss G: 0.9660 (0.8093) Acc G: 8.783% 
LR: 2.000e-04 

2023-03-02 01:45:52,559 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1403 (0.1816) Acc D Real: 89.783% 
Loss D Fake: 0.4847 (0.5946) Acc D Fake: 91.220% 
Loss D: 0.625 
Loss G: 0.9702 (0.8105) Acc G: 8.744% 
LR: 2.000e-04 

2023-03-02 01:45:52,568 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.1281 (0.1813) Acc D Real: 89.798% 
Loss D Fake: 0.4822 (0.5938) Acc D Fake: 91.258% 
Loss D: 0.610 
Loss G: 0.9750 (0.8116) Acc G: 8.706% 
LR: 2.000e-04 

2023-03-02 01:45:52,576 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1490 (0.1810) Acc D Real: 89.803% 
Loss D Fake: 0.4793 (0.5930) Acc D Fake: 91.296% 
Loss D: 0.628 
Loss G: 0.9805 (0.8128) Acc G: 8.657% 
LR: 2.000e-04 

2023-03-02 01:45:52,585 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.1474 (0.1808) Acc D Real: 89.812% 
Loss D Fake: 0.4760 (0.5922) Acc D Fake: 91.344% 
Loss D: 0.623 
Loss G: 0.9864 (0.8140) Acc G: 8.609% 
LR: 2.000e-04 

2023-03-02 01:45:52,593 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.0987 (0.1802) Acc D Real: 89.846% 
Loss D Fake: 0.4724 (0.5914) Acc D Fake: 91.404% 
Loss D: 0.571 
Loss G: 0.9931 (0.8152) Acc G: 8.550% 
LR: 2.000e-04 

2023-03-02 01:45:52,602 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1511 (0.1800) Acc D Real: 89.842% 
Loss D Fake: 0.4686 (0.5905) Acc D Fake: 91.462% 
Loss D: 0.620 
Loss G: 1.0002 (0.8165) Acc G: 8.492% 
LR: 2.000e-04 

2023-03-02 01:45:52,610 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.1319 (0.1797) Acc D Real: 89.846% 
Loss D Fake: 0.4646 (0.5897) Acc D Fake: 91.520% 
Loss D: 0.597 
Loss G: 1.0075 (0.8178) Acc G: 8.435% 
LR: 2.000e-04 

2023-03-02 01:45:52,618 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.1300 (0.1794) Acc D Real: 89.858% 
Loss D Fake: 0.4606 (0.5888) Acc D Fake: 91.577% 
Loss D: 0.591 
Loss G: 1.0152 (0.8191) Acc G: 8.378% 
LR: 2.000e-04 

2023-03-02 01:45:52,626 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.1123 (0.1789) Acc D Real: 89.884% 
Loss D Fake: 0.4563 (0.5879) Acc D Fake: 91.633% 
Loss D: 0.569 
Loss G: 1.0235 (0.8205) Acc G: 8.322% 
LR: 2.000e-04 

2023-03-02 01:45:52,634 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.1741 (0.1789) Acc D Real: 89.861% 
Loss D Fake: 0.4520 (0.5870) Acc D Fake: 91.688% 
Loss D: 0.626 
Loss G: 1.0314 (0.8219) Acc G: 8.267% 
LR: 2.000e-04 

2023-03-02 01:45:52,641 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.1232 (0.1785) Acc D Real: 89.876% 
Loss D Fake: 0.4479 (0.5861) Acc D Fake: 91.743% 
Loss D: 0.571 
Loss G: 1.0395 (0.8233) Acc G: 8.213% 
LR: 2.000e-04 

2023-03-02 01:45:52,648 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2039 (0.1787) Acc D Real: 89.837% 
Loss D Fake: 0.4438 (0.5852) Acc D Fake: 91.797% 
Loss D: 0.648 
Loss G: 1.0471 (0.8247) Acc G: 8.159% 
LR: 2.000e-04 

2023-03-02 01:45:52,655 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.1283 (0.1784) Acc D Real: 89.846% 
Loss D Fake: 0.4399 (0.5843) Acc D Fake: 91.850% 
Loss D: 0.568 
Loss G: 1.0550 (0.8262) Acc G: 8.106% 
LR: 2.000e-04 

2023-03-02 01:45:52,663 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.1543 (0.1782) Acc D Real: 89.837% 
Loss D Fake: 0.4359 (0.5833) Acc D Fake: 91.903% 
Loss D: 0.590 
Loss G: 1.0634 (0.8278) Acc G: 8.054% 
LR: 2.000e-04 

2023-03-02 01:45:52,670 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.1959 (0.1783) Acc D Real: 89.804% 
Loss D Fake: 0.4318 (0.5823) Acc D Fake: 91.955% 
Loss D: 0.628 
Loss G: 1.0716 (0.8293) Acc G: 8.002% 
LR: 2.000e-04 

2023-03-02 01:45:52,677 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.1635 (0.1782) Acc D Real: 89.785% 
Loss D Fake: 0.4279 (0.5813) Acc D Fake: 92.006% 
Loss D: 0.591 
Loss G: 1.0801 (0.8309) Acc G: 7.951% 
LR: 2.000e-04 

2023-03-02 01:45:52,684 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.1843 (0.1783) Acc D Real: 89.751% 
Loss D Fake: 0.4239 (0.5803) Acc D Fake: 92.057% 
Loss D: 0.608 
Loss G: 1.0884 (0.8326) Acc G: 7.901% 
LR: 2.000e-04 

2023-03-02 01:45:52,692 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.1832 (0.1783) Acc D Real: 89.726% 
Loss D Fake: 0.4201 (0.5793) Acc D Fake: 92.107% 
Loss D: 0.603 
Loss G: 1.0965 (0.8342) Acc G: 7.851% 
LR: 2.000e-04 

2023-03-02 01:45:52,699 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.1574 (0.1782) Acc D Real: 89.724% 
Loss D Fake: 0.4164 (0.5783) Acc D Fake: 92.156% 
Loss D: 0.574 
Loss G: 1.1048 (0.8359) Acc G: 7.802% 
LR: 2.000e-04 

2023-03-02 01:45:52,707 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.1020 (0.1777) Acc D Real: 89.753% 
Loss D Fake: 0.4125 (0.5773) Acc D Fake: 92.205% 
Loss D: 0.514 
Loss G: 1.1148 (0.8376) Acc G: 7.754% 
LR: 2.000e-04 

2023-03-02 01:45:52,714 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.1831 (0.1777) Acc D Real: 89.725% 
Loss D Fake: 0.4079 (0.5762) Acc D Fake: 92.253% 
Loss D: 0.591 
Loss G: 1.1253 (0.8394) Acc G: 7.706% 
LR: 2.000e-04 

2023-03-02 01:45:52,721 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.1388 (0.1775) Acc D Real: 89.723% 
Loss D Fake: 0.4029 (0.5752) Acc D Fake: 92.300% 
Loss D: 0.542 
Loss G: 1.1377 (0.8413) Acc G: 7.658% 
LR: 2.000e-04 

2023-03-02 01:45:52,728 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.0985 (0.1770) Acc D Real: 89.751% 
Loss D Fake: 0.3969 (0.5741) Acc D Fake: 92.347% 
Loss D: 0.495 
Loss G: 1.1520 (0.8431) Acc G: 7.612% 
LR: 2.000e-04 

2023-03-02 01:45:52,736 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1612 (0.1769) Acc D Real: 89.738% 
Loss D Fake: 0.3911 (0.5730) Acc D Fake: 92.394% 
Loss D: 0.552 
Loss G: 1.1628 (0.8451) Acc G: 7.566% 
LR: 2.000e-04 

2023-03-02 01:45:52,744 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.1881 (0.1770) Acc D Real: 89.707% 
Loss D Fake: 0.3873 (0.5719) Acc D Fake: 92.439% 
Loss D: 0.575 
Loss G: 1.1727 (0.8471) Acc G: 7.520% 
LR: 2.000e-04 

2023-03-02 01:45:52,751 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.1349 (0.1767) Acc D Real: 89.711% 
Loss D Fake: 0.3831 (0.5707) Acc D Fake: 92.485% 
Loss D: 0.518 
Loss G: 1.1844 (0.8491) Acc G: 7.475% 
LR: 2.000e-04 

2023-03-02 01:45:52,758 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.1557 (0.1766) Acc D Real: 89.704% 
Loss D Fake: 0.3784 (0.5696) Acc D Fake: 92.529% 
Loss D: 0.534 
Loss G: 1.1967 (0.8511) Acc G: 7.431% 
LR: 2.000e-04 

2023-03-02 01:45:52,766 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.1592 (0.1765) Acc D Real: 89.697% 
Loss D Fake: 0.3737 (0.5684) Acc D Fake: 92.574% 
Loss D: 0.533 
Loss G: 1.2093 (0.8533) Acc G: 7.387% 
LR: 2.000e-04 

2023-03-02 01:45:52,773 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.2331 (0.1768) Acc D Real: 89.648% 
Loss D Fake: 0.3692 (0.5673) Acc D Fake: 92.617% 
Loss D: 0.602 
Loss G: 1.2208 (0.8554) Acc G: 7.343% 
LR: 2.000e-04 

2023-03-02 01:45:52,780 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.1833 (0.1769) Acc D Real: 89.626% 
Loss D Fake: 0.3652 (0.5661) Acc D Fake: 92.661% 
Loss D: 0.549 
Loss G: 1.2316 (0.8576) Acc G: 7.300% 
LR: 2.000e-04 

2023-03-02 01:45:52,788 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.1616 (0.1768) Acc D Real: 89.615% 
Loss D Fake: 0.3614 (0.5649) Acc D Fake: 92.703% 
Loss D: 0.523 
Loss G: 1.2423 (0.8599) Acc G: 7.258% 
LR: 2.000e-04 

2023-03-02 01:45:52,796 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.1134 (0.1764) Acc D Real: 89.631% 
Loss D Fake: 0.3575 (0.5637) Acc D Fake: 92.745% 
Loss D: 0.471 
Loss G: 1.2542 (0.8621) Acc G: 7.216% 
LR: 2.000e-04 

2023-03-02 01:45:52,804 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1106 (0.1760) Acc D Real: 89.651% 
Loss D Fake: 0.3530 (0.5625) Acc D Fake: 92.787% 
Loss D: 0.464 
Loss G: 1.2690 (0.8645) Acc G: 7.174% 
LR: 2.000e-04 

2023-03-02 01:45:52,811 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.2694 (0.1766) Acc D Real: 89.597% 
Loss D Fake: 0.3481 (0.5613) Acc D Fake: 92.828% 
Loss D: 0.617 
Loss G: 1.2818 (0.8669) Acc G: 7.133% 
LR: 2.000e-04 

2023-03-02 01:45:52,819 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.1644 (0.1765) Acc D Real: 89.588% 
Loss D Fake: 0.3446 (0.5600) Acc D Fake: 92.869% 
Loss D: 0.509 
Loss G: 1.2918 (0.8693) Acc G: 7.093% 
LR: 2.000e-04 

2023-03-02 01:45:52,826 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.0973 (0.1761) Acc D Real: 89.613% 
Loss D Fake: 0.3415 (0.5588) Acc D Fake: 92.909% 
Loss D: 0.439 
Loss G: 1.3046 (0.8717) Acc G: 7.053% 
LR: 2.000e-04 

2023-03-02 01:45:52,833 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.1229 (0.1758) Acc D Real: 89.625% 
Loss D Fake: 0.3372 (0.5575) Acc D Fake: 92.949% 
Loss D: 0.460 
Loss G: 1.3205 (0.8743) Acc G: 7.013% 
LR: 2.000e-04 

2023-03-02 01:45:52,841 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.1385 (0.1755) Acc D Real: 89.628% 
Loss D Fake: 0.3322 (0.5563) Acc D Fake: 92.989% 
Loss D: 0.471 
Loss G: 1.3383 (0.8769) Acc G: 6.974% 
LR: 2.000e-04 

2023-03-02 01:45:52,848 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.1805 (0.1756) Acc D Real: 89.615% 
Loss D Fake: 0.3286 (0.5550) Acc D Fake: 93.027% 
Loss D: 0.509 
Loss G: 1.3369 (0.8794) Acc G: 6.935% 
LR: 2.000e-04 

2023-03-02 01:45:52,856 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.2109 (0.1758) Acc D Real: 89.589% 
Loss D Fake: 0.3324 (0.5538) Acc D Fake: 93.066% 
Loss D: 0.543 
Loss G: 1.3135 (0.8818) Acc G: 6.897% 
LR: 2.000e-04 

2023-03-02 01:45:52,863 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.1486 (0.1756) Acc D Real: 89.592% 
Loss D Fake: 0.3449 (0.5526) Acc D Fake: 93.104% 
Loss D: 0.494 
Loss G: 1.2770 (0.8840) Acc G: 6.859% 
LR: 2.000e-04 

2023-03-02 01:45:52,871 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.0941 (0.1752) Acc D Real: 89.623% 
Loss D Fake: 0.3607 (0.5516) Acc D Fake: 93.142% 
Loss D: 0.455 
Loss G: 1.3221 (0.8864) Acc G: 6.821% 
LR: 2.000e-04 

2023-03-02 01:45:52,878 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.1298 (0.1749) Acc D Real: 89.638% 
Loss D Fake: 0.3237 (0.5504) Acc D Fake: 93.179% 
Loss D: 0.453 
Loss G: 1.3865 (0.8891) Acc G: 6.784% 
LR: 2.000e-04 

2023-03-02 01:45:52,885 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.1263 (0.1747) Acc D Real: 89.649% 
Loss D Fake: 0.3091 (0.5490) Acc D Fake: 93.216% 
Loss D: 0.435 
Loss G: 1.4305 (0.8920) Acc G: 6.748% 
LR: 2.000e-04 

2023-03-02 01:45:52,893 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1726 (0.1747) Acc D Real: 89.638% 
Loss D Fake: 0.2987 (0.5477) Acc D Fake: 93.252% 
Loss D: 0.471 
Loss G: 1.4670 (0.8951) Acc G: 6.711% 
LR: 2.000e-04 

2023-03-02 01:45:52,900 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.1427 (0.1745) Acc D Real: 89.648% 
Loss D Fake: 0.2900 (0.5463) Acc D Fake: 93.288% 
Loss D: 0.433 
Loss G: 1.4998 (0.8983) Acc G: 6.676% 
LR: 2.000e-04 

2023-03-02 01:45:52,908 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.1349 (0.1743) Acc D Real: 89.650% 
Loss D Fake: 0.2826 (0.5449) Acc D Fake: 93.324% 
Loss D: 0.418 
Loss G: 1.5279 (0.9017) Acc G: 6.640% 
LR: 2.000e-04 

2023-03-02 01:45:52,915 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.1547 (0.1742) Acc D Real: 89.657% 
Loss D Fake: 0.2766 (0.5435) Acc D Fake: 93.360% 
Loss D: 0.431 
Loss G: 1.5515 (0.9051) Acc G: 6.605% 
LR: 2.000e-04 

2023-03-02 01:45:52,922 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.1444 (0.1740) Acc D Real: 89.663% 
Loss D Fake: 0.2716 (0.5421) Acc D Fake: 93.394% 
Loss D: 0.416 
Loss G: 1.5735 (0.9086) Acc G: 6.570% 
LR: 2.000e-04 

2023-03-02 01:45:52,930 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.1485 (0.1739) Acc D Real: 89.673% 
Loss D Fake: 0.2669 (0.5406) Acc D Fake: 93.429% 
Loss D: 0.415 
Loss G: 1.5943 (0.9122) Acc G: 6.536% 
LR: 2.000e-04 

2023-03-02 01:45:52,937 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.1487 (0.1738) Acc D Real: 89.673% 
Loss D Fake: 0.2633 (0.5392) Acc D Fake: 93.463% 
Loss D: 0.412 
Loss G: 1.6033 (0.9158) Acc G: 6.502% 
LR: 2.000e-04 

2023-03-02 01:45:52,945 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.1418 (0.1736) Acc D Real: 89.683% 
Loss D Fake: 0.2619 (0.5377) Acc D Fake: 93.497% 
Loss D: 0.404 
Loss G: 1.6100 (0.9194) Acc G: 6.468% 
LR: 2.000e-04 

2023-03-02 01:45:52,952 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2070 (0.1738) Acc D Real: 89.669% 
Loss D Fake: 0.2607 (0.5363) Acc D Fake: 93.531% 
Loss D: 0.468 
Loss G: 1.6128 (0.9230) Acc G: 6.435% 
LR: 2.000e-04 

2023-03-02 01:45:52,959 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.0927 (0.1733) Acc D Real: 89.694% 
Loss D Fake: 0.2605 (0.5349) Acc D Fake: 93.564% 
Loss D: 0.353 
Loss G: 1.6189 (0.9266) Acc G: 6.402% 
LR: 2.000e-04 

2023-03-02 01:45:52,967 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.0862 (0.1729) Acc D Real: 89.726% 
Loss D Fake: 0.2591 (0.5335) Acc D Fake: 93.597% 
Loss D: 0.345 
Loss G: 1.6308 (0.9302) Acc G: 6.369% 
LR: 2.000e-04 

2023-03-02 01:45:52,975 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.1120 (0.1726) Acc D Real: 89.745% 
Loss D Fake: 0.2567 (0.5321) Acc D Fake: 93.629% 
Loss D: 0.369 
Loss G: 1.6457 (0.9338) Acc G: 6.337% 
LR: 2.000e-04 

2023-03-02 01:45:52,983 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.1713 (0.1726) Acc D Real: 89.741% 
Loss D Fake: 0.2543 (0.5307) Acc D Fake: 93.661% 
Loss D: 0.426 
Loss G: 1.6570 (0.9375) Acc G: 6.305% 
LR: 2.000e-04 

2023-03-02 01:45:52,991 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.1939 (0.1727) Acc D Real: 89.727% 
Loss D Fake: 0.2530 (0.5293) Acc D Fake: 93.693% 
Loss D: 0.447 
Loss G: 1.6590 (0.9411) Acc G: 6.273% 
LR: 2.000e-04 

2023-03-02 01:45:52,998 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.1309 (0.1725) Acc D Real: 89.734% 
Loss D Fake: 0.2533 (0.5279) Acc D Fake: 93.725% 
Loss D: 0.384 
Loss G: 1.6559 (0.9447) Acc G: 6.242% 
LR: 2.000e-04 

2023-03-02 01:45:53,006 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2981 (0.1731) Acc D Real: 89.682% 
Loss D Fake: 0.2568 (0.5266) Acc D Fake: 93.756% 
Loss D: 0.555 
Loss G: 1.5938 (0.9479) Acc G: 6.211% 
LR: 2.000e-04 

2023-03-02 01:45:53,014 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1607 (0.1730) Acc D Real: 89.678% 
Loss D Fake: 0.3525 (0.5257) Acc D Fake: 93.762% 
Loss D: 0.513 
Loss G: 1.6486 (0.9514) Acc G: 6.180% 
LR: 2.000e-04 

2023-03-02 01:45:53,023 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.1120 (0.1727) Acc D Real: 89.698% 
Loss D Fake: 0.2454 (0.5243) Acc D Fake: 93.793% 
Loss D: 0.357 
Loss G: 1.7322 (0.9552) Acc G: 6.149% 
LR: 2.000e-04 

2023-03-02 01:45:53,031 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.1817 (0.1728) Acc D Real: 89.692% 
Loss D Fake: 0.2349 (0.5229) Acc D Fake: 93.823% 
Loss D: 0.417 
Loss G: 1.7795 (0.9592) Acc G: 6.119% 
LR: 2.000e-04 

2023-03-02 01:45:53,039 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2571 (0.1732) Acc D Real: 89.664% 
Loss D Fake: 0.2285 (0.5215) Acc D Fake: 93.853% 
Loss D: 0.486 
Loss G: 1.8112 (0.9634) Acc G: 6.089% 
LR: 2.000e-04 

2023-03-02 01:45:53,048 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.1245 (0.1730) Acc D Real: 89.685% 
Loss D Fake: 0.2241 (0.5200) Acc D Fake: 93.883% 
Loss D: 0.349 
Loss G: 1.8360 (0.9676) Acc G: 6.060% 
LR: 2.000e-04 

2023-03-02 01:45:53,056 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.1872 (0.1730) Acc D Real: 89.688% 
Loss D Fake: 0.2206 (0.5186) Acc D Fake: 93.913% 
Loss D: 0.408 
Loss G: 1.8555 (0.9719) Acc G: 6.031% 
LR: 2.000e-04 

2023-03-02 01:45:53,064 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2369 (0.1733) Acc D Real: 89.670% 
Loss D Fake: 0.2186 (0.5171) Acc D Fake: 93.942% 
Loss D: 0.456 
Loss G: 1.8634 (0.9762) Acc G: 6.002% 
LR: 2.000e-04 

2023-03-02 01:45:53,071 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1224 (0.1731) Acc D Real: 89.694% 
Loss D Fake: 0.2190 (0.5157) Acc D Fake: 93.971% 
Loss D: 0.341 
Loss G: 1.8672 (0.9805) Acc G: 5.973% 
LR: 2.000e-04 

2023-03-02 01:45:53,079 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2493 (0.1735) Acc D Real: 89.676% 
Loss D Fake: 0.2198 (0.5143) Acc D Fake: 94.000% 
Loss D: 0.469 
Loss G: 1.8670 (0.9847) Acc G: 5.944% 
LR: 2.000e-04 

2023-03-02 01:45:53,087 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.1758 (0.1735) Acc D Real: 89.667% 
Loss D Fake: 0.2218 (0.5129) Acc D Fake: 94.028% 
Loss D: 0.398 
Loss G: 1.8566 (0.9888) Acc G: 5.916% 
LR: 2.000e-04 

2023-03-02 01:45:53,094 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3428 (0.1743) Acc D Real: 89.625% 
Loss D Fake: 0.2255 (0.5116) Acc D Fake: 94.056% 
Loss D: 0.568 
Loss G: 1.8399 (0.9928) Acc G: 5.888% 
LR: 2.000e-04 

2023-03-02 01:45:53,102 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2997 (0.1749) Acc D Real: 89.585% 
Loss D Fake: 0.2304 (0.5102) Acc D Fake: 94.084% 
Loss D: 0.530 
Loss G: 1.8121 (0.9967) Acc G: 5.861% 
LR: 2.000e-04 

2023-03-02 01:45:53,109 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.1433 (0.1747) Acc D Real: 89.591% 
Loss D Fake: 0.2372 (0.5090) Acc D Fake: 94.112% 
Loss D: 0.380 
Loss G: 1.7840 (1.0004) Acc G: 5.833% 
LR: 2.000e-04 

2023-03-02 01:45:53,116 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.1441 (0.1746) Acc D Real: 89.602% 
Loss D Fake: 0.2433 (0.5077) Acc D Fake: 94.139% 
Loss D: 0.387 
Loss G: 1.7615 (1.0039) Acc G: 5.814% 
LR: 2.000e-04 

2023-03-02 01:45:53,124 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.0878 (0.1742) Acc D Real: 89.632% 
Loss D Fake: 0.2482 (0.5065) Acc D Fake: 94.159% 
Loss D: 0.336 
Loss G: 1.7461 (1.0073) Acc G: 5.795% 
LR: 2.000e-04 

2023-03-02 01:45:53,131 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.1480 (0.1740) Acc D Real: 89.641% 
Loss D Fake: 0.2519 (0.5054) Acc D Fake: 94.178% 
Loss D: 0.400 
Loss G: 1.7338 (1.0107) Acc G: 5.776% 
LR: 2.000e-04 

2023-03-02 01:45:53,139 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.1461 (0.1739) Acc D Real: 89.648% 
Loss D Fake: 0.2579 (0.5042) Acc D Fake: 94.197% 
Loss D: 0.404 
Loss G: 1.6922 (1.0138) Acc G: 5.765% 
LR: 2.000e-04 

2023-03-02 01:45:53,146 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.1072 (0.1736) Acc D Real: 89.663% 
Loss D Fake: 0.2703 (0.5031) Acc D Fake: 94.208% 
Loss D: 0.377 
Loss G: 1.6502 (1.0167) Acc G: 5.761% 
LR: 2.000e-04 

2023-03-02 01:45:53,153 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2930 (0.1742) Acc D Real: 89.635% 
Loss D Fake: 0.2818 (0.5021) Acc D Fake: 94.212% 
Loss D: 0.575 
Loss G: 1.6080 (1.0194) Acc G: 5.758% 
LR: 2.000e-04 

2023-03-02 01:45:53,161 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2457 (0.1745) Acc D Real: 89.617% 
Loss D Fake: 0.2942 (0.5012) Acc D Fake: 94.215% 
Loss D: 0.540 
Loss G: 1.5807 (1.0219) Acc G: 5.754% 
LR: 2.000e-04 

2023-03-02 01:45:53,169 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.1491 (0.1744) Acc D Real: 89.627% 
Loss D Fake: 0.2966 (0.5003) Acc D Fake: 94.219% 
Loss D: 0.446 
Loss G: 1.5758 (1.0244) Acc G: 5.751% 
LR: 2.000e-04 

2023-03-02 01:45:53,177 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.1267 (0.1741) Acc D Real: 89.641% 
Loss D Fake: 0.2914 (0.4993) Acc D Fake: 94.222% 
Loss D: 0.418 
Loss G: 1.5829 (1.0269) Acc G: 5.747% 
LR: 2.000e-04 

2023-03-02 01:45:53,185 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2384 (0.1744) Acc D Real: 89.625% 
Loss D Fake: 0.2853 (0.4984) Acc D Fake: 94.233% 
Loss D: 0.524 
Loss G: 1.5905 (1.0295) Acc G: 5.737% 
LR: 2.000e-04 

2023-03-02 01:45:53,192 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.1396 (0.1743) Acc D Real: 89.625% 
Loss D Fake: 0.2805 (0.4974) Acc D Fake: 94.244% 
Loss D: 0.420 
Loss G: 1.5979 (1.0320) Acc G: 5.726% 
LR: 2.000e-04 

2023-03-02 01:45:53,199 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.0850 (0.1739) Acc D Real: 89.630% 
Loss D Fake: 0.2763 (0.4964) Acc D Fake: 94.249% 
Loss D: 0.361 
Loss G: 1.6074 (1.0345) Acc G: 5.721% 
LR: 2.000e-04 

2023-03-02 01:45:53,212 -                train: [    INFO] - Best Loss 0.867 to 0.852
2023-03-02 01:45:53,212 -                train: [    INFO] - 
Epoch: 3/20
2023-03-02 01:45:53,383 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.0889 (0.0872) Acc D Real: 95.703% 
Loss D Fake: 0.2695 (0.2708) Acc D Fake: 99.167% 
Loss D: 0.358 
Loss G: 1.6196 (1.6192) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:53,392 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.1588 (0.1110) Acc D Real: 93.681% 
Loss D Fake: 0.2714 (0.2710) Acc D Fake: 99.444% 
Loss D: 0.430 
Loss G: 1.6158 (1.6181) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:53,400 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.1127 (0.1115) Acc D Real: 93.867% 
Loss D Fake: 0.2737 (0.2717) Acc D Fake: 99.583% 
Loss D: 0.386 
Loss G: 1.6130 (1.6168) Acc G: 0.417% 
LR: 2.000e-04 

2023-03-02 01:45:53,418 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.1442 (0.1180) Acc D Real: 93.594% 
Loss D Fake: 0.2760 (0.2725) Acc D Fake: 99.333% 
Loss D: 0.420 
Loss G: 1.6110 (1.6156) Acc G: 0.667% 
LR: 2.000e-04 

2023-03-02 01:45:53,425 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.0801 (0.1117) Acc D Real: 94.149% 
Loss D Fake: 0.2776 (0.2734) Acc D Fake: 99.167% 
Loss D: 0.358 
Loss G: 1.6149 (1.6155) Acc G: 0.833% 
LR: 2.000e-04 

2023-03-02 01:45:53,432 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.1286 (0.1141) Acc D Real: 93.951% 
Loss D Fake: 0.2772 (0.2739) Acc D Fake: 99.286% 
Loss D: 0.406 
Loss G: 1.6211 (1.6163) Acc G: 0.714% 
LR: 2.000e-04 

2023-03-02 01:45:53,439 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.1257 (0.1156) Acc D Real: 93.854% 
Loss D Fake: 0.2760 (0.2742) Acc D Fake: 99.375% 
Loss D: 0.402 
Loss G: 1.6293 (1.6179) Acc G: 0.625% 
LR: 2.000e-04 

2023-03-02 01:45:53,447 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.1278 (0.1169) Acc D Real: 93.628% 
Loss D Fake: 0.2733 (0.2741) Acc D Fake: 99.444% 
Loss D: 0.401 
Loss G: 1.6426 (1.6207) Acc G: 0.556% 
LR: 2.000e-04 

2023-03-02 01:45:53,454 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2126 (0.1265) Acc D Real: 92.979% 
Loss D Fake: 0.2692 (0.2736) Acc D Fake: 99.500% 
Loss D: 0.482 
Loss G: 1.6563 (1.6242) Acc G: 0.500% 
LR: 2.000e-04 

2023-03-02 01:45:53,461 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1564 (0.1292) Acc D Real: 92.794% 
Loss D Fake: 0.2651 (0.2728) Acc D Fake: 99.545% 
Loss D: 0.421 
Loss G: 1.6708 (1.6285) Acc G: 0.455% 
LR: 2.000e-04 

2023-03-02 01:45:53,468 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1372 (0.1299) Acc D Real: 92.595% 
Loss D Fake: 0.2607 (0.2718) Acc D Fake: 99.583% 
Loss D: 0.398 
Loss G: 1.6860 (1.6333) Acc G: 0.417% 
LR: 2.000e-04 

2023-03-02 01:45:53,475 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.1476 (0.1312) Acc D Real: 92.556% 
Loss D Fake: 0.2563 (0.2706) Acc D Fake: 99.615% 
Loss D: 0.404 
Loss G: 1.7008 (1.6385) Acc G: 0.385% 
LR: 2.000e-04 

2023-03-02 01:45:53,482 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.0708 (0.1269) Acc D Real: 92.920% 
Loss D Fake: 0.2521 (0.2693) Acc D Fake: 99.643% 
Loss D: 0.323 
Loss G: 1.7170 (1.6441) Acc G: 0.357% 
LR: 2.000e-04 

2023-03-02 01:45:53,489 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.0839 (0.1240) Acc D Real: 93.104% 
Loss D Fake: 0.2478 (0.2679) Acc D Fake: 99.667% 
Loss D: 0.332 
Loss G: 1.7341 (1.6501) Acc G: 0.333% 
LR: 2.000e-04 

2023-03-02 01:45:53,497 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2106 (0.1295) Acc D Real: 92.669% 
Loss D Fake: 0.2444 (0.2664) Acc D Fake: 99.688% 
Loss D: 0.455 
Loss G: 1.7362 (1.6555) Acc G: 0.312% 
LR: 2.000e-04 

2023-03-02 01:45:53,506 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.0871 (0.1270) Acc D Real: 92.822% 
Loss D Fake: 0.2432 (0.2650) Acc D Fake: 99.706% 
Loss D: 0.330 
Loss G: 1.7353 (1.6602) Acc G: 0.294% 
LR: 2.000e-04 

2023-03-02 01:45:53,513 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1672 (0.1292) Acc D Real: 92.523% 
Loss D Fake: 0.2421 (0.2638) Acc D Fake: 99.722% 
Loss D: 0.409 
Loss G: 1.7322 (1.6642) Acc G: 0.278% 
LR: 2.000e-04 

2023-03-02 01:45:53,521 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.1640 (0.1310) Acc D Real: 92.349% 
Loss D Fake: 0.2418 (0.2626) Acc D Fake: 99.737% 
Loss D: 0.406 
Loss G: 1.7277 (1.6675) Acc G: 0.263% 
LR: 2.000e-04 

2023-03-02 01:45:53,529 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1957 (0.1343) Acc D Real: 92.060% 
Loss D Fake: 0.2419 (0.2616) Acc D Fake: 99.750% 
Loss D: 0.438 
Loss G: 1.7185 (1.6701) Acc G: 0.250% 
LR: 2.000e-04 

2023-03-02 01:45:53,537 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.1797 (0.1364) Acc D Real: 91.801% 
Loss D Fake: 0.2430 (0.2607) Acc D Fake: 99.762% 
Loss D: 0.423 
Loss G: 1.7075 (1.6718) Acc G: 0.238% 
LR: 2.000e-04 

2023-03-02 01:45:53,545 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1660 (0.1378) Acc D Real: 91.662% 
Loss D Fake: 0.2443 (0.2599) Acc D Fake: 99.773% 
Loss D: 0.410 
Loss G: 1.6985 (1.6730) Acc G: 0.227% 
LR: 2.000e-04 

2023-03-02 01:45:53,553 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1226 (0.1371) Acc D Real: 91.617% 
Loss D Fake: 0.2450 (0.2593) Acc D Fake: 99.783% 
Loss D: 0.368 
Loss G: 1.6964 (1.6741) Acc G: 0.217% 
LR: 2.000e-04 

2023-03-02 01:45:53,561 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.0838 (0.1349) Acc D Real: 91.780% 
Loss D Fake: 0.2440 (0.2586) Acc D Fake: 99.792% 
Loss D: 0.328 
Loss G: 1.7080 (1.6755) Acc G: 0.208% 
LR: 2.000e-04 

2023-03-02 01:45:53,569 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1118 (0.1340) Acc D Real: 91.862% 
Loss D Fake: 0.2408 (0.2579) Acc D Fake: 99.800% 
Loss D: 0.353 
Loss G: 1.7276 (1.6776) Acc G: 0.200% 
LR: 2.000e-04 

2023-03-02 01:45:53,576 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.0896 (0.1323) Acc D Real: 91.967% 
Loss D Fake: 0.2371 (0.2571) Acc D Fake: 99.808% 
Loss D: 0.327 
Loss G: 1.7532 (1.6805) Acc G: 0.192% 
LR: 2.000e-04 

2023-03-02 01:45:53,584 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1296 (0.1322) Acc D Real: 91.916% 
Loss D Fake: 0.2328 (0.2562) Acc D Fake: 99.815% 
Loss D: 0.362 
Loss G: 1.7794 (1.6841) Acc G: 0.185% 
LR: 2.000e-04 

2023-03-02 01:45:53,592 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1405 (0.1325) Acc D Real: 91.931% 
Loss D Fake: 0.2287 (0.2552) Acc D Fake: 99.821% 
Loss D: 0.369 
Loss G: 1.8057 (1.6885) Acc G: 0.179% 
LR: 2.000e-04 

2023-03-02 01:45:53,599 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1872 (0.1344) Acc D Real: 91.778% 
Loss D Fake: 0.2254 (0.2542) Acc D Fake: 99.828% 
Loss D: 0.413 
Loss G: 1.8180 (1.6929) Acc G: 0.172% 
LR: 2.000e-04 

2023-03-02 01:45:53,607 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.0843 (0.1327) Acc D Real: 91.905% 
Loss D Fake: 0.2237 (0.2532) Acc D Fake: 99.833% 
Loss D: 0.308 
Loss G: 1.8304 (1.6975) Acc G: 0.167% 
LR: 2.000e-04 

2023-03-02 01:45:53,614 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2183 (0.1354) Acc D Real: 91.778% 
Loss D Fake: 0.2216 (0.2522) Acc D Fake: 99.839% 
Loss D: 0.440 
Loss G: 1.8430 (1.7022) Acc G: 0.161% 
LR: 2.000e-04 

2023-03-02 01:45:53,622 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1660 (0.1364) Acc D Real: 91.733% 
Loss D Fake: 0.2197 (0.2512) Acc D Fake: 99.844% 
Loss D: 0.386 
Loss G: 1.8540 (1.7070) Acc G: 0.156% 
LR: 2.000e-04 

2023-03-02 01:45:53,630 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.1650 (0.1373) Acc D Real: 91.649% 
Loss D Fake: 0.2184 (0.2502) Acc D Fake: 99.848% 
Loss D: 0.383 
Loss G: 1.8595 (1.7116) Acc G: 0.152% 
LR: 2.000e-04 

2023-03-02 01:45:53,638 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1170 (0.1367) Acc D Real: 91.650% 
Loss D Fake: 0.2174 (0.2492) Acc D Fake: 99.853% 
Loss D: 0.334 
Loss G: 1.8663 (1.7161) Acc G: 0.147% 
LR: 2.000e-04 

2023-03-02 01:45:53,645 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3007 (0.1414) Acc D Real: 91.399% 
Loss D Fake: 0.2165 (0.2483) Acc D Fake: 99.857% 
Loss D: 0.517 
Loss G: 1.8645 (1.7204) Acc G: 0.143% 
LR: 2.000e-04 

2023-03-02 01:45:53,653 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3486 (0.1471) Acc D Real: 91.082% 
Loss D Fake: 0.2178 (0.2474) Acc D Fake: 99.861% 
Loss D: 0.566 
Loss G: 1.8506 (1.7240) Acc G: 0.139% 
LR: 2.000e-04 

2023-03-02 01:45:53,660 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.1534 (0.1473) Acc D Real: 91.077% 
Loss D Fake: 0.2207 (0.2467) Acc D Fake: 99.865% 
Loss D: 0.374 
Loss G: 1.8396 (1.7271) Acc G: 0.135% 
LR: 2.000e-04 

2023-03-02 01:45:53,668 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1385 (0.1471) Acc D Real: 91.133% 
Loss D Fake: 0.2220 (0.2461) Acc D Fake: 99.868% 
Loss D: 0.361 
Loss G: 1.8440 (1.7302) Acc G: 0.132% 
LR: 2.000e-04 

2023-03-02 01:45:53,675 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.0818 (0.1454) Acc D Real: 91.239% 
Loss D Fake: 0.2206 (0.2454) Acc D Fake: 99.872% 
Loss D: 0.302 
Loss G: 1.8629 (1.7336) Acc G: 0.128% 
LR: 2.000e-04 

2023-03-02 01:45:53,683 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.0887 (0.1440) Acc D Real: 91.329% 
Loss D Fake: 0.2176 (0.2447) Acc D Fake: 99.875% 
Loss D: 0.306 
Loss G: 1.8892 (1.7375) Acc G: 0.125% 
LR: 2.000e-04 

2023-03-02 01:45:53,690 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1417 (0.1439) Acc D Real: 91.317% 
Loss D Fake: 0.2145 (0.2440) Acc D Fake: 99.878% 
Loss D: 0.356 
Loss G: 1.9081 (1.7416) Acc G: 0.122% 
LR: 2.000e-04 

2023-03-02 01:45:53,698 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1252 (0.1435) Acc D Real: 91.308% 
Loss D Fake: 0.2126 (0.2432) Acc D Fake: 99.881% 
Loss D: 0.338 
Loss G: 1.9244 (1.7460) Acc G: 0.119% 
LR: 2.000e-04 

2023-03-02 01:45:53,706 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2663 (0.1463) Acc D Real: 91.164% 
Loss D Fake: 0.2113 (0.2425) Acc D Fake: 99.884% 
Loss D: 0.478 
Loss G: 1.9278 (1.7502) Acc G: 0.116% 
LR: 2.000e-04 

2023-03-02 01:45:53,713 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.1550 (0.1465) Acc D Real: 91.177% 
Loss D Fake: 0.2115 (0.2418) Acc D Fake: 99.886% 
Loss D: 0.366 
Loss G: 1.9301 (1.7543) Acc G: 0.114% 
LR: 2.000e-04 

2023-03-02 01:45:53,720 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.1036 (0.1456) Acc D Real: 91.227% 
Loss D Fake: 0.2111 (0.2411) Acc D Fake: 99.889% 
Loss D: 0.315 
Loss G: 1.9379 (1.7584) Acc G: 0.111% 
LR: 2.000e-04 

2023-03-02 01:45:53,729 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.1086 (0.1448) Acc D Real: 91.278% 
Loss D Fake: 0.2099 (0.2404) Acc D Fake: 99.891% 
Loss D: 0.318 
Loss G: 1.9498 (1.7626) Acc G: 0.109% 
LR: 2.000e-04 

2023-03-02 01:45:53,736 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1439 (0.1447) Acc D Real: 91.307% 
Loss D Fake: 0.2082 (0.2397) Acc D Fake: 99.894% 
Loss D: 0.352 
Loss G: 1.9628 (1.7668) Acc G: 0.106% 
LR: 2.000e-04 

2023-03-02 01:45:53,744 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.2580 (0.1471) Acc D Real: 91.127% 
Loss D Fake: 0.2079 (0.2391) Acc D Fake: 99.896% 
Loss D: 0.466 
Loss G: 1.9284 (1.7702) Acc G: 0.104% 
LR: 2.000e-04 

2023-03-02 01:45:53,752 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.1891 (0.1480) Acc D Real: 91.083% 
Loss D Fake: 0.2172 (0.2386) Acc D Fake: 99.898% 
Loss D: 0.406 
Loss G: 1.8230 (1.7713) Acc G: 0.102% 
LR: 2.000e-04 

2023-03-02 01:45:53,759 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.0985 (0.1470) Acc D Real: 91.126% 
Loss D Fake: 0.4048 (0.2419) Acc D Fake: 99.767% 
Loss D: 0.503 
Loss G: 2.0837 (1.7775) Acc G: 0.100% 
LR: 2.000e-04 

2023-03-02 01:45:53,767 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.1984 (0.1480) Acc D Real: 91.103% 
Loss D Fake: 0.1855 (0.2408) Acc D Fake: 99.771% 
Loss D: 0.384 
Loss G: 2.1886 (1.7856) Acc G: 0.098% 
LR: 2.000e-04 

2023-03-02 01:45:53,774 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2822 (0.1506) Acc D Real: 90.997% 
Loss D Fake: 0.1809 (0.2397) Acc D Fake: 99.776% 
Loss D: 0.463 
Loss G: 2.2321 (1.7942) Acc G: 0.096% 
LR: 2.000e-04 

2023-03-02 01:45:53,781 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3350 (0.1540) Acc D Real: 90.867% 
Loss D Fake: 0.1831 (0.2386) Acc D Fake: 99.780% 
Loss D: 0.518 
Loss G: 2.2390 (1.8025) Acc G: 0.094% 
LR: 2.000e-04 

2023-03-02 01:45:53,789 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4614 (0.1597) Acc D Real: 90.592% 
Loss D Fake: 0.1927 (0.2378) Acc D Fake: 99.784% 
Loss D: 0.654 
Loss G: 2.1875 (1.8097) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:45:53,796 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2981 (0.1622) Acc D Real: 90.496% 
Loss D Fake: 3.3432 (0.2942) Acc D Fake: 98.487% 
Loss D: 3.641 
Loss G: 2.3204 (1.8190) Acc G: 0.091% 
LR: 2.000e-04 

2023-03-02 01:45:53,804 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4151 (0.1668) Acc D Real: 90.321% 
Loss D Fake: 0.1609 (0.2919) Acc D Fake: 98.514% 
Loss D: 0.576 
Loss G: 2.4129 (1.8296) Acc G: 0.089% 
LR: 2.000e-04 

2023-03-02 01:45:53,811 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4422 (0.1716) Acc D Real: 90.119% 
Loss D Fake: 0.1482 (0.2893) Acc D Fake: 98.540% 
Loss D: 0.590 
Loss G: 2.4583 (1.8406) Acc G: 0.088% 
LR: 2.000e-04 

2023-03-02 01:45:53,819 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.9359 (0.1848) Acc D Real: 89.599% 
Loss D Fake: 0.1414 (0.2868) Acc D Fake: 98.565% 
Loss D: 1.077 
Loss G: 2.4830 (1.8517) Acc G: 0.086% 
LR: 2.000e-04 

2023-03-02 01:45:53,826 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.9898 (0.1984) Acc D Real: 89.034% 
Loss D Fake: 0.1375 (0.2843) Acc D Fake: 98.589% 
Loss D: 1.127 
Loss G: 2.4935 (1.8626) Acc G: 0.085% 
LR: 2.000e-04 

2023-03-02 01:45:53,834 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.7314 (0.2073) Acc D Real: 88.676% 
Loss D Fake: 0.1354 (0.2818) Acc D Fake: 98.613% 
Loss D: 0.867 
Loss G: 2.4956 (1.8731) Acc G: 0.083% 
LR: 2.000e-04 

2023-03-02 01:45:53,841 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.6120 (0.2139) Acc D Real: 88.419% 
Loss D Fake: 0.1342 (0.2794) Acc D Fake: 98.636% 
Loss D: 0.746 
Loss G: 2.4927 (1.8833) Acc G: 0.082% 
LR: 2.000e-04 

2023-03-02 01:45:53,848 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.7527 (0.2226) Acc D Real: 88.070% 
Loss D Fake: 0.1337 (0.2770) Acc D Fake: 98.658% 
Loss D: 0.886 
Loss G: 2.4853 (1.8930) Acc G: 0.081% 
LR: 2.000e-04 

2023-03-02 01:45:53,856 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 1.0647 (0.2360) Acc D Real: 87.519% 
Loss D Fake: 0.1339 (0.2747) Acc D Fake: 98.679% 
Loss D: 1.199 
Loss G: 2.4718 (1.9022) Acc G: 0.079% 
LR: 2.000e-04 

2023-03-02 01:45:53,863 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.6377 (0.2423) Acc D Real: 87.268% 
Loss D Fake: 0.1347 (0.2725) Acc D Fake: 98.700% 
Loss D: 0.772 
Loss G: 2.4562 (1.9108) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:45:53,871 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.8115 (0.2510) Acc D Real: 86.913% 
Loss D Fake: 0.1358 (0.2704) Acc D Fake: 98.720% 
Loss D: 0.947 
Loss G: 2.4388 (1.9189) Acc G: 0.077% 
LR: 2.000e-04 

2023-03-02 01:45:53,878 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.7021 (0.2579) Acc D Real: 86.652% 
Loss D Fake: 0.1371 (0.2684) Acc D Fake: 98.739% 
Loss D: 0.839 
Loss G: 2.4206 (1.9265) Acc G: 0.076% 
LR: 2.000e-04 

2023-03-02 01:45:53,886 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.7748 (0.2656) Acc D Real: 86.339% 
Loss D Fake: 0.1385 (0.2665) Acc D Fake: 98.758% 
Loss D: 0.913 
Loss G: 2.4016 (1.9336) Acc G: 0.075% 
LR: 2.000e-04 

2023-03-02 01:45:53,893 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.8222 (0.2738) Acc D Real: 86.006% 
Loss D Fake: 0.1401 (0.2646) Acc D Fake: 98.776% 
Loss D: 0.962 
Loss G: 2.3817 (1.9402) Acc G: 0.074% 
LR: 2.000e-04 

2023-03-02 01:45:53,900 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.6072 (0.2786) Acc D Real: 85.822% 
Loss D Fake: 0.1418 (0.2628) Acc D Fake: 98.794% 
Loss D: 0.749 
Loss G: 2.3621 (1.9463) Acc G: 0.072% 
LR: 2.000e-04 

2023-03-02 01:45:53,908 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.8136 (0.2862) Acc D Real: 85.490% 
Loss D Fake: 0.1435 (0.2611) Acc D Fake: 98.811% 
Loss D: 0.957 
Loss G: 2.3419 (1.9520) Acc G: 0.071% 
LR: 2.000e-04 

2023-03-02 01:45:53,916 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.6204 (0.2909) Acc D Real: 85.294% 
Loss D Fake: 0.1454 (0.2595) Acc D Fake: 98.828% 
Loss D: 0.766 
Loss G: 2.3221 (1.9572) Acc G: 0.070% 
LR: 2.000e-04 

2023-03-02 01:45:53,923 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 1.0336 (0.3013) Acc D Real: 84.841% 
Loss D Fake: 0.1473 (0.2580) Acc D Fake: 98.844% 
Loss D: 1.181 
Loss G: 2.3008 (1.9620) Acc G: 0.069% 
LR: 2.000e-04 

2023-03-02 01:45:53,931 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.9244 (0.3098) Acc D Real: 84.473% 
Loss D Fake: 0.1495 (0.2565) Acc D Fake: 98.860% 
Loss D: 1.074 
Loss G: 2.2782 (1.9663) Acc G: 0.068% 
LR: 2.000e-04 

2023-03-02 01:45:53,938 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.7037 (0.3151) Acc D Real: 84.246% 
Loss D Fake: 0.1517 (0.2550) Acc D Fake: 98.875% 
Loss D: 0.855 
Loss G: 2.2561 (1.9702) Acc G: 0.068% 
LR: 2.000e-04 

2023-03-02 01:45:53,945 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.6100 (0.3190) Acc D Real: 84.062% 
Loss D Fake: 0.1540 (0.2537) Acc D Fake: 98.890% 
Loss D: 0.764 
Loss G: 2.2350 (1.9737) Acc G: 0.067% 
LR: 2.000e-04 

2023-03-02 01:45:53,953 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.6616 (0.3236) Acc D Real: 83.851% 
Loss D Fake: 0.1562 (0.2524) Acc D Fake: 98.905% 
Loss D: 0.818 
Loss G: 2.2149 (1.9769) Acc G: 0.066% 
LR: 2.000e-04 

2023-03-02 01:45:53,960 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.9993 (0.3323) Acc D Real: 83.435% 
Loss D Fake: 0.1584 (0.2512) Acc D Fake: 98.919% 
Loss D: 1.158 
Loss G: 2.1937 (1.9797) Acc G: 0.065% 
LR: 2.000e-04 

2023-03-02 01:45:53,968 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.6583 (0.3365) Acc D Real: 83.243% 
Loss D Fake: 0.1608 (0.2500) Acc D Fake: 98.933% 
Loss D: 0.819 
Loss G: 2.1729 (1.9822) Acc G: 0.064% 
LR: 2.000e-04 

2023-03-02 01:45:53,975 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.6327 (0.3403) Acc D Real: 83.058% 
Loss D Fake: 0.1632 (0.2489) Acc D Fake: 98.946% 
Loss D: 0.796 
Loss G: 2.1528 (1.9844) Acc G: 0.063% 
LR: 2.000e-04 

2023-03-02 01:45:53,982 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.9949 (0.3484) Acc D Real: 82.667% 
Loss D Fake: 0.1656 (0.2479) Acc D Fake: 98.960% 
Loss D: 1.161 
Loss G: 2.1318 (1.9862) Acc G: 0.062% 
LR: 2.000e-04 

2023-03-02 01:45:53,990 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.8930 (0.3552) Acc D Real: 82.349% 
Loss D Fake: 0.1682 (0.2469) Acc D Fake: 98.972% 
Loss D: 1.061 
Loss G: 2.1104 (1.9877) Acc G: 0.062% 
LR: 2.000e-04 

2023-03-02 01:45:53,997 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.9123 (0.3620) Acc D Real: 82.019% 
Loss D Fake: 0.1709 (0.2460) Acc D Fake: 98.985% 
Loss D: 1.083 
Loss G: 2.0889 (1.9890) Acc G: 0.061% 
LR: 2.000e-04 

2023-03-02 01:45:54,005 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.8842 (0.3683) Acc D Real: 81.702% 
Loss D Fake: 0.1737 (0.2451) Acc D Fake: 98.997% 
Loss D: 1.058 
Loss G: 2.0673 (1.9899) Acc G: 0.060% 
LR: 2.000e-04 

2023-03-02 01:45:54,012 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4962 (0.3698) Acc D Real: 81.621% 
Loss D Fake: 0.1764 (0.2443) Acc D Fake: 99.009% 
Loss D: 0.673 
Loss G: 2.0476 (1.9906) Acc G: 0.060% 
LR: 2.000e-04 

2023-03-02 01:45:54,020 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.7112 (0.3738) Acc D Real: 81.403% 
Loss D Fake: 0.1790 (0.2435) Acc D Fake: 99.021% 
Loss D: 0.890 
Loss G: 2.0283 (1.9911) Acc G: 0.059% 
LR: 2.000e-04 

2023-03-02 01:45:54,027 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.7671 (0.3784) Acc D Real: 81.154% 
Loss D Fake: 0.1825 (0.2428) Acc D Fake: 99.032% 
Loss D: 0.950 
Loss G: 1.9961 (1.9911) Acc G: 0.058% 
LR: 2.000e-04 

2023-03-02 01:45:54,034 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.4812 (0.3795) Acc D Real: 81.087% 
Loss D Fake: 0.1880 (0.2422) Acc D Fake: 99.043% 
Loss D: 0.669 
Loss G: 1.9639 (1.9908) Acc G: 0.057% 
LR: 2.000e-04 

2023-03-02 01:45:54,042 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.5898 (0.3819) Acc D Real: 80.953% 
Loss D Fake: 0.1930 (0.2416) Acc D Fake: 99.054% 
Loss D: 0.783 
Loss G: 1.9351 (1.9902) Acc G: 0.057% 
LR: 2.000e-04 

2023-03-02 01:45:54,049 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.7182 (0.3857) Acc D Real: 80.742% 
Loss D Fake: 0.1976 (0.2411) Acc D Fake: 99.065% 
Loss D: 0.916 
Loss G: 1.9090 (1.9893) Acc G: 0.056% 
LR: 2.000e-04 

2023-03-02 01:45:54,057 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.5585 (0.3876) Acc D Real: 80.627% 
Loss D Fake: 0.2019 (0.2407) Acc D Fake: 99.075% 
Loss D: 0.760 
Loss G: 1.8856 (1.9881) Acc G: 0.056% 
LR: 2.000e-04 

2023-03-02 01:45:54,064 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.6161 (0.3901) Acc D Real: 80.465% 
Loss D Fake: 0.2058 (0.2403) Acc D Fake: 99.085% 
Loss D: 0.822 
Loss G: 1.8641 (1.9867) Acc G: 0.055% 
LR: 2.000e-04 

2023-03-02 01:45:54,071 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4397 (0.3907) Acc D Real: 80.417% 
Loss D Fake: 0.2095 (0.2400) Acc D Fake: 99.095% 
Loss D: 0.649 
Loss G: 1.8452 (1.9852) Acc G: 0.054% 
LR: 2.000e-04 

2023-03-02 01:45:54,079 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.5404 (0.3923) Acc D Real: 80.314% 
Loss D Fake: 0.2128 (0.2397) Acc D Fake: 99.105% 
Loss D: 0.753 
Loss G: 1.8281 (1.9835) Acc G: 0.054% 
LR: 2.000e-04 

2023-03-02 01:45:54,086 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.5170 (0.3936) Acc D Real: 80.211% 
Loss D Fake: 0.2158 (0.2394) Acc D Fake: 99.097% 
Loss D: 0.733 
Loss G: 1.8121 (1.9817) Acc G: 0.071% 
LR: 2.000e-04 

2023-03-02 01:45:54,094 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.6399 (0.3962) Acc D Real: 80.043% 
Loss D Fake: 0.2189 (0.2392) Acc D Fake: 99.089% 
Loss D: 0.859 
Loss G: 1.7961 (1.9797) Acc G: 0.105% 
LR: 2.000e-04 

2023-03-02 01:45:54,101 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.5120 (0.3974) Acc D Real: 79.953% 
Loss D Fake: 0.2219 (0.2390) Acc D Fake: 99.064% 
Loss D: 0.734 
Loss G: 1.7812 (1.9777) Acc G: 0.139% 
LR: 2.000e-04 

2023-03-02 01:45:54,108 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.5029 (0.3985) Acc D Real: 79.876% 
Loss D Fake: 0.2247 (0.2389) Acc D Fake: 99.039% 
Loss D: 0.728 
Loss G: 1.7674 (1.9755) Acc G: 0.172% 
LR: 2.000e-04 

2023-03-02 01:45:54,116 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.5838 (0.4004) Acc D Real: 79.749% 
Loss D Fake: 0.2274 (0.2388) Acc D Fake: 99.015% 
Loss D: 0.811 
Loss G: 1.7540 (1.9732) Acc G: 0.204% 
LR: 2.000e-04 

2023-03-02 01:45:54,123 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3431 (0.3998) Acc D Real: 79.766% 
Loss D Fake: 0.2300 (0.2387) Acc D Fake: 98.991% 
Loss D: 0.573 
Loss G: 1.7421 (1.9709) Acc G: 0.236% 
LR: 2.000e-04 

2023-03-02 01:45:54,131 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.5557 (0.4014) Acc D Real: 79.665% 
Loss D Fake: 0.2324 (0.2386) Acc D Fake: 98.968% 
Loss D: 0.788 
Loss G: 1.7305 (1.9685) Acc G: 0.267% 
LR: 2.000e-04 

2023-03-02 01:45:54,138 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.6893 (0.4042) Acc D Real: 79.478% 
Loss D Fake: 0.2349 (0.2386) Acc D Fake: 98.945% 
Loss D: 0.924 
Loss G: 1.7177 (1.9660) Acc G: 0.297% 
LR: 2.000e-04 

2023-03-02 01:45:54,146 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4644 (0.4048) Acc D Real: 79.416% 
Loss D Fake: 0.2377 (0.2386) Acc D Fake: 98.923% 
Loss D: 0.702 
Loss G: 1.7053 (1.9635) Acc G: 0.327% 
LR: 2.000e-04 

2023-03-02 01:45:54,153 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3612 (0.4044) Acc D Real: 79.429% 
Loss D Fake: 0.2404 (0.2386) Acc D Fake: 98.901% 
Loss D: 0.602 
Loss G: 1.6941 (1.9608) Acc G: 0.356% 
LR: 2.000e-04 

2023-03-02 01:45:54,161 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2153 (0.4026) Acc D Real: 79.530% 
Loss D Fake: 0.2427 (0.2386) Acc D Fake: 98.879% 
Loss D: 0.458 
Loss G: 1.6848 (1.9582) Acc G: 0.385% 
LR: 2.000e-04 

2023-03-02 01:45:54,168 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2637 (0.4013) Acc D Real: 79.597% 
Loss D Fake: 0.2447 (0.2387) Acc D Fake: 98.858% 
Loss D: 0.508 
Loss G: 1.6771 (1.9555) Acc G: 0.413% 
LR: 2.000e-04 

2023-03-02 01:45:54,176 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3217 (0.4005) Acc D Real: 79.631% 
Loss D Fake: 0.2463 (0.2388) Acc D Fake: 98.837% 
Loss D: 0.568 
Loss G: 1.6703 (1.9528) Acc G: 0.456% 
LR: 2.000e-04 

2023-03-02 01:45:54,183 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4809 (0.4013) Acc D Real: 79.567% 
Loss D Fake: 0.2479 (0.2388) Acc D Fake: 98.802% 
Loss D: 0.729 
Loss G: 1.6632 (1.9501) Acc G: 0.498% 
LR: 2.000e-04 

2023-03-02 01:45:54,191 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2698 (0.4000) Acc D Real: 79.633% 
Loss D Fake: 0.2496 (0.2389) Acc D Fake: 98.766% 
Loss D: 0.519 
Loss G: 1.6566 (1.9474) Acc G: 0.540% 
LR: 2.000e-04 

2023-03-02 01:45:54,198 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.2685 (0.3988) Acc D Real: 79.689% 
Loss D Fake: 0.2511 (0.2391) Acc D Fake: 98.732% 
Loss D: 0.520 
Loss G: 1.6508 (1.9447) Acc G: 0.581% 
LR: 2.000e-04 

2023-03-02 01:45:54,205 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.2295 (0.3973) Acc D Real: 79.779% 
Loss D Fake: 0.2524 (0.2392) Acc D Fake: 98.698% 
Loss D: 0.482 
Loss G: 1.6460 (1.9420) Acc G: 0.621% 
LR: 2.000e-04 

2023-03-02 01:45:54,213 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.2597 (0.3961) Acc D Real: 79.847% 
Loss D Fake: 0.2535 (0.2393) Acc D Fake: 98.665% 
Loss D: 0.513 
Loss G: 1.6420 (1.9393) Acc G: 0.661% 
LR: 2.000e-04 

2023-03-02 01:45:54,220 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3066 (0.3953) Acc D Real: 79.891% 
Loss D Fake: 0.2544 (0.2394) Acc D Fake: 98.632% 
Loss D: 0.561 
Loss G: 1.6384 (1.9366) Acc G: 0.699% 
LR: 2.000e-04 

2023-03-02 01:45:54,228 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4402 (0.3957) Acc D Real: 79.835% 
Loss D Fake: 0.2553 (0.2396) Acc D Fake: 98.600% 
Loss D: 0.696 
Loss G: 1.6344 (1.9339) Acc G: 0.737% 
LR: 2.000e-04 

2023-03-02 01:45:54,235 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2474 (0.3944) Acc D Real: 79.910% 
Loss D Fake: 0.2563 (0.2397) Acc D Fake: 98.568% 
Loss D: 0.504 
Loss G: 1.6306 (1.9312) Acc G: 0.775% 
LR: 2.000e-04 

2023-03-02 01:45:54,243 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3051 (0.3936) Acc D Real: 79.942% 
Loss D Fake: 0.2571 (0.2399) Acc D Fake: 98.537% 
Loss D: 0.562 
Loss G: 1.6272 (1.9286) Acc G: 0.812% 
LR: 2.000e-04 

2023-03-02 01:45:54,250 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4576 (0.3941) Acc D Real: 79.889% 
Loss D Fake: 0.2580 (0.2400) Acc D Fake: 98.507% 
Loss D: 0.716 
Loss G: 1.6233 (1.9260) Acc G: 0.848% 
LR: 2.000e-04 

2023-03-02 01:45:54,257 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2456 (0.3929) Acc D Real: 79.956% 
Loss D Fake: 0.2589 (0.2402) Acc D Fake: 98.477% 
Loss D: 0.505 
Loss G: 1.6196 (1.9233) Acc G: 0.883% 
LR: 2.000e-04 

2023-03-02 01:45:54,265 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3991 (0.3929) Acc D Real: 79.943% 
Loss D Fake: 0.2598 (0.2404) Acc D Fake: 98.447% 
Loss D: 0.659 
Loss G: 1.6158 (1.9207) Acc G: 0.918% 
LR: 2.000e-04 

2023-03-02 01:45:54,272 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3654 (0.3927) Acc D Real: 79.948% 
Loss D Fake: 0.2608 (0.2405) Acc D Fake: 98.418% 
Loss D: 0.626 
Loss G: 1.6119 (1.9181) Acc G: 0.952% 
LR: 2.000e-04 

2023-03-02 01:45:54,280 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2873 (0.3918) Acc D Real: 79.993% 
Loss D Fake: 0.2618 (0.2407) Acc D Fake: 98.390% 
Loss D: 0.549 
Loss G: 1.6083 (1.9156) Acc G: 0.986% 
LR: 2.000e-04 

2023-03-02 01:45:54,287 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.1750 (0.3900) Acc D Real: 80.093% 
Loss D Fake: 0.2626 (0.2409) Acc D Fake: 98.362% 
Loss D: 0.438 
Loss G: 1.6057 (1.9130) Acc G: 1.019% 
LR: 2.000e-04 

2023-03-02 01:45:54,295 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4166 (0.3902) Acc D Real: 80.064% 
Loss D Fake: 0.2634 (0.2411) Acc D Fake: 98.334% 
Loss D: 0.680 
Loss G: 1.6014 (1.9104) Acc G: 1.052% 
LR: 2.000e-04 

2023-03-02 01:45:54,302 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3447 (0.3899) Acc D Real: 80.068% 
Loss D Fake: 0.2649 (0.2413) Acc D Fake: 98.307% 
Loss D: 0.610 
Loss G: 1.5959 (1.9079) Acc G: 1.084% 
LR: 2.000e-04 

2023-03-02 01:45:54,309 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4016 (0.3900) Acc D Real: 80.059% 
Loss D Fake: 0.2666 (0.2415) Acc D Fake: 98.280% 
Loss D: 0.668 
Loss G: 1.5901 (1.9053) Acc G: 1.116% 
LR: 2.000e-04 

2023-03-02 01:45:54,317 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.2401 (0.3888) Acc D Real: 80.124% 
Loss D Fake: 0.2684 (0.2417) Acc D Fake: 98.254% 
Loss D: 0.509 
Loss G: 1.5839 (1.9028) Acc G: 1.147% 
LR: 2.000e-04 

2023-03-02 01:45:54,324 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4720 (0.3894) Acc D Real: 80.064% 
Loss D Fake: 0.2704 (0.2419) Acc D Fake: 98.228% 
Loss D: 0.742 
Loss G: 1.5772 (1.9002) Acc G: 1.177% 
LR: 2.000e-04 

2023-03-02 01:45:54,332 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3408 (0.3890) Acc D Real: 80.073% 
Loss D Fake: 0.2725 (0.2422) Acc D Fake: 98.203% 
Loss D: 0.613 
Loss G: 1.5707 (1.8976) Acc G: 1.207% 
LR: 2.000e-04 

2023-03-02 01:45:54,339 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.2866 (0.3882) Acc D Real: 80.113% 
Loss D Fake: 0.2744 (0.2424) Acc D Fake: 98.178% 
Loss D: 0.561 
Loss G: 1.5649 (1.8950) Acc G: 1.250% 
LR: 2.000e-04 

2023-03-02 01:45:54,347 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.5245 (0.3893) Acc D Real: 80.036% 
Loss D Fake: 0.2761 (0.2427) Acc D Fake: 98.140% 
Loss D: 0.801 
Loss G: 1.5590 (1.8924) Acc G: 1.292% 
LR: 2.000e-04 

2023-03-02 01:45:54,354 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3840 (0.3892) Acc D Real: 80.026% 
Loss D Fake: 0.2778 (0.2429) Acc D Fake: 98.103% 
Loss D: 0.662 
Loss G: 1.5533 (1.8898) Acc G: 1.333% 
LR: 2.000e-04 

2023-03-02 01:45:54,362 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.1663 (0.3875) Acc D Real: 80.127% 
Loss D Fake: 0.2793 (0.2432) Acc D Fake: 98.067% 
Loss D: 0.446 
Loss G: 1.5488 (1.8872) Acc G: 1.374% 
LR: 2.000e-04 

2023-03-02 01:45:54,369 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3279 (0.3871) Acc D Real: 80.140% 
Loss D Fake: 0.2805 (0.2435) Acc D Fake: 98.031% 
Loss D: 0.608 
Loss G: 1.5444 (1.8846) Acc G: 1.414% 
LR: 2.000e-04 

2023-03-02 01:45:54,377 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4154 (0.3873) Acc D Real: 80.114% 
Loss D Fake: 0.2818 (0.2438) Acc D Fake: 97.996% 
Loss D: 0.697 
Loss G: 1.5399 (1.8820) Acc G: 1.454% 
LR: 2.000e-04 

2023-03-02 01:45:54,384 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2497 (0.3863) Acc D Real: 80.167% 
Loss D Fake: 0.2833 (0.2441) Acc D Fake: 97.961% 
Loss D: 0.533 
Loss G: 1.5342 (1.8794) Acc G: 1.493% 
LR: 2.000e-04 

2023-03-02 01:45:54,391 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3456 (0.3860) Acc D Real: 80.175% 
Loss D Fake: 0.2852 (0.2444) Acc D Fake: 97.927% 
Loss D: 0.631 
Loss G: 1.5286 (1.8768) Acc G: 1.531% 
LR: 2.000e-04 

2023-03-02 01:45:54,399 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3303 (0.3856) Acc D Real: 80.187% 
Loss D Fake: 0.2870 (0.2447) Acc D Fake: 97.893% 
Loss D: 0.617 
Loss G: 1.5235 (1.8742) Acc G: 1.569% 
LR: 2.000e-04 

2023-03-02 01:45:54,406 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3116 (0.3850) Acc D Real: 80.208% 
Loss D Fake: 0.2885 (0.2450) Acc D Fake: 97.860% 
Loss D: 0.600 
Loss G: 1.5188 (1.8716) Acc G: 1.606% 
LR: 2.000e-04 

2023-03-02 01:45:54,413 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1738 (0.3835) Acc D Real: 80.292% 
Loss D Fake: 0.2906 (0.2454) Acc D Fake: 97.827% 
Loss D: 0.464 
Loss G: 1.5103 (1.8690) Acc G: 1.643% 
LR: 2.000e-04 

2023-03-02 01:45:54,421 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.2378 (0.3824) Acc D Real: 80.344% 
Loss D Fake: 0.2945 (0.2457) Acc D Fake: 97.795% 
Loss D: 0.532 
Loss G: 1.5011 (1.8663) Acc G: 1.679% 
LR: 2.000e-04 

2023-03-02 01:45:54,428 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3230 (0.3820) Acc D Real: 80.348% 
Loss D Fake: 0.2982 (0.2461) Acc D Fake: 97.763% 
Loss D: 0.621 
Loss G: 1.4920 (1.8637) Acc G: 1.714% 
LR: 2.000e-04 

2023-03-02 01:45:54,436 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.2035 (0.3808) Acc D Real: 80.418% 
Loss D Fake: 0.3017 (0.2465) Acc D Fake: 97.731% 
Loss D: 0.505 
Loss G: 1.4835 (1.8610) Acc G: 1.749% 
LR: 2.000e-04 

2023-03-02 01:45:54,443 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.2014 (0.3795) Acc D Real: 80.487% 
Loss D Fake: 0.3049 (0.2469) Acc D Fake: 97.700% 
Loss D: 0.506 
Loss G: 1.4767 (1.8582) Acc G: 1.784% 
LR: 2.000e-04 

2023-03-02 01:45:54,451 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2479 (0.3786) Acc D Real: 80.537% 
Loss D Fake: 0.3074 (0.2473) Acc D Fake: 97.670% 
Loss D: 0.555 
Loss G: 1.4715 (1.8555) Acc G: 1.818% 
LR: 2.000e-04 

2023-03-02 01:45:54,459 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3372 (0.3783) Acc D Real: 80.541% 
Loss D Fake: 0.3093 (0.2477) Acc D Fake: 97.640% 
Loss D: 0.646 
Loss G: 1.4679 (1.8529) Acc G: 1.852% 
LR: 2.000e-04 

2023-03-02 01:45:54,466 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2562 (0.3774) Acc D Real: 80.579% 
Loss D Fake: 0.3104 (0.2482) Acc D Fake: 97.610% 
Loss D: 0.567 
Loss G: 1.4660 (1.8502) Acc G: 1.885% 
LR: 2.000e-04 

2023-03-02 01:45:54,474 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.2536 (0.3766) Acc D Real: 80.615% 
Loss D Fake: 0.3107 (0.2486) Acc D Fake: 97.581% 
Loss D: 0.564 
Loss G: 1.4657 (1.8476) Acc G: 1.918% 
LR: 2.000e-04 

2023-03-02 01:45:54,481 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.2927 (0.3760) Acc D Real: 80.635% 
Loss D Fake: 0.3106 (0.2490) Acc D Fake: 97.552% 
Loss D: 0.603 
Loss G: 1.4658 (1.8450) Acc G: 1.950% 
LR: 2.000e-04 

2023-03-02 01:45:54,488 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.1781 (0.3747) Acc D Real: 80.711% 
Loss D Fake: 0.3101 (0.2494) Acc D Fake: 97.523% 
Loss D: 0.488 
Loss G: 1.4672 (1.8424) Acc G: 1.982% 
LR: 2.000e-04 

2023-03-02 01:45:54,495 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4012 (0.3749) Acc D Real: 80.692% 
Loss D Fake: 0.3092 (0.2498) Acc D Fake: 97.495% 
Loss D: 0.710 
Loss G: 1.4686 (1.8399) Acc G: 2.013% 
LR: 2.000e-04 

2023-03-02 01:45:54,503 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.2321 (0.3739) Acc D Real: 80.734% 
Loss D Fake: 0.3084 (0.2502) Acc D Fake: 97.467% 
Loss D: 0.541 
Loss G: 1.4703 (1.8374) Acc G: 2.044% 
LR: 2.000e-04 

2023-03-02 01:45:54,511 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.2040 (0.3728) Acc D Real: 80.794% 
Loss D Fake: 0.3074 (0.2506) Acc D Fake: 97.440% 
Loss D: 0.511 
Loss G: 1.4725 (1.8350) Acc G: 2.075% 
LR: 2.000e-04 

2023-03-02 01:45:54,518 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3849 (0.3729) Acc D Real: 80.779% 
Loss D Fake: 0.3066 (0.2510) Acc D Fake: 97.413% 
Loss D: 0.691 
Loss G: 1.4724 (1.8326) Acc G: 2.105% 
LR: 2.000e-04 

2023-03-02 01:45:54,526 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2997 (0.3724) Acc D Real: 80.793% 
Loss D Fake: 0.3077 (0.2513) Acc D Fake: 97.386% 
Loss D: 0.607 
Loss G: 1.4653 (1.8302) Acc G: 2.135% 
LR: 2.000e-04 

2023-03-02 01:45:54,533 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.2267 (0.3714) Acc D Real: 80.843% 
Loss D Fake: 0.3115 (0.2517) Acc D Fake: 97.349% 
Loss D: 0.538 
Loss G: 1.4571 (1.8278) Acc G: 2.175% 
LR: 2.000e-04 

2023-03-02 01:45:54,540 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3302 (0.3712) Acc D Real: 80.843% 
Loss D Fake: 0.3147 (0.2521) Acc D Fake: 97.312% 
Loss D: 0.645 
Loss G: 1.4510 (1.8254) Acc G: 2.215% 
LR: 2.000e-04 

2023-03-02 01:45:54,548 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.1999 (0.3701) Acc D Real: 80.898% 
Loss D Fake: 0.3170 (0.2526) Acc D Fake: 97.276% 
Loss D: 0.517 
Loss G: 1.4457 (1.8229) Acc G: 2.254% 
LR: 2.000e-04 

2023-03-02 01:45:54,555 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.2795 (0.3695) Acc D Real: 80.927% 
Loss D Fake: 0.3186 (0.2530) Acc D Fake: 97.241% 
Loss D: 0.598 
Loss G: 1.4429 (1.8205) Acc G: 2.293% 
LR: 2.000e-04 

2023-03-02 01:45:54,562 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3208 (0.3692) Acc D Real: 80.934% 
Loss D Fake: 0.3188 (0.2534) Acc D Fake: 97.205% 
Loss D: 0.640 
Loss G: 1.4424 (1.8181) Acc G: 2.331% 
LR: 2.000e-04 

2023-03-02 01:45:54,569 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.1941 (0.3681) Acc D Real: 80.999% 
Loss D Fake: 0.3181 (0.2538) Acc D Fake: 97.170% 
Loss D: 0.512 
Loss G: 1.4441 (1.8158) Acc G: 2.369% 
LR: 2.000e-04 

2023-03-02 01:45:54,577 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.1236 (0.3666) Acc D Real: 81.087% 
Loss D Fake: 0.3164 (0.2542) Acc D Fake: 97.136% 
Loss D: 0.440 
Loss G: 1.4477 (1.8135) Acc G: 2.406% 
LR: 2.000e-04 

2023-03-02 01:45:54,584 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.1548 (0.3653) Acc D Real: 81.166% 
Loss D Fake: 0.3144 (0.2546) Acc D Fake: 97.102% 
Loss D: 0.469 
Loss G: 1.4517 (1.8112) Acc G: 2.443% 
LR: 2.000e-04 

2023-03-02 01:45:54,591 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.1558 (0.3640) Acc D Real: 81.235% 
Loss D Fake: 0.3124 (0.2549) Acc D Fake: 97.069% 
Loss D: 0.468 
Loss G: 1.4559 (1.8090) Acc G: 2.479% 
LR: 2.000e-04 

2023-03-02 01:45:54,598 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.2178 (0.3631) Acc D Real: 81.270% 
Loss D Fake: 0.3106 (0.2553) Acc D Fake: 97.035% 
Loss D: 0.528 
Loss G: 1.4594 (1.8069) Acc G: 2.515% 
LR: 2.000e-04 

2023-03-02 01:45:54,606 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3031 (0.3627) Acc D Real: 81.291% 
Loss D Fake: 0.3090 (0.2556) Acc D Fake: 97.003% 
Loss D: 0.612 
Loss G: 1.4624 (1.8048) Acc G: 2.551% 
LR: 2.000e-04 

2023-03-02 01:45:54,613 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1973 (0.3617) Acc D Real: 81.340% 
Loss D Fake: 0.3076 (0.2559) Acc D Fake: 96.970% 
Loss D: 0.505 
Loss G: 1.4653 (1.8027) Acc G: 2.586% 
LR: 2.000e-04 

2023-03-02 01:45:54,620 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4102 (0.3620) Acc D Real: 81.303% 
Loss D Fake: 0.3063 (0.2562) Acc D Fake: 96.938% 
Loss D: 0.716 
Loss G: 1.4673 (1.8007) Acc G: 2.620% 
LR: 2.000e-04 

2023-03-02 01:45:54,628 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2320 (0.3612) Acc D Real: 81.338% 
Loss D Fake: 0.3053 (0.2565) Acc D Fake: 96.907% 
Loss D: 0.537 
Loss G: 1.4689 (1.7987) Acc G: 2.655% 
LR: 2.000e-04 

2023-03-02 01:45:54,636 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.2366 (0.3605) Acc D Real: 81.378% 
Loss D Fake: 0.3044 (0.2568) Acc D Fake: 96.876% 
Loss D: 0.541 
Loss G: 1.4704 (1.7968) Acc G: 2.688% 
LR: 2.000e-04 

2023-03-02 01:45:54,644 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.2888 (0.3600) Acc D Real: 81.396% 
Loss D Fake: 0.3036 (0.2571) Acc D Fake: 96.845% 
Loss D: 0.592 
Loss G: 1.4715 (1.7948) Acc G: 2.722% 
LR: 2.000e-04 

2023-03-02 01:45:54,651 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3073 (0.3597) Acc D Real: 81.403% 
Loss D Fake: 0.3031 (0.2573) Acc D Fake: 96.814% 
Loss D: 0.610 
Loss G: 1.4721 (1.7929) Acc G: 2.755% 
LR: 2.000e-04 

2023-03-02 01:45:54,658 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3781 (0.3598) Acc D Real: 81.379% 
Loss D Fake: 0.3033 (0.2576) Acc D Fake: 96.784% 
Loss D: 0.681 
Loss G: 1.4667 (1.7910) Acc G: 2.788% 
LR: 2.000e-04 

2023-03-02 01:45:54,666 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3371 (0.3597) Acc D Real: 81.379% 
Loss D Fake: 0.3052 (0.2579) Acc D Fake: 96.754% 
Loss D: 0.642 
Loss G: 1.4602 (1.7891) Acc G: 2.820% 
LR: 2.000e-04 

2023-03-02 01:45:54,673 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4482 (0.3602) Acc D Real: 81.327% 
Loss D Fake: 0.3070 (0.2582) Acc D Fake: 96.725% 
Loss D: 0.755 
Loss G: 1.4538 (1.7872) Acc G: 2.852% 
LR: 2.000e-04 

2023-03-02 01:45:54,680 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1913 (0.3592) Acc D Real: 81.382% 
Loss D Fake: 0.3087 (0.2585) Acc D Fake: 96.696% 
Loss D: 0.500 
Loss G: 1.4485 (1.7852) Acc G: 2.883% 
LR: 2.000e-04 

2023-03-02 01:45:54,687 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.2595 (0.3587) Acc D Real: 81.409% 
Loss D Fake: 0.3101 (0.2588) Acc D Fake: 96.667% 
Loss D: 0.570 
Loss G: 1.4431 (1.7833) Acc G: 2.914% 
LR: 2.000e-04 

2023-03-02 01:45:54,694 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.2499 (0.3581) Acc D Real: 81.431% 
Loss D Fake: 0.3117 (0.2591) Acc D Fake: 96.639% 
Loss D: 0.562 
Loss G: 1.4383 (1.7813) Acc G: 2.945% 
LR: 2.000e-04 

2023-03-02 01:45:54,702 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2919 (0.3577) Acc D Real: 81.443% 
Loss D Fake: 0.3129 (0.2594) Acc D Fake: 96.611% 
Loss D: 0.605 
Loss G: 1.4343 (1.7794) Acc G: 2.976% 
LR: 2.000e-04 

2023-03-02 01:45:54,710 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.1606 (0.3566) Acc D Real: 81.507% 
Loss D Fake: 0.3139 (0.2597) Acc D Fake: 96.583% 
Loss D: 0.474 
Loss G: 1.4316 (1.7774) Acc G: 3.006% 
LR: 2.000e-04 

2023-03-02 01:45:54,719 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.2146 (0.3558) Acc D Real: 81.546% 
Loss D Fake: 0.3144 (0.2600) Acc D Fake: 96.556% 
Loss D: 0.529 
Loss G: 1.4299 (1.7755) Acc G: 3.035% 
LR: 2.000e-04 

2023-03-02 01:45:54,727 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.1526 (0.3547) Acc D Real: 81.610% 
Loss D Fake: 0.3147 (0.2603) Acc D Fake: 96.528% 
Loss D: 0.467 
Loss G: 1.4291 (1.7735) Acc G: 3.065% 
LR: 2.000e-04 

2023-03-02 01:45:54,735 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.1267 (0.3534) Acc D Real: 81.690% 
Loss D Fake: 0.3147 (0.2606) Acc D Fake: 96.501% 
Loss D: 0.441 
Loss G: 1.4293 (1.7716) Acc G: 3.094% 
LR: 2.000e-04 

2023-03-02 01:45:54,744 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.2224 (0.3527) Acc D Real: 81.728% 
Loss D Fake: 0.3148 (0.2609) Acc D Fake: 96.475% 
Loss D: 0.537 
Loss G: 1.4272 (1.7697) Acc G: 3.123% 
LR: 2.000e-04 

2023-03-02 01:45:54,752 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.1587 (0.3516) Acc D Real: 81.784% 
Loss D Fake: 0.3157 (0.2612) Acc D Fake: 96.449% 
Loss D: 0.474 
Loss G: 1.4253 (1.7679) Acc G: 3.151% 
LR: 2.000e-04 

2023-03-02 01:45:54,760 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.1944 (0.3508) Acc D Real: 81.833% 
Loss D Fake: 0.3162 (0.2615) Acc D Fake: 96.423% 
Loss D: 0.511 
Loss G: 1.4244 (1.7660) Acc G: 3.179% 
LR: 2.000e-04 

2023-03-02 01:45:54,769 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2708 (0.3503) Acc D Real: 81.852% 
Loss D Fake: 0.3164 (0.2618) Acc D Fake: 96.397% 
Loss D: 0.587 
Loss G: 1.4239 (1.7641) Acc G: 3.207% 
LR: 2.000e-04 

2023-03-02 01:45:54,776 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.2455 (0.3498) Acc D Real: 81.885% 
Loss D Fake: 0.3165 (0.2621) Acc D Fake: 96.372% 
Loss D: 0.562 
Loss G: 1.4238 (1.7623) Acc G: 3.235% 
LR: 2.000e-04 

2023-03-02 01:45:54,783 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3598 (0.3498) Acc D Real: 81.870% 
Loss D Fake: 0.3164 (0.2624) Acc D Fake: 96.346% 
Loss D: 0.676 
Loss G: 1.4235 (1.7605) Acc G: 3.262% 
LR: 2.000e-04 

2023-03-02 01:45:54,790 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.2317 (0.3492) Acc D Real: 81.904% 
Loss D Fake: 0.3164 (0.2626) Acc D Fake: 96.321% 
Loss D: 0.548 
Loss G: 1.4232 (1.7587) Acc G: 3.289% 
LR: 2.000e-04 

2023-03-02 01:45:54,798 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.1578 (0.3482) Acc D Real: 81.963% 
Loss D Fake: 0.3163 (0.2629) Acc D Fake: 96.297% 
Loss D: 0.474 
Loss G: 1.4238 (1.7569) Acc G: 3.316% 
LR: 2.000e-04 

2023-03-02 01:45:54,805 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.2471 (0.3476) Acc D Real: 81.987% 
Loss D Fake: 0.3159 (0.2632) Acc D Fake: 96.272% 
Loss D: 0.563 
Loss G: 1.4246 (1.7552) Acc G: 3.342% 
LR: 2.000e-04 

2023-03-02 01:45:54,812 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.2335 (0.3471) Acc D Real: 82.014% 
Loss D Fake: 0.3155 (0.2635) Acc D Fake: 96.248% 
Loss D: 0.549 
Loss G: 1.4253 (1.7535) Acc G: 3.368% 
LR: 2.000e-04 

2023-03-02 01:45:54,819 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.2777 (0.3467) Acc D Real: 82.031% 
Loss D Fake: 0.3152 (0.2637) Acc D Fake: 96.225% 
Loss D: 0.593 
Loss G: 1.4257 (1.7517) Acc G: 3.394% 
LR: 2.000e-04 

2023-03-02 01:45:54,826 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.2178 (0.3460) Acc D Real: 82.066% 
Loss D Fake: 0.3151 (0.2640) Acc D Fake: 96.201% 
Loss D: 0.533 
Loss G: 1.4251 (1.7501) Acc G: 3.420% 
LR: 2.000e-04 

2023-03-02 01:45:54,833 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2971 (0.3458) Acc D Real: 82.076% 
Loss D Fake: 0.3154 (0.2643) Acc D Fake: 96.178% 
Loss D: 0.612 
Loss G: 1.4245 (1.7484) Acc G: 3.445% 
LR: 2.000e-04 

2023-03-02 01:45:54,840 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.1354 (0.3447) Acc D Real: 82.137% 
Loss D Fake: 0.3155 (0.2645) Acc D Fake: 96.154% 
Loss D: 0.451 
Loss G: 1.4245 (1.7467) Acc G: 3.470% 
LR: 2.000e-04 

2023-03-02 01:45:54,847 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.1559 (0.3437) Acc D Real: 82.190% 
Loss D Fake: 0.3154 (0.2648) Acc D Fake: 96.131% 
Loss D: 0.471 
Loss G: 1.4253 (1.7451) Acc G: 3.495% 
LR: 2.000e-04 

2023-03-02 01:45:54,854 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3784 (0.3439) Acc D Real: 82.175% 
Loss D Fake: 0.3151 (0.2651) Acc D Fake: 96.109% 
Loss D: 0.693 
Loss G: 1.4257 (1.7435) Acc G: 3.519% 
LR: 2.000e-04 

2023-03-02 01:45:54,861 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.1752 (0.3431) Acc D Real: 82.222% 
Loss D Fake: 0.3149 (0.2653) Acc D Fake: 96.086% 
Loss D: 0.490 
Loss G: 1.4263 (1.7419) Acc G: 3.544% 
LR: 2.000e-04 

2023-03-02 01:45:54,868 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2313 (0.3425) Acc D Real: 82.255% 
Loss D Fake: 0.3147 (0.2656) Acc D Fake: 96.064% 
Loss D: 0.546 
Loss G: 1.4268 (1.7403) Acc G: 3.568% 
LR: 2.000e-04 

2023-03-02 01:45:54,875 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2480 (0.3420) Acc D Real: 82.274% 
Loss D Fake: 0.3146 (0.2658) Acc D Fake: 96.042% 
Loss D: 0.563 
Loss G: 1.4264 (1.7387) Acc G: 3.592% 
LR: 2.000e-04 

2023-03-02 01:45:54,882 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2404 (0.3415) Acc D Real: 82.298% 
Loss D Fake: 0.3150 (0.2660) Acc D Fake: 96.020% 
Loss D: 0.555 
Loss G: 1.4260 (1.7371) Acc G: 3.615% 
LR: 2.000e-04 

2023-03-02 01:45:54,890 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1874 (0.3407) Acc D Real: 82.339% 
Loss D Fake: 0.3151 (0.2663) Acc D Fake: 95.999% 
Loss D: 0.502 
Loss G: 1.4261 (1.7356) Acc G: 3.639% 
LR: 2.000e-04 

2023-03-02 01:45:54,897 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4066 (0.3411) Acc D Real: 82.305% 
Loss D Fake: 0.3151 (0.2665) Acc D Fake: 95.978% 
Loss D: 0.722 
Loss G: 1.4252 (1.7341) Acc G: 3.662% 
LR: 2.000e-04 

2023-03-02 01:45:54,904 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.2414 (0.3406) Acc D Real: 82.328% 
Loss D Fake: 0.3155 (0.2668) Acc D Fake: 95.956% 
Loss D: 0.557 
Loss G: 1.4244 (1.7326) Acc G: 3.685% 
LR: 2.000e-04 

2023-03-02 01:45:54,911 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2579 (0.3402) Acc D Real: 82.344% 
Loss D Fake: 0.3157 (0.2670) Acc D Fake: 95.935% 
Loss D: 0.574 
Loss G: 1.4231 (1.7310) Acc G: 3.707% 
LR: 2.000e-04 

2023-03-02 01:45:54,918 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.2890 (0.3399) Acc D Real: 82.351% 
Loss D Fake: 0.3165 (0.2673) Acc D Fake: 95.915% 
Loss D: 0.606 
Loss G: 1.4189 (1.7295) Acc G: 3.730% 
LR: 2.000e-04 

2023-03-02 01:45:54,925 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.2368 (0.3394) Acc D Real: 82.374% 
Loss D Fake: 0.3183 (0.2675) Acc D Fake: 95.894% 
Loss D: 0.555 
Loss G: 1.4147 (1.7280) Acc G: 3.752% 
LR: 2.000e-04 

2023-03-02 01:45:54,932 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.0720 (0.3381) Acc D Real: 82.452% 
Loss D Fake: 0.3195 (0.2677) Acc D Fake: 95.874% 
Loss D: 0.392 
Loss G: 1.4125 (1.7265) Acc G: 3.774% 
LR: 2.000e-04 

2023-03-02 01:45:54,939 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1866 (0.3374) Acc D Real: 82.489% 
Loss D Fake: 0.3201 (0.2680) Acc D Fake: 95.854% 
Loss D: 0.507 
Loss G: 1.4114 (1.7250) Acc G: 3.796% 
LR: 2.000e-04 

2023-03-02 01:45:54,946 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2918 (0.3372) Acc D Real: 82.498% 
Loss D Fake: 0.3204 (0.2682) Acc D Fake: 95.834% 
Loss D: 0.612 
Loss G: 1.4103 (1.7235) Acc G: 3.817% 
LR: 2.000e-04 

2023-03-02 01:45:54,953 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2291 (0.3367) Acc D Real: 82.523% 
Loss D Fake: 0.3208 (0.2685) Acc D Fake: 95.814% 
Loss D: 0.550 
Loss G: 1.4092 (1.7220) Acc G: 3.839% 
LR: 2.000e-04 

2023-03-02 01:45:54,960 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.2797 (0.3364) Acc D Real: 82.529% 
Loss D Fake: 0.3212 (0.2687) Acc D Fake: 95.795% 
Loss D: 0.601 
Loss G: 1.4079 (1.7205) Acc G: 3.860% 
LR: 2.000e-04 

2023-03-02 01:45:54,967 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3145 (0.3363) Acc D Real: 82.532% 
Loss D Fake: 0.3218 (0.2690) Acc D Fake: 95.775% 
Loss D: 0.636 
Loss G: 1.4054 (1.7190) Acc G: 3.881% 
LR: 2.000e-04 

2023-03-02 01:45:54,974 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.2113 (0.3357) Acc D Real: 82.560% 
Loss D Fake: 0.3228 (0.2692) Acc D Fake: 95.756% 
Loss D: 0.534 
Loss G: 1.4034 (1.7176) Acc G: 3.902% 
LR: 2.000e-04 

2023-03-02 01:45:54,981 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.2429 (0.3353) Acc D Real: 82.579% 
Loss D Fake: 0.3233 (0.2695) Acc D Fake: 95.737% 
Loss D: 0.566 
Loss G: 1.4019 (1.7161) Acc G: 3.922% 
LR: 2.000e-04 

2023-03-02 01:45:54,988 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.0815 (0.3341) Acc D Real: 82.649% 
Loss D Fake: 0.3236 (0.2697) Acc D Fake: 95.718% 
Loss D: 0.405 
Loss G: 1.4017 (1.7146) Acc G: 3.943% 
LR: 2.000e-04 

2023-03-02 01:45:54,995 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2096 (0.3336) Acc D Real: 82.679% 
Loss D Fake: 0.3235 (0.2700) Acc D Fake: 95.699% 
Loss D: 0.533 
Loss G: 1.4018 (1.7132) Acc G: 3.963% 
LR: 2.000e-04 

2023-03-02 01:45:55,002 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2334 (0.3331) Acc D Real: 82.701% 
Loss D Fake: 0.3233 (0.2702) Acc D Fake: 95.681% 
Loss D: 0.557 
Loss G: 1.4022 (1.7118) Acc G: 3.983% 
LR: 2.000e-04 

2023-03-02 01:45:55,009 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.1536 (0.3323) Acc D Real: 82.747% 
Loss D Fake: 0.3230 (0.2705) Acc D Fake: 95.663% 
Loss D: 0.477 
Loss G: 1.4030 (1.7104) Acc G: 4.003% 
LR: 2.000e-04 

2023-03-02 01:45:55,016 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2327 (0.3318) Acc D Real: 82.770% 
Loss D Fake: 0.3228 (0.2707) Acc D Fake: 95.644% 
Loss D: 0.556 
Loss G: 1.4020 (1.7090) Acc G: 4.023% 
LR: 2.000e-04 

2023-03-02 01:45:55,023 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.1554 (0.3310) Acc D Real: 82.821% 
Loss D Fake: 0.3234 (0.2710) Acc D Fake: 95.626% 
Loss D: 0.479 
Loss G: 1.4014 (1.7076) Acc G: 4.042% 
LR: 2.000e-04 

2023-03-02 01:45:55,030 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.1566 (0.3302) Acc D Real: 82.870% 
Loss D Fake: 0.3236 (0.2712) Acc D Fake: 95.609% 
Loss D: 0.480 
Loss G: 1.4017 (1.7062) Acc G: 4.062% 
LR: 2.000e-04 

2023-03-02 01:45:55,037 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2044 (0.3297) Acc D Real: 82.897% 
Loss D Fake: 0.3237 (0.2714) Acc D Fake: 95.591% 
Loss D: 0.528 
Loss G: 1.4000 (1.7048) Acc G: 4.081% 
LR: 2.000e-04 

2023-03-02 01:45:55,044 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.1100 (0.3287) Acc D Real: 82.956% 
Loss D Fake: 0.3245 (0.2717) Acc D Fake: 95.566% 
Loss D: 0.434 
Loss G: 1.3989 (1.7035) Acc G: 4.107% 
LR: 2.000e-04 

2023-03-02 01:45:55,051 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.1802 (0.3280) Acc D Real: 82.997% 
Loss D Fake: 0.3247 (0.2719) Acc D Fake: 95.541% 
Loss D: 0.505 
Loss G: 1.3990 (1.7021) Acc G: 4.133% 
LR: 2.000e-04 

2023-03-02 01:45:55,058 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2234 (0.3276) Acc D Real: 83.003% 
Loss D Fake: 0.3246 (0.2721) Acc D Fake: 95.535% 
Loss D: 0.548 
Loss G: 1.3996 (1.7008) Acc G: 4.140% 
LR: 2.000e-04 

2023-03-02 01:45:55,070 -                train: [    INFO] - 
Epoch: 4/20
2023-03-02 01:45:55,248 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.0865 (0.1472) Acc D Real: 93.255% 
Loss D Fake: 0.3236 (0.3239) Acc D Fake: 90.000% 
Loss D: 0.410 
Loss G: 1.4030 (1.4019) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,258 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3397 (0.2114) Acc D Real: 88.958% 
Loss D Fake: 0.3228 (0.3235) Acc D Fake: 90.000% 
Loss D: 0.663 
Loss G: 1.4038 (1.4025) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,266 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2208 (0.2137) Acc D Real: 88.763% 
Loss D Fake: 0.3227 (0.3233) Acc D Fake: 90.000% 
Loss D: 0.544 
Loss G: 1.4022 (1.4024) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,283 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2372 (0.2184) Acc D Real: 88.583% 
Loss D Fake: 0.3233 (0.3233) Acc D Fake: 90.000% 
Loss D: 0.560 
Loss G: 1.4003 (1.4020) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,290 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2119 (0.2173) Acc D Real: 88.750% 
Loss D Fake: 0.3237 (0.3234) Acc D Fake: 90.000% 
Loss D: 0.536 
Loss G: 1.3989 (1.4015) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,298 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.0954 (0.1999) Acc D Real: 89.918% 
Loss D Fake: 0.3239 (0.3235) Acc D Fake: 90.000% 
Loss D: 0.419 
Loss G: 1.3985 (1.4011) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,305 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3112 (0.2138) Acc D Real: 88.939% 
Loss D Fake: 0.3239 (0.3235) Acc D Fake: 90.000% 
Loss D: 0.635 
Loss G: 1.3977 (1.4006) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,312 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2748 (0.2206) Acc D Real: 88.403% 
Loss D Fake: 0.3241 (0.3236) Acc D Fake: 90.000% 
Loss D: 0.599 
Loss G: 1.3968 (1.4002) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,319 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2965 (0.2282) Acc D Real: 87.885% 
Loss D Fake: 0.3244 (0.3237) Acc D Fake: 90.000% 
Loss D: 0.621 
Loss G: 1.3956 (1.3997) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,326 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1698 (0.2229) Acc D Real: 88.163% 
Loss D Fake: 0.3248 (0.3238) Acc D Fake: 90.000% 
Loss D: 0.495 
Loss G: 1.3940 (1.3992) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,333 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1971 (0.2207) Acc D Real: 88.312% 
Loss D Fake: 0.3252 (0.3239) Acc D Fake: 90.000% 
Loss D: 0.522 
Loss G: 1.3929 (1.3987) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,340 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3319 (0.2293) Acc D Real: 87.688% 
Loss D Fake: 0.3255 (0.3240) Acc D Fake: 90.000% 
Loss D: 0.657 
Loss G: 1.3919 (1.3982) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,347 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2006 (0.2272) Acc D Real: 87.891% 
Loss D Fake: 0.3257 (0.3241) Acc D Fake: 90.000% 
Loss D: 0.526 
Loss G: 1.3914 (1.3977) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,354 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2792 (0.2307) Acc D Real: 87.639% 
Loss D Fake: 0.3257 (0.3242) Acc D Fake: 90.000% 
Loss D: 0.605 
Loss G: 1.3906 (1.3972) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,361 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2127 (0.2296) Acc D Real: 87.757% 
Loss D Fake: 0.3259 (0.3243) Acc D Fake: 90.000% 
Loss D: 0.539 
Loss G: 1.3900 (1.3968) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,369 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.2130 (0.2286) Acc D Real: 87.791% 
Loss D Fake: 0.3260 (0.3244) Acc D Fake: 90.000% 
Loss D: 0.539 
Loss G: 1.3897 (1.3963) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,376 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2722 (0.2310) Acc D Real: 87.624% 
Loss D Fake: 0.3261 (0.3245) Acc D Fake: 90.000% 
Loss D: 0.598 
Loss G: 1.3893 (1.3960) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,383 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.2307 (0.2310) Acc D Real: 87.560% 
Loss D Fake: 0.3261 (0.3246) Acc D Fake: 90.000% 
Loss D: 0.557 
Loss G: 1.3885 (1.3956) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,390 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1947 (0.2292) Acc D Real: 87.677% 
Loss D Fake: 0.3262 (0.3247) Acc D Fake: 90.000% 
Loss D: 0.521 
Loss G: 1.3878 (1.3952) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,398 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.1521 (0.2255) Acc D Real: 87.966% 
Loss D Fake: 0.3263 (0.3248) Acc D Fake: 90.000% 
Loss D: 0.478 
Loss G: 1.3876 (1.3948) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,405 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1825 (0.2236) Acc D Real: 88.106% 
Loss D Fake: 0.3262 (0.3248) Acc D Fake: 90.000% 
Loss D: 0.509 
Loss G: 1.3877 (1.3945) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,413 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1421 (0.2200) Acc D Real: 88.361% 
Loss D Fake: 0.3261 (0.3249) Acc D Fake: 90.000% 
Loss D: 0.468 
Loss G: 1.3883 (1.3942) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,420 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.1082 (0.2154) Acc D Real: 88.704% 
Loss D Fake: 0.3257 (0.3249) Acc D Fake: 90.000% 
Loss D: 0.434 
Loss G: 1.3897 (1.3940) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,426 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1535 (0.2129) Acc D Real: 88.908% 
Loss D Fake: 0.3251 (0.3249) Acc D Fake: 90.000% 
Loss D: 0.479 
Loss G: 1.3916 (1.3939) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,433 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2174 (0.2131) Acc D Real: 88.900% 
Loss D Fake: 0.3244 (0.3249) Acc D Fake: 90.000% 
Loss D: 0.542 
Loss G: 1.3934 (1.3939) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,440 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1593 (0.2111) Acc D Real: 89.097% 
Loss D Fake: 0.3238 (0.3249) Acc D Fake: 90.000% 
Loss D: 0.483 
Loss G: 1.3954 (1.3940) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,446 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1127 (0.2076) Acc D Real: 89.327% 
Loss D Fake: 0.3232 (0.3248) Acc D Fake: 90.000% 
Loss D: 0.436 
Loss G: 1.3974 (1.3941) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,453 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1221 (0.2046) Acc D Real: 89.547% 
Loss D Fake: 0.3225 (0.3247) Acc D Fake: 90.000% 
Loss D: 0.445 
Loss G: 1.3998 (1.3943) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,460 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.1931 (0.2042) Acc D Real: 89.571% 
Loss D Fake: 0.3218 (0.3246) Acc D Fake: 90.000% 
Loss D: 0.515 
Loss G: 1.4019 (1.3945) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,468 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.1919 (0.2038) Acc D Real: 89.617% 
Loss D Fake: 0.3212 (0.3245) Acc D Fake: 90.000% 
Loss D: 0.513 
Loss G: 1.4037 (1.3948) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,475 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1760 (0.2030) Acc D Real: 89.657% 
Loss D Fake: 0.3207 (0.3244) Acc D Fake: 90.000% 
Loss D: 0.497 
Loss G: 1.4046 (1.3951) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,482 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2296 (0.2038) Acc D Real: 89.637% 
Loss D Fake: 0.3205 (0.3243) Acc D Fake: 90.000% 
Loss D: 0.550 
Loss G: 1.4050 (1.3954) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,489 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1270 (0.2015) Acc D Real: 89.786% 
Loss D Fake: 0.3203 (0.3242) Acc D Fake: 90.000% 
Loss D: 0.447 
Loss G: 1.4057 (1.3957) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,496 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1246 (0.1993) Acc D Real: 89.960% 
Loss D Fake: 0.3201 (0.3241) Acc D Fake: 90.000% 
Loss D: 0.445 
Loss G: 1.4067 (1.3961) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,504 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1553 (0.1981) Acc D Real: 90.067% 
Loss D Fake: 0.3199 (0.3239) Acc D Fake: 90.000% 
Loss D: 0.475 
Loss G: 1.4072 (1.3964) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,511 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2216 (0.1987) Acc D Real: 90.034% 
Loss D Fake: 0.3198 (0.3238) Acc D Fake: 90.000% 
Loss D: 0.541 
Loss G: 1.4076 (1.3967) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,518 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1813 (0.1983) Acc D Real: 90.073% 
Loss D Fake: 0.3198 (0.3237) Acc D Fake: 90.000% 
Loss D: 0.501 
Loss G: 1.4079 (1.3970) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,525 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1340 (0.1966) Acc D Real: 90.192% 
Loss D Fake: 0.3199 (0.3236) Acc D Fake: 90.000% 
Loss D: 0.454 
Loss G: 1.4074 (1.3972) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,533 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.1585 (0.1957) Acc D Real: 90.258% 
Loss D Fake: 0.3201 (0.3235) Acc D Fake: 90.000% 
Loss D: 0.479 
Loss G: 1.4073 (1.3975) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,541 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1387 (0.1943) Acc D Real: 90.360% 
Loss D Fake: 0.3200 (0.3235) Acc D Fake: 90.000% 
Loss D: 0.459 
Loss G: 1.4078 (1.3977) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,548 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1726 (0.1938) Acc D Real: 90.387% 
Loss D Fake: 0.3198 (0.3234) Acc D Fake: 90.000% 
Loss D: 0.492 
Loss G: 1.4086 (1.3980) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,556 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1875 (0.1936) Acc D Real: 90.405% 
Loss D Fake: 0.3196 (0.3233) Acc D Fake: 90.000% 
Loss D: 0.507 
Loss G: 1.4094 (1.3983) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,563 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.0988 (0.1915) Acc D Real: 90.575% 
Loss D Fake: 0.3194 (0.3232) Acc D Fake: 90.000% 
Loss D: 0.418 
Loss G: 1.4107 (1.3985) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,570 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.1905 (0.1914) Acc D Real: 90.579% 
Loss D Fake: 0.3190 (0.3231) Acc D Fake: 90.000% 
Loss D: 0.509 
Loss G: 1.4122 (1.3988) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,577 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.1286 (0.1901) Acc D Real: 90.670% 
Loss D Fake: 0.3185 (0.3230) Acc D Fake: 90.000% 
Loss D: 0.447 
Loss G: 1.4142 (1.3992) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,584 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.2548 (0.1914) Acc D Real: 90.582% 
Loss D Fake: 0.3180 (0.3229) Acc D Fake: 90.000% 
Loss D: 0.573 
Loss G: 1.4157 (1.3995) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,592 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.1298 (0.1902) Acc D Real: 90.696% 
Loss D Fake: 0.3176 (0.3228) Acc D Fake: 90.000% 
Loss D: 0.447 
Loss G: 1.4175 (1.3999) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,599 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2176 (0.1907) Acc D Real: 90.643% 
Loss D Fake: 0.3171 (0.3227) Acc D Fake: 90.000% 
Loss D: 0.535 
Loss G: 1.4190 (1.4003) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,607 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.0864 (0.1886) Acc D Real: 90.783% 
Loss D Fake: 0.3167 (0.3225) Acc D Fake: 90.000% 
Loss D: 0.403 
Loss G: 1.4206 (1.4007) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,614 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2449 (0.1897) Acc D Real: 90.699% 
Loss D Fake: 0.3164 (0.3224) Acc D Fake: 90.000% 
Loss D: 0.561 
Loss G: 1.4213 (1.4011) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,621 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.1586 (0.1891) Acc D Real: 90.738% 
Loss D Fake: 0.3162 (0.3223) Acc D Fake: 90.000% 
Loss D: 0.475 
Loss G: 1.4220 (1.4015) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,628 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2500 (0.1903) Acc D Real: 90.682% 
Loss D Fake: 0.3161 (0.3222) Acc D Fake: 90.000% 
Loss D: 0.566 
Loss G: 1.4225 (1.4019) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,636 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.1556 (0.1896) Acc D Real: 90.742% 
Loss D Fake: 0.3160 (0.3221) Acc D Fake: 90.000% 
Loss D: 0.472 
Loss G: 1.4230 (1.4023) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,643 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2097 (0.1900) Acc D Real: 90.726% 
Loss D Fake: 0.3160 (0.3220) Acc D Fake: 90.000% 
Loss D: 0.526 
Loss G: 1.4235 (1.4027) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,650 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.1930 (0.1901) Acc D Real: 90.712% 
Loss D Fake: 0.3169 (0.3219) Acc D Fake: 90.000% 
Loss D: 0.510 
Loss G: 1.4136 (1.4029) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,658 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.1530 (0.1894) Acc D Real: 90.763% 
Loss D Fake: 0.3206 (0.3218) Acc D Fake: 90.000% 
Loss D: 0.474 
Loss G: 1.4012 (1.4028) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,665 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2021 (0.1896) Acc D Real: 90.767% 
Loss D Fake: 0.3240 (0.3219) Acc D Fake: 90.000% 
Loss D: 0.526 
Loss G: 1.3902 (1.4026) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,672 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.1696 (0.1893) Acc D Real: 90.774% 
Loss D Fake: 0.3273 (0.3220) Acc D Fake: 90.000% 
Loss D: 0.497 
Loss G: 1.3802 (1.4022) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:55,681 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.2024 (0.1895) Acc D Real: 90.765% 
Loss D Fake: 0.3303 (0.3221) Acc D Fake: 89.972% 
Loss D: 0.533 
Loss G: 1.3714 (1.4017) Acc G: 10.028% 
LR: 2.000e-04 

2023-03-02 01:45:55,688 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.1137 (0.1883) Acc D Real: 90.850% 
Loss D Fake: 0.3334 (0.3223) Acc D Fake: 89.945% 
Loss D: 0.447 
Loss G: 1.3598 (1.4010) Acc G: 10.055% 
LR: 2.000e-04 

2023-03-02 01:45:55,696 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2752 (0.1897) Acc D Real: 90.764% 
Loss D Fake: 0.3375 (0.3225) Acc D Fake: 89.919% 
Loss D: 0.613 
Loss G: 1.3475 (1.4002) Acc G: 10.081% 
LR: 2.000e-04 

2023-03-02 01:45:55,703 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.1464 (0.1890) Acc D Real: 90.811% 
Loss D Fake: 0.3414 (0.3228) Acc D Fake: 89.894% 
Loss D: 0.488 
Loss G: 1.3371 (1.3992) Acc G: 10.106% 
LR: 2.000e-04 

2023-03-02 01:45:55,711 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.0850 (0.1874) Acc D Real: 90.916% 
Loss D Fake: 0.3446 (0.3232) Acc D Fake: 89.870% 
Loss D: 0.430 
Loss G: 1.3290 (1.3981) Acc G: 10.130% 
LR: 2.000e-04 

2023-03-02 01:45:55,718 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.1632 (0.1870) Acc D Real: 90.933% 
Loss D Fake: 0.3471 (0.3236) Acc D Fake: 89.846% 
Loss D: 0.510 
Loss G: 1.3224 (1.3969) Acc G: 10.154% 
LR: 2.000e-04 

2023-03-02 01:45:55,726 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.2217 (0.1875) Acc D Real: 90.905% 
Loss D Fake: 0.3498 (0.3239) Acc D Fake: 89.823% 
Loss D: 0.571 
Loss G: 1.3120 (1.3956) Acc G: 10.177% 
LR: 2.000e-04 

2023-03-02 01:45:55,734 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1133 (0.1864) Acc D Real: 90.979% 
Loss D Fake: 0.3538 (0.3244) Acc D Fake: 89.801% 
Loss D: 0.467 
Loss G: 1.3020 (1.3942) Acc G: 10.199% 
LR: 2.000e-04 

2023-03-02 01:45:55,742 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1604 (0.1860) Acc D Real: 91.008% 
Loss D Fake: 0.3571 (0.3249) Acc D Fake: 89.779% 
Loss D: 0.517 
Loss G: 1.2937 (1.3928) Acc G: 10.221% 
LR: 2.000e-04 

2023-03-02 01:45:55,749 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.0904 (0.1846) Acc D Real: 91.098% 
Loss D Fake: 0.3598 (0.3254) Acc D Fake: 89.758% 
Loss D: 0.450 
Loss G: 1.2873 (1.3912) Acc G: 10.242% 
LR: 2.000e-04 

2023-03-02 01:45:55,757 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1033 (0.1835) Acc D Real: 91.174% 
Loss D Fake: 0.3618 (0.3259) Acc D Fake: 89.738% 
Loss D: 0.465 
Loss G: 1.2830 (1.3897) Acc G: 10.262% 
LR: 2.000e-04 

2023-03-02 01:45:55,764 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.1058 (0.1824) Acc D Real: 91.262% 
Loss D Fake: 0.3631 (0.3264) Acc D Fake: 89.718% 
Loss D: 0.469 
Loss G: 1.2812 (1.3882) Acc G: 10.282% 
LR: 2.000e-04 

2023-03-02 01:45:55,771 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.0662 (0.1808) Acc D Real: 91.381% 
Loss D Fake: 0.3634 (0.3269) Acc D Fake: 89.699% 
Loss D: 0.430 
Loss G: 1.2821 (1.3867) Acc G: 10.301% 
LR: 2.000e-04 

2023-03-02 01:45:55,779 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.2112 (0.1812) Acc D Real: 91.351% 
Loss D Fake: 0.3634 (0.3274) Acc D Fake: 89.680% 
Loss D: 0.575 
Loss G: 1.2802 (1.3852) Acc G: 10.320% 
LR: 2.000e-04 

2023-03-02 01:45:55,786 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1789 (0.1812) Acc D Real: 91.345% 
Loss D Fake: 0.3645 (0.3279) Acc D Fake: 89.662% 
Loss D: 0.543 
Loss G: 1.2782 (1.3838) Acc G: 10.338% 
LR: 2.000e-04 

2023-03-02 01:45:55,793 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.1109 (0.1802) Acc D Real: 91.412% 
Loss D Fake: 0.3649 (0.3284) Acc D Fake: 89.644% 
Loss D: 0.476 
Loss G: 1.2787 (1.3824) Acc G: 10.356% 
LR: 2.000e-04 

2023-03-02 01:45:55,801 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.1345 (0.1796) Acc D Real: 91.454% 
Loss D Fake: 0.3644 (0.3289) Acc D Fake: 89.627% 
Loss D: 0.499 
Loss G: 1.2816 (1.3811) Acc G: 10.373% 
LR: 2.000e-04 

2023-03-02 01:45:55,808 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.0629 (0.1781) Acc D Real: 91.560% 
Loss D Fake: 0.3631 (0.3293) Acc D Fake: 89.610% 
Loss D: 0.426 
Loss G: 1.2876 (1.3798) Acc G: 10.390% 
LR: 2.000e-04 

2023-03-02 01:45:55,815 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1335 (0.1775) Acc D Real: 91.597% 
Loss D Fake: 0.3608 (0.3298) Acc D Fake: 89.594% 
Loss D: 0.494 
Loss G: 1.2951 (1.3787) Acc G: 10.406% 
LR: 2.000e-04 

2023-03-02 01:45:55,823 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.0671 (0.1761) Acc D Real: 91.698% 
Loss D Fake: 0.3581 (0.3301) Acc D Fake: 89.578% 
Loss D: 0.425 
Loss G: 1.3043 (1.3778) Acc G: 10.422% 
LR: 2.000e-04 

2023-03-02 01:45:55,830 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1272 (0.1755) Acc D Real: 91.742% 
Loss D Fake: 0.3556 (0.3304) Acc D Fake: 89.562% 
Loss D: 0.483 
Loss G: 1.3103 (1.3770) Acc G: 10.438% 
LR: 2.000e-04 

2023-03-02 01:45:55,837 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.0665 (0.1742) Acc D Real: 91.842% 
Loss D Fake: 0.3554 (0.3307) Acc D Fake: 89.527% 
Loss D: 0.422 
Loss G: 1.3151 (1.3762) Acc G: 10.473% 
LR: 2.000e-04 

2023-03-02 01:45:55,844 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.0744 (0.1730) Acc D Real: 91.924% 
Loss D Fake: 0.3553 (0.3310) Acc D Fake: 89.492% 
Loss D: 0.430 
Loss G: 1.3191 (1.3755) Acc G: 10.508% 
LR: 2.000e-04 

2023-03-02 01:45:55,852 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.0850 (0.1719) Acc D Real: 91.999% 
Loss D Fake: 0.3553 (0.3313) Acc D Fake: 89.458% 
Loss D: 0.440 
Loss G: 1.3230 (1.3749) Acc G: 10.542% 
LR: 2.000e-04 

2023-03-02 01:45:55,860 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.1146 (0.1712) Acc D Real: 92.052% 
Loss D Fake: 0.3551 (0.3316) Acc D Fake: 89.425% 
Loss D: 0.470 
Loss G: 1.3272 (1.3743) Acc G: 10.595% 
LR: 2.000e-04 

2023-03-02 01:45:55,868 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.0690 (0.1700) Acc D Real: 92.142% 
Loss D Fake: 0.3546 (0.3319) Acc D Fake: 89.373% 
Loss D: 0.424 
Loss G: 1.3323 (1.3738) Acc G: 10.647% 
LR: 2.000e-04 

2023-03-02 01:45:55,876 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1262 (0.1695) Acc D Real: 92.184% 
Loss D Fake: 0.3541 (0.3321) Acc D Fake: 89.322% 
Loss D: 0.480 
Loss G: 1.3354 (1.3734) Acc G: 10.698% 
LR: 2.000e-04 

2023-03-02 01:45:55,885 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.0701 (0.1684) Acc D Real: 92.268% 
Loss D Fake: 0.3544 (0.3324) Acc D Fake: 89.272% 
Loss D: 0.425 
Loss G: 1.3388 (1.3730) Acc G: 10.747% 
LR: 2.000e-04 

2023-03-02 01:45:55,893 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.0692 (0.1672) Acc D Real: 92.347% 
Loss D Fake: 0.3541 (0.3326) Acc D Fake: 89.223% 
Loss D: 0.423 
Loss G: 1.3434 (1.3726) Acc G: 10.795% 
LR: 2.000e-04 

2023-03-02 01:45:55,902 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.0578 (0.1660) Acc D Real: 92.433% 
Loss D Fake: 0.3531 (0.3329) Acc D Fake: 89.176% 
Loss D: 0.411 
Loss G: 1.3494 (1.3724) Acc G: 10.843% 
LR: 2.000e-04 

2023-03-02 01:45:55,910 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.1145 (0.1654) Acc D Real: 92.485% 
Loss D Fake: 0.3515 (0.3331) Acc D Fake: 89.130% 
Loss D: 0.466 
Loss G: 1.3562 (1.3722) Acc G: 10.889% 
LR: 2.000e-04 

2023-03-02 01:45:55,919 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.0789 (0.1645) Acc D Real: 92.548% 
Loss D Fake: 0.3496 (0.3333) Acc D Fake: 89.084% 
Loss D: 0.428 
Loss G: 1.3638 (1.3721) Acc G: 10.934% 
LR: 2.000e-04 

2023-03-02 01:45:55,928 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.0608 (0.1634) Acc D Real: 92.626% 
Loss D Fake: 0.3472 (0.3334) Acc D Fake: 89.040% 
Loss D: 0.408 
Loss G: 1.3723 (1.3721) Acc G: 10.978% 
LR: 2.000e-04 

2023-03-02 01:45:55,936 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.1052 (0.1627) Acc D Real: 92.674% 
Loss D Fake: 0.3449 (0.3335) Acc D Fake: 88.996% 
Loss D: 0.450 
Loss G: 1.3773 (1.3722) Acc G: 11.022% 
LR: 2.000e-04 

2023-03-02 01:45:55,945 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.1033 (0.1621) Acc D Real: 92.722% 
Loss D Fake: 0.3435 (0.3336) Acc D Fake: 88.954% 
Loss D: 0.447 
Loss G: 1.3816 (1.3723) Acc G: 11.064% 
LR: 2.000e-04 

2023-03-02 01:45:55,953 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.0702 (0.1611) Acc D Real: 92.789% 
Loss D Fake: 0.3417 (0.3337) Acc D Fake: 88.912% 
Loss D: 0.412 
Loss G: 1.3874 (1.3724) Acc G: 11.105% 
LR: 2.000e-04 

2023-03-02 01:45:55,962 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.1012 (0.1605) Acc D Real: 92.836% 
Loss D Fake: 0.3394 (0.3338) Acc D Fake: 88.872% 
Loss D: 0.441 
Loss G: 1.3939 (1.3726) Acc G: 11.146% 
LR: 2.000e-04 

2023-03-02 01:45:55,969 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1190 (0.1601) Acc D Real: 92.867% 
Loss D Fake: 0.3371 (0.3338) Acc D Fake: 88.832% 
Loss D: 0.456 
Loss G: 1.4005 (1.3729) Acc G: 11.186% 
LR: 2.000e-04 

2023-03-02 01:45:55,976 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.0821 (0.1593) Acc D Real: 92.934% 
Loss D Fake: 0.3348 (0.3338) Acc D Fake: 88.793% 
Loss D: 0.417 
Loss G: 1.4077 (1.3733) Acc G: 11.207% 
LR: 2.000e-04 

2023-03-02 01:45:55,984 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.0736 (0.1584) Acc D Real: 92.993% 
Loss D Fake: 0.3323 (0.3338) Acc D Fake: 88.771% 
Loss D: 0.406 
Loss G: 1.4155 (1.3737) Acc G: 11.229% 
LR: 2.000e-04 

2023-03-02 01:45:55,992 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.0824 (0.1577) Acc D Real: 93.048% 
Loss D Fake: 0.3297 (0.3338) Acc D Fake: 88.750% 
Loss D: 0.412 
Loss G: 1.4238 (1.3742) Acc G: 11.250% 
LR: 2.000e-04 

2023-03-02 01:45:55,999 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.1065 (0.1572) Acc D Real: 93.089% 
Loss D Fake: 0.3270 (0.3337) Acc D Fake: 88.729% 
Loss D: 0.434 
Loss G: 1.4321 (1.3748) Acc G: 11.271% 
LR: 2.000e-04 

2023-03-02 01:45:56,006 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.0968 (0.1566) Acc D Real: 93.135% 
Loss D Fake: 0.3249 (0.3336) Acc D Fake: 88.709% 
Loss D: 0.422 
Loss G: 1.4381 (1.3754) Acc G: 11.291% 
LR: 2.000e-04 

2023-03-02 01:45:56,014 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.0591 (0.1556) Acc D Real: 93.195% 
Loss D Fake: 0.3238 (0.3335) Acc D Fake: 88.689% 
Loss D: 0.383 
Loss G: 1.4441 (1.3761) Acc G: 11.311% 
LR: 2.000e-04 

2023-03-02 01:45:56,021 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.1369 (0.1554) Acc D Real: 93.202% 
Loss D Fake: 0.3231 (0.3334) Acc D Fake: 88.670% 
Loss D: 0.460 
Loss G: 1.4435 (1.3767) Acc G: 11.330% 
LR: 2.000e-04 

2023-03-02 01:45:56,029 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.0622 (0.1545) Acc D Real: 93.261% 
Loss D Fake: 0.3240 (0.3333) Acc D Fake: 88.651% 
Loss D: 0.386 
Loss G: 1.4424 (1.3773) Acc G: 11.349% 
LR: 2.000e-04 

2023-03-02 01:45:56,036 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.1210 (0.1542) Acc D Real: 93.295% 
Loss D Fake: 0.3243 (0.3333) Acc D Fake: 88.632% 
Loss D: 0.445 
Loss G: 1.4426 (1.3780) Acc G: 11.368% 
LR: 2.000e-04 

2023-03-02 01:45:56,044 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.0919 (0.1536) Acc D Real: 93.331% 
Loss D Fake: 0.3245 (0.3332) Acc D Fake: 88.614% 
Loss D: 0.416 
Loss G: 1.4419 (1.3786) Acc G: 11.386% 
LR: 2.000e-04 

2023-03-02 01:45:56,051 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.0607 (0.1528) Acc D Real: 93.387% 
Loss D Fake: 0.3249 (0.3331) Acc D Fake: 88.596% 
Loss D: 0.386 
Loss G: 1.4426 (1.3791) Acc G: 11.404% 
LR: 2.000e-04 

2023-03-02 01:45:56,059 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.0897 (0.1522) Acc D Real: 93.422% 
Loss D Fake: 0.3436 (0.3332) Acc D Fake: 88.578% 
Loss D: 0.433 
Loss G: 0.3303 (1.3695) Acc G: 12.003% 
LR: 2.000e-04 

2023-03-02 01:45:56,066 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.0933 (0.1517) Acc D Real: 93.457% 
Loss D Fake: 5.5149 (0.3803) Acc D Fake: 87.909% 
Loss D: 5.608 
Loss G: 0.2132 (1.3590) Acc G: 12.712% 
LR: 2.000e-04 

2023-03-02 01:45:56,074 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.0677 (0.1509) Acc D Real: 93.515% 
Loss D Fake: 5.9085 (0.4301) Acc D Fake: 87.177% 
Loss D: 5.976 
Loss G: 0.1788 (1.3484) Acc G: 13.483% 
LR: 2.000e-04 

2023-03-02 01:45:56,081 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.0523 (0.1500) Acc D Real: 93.573% 
Loss D Fake: 6.0087 (0.4799) Acc D Fake: 86.399% 
Loss D: 6.061 
Loss G: 0.1634 (1.3378) Acc G: 14.256% 
LR: 2.000e-04 

2023-03-02 01:45:56,089 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.1045 (0.1496) Acc D Real: 93.608% 
Loss D Fake: 5.9814 (0.5286) Acc D Fake: 85.634% 
Loss D: 6.086 
Loss G: 0.1561 (1.3273) Acc G: 15.015% 
LR: 2.000e-04 

2023-03-02 01:45:56,096 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.0567 (0.1488) Acc D Real: 93.664% 
Loss D Fake: 5.8786 (0.5755) Acc D Fake: 84.883% 
Loss D: 5.935 
Loss G: 0.1531 (1.3170) Acc G: 15.760% 
LR: 2.000e-04 

2023-03-02 01:45:56,104 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.0684 (0.1481) Acc D Real: 93.719% 
Loss D Fake: 5.7185 (0.6202) Acc D Fake: 84.145% 
Loss D: 5.787 
Loss G: 0.1530 (1.3069) Acc G: 16.493% 
LR: 2.000e-04 

2023-03-02 01:45:56,111 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.0645 (0.1474) Acc D Real: 93.773% 
Loss D Fake: 5.5063 (0.6624) Acc D Fake: 83.420% 
Loss D: 5.571 
Loss G: 0.1549 (1.2970) Acc G: 17.213% 
LR: 2.000e-04 

2023-03-02 01:45:56,119 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.0708 (0.1467) Acc D Real: 93.820% 
Loss D Fake: 5.2673 (0.7017) Acc D Fake: 82.707% 
Loss D: 5.338 
Loss G: 0.1583 (1.2873) Acc G: 17.920% 
LR: 2.000e-04 

2023-03-02 01:45:56,126 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.0727 (0.1461) Acc D Real: 93.873% 
Loss D Fake: 5.0379 (0.7385) Acc D Fake: 82.006% 
Loss D: 5.111 
Loss G: 0.1631 (1.2777) Acc G: 18.616% 
LR: 2.000e-04 

2023-03-02 01:45:56,134 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.0750 (0.1455) Acc D Real: 93.918% 
Loss D Fake: 4.8172 (0.7727) Acc D Fake: 81.317% 
Loss D: 4.892 
Loss G: 0.1692 (1.2684) Acc G: 19.300% 
LR: 2.000e-04 

2023-03-02 01:45:56,141 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.1190 (0.1453) Acc D Real: 93.945% 
Loss D Fake: 4.5982 (0.8046) Acc D Fake: 80.639% 
Loss D: 4.717 
Loss G: 0.1764 (1.2593) Acc G: 19.972% 
LR: 2.000e-04 

2023-03-02 01:45:56,150 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.0678 (0.1447) Acc D Real: 93.995% 
Loss D Fake: 4.3928 (0.8343) Acc D Fake: 79.972% 
Loss D: 4.461 
Loss G: 0.1855 (1.2504) Acc G: 20.634% 
LR: 2.000e-04 

2023-03-02 01:45:56,157 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.0784 (0.1441) Acc D Real: 94.045% 
Loss D Fake: 4.2008 (0.8619) Acc D Fake: 79.317% 
Loss D: 4.279 
Loss G: 0.1966 (1.2418) Acc G: 21.257% 
LR: 2.000e-04 

2023-03-02 01:45:56,165 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.0721 (0.1435) Acc D Real: 94.090% 
Loss D Fake: 4.0161 (0.8875) Acc D Fake: 78.713% 
Loss D: 4.088 
Loss G: 0.2103 (1.2334) Acc G: 21.843% 
LR: 2.000e-04 

2023-03-02 01:45:56,172 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.1100 (0.1433) Acc D Real: 94.118% 
Loss D Fake: 3.8280 (0.9112) Acc D Fake: 78.159% 
Loss D: 3.938 
Loss G: 0.2280 (1.2253) Acc G: 22.392% 
LR: 2.000e-04 

2023-03-02 01:45:56,180 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.0632 (0.1426) Acc D Real: 94.165% 
Loss D Fake: 3.6239 (0.9329) Acc D Fake: 77.617% 
Loss D: 3.687 
Loss G: 0.2526 (1.2175) Acc G: 22.906% 
LR: 2.000e-04 

2023-03-02 01:45:56,187 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.0686 (0.1420) Acc D Real: 94.211% 
Loss D Fake: 3.3831 (0.9524) Acc D Fake: 77.121% 
Loss D: 3.452 
Loss G: 0.2911 (1.2102) Acc G: 23.372% 
LR: 2.000e-04 

2023-03-02 01:45:56,195 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.0816 (0.1416) Acc D Real: 94.250% 
Loss D Fake: 3.0469 (0.9689) Acc D Fake: 76.663% 
Loss D: 3.129 
Loss G: 0.3730 (1.2036) Acc G: 23.792% 
LR: 2.000e-04 

2023-03-02 01:45:56,202 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.0814 (0.1411) Acc D Real: 94.295% 
Loss D Fake: 2.0483 (0.9773) Acc D Fake: 76.298% 
Loss D: 2.130 
Loss G: 0.1550 (1.1954) Acc G: 24.387% 
LR: 2.000e-04 

2023-03-02 01:45:56,209 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.0971 (0.1407) Acc D Real: 94.337% 
Loss D Fake: 3.3367 (0.9956) Acc D Fake: 75.707% 
Loss D: 3.434 
Loss G: 0.1233 (1.1871) Acc G: 24.973% 
LR: 2.000e-04 

2023-03-02 01:45:56,217 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.0910 (0.1404) Acc D Real: 94.381% 
Loss D Fake: 3.4060 (1.0141) Acc D Fake: 75.125% 
Loss D: 3.497 
Loss G: 0.1125 (1.1788) Acc G: 25.550% 
LR: 2.000e-04 

2023-03-02 01:45:56,224 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.0725 (0.1398) Acc D Real: 94.424% 
Loss D Fake: 3.4062 (1.0324) Acc D Fake: 74.551% 
Loss D: 3.479 
Loss G: 0.1075 (1.1706) Acc G: 26.119% 
LR: 2.000e-04 

2023-03-02 01:45:56,231 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.0774 (0.1394) Acc D Real: 94.466% 
Loss D Fake: 3.3755 (1.0501) Acc D Fake: 73.986% 
Loss D: 3.453 
Loss G: 0.1052 (1.1626) Acc G: 26.679% 
LR: 2.000e-04 

2023-03-02 01:45:56,239 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.0707 (0.1389) Acc D Real: 94.508% 
Loss D Fake: 3.3261 (1.0673) Acc D Fake: 73.430% 
Loss D: 3.397 
Loss G: 0.1045 (1.1546) Acc G: 27.230% 
LR: 2.000e-04 

2023-03-02 01:45:56,246 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.0711 (0.1383) Acc D Real: 94.549% 
Loss D Fake: 3.2627 (1.0836) Acc D Fake: 72.882% 
Loss D: 3.334 
Loss G: 0.1051 (1.1468) Acc G: 27.773% 
LR: 2.000e-04 

2023-03-02 01:45:56,254 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.0699 (0.1378) Acc D Real: 94.589% 
Loss D Fake: 3.1857 (1.0992) Acc D Fake: 72.342% 
Loss D: 3.256 
Loss G: 0.1068 (1.1391) Acc G: 28.308% 
LR: 2.000e-04 

2023-03-02 01:45:56,261 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.0790 (0.1374) Acc D Real: 94.628% 
Loss D Fake: 3.0962 (1.1139) Acc D Fake: 71.810% 
Loss D: 3.175 
Loss G: 0.1093 (1.1315) Acc G: 28.835% 
LR: 2.000e-04 

2023-03-02 01:45:56,269 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.0740 (0.1369) Acc D Real: 94.665% 
Loss D Fake: 3.0043 (1.1277) Acc D Fake: 71.298% 
Loss D: 3.078 
Loss G: 0.1122 (1.1241) Acc G: 29.342% 
LR: 2.000e-04 

2023-03-02 01:45:56,276 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.0921 (0.1366) Acc D Real: 94.698% 
Loss D Fake: 2.9201 (1.1407) Acc D Fake: 70.794% 
Loss D: 3.012 
Loss G: 0.1154 (1.1168) Acc G: 29.842% 
LR: 2.000e-04 

2023-03-02 01:45:56,283 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.0826 (0.1362) Acc D Real: 94.732% 
Loss D Fake: 2.8446 (1.1529) Acc D Fake: 70.296% 
Loss D: 2.927 
Loss G: 0.1186 (1.1096) Acc G: 30.335% 
LR: 2.000e-04 

2023-03-02 01:45:56,291 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.0915 (0.1359) Acc D Real: 94.764% 
Loss D Fake: 2.7759 (1.1645) Acc D Fake: 69.806% 
Loss D: 2.867 
Loss G: 0.1220 (1.1025) Acc G: 30.821% 
LR: 2.000e-04 

2023-03-02 01:45:56,298 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.0798 (0.1355) Acc D Real: 94.798% 
Loss D Fake: 2.7122 (1.1755) Acc D Fake: 69.323% 
Loss D: 2.792 
Loss G: 0.1254 (1.0956) Acc G: 31.299% 
LR: 2.000e-04 

2023-03-02 01:45:56,306 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.0901 (0.1352) Acc D Real: 94.829% 
Loss D Fake: 2.6521 (1.1859) Acc D Fake: 68.846% 
Loss D: 2.742 
Loss G: 0.1290 (1.0888) Acc G: 31.772% 
LR: 2.000e-04 

2023-03-02 01:45:56,313 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.0828 (0.1348) Acc D Real: 94.859% 
Loss D Fake: 2.5951 (1.1958) Acc D Fake: 68.377% 
Loss D: 2.678 
Loss G: 0.1326 (1.0821) Acc G: 32.237% 
LR: 2.000e-04 

2023-03-02 01:45:56,321 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.0841 (0.1345) Acc D Real: 94.889% 
Loss D Fake: 2.5405 (1.2051) Acc D Fake: 67.913% 
Loss D: 2.625 
Loss G: 0.1364 (1.0755) Acc G: 32.696% 
LR: 2.000e-04 

2023-03-02 01:45:56,328 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.0839 (0.1341) Acc D Real: 94.920% 
Loss D Fake: 2.4880 (1.2140) Acc D Fake: 67.457% 
Loss D: 2.572 
Loss G: 0.1403 (1.0691) Acc G: 33.149% 
LR: 2.000e-04 

2023-03-02 01:45:56,335 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.0951 (0.1339) Acc D Real: 94.948% 
Loss D Fake: 2.4373 (1.2223) Acc D Fake: 67.006% 
Loss D: 2.532 
Loss G: 0.1443 (1.0627) Acc G: 33.595% 
LR: 2.000e-04 

2023-03-02 01:45:56,343 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.0860 (0.1335) Acc D Real: 94.978% 
Loss D Fake: 2.3885 (1.2303) Acc D Fake: 66.561% 
Loss D: 2.474 
Loss G: 0.1484 (1.0565) Acc G: 34.036% 
LR: 2.000e-04 

2023-03-02 01:45:56,350 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.0854 (0.1332) Acc D Real: 95.005% 
Loss D Fake: 2.3412 (1.2378) Acc D Fake: 66.123% 
Loss D: 2.427 
Loss G: 0.1525 (1.0504) Acc G: 34.470% 
LR: 2.000e-04 

2023-03-02 01:45:56,358 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.0765 (0.1328) Acc D Real: 95.033% 
Loss D Fake: 2.2954 (1.2449) Acc D Fake: 65.690% 
Loss D: 2.372 
Loss G: 0.1568 (1.0444) Acc G: 34.899% 
LR: 2.000e-04 

2023-03-02 01:45:56,365 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.1007 (0.1326) Acc D Real: 95.058% 
Loss D Fake: 2.2508 (1.2516) Acc D Fake: 65.275% 
Loss D: 2.351 
Loss G: 0.1612 (1.0385) Acc G: 35.310% 
LR: 2.000e-04 

2023-03-02 01:45:56,372 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.1009 (0.1324) Acc D Real: 95.081% 
Loss D Fake: 2.2077 (1.2579) Acc D Fake: 64.864% 
Loss D: 2.309 
Loss G: 0.1657 (1.0327) Acc G: 35.717% 
LR: 2.000e-04 

2023-03-02 01:45:56,380 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.0949 (0.1322) Acc D Real: 95.106% 
Loss D Fake: 2.1659 (1.2639) Acc D Fake: 64.460% 
Loss D: 2.261 
Loss G: 0.1702 (1.0271) Acc G: 36.118% 
LR: 2.000e-04 

2023-03-02 01:45:56,387 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.0898 (0.1319) Acc D Real: 95.130% 
Loss D Fake: 2.1252 (1.2695) Acc D Fake: 64.060% 
Loss D: 2.215 
Loss G: 0.1749 (1.0215) Acc G: 36.513% 
LR: 2.000e-04 

2023-03-02 01:45:56,394 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.1114 (0.1317) Acc D Real: 95.153% 
Loss D Fake: 2.0857 (1.2748) Acc D Fake: 63.666% 
Loss D: 2.197 
Loss G: 0.1796 (1.0160) Acc G: 36.904% 
LR: 2.000e-04 

2023-03-02 01:45:56,402 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.0972 (0.1315) Acc D Real: 95.175% 
Loss D Fake: 2.0472 (1.2798) Acc D Fake: 63.277% 
Loss D: 2.144 
Loss G: 0.1845 (1.0107) Acc G: 37.290% 
LR: 2.000e-04 

2023-03-02 01:45:56,409 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.1055 (0.1314) Acc D Real: 95.196% 
Loss D Fake: 2.0098 (1.2845) Acc D Fake: 62.892% 
Loss D: 2.115 
Loss G: 0.1893 (1.0054) Acc G: 37.670% 
LR: 2.000e-04 

2023-03-02 01:45:56,416 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.1034 (0.1312) Acc D Real: 95.217% 
Loss D Fake: 1.9734 (1.2889) Acc D Fake: 62.513% 
Loss D: 2.077 
Loss G: 0.1943 (1.0002) Acc G: 38.046% 
LR: 2.000e-04 

2023-03-02 01:45:56,425 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.1061 (0.1310) Acc D Real: 95.237% 
Loss D Fake: 1.9380 (1.2930) Acc D Fake: 62.138% 
Loss D: 2.044 
Loss G: 0.1994 (0.9952) Acc G: 38.417% 
LR: 2.000e-04 

2023-03-02 01:45:56,433 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.1009 (0.1308) Acc D Real: 95.258% 
Loss D Fake: 1.9036 (1.2968) Acc D Fake: 61.779% 
Loss D: 2.005 
Loss G: 0.2045 (0.9902) Acc G: 38.773% 
LR: 2.000e-04 

2023-03-02 01:45:56,440 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.1059 (0.1307) Acc D Real: 95.277% 
Loss D Fake: 1.8700 (1.3004) Acc D Fake: 61.424% 
Loss D: 1.976 
Loss G: 0.2098 (0.9853) Acc G: 39.124% 
LR: 2.000e-04 

2023-03-02 01:45:56,448 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.1002 (0.1305) Acc D Real: 95.295% 
Loss D Fake: 1.8371 (1.3037) Acc D Fake: 61.074% 
Loss D: 1.937 
Loss G: 0.2151 (0.9805) Acc G: 39.471% 
LR: 2.000e-04 

2023-03-02 01:45:56,456 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.1022 (0.1303) Acc D Real: 95.313% 
Loss D Fake: 1.8050 (1.3068) Acc D Fake: 60.728% 
Loss D: 1.907 
Loss G: 0.2205 (0.9758) Acc G: 39.814% 
LR: 2.000e-04 

2023-03-02 01:45:56,464 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.1254 (0.1303) Acc D Real: 95.329% 
Loss D Fake: 1.7737 (1.3097) Acc D Fake: 60.386% 
Loss D: 1.899 
Loss G: 0.2260 (0.9712) Acc G: 40.153% 
LR: 2.000e-04 

2023-03-02 01:45:56,472 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.1171 (0.1302) Acc D Real: 95.345% 
Loss D Fake: 1.7434 (1.3123) Acc D Fake: 60.048% 
Loss D: 1.860 
Loss G: 0.2315 (0.9667) Acc G: 40.487% 
LR: 2.000e-04 

2023-03-02 01:45:56,479 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1128 (0.1301) Acc D Real: 95.360% 
Loss D Fake: 1.7138 (1.3148) Acc D Fake: 59.714% 
Loss D: 1.827 
Loss G: 0.2371 (0.9623) Acc G: 40.818% 
LR: 2.000e-04 

2023-03-02 01:45:56,487 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.1168 (0.1300) Acc D Real: 95.377% 
Loss D Fake: 1.6849 (1.3170) Acc D Fake: 59.385% 
Loss D: 1.802 
Loss G: 0.2428 (0.9580) Acc G: 41.144% 
LR: 2.000e-04 

2023-03-02 01:45:56,494 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.1075 (0.1299) Acc D Real: 95.394% 
Loss D Fake: 1.6568 (1.3190) Acc D Fake: 59.059% 
Loss D: 1.764 
Loss G: 0.2485 (0.9537) Acc G: 41.456% 
LR: 2.000e-04 

2023-03-02 01:45:56,502 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.0960 (0.1297) Acc D Real: 95.412% 
Loss D Fake: 1.6291 (1.3209) Acc D Fake: 58.747% 
Loss D: 1.725 
Loss G: 0.2544 (0.9496) Acc G: 41.765% 
LR: 2.000e-04 

2023-03-02 01:45:56,509 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.1185 (0.1296) Acc D Real: 95.426% 
Loss D Fake: 1.6022 (1.3225) Acc D Fake: 58.439% 
Loss D: 1.721 
Loss G: 0.2603 (0.9455) Acc G: 42.070% 
LR: 2.000e-04 

2023-03-02 01:45:56,516 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.1275 (0.1296) Acc D Real: 95.439% 
Loss D Fake: 1.5758 (1.3240) Acc D Fake: 58.134% 
Loss D: 1.703 
Loss G: 0.2662 (0.9415) Acc G: 42.372% 
LR: 2.000e-04 

2023-03-02 01:45:56,524 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.1247 (0.1296) Acc D Real: 95.452% 
Loss D Fake: 1.5502 (1.3253) Acc D Fake: 57.834% 
Loss D: 1.675 
Loss G: 0.2722 (0.9376) Acc G: 42.670% 
LR: 2.000e-04 

2023-03-02 01:45:56,531 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.0963 (0.1294) Acc D Real: 95.469% 
Loss D Fake: 1.5252 (1.3265) Acc D Fake: 57.536% 
Loss D: 1.621 
Loss G: 0.2783 (0.9337) Acc G: 42.965% 
LR: 2.000e-04 

2023-03-02 01:45:56,539 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.1527 (0.1295) Acc D Real: 95.480% 
Loss D Fake: 1.5007 (1.3275) Acc D Fake: 57.242% 
Loss D: 1.653 
Loss G: 0.2844 (0.9300) Acc G: 43.256% 
LR: 2.000e-04 

2023-03-02 01:45:56,546 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1466 (0.1296) Acc D Real: 95.489% 
Loss D Fake: 1.4771 (1.3284) Acc D Fake: 56.951% 
Loss D: 1.624 
Loss G: 0.2905 (0.9263) Acc G: 43.543% 
LR: 2.000e-04 

2023-03-02 01:45:56,553 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.1509 (0.1297) Acc D Real: 95.499% 
Loss D Fake: 1.4542 (1.3291) Acc D Fake: 56.664% 
Loss D: 1.605 
Loss G: 0.2966 (0.9227) Acc G: 43.828% 
LR: 2.000e-04 

2023-03-02 01:45:56,561 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.1568 (0.1299) Acc D Real: 95.506% 
Loss D Fake: 1.4321 (1.3297) Acc D Fake: 56.380% 
Loss D: 1.589 
Loss G: 0.3027 (0.9192) Acc G: 44.109% 
LR: 2.000e-04 

2023-03-02 01:45:56,568 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.1627 (0.1301) Acc D Real: 95.515% 
Loss D Fake: 1.4108 (1.3301) Acc D Fake: 56.108% 
Loss D: 1.573 
Loss G: 0.3087 (0.9157) Acc G: 44.378% 
LR: 2.000e-04 

2023-03-02 01:45:56,576 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.1264 (0.1301) Acc D Real: 95.527% 
Loss D Fake: 1.3900 (1.3305) Acc D Fake: 55.840% 
Loss D: 1.516 
Loss G: 0.3149 (0.9124) Acc G: 44.644% 
LR: 2.000e-04 

2023-03-02 01:45:56,583 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.1337 (0.1301) Acc D Real: 95.537% 
Loss D Fake: 1.3695 (1.3307) Acc D Fake: 55.575% 
Loss D: 1.503 
Loss G: 0.3211 (0.9091) Acc G: 44.906% 
LR: 2.000e-04 

2023-03-02 01:45:56,592 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.1102 (0.1300) Acc D Real: 95.550% 
Loss D Fake: 1.3493 (1.3308) Acc D Fake: 55.312% 
Loss D: 1.459 
Loss G: 0.3274 (0.9058) Acc G: 45.166% 
LR: 2.000e-04 

2023-03-02 01:45:56,600 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.1616 (0.1301) Acc D Real: 95.556% 
Loss D Fake: 1.3293 (1.3308) Acc D Fake: 55.053% 
Loss D: 1.491 
Loss G: 0.3337 (0.9027) Acc G: 45.423% 
LR: 2.000e-04 

2023-03-02 01:45:56,607 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.1816 (0.1304) Acc D Real: 95.561% 
Loss D Fake: 1.3100 (1.3307) Acc D Fake: 54.796% 
Loss D: 1.492 
Loss G: 0.3400 (0.8996) Acc G: 45.677% 
LR: 2.000e-04 

2023-03-02 01:45:56,615 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.1638 (0.1306) Acc D Real: 95.566% 
Loss D Fake: 1.2914 (1.3305) Acc D Fake: 54.542% 
Loss D: 1.455 
Loss G: 0.3463 (0.8966) Acc G: 45.928% 
LR: 2.000e-04 

2023-03-02 01:45:56,622 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.1121 (0.1305) Acc D Real: 95.580% 
Loss D Fake: 1.2732 (1.3302) Acc D Fake: 54.291% 
Loss D: 1.385 
Loss G: 0.3527 (0.8936) Acc G: 46.177% 
LR: 2.000e-04 

2023-03-02 01:45:56,629 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.1766 (0.1308) Acc D Real: 95.586% 
Loss D Fake: 1.2551 (1.3297) Acc D Fake: 54.043% 
Loss D: 1.432 
Loss G: 0.3591 (0.8907) Acc G: 46.423% 
LR: 2.000e-04 

2023-03-02 01:45:56,636 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1858 (0.1310) Acc D Real: 95.588% 
Loss D Fake: 1.2376 (1.3292) Acc D Fake: 53.797% 
Loss D: 1.423 
Loss G: 0.3655 (0.8879) Acc G: 46.666% 
LR: 2.000e-04 

2023-03-02 01:45:56,643 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.1352 (0.1311) Acc D Real: 95.599% 
Loss D Fake: 1.2208 (1.3287) Acc D Fake: 53.554% 
Loss D: 1.356 
Loss G: 0.3718 (0.8851) Acc G: 46.907% 
LR: 2.000e-04 

2023-03-02 01:45:56,650 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.1952 (0.1314) Acc D Real: 95.602% 
Loss D Fake: 1.2043 (1.3280) Acc D Fake: 53.313% 
Loss D: 1.400 
Loss G: 0.3781 (0.8824) Acc G: 47.145% 
LR: 2.000e-04 

2023-03-02 01:45:56,659 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.1373 (0.1314) Acc D Real: 95.615% 
Loss D Fake: 1.1884 (1.3273) Acc D Fake: 53.084% 
Loss D: 1.326 
Loss G: 0.3844 (0.8798) Acc G: 47.372% 
LR: 2.000e-04 

2023-03-02 01:45:56,667 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.1460 (0.1315) Acc D Real: 95.624% 
Loss D Fake: 1.1726 (1.3265) Acc D Fake: 52.857% 
Loss D: 1.319 
Loss G: 0.3908 (0.8772) Acc G: 47.596% 
LR: 2.000e-04 

2023-03-02 01:45:56,676 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.1619 (0.1317) Acc D Real: 95.630% 
Loss D Fake: 1.1571 (1.3256) Acc D Fake: 52.633% 
Loss D: 1.319 
Loss G: 0.3973 (0.8747) Acc G: 47.818% 
LR: 2.000e-04 

2023-03-02 01:45:56,684 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.1712 (0.1319) Acc D Real: 95.636% 
Loss D Fake: 1.1419 (1.3246) Acc D Fake: 52.411% 
Loss D: 1.313 
Loss G: 0.4037 (0.8723) Acc G: 48.038% 
LR: 2.000e-04 

2023-03-02 01:45:56,692 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.1361 (0.1319) Acc D Real: 95.645% 
Loss D Fake: 1.1271 (1.3236) Acc D Fake: 52.191% 
Loss D: 1.263 
Loss G: 0.4102 (0.8699) Acc G: 48.255% 
LR: 2.000e-04 

2023-03-02 01:45:56,701 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2203 (0.1324) Acc D Real: 95.645% 
Loss D Fake: 1.1125 (1.3225) Acc D Fake: 51.974% 
Loss D: 1.333 
Loss G: 0.4166 (0.8675) Acc G: 48.470% 
LR: 2.000e-04 

2023-03-02 01:45:56,709 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.1634 (0.1325) Acc D Real: 95.651% 
Loss D Fake: 1.0986 (1.3214) Acc D Fake: 51.758% 
Loss D: 1.262 
Loss G: 0.4230 (0.8653) Acc G: 48.683% 
LR: 2.000e-04 

2023-03-02 01:45:56,717 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.1681 (0.1327) Acc D Real: 95.658% 
Loss D Fake: 1.0849 (1.3201) Acc D Fake: 51.545% 
Loss D: 1.253 
Loss G: 0.4293 (0.8630) Acc G: 48.894% 
LR: 2.000e-04 

2023-03-02 01:45:56,724 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.1391 (0.1327) Acc D Real: 95.665% 
Loss D Fake: 1.0715 (1.3189) Acc D Fake: 51.334% 
Loss D: 1.211 
Loss G: 0.4358 (0.8609) Acc G: 49.103% 
LR: 2.000e-04 

2023-03-02 01:45:56,731 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.1325 (0.1327) Acc D Real: 95.674% 
Loss D Fake: 1.0580 (1.3176) Acc D Fake: 51.126% 
Loss D: 1.190 
Loss G: 0.4425 (0.8587) Acc G: 49.309% 
LR: 2.000e-04 

2023-03-02 01:45:56,739 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.1900 (0.1330) Acc D Real: 95.677% 
Loss D Fake: 1.0446 (1.3162) Acc D Fake: 50.919% 
Loss D: 1.235 
Loss G: 0.4491 (0.8567) Acc G: 49.514% 
LR: 2.000e-04 

2023-03-02 01:45:56,746 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.1508 (0.1331) Acc D Real: 95.684% 
Loss D Fake: 1.0317 (1.3148) Acc D Fake: 50.714% 
Loss D: 1.183 
Loss G: 0.4557 (0.8547) Acc G: 49.716% 
LR: 2.000e-04 

2023-03-02 01:45:56,753 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.1823 (0.1334) Acc D Real: 95.687% 
Loss D Fake: 1.0190 (1.3133) Acc D Fake: 50.512% 
Loss D: 1.201 
Loss G: 0.4623 (0.8527) Acc G: 49.917% 
LR: 2.000e-04 

2023-03-02 01:45:56,761 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1927 (0.1336) Acc D Real: 95.686% 
Loss D Fake: 1.0066 (1.3118) Acc D Fake: 50.311% 
Loss D: 1.199 
Loss G: 0.4688 (0.8508) Acc G: 50.115% 
LR: 2.000e-04 

2023-03-02 01:45:56,768 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.1739 (0.1338) Acc D Real: 95.690% 
Loss D Fake: 0.9947 (1.3102) Acc D Fake: 50.113% 
Loss D: 1.169 
Loss G: 0.4753 (0.8490) Acc G: 50.311% 
LR: 2.000e-04 

2023-03-02 01:45:56,775 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.1712 (0.1340) Acc D Real: 95.696% 
Loss D Fake: 0.9828 (1.3086) Acc D Fake: 49.916% 
Loss D: 1.154 
Loss G: 0.4819 (0.8472) Acc G: 50.506% 
LR: 2.000e-04 

2023-03-02 01:45:56,782 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.1567 (0.1341) Acc D Real: 95.701% 
Loss D Fake: 0.9711 (1.3070) Acc D Fake: 49.729% 
Loss D: 1.128 
Loss G: 0.4886 (0.8454) Acc G: 50.691% 
LR: 2.000e-04 

2023-03-02 01:45:56,789 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.1549 (0.1342) Acc D Real: 95.709% 
Loss D Fake: 0.9596 (1.3053) Acc D Fake: 49.545% 
Loss D: 1.115 
Loss G: 0.4953 (0.8437) Acc G: 50.873% 
LR: 2.000e-04 

2023-03-02 01:45:56,796 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.2746 (0.1349) Acc D Real: 95.702% 
Loss D Fake: 0.9484 (1.3036) Acc D Fake: 49.362% 
Loss D: 1.223 
Loss G: 0.5017 (0.8421) Acc G: 51.054% 
LR: 2.000e-04 

2023-03-02 01:45:56,803 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2063 (0.1353) Acc D Real: 95.705% 
Loss D Fake: 0.9380 (1.3018) Acc D Fake: 49.180% 
Loss D: 1.144 
Loss G: 0.5078 (0.8405) Acc G: 51.233% 
LR: 2.000e-04 

2023-03-02 01:45:56,810 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1486 (0.1353) Acc D Real: 95.711% 
Loss D Fake: 0.9279 (1.3000) Acc D Fake: 49.001% 
Loss D: 1.076 
Loss G: 0.5141 (0.8389) Acc G: 51.411% 
LR: 2.000e-04 

2023-03-02 01:45:56,817 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.1607 (0.1354) Acc D Real: 95.717% 
Loss D Fake: 0.9176 (1.2982) Acc D Fake: 48.823% 
Loss D: 1.078 
Loss G: 0.5206 (0.8374) Acc G: 51.587% 
LR: 2.000e-04 

2023-03-02 01:45:56,824 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2040 (0.1358) Acc D Real: 95.718% 
Loss D Fake: 0.9075 (1.2963) Acc D Fake: 48.647% 
Loss D: 1.112 
Loss G: 0.5269 (0.8359) Acc G: 51.761% 
LR: 2.000e-04 

2023-03-02 01:45:56,832 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.1754 (0.1360) Acc D Real: 95.724% 
Loss D Fake: 0.8977 (1.2945) Acc D Fake: 48.473% 
Loss D: 1.073 
Loss G: 0.5333 (0.8345) Acc G: 51.933% 
LR: 2.000e-04 

2023-03-02 01:45:56,839 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.1911 (0.1362) Acc D Real: 95.728% 
Loss D Fake: 0.8881 (1.2926) Acc D Fake: 48.300% 
Loss D: 1.079 
Loss G: 0.5396 (0.8331) Acc G: 52.104% 
LR: 2.000e-04 

2023-03-02 01:45:56,846 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.1341 (0.1362) Acc D Real: 95.734% 
Loss D Fake: 0.8786 (1.2906) Acc D Fake: 48.129% 
Loss D: 1.013 
Loss G: 0.5461 (0.8318) Acc G: 52.274% 
LR: 2.000e-04 

2023-03-02 01:45:56,854 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3069 (0.1370) Acc D Real: 95.724% 
Loss D Fake: 0.8691 (1.2887) Acc D Fake: 47.959% 
Loss D: 1.176 
Loss G: 0.5522 (0.8305) Acc G: 52.441% 
LR: 2.000e-04 

2023-03-02 01:45:56,861 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2283 (0.1374) Acc D Real: 95.724% 
Loss D Fake: 0.8606 (1.2867) Acc D Fake: 47.791% 
Loss D: 1.089 
Loss G: 0.5581 (0.8292) Acc G: 52.608% 
LR: 2.000e-04 

2023-03-02 01:45:56,868 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2103 (0.1378) Acc D Real: 95.726% 
Loss D Fake: 0.8524 (1.2847) Acc D Fake: 47.625% 
Loss D: 1.063 
Loss G: 0.5639 (0.8280) Acc G: 52.772% 
LR: 2.000e-04 

2023-03-02 01:45:56,875 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2496 (0.1383) Acc D Real: 95.723% 
Loss D Fake: 0.8444 (1.2827) Acc D Fake: 47.460% 
Loss D: 1.094 
Loss G: 0.5695 (0.8268) Acc G: 52.935% 
LR: 2.000e-04 

2023-03-02 01:45:56,882 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2186 (0.1386) Acc D Real: 95.724% 
Loss D Fake: 0.8368 (1.2806) Acc D Fake: 47.296% 
Loss D: 1.055 
Loss G: 0.5750 (0.8257) Acc G: 53.097% 
LR: 2.000e-04 

2023-03-02 01:45:56,889 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.1270 (0.1386) Acc D Real: 95.732% 
Loss D Fake: 0.8292 (1.2786) Acc D Fake: 47.134% 
Loss D: 0.956 
Loss G: 0.5808 (0.8245) Acc G: 53.257% 
LR: 2.000e-04 

2023-03-02 01:45:56,897 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2243 (0.1390) Acc D Real: 95.735% 
Loss D Fake: 0.8212 (1.2765) Acc D Fake: 46.974% 
Loss D: 1.046 
Loss G: 0.5867 (0.8235) Acc G: 53.416% 
LR: 2.000e-04 

2023-03-02 01:45:56,904 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3371 (0.1399) Acc D Real: 95.729% 
Loss D Fake: 0.8138 (1.2744) Acc D Fake: 46.815% 
Loss D: 1.151 
Loss G: 0.5920 (0.8224) Acc G: 53.573% 
LR: 2.000e-04 

2023-03-02 01:45:56,911 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2223 (0.1402) Acc D Real: 95.732% 
Loss D Fake: 0.8072 (1.2723) Acc D Fake: 46.657% 
Loss D: 1.030 
Loss G: 0.5971 (0.8214) Acc G: 53.729% 
LR: 2.000e-04 

2023-03-02 01:45:56,918 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.1664 (0.1404) Acc D Real: 95.736% 
Loss D Fake: 0.8005 (1.2702) Acc D Fake: 46.501% 
Loss D: 0.967 
Loss G: 0.6026 (0.8204) Acc G: 53.883% 
LR: 2.000e-04 

2023-03-02 01:45:56,925 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.1906 (0.1406) Acc D Real: 95.739% 
Loss D Fake: 0.7935 (1.2681) Acc D Fake: 46.346% 
Loss D: 0.984 
Loss G: 0.6082 (0.8195) Acc G: 54.029% 
LR: 2.000e-04 

2023-03-02 01:45:56,932 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.1420 (0.1406) Acc D Real: 95.741% 
Loss D Fake: 0.7864 (1.2660) Acc D Fake: 46.309% 
Loss D: 0.928 
Loss G: 0.6141 (0.8186) Acc G: 54.065% 
LR: 2.000e-04 

2023-03-02 01:45:56,942 -                train: [    INFO] - 
Epoch: 5/20
2023-03-02 01:45:57,124 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2659 (0.2778) Acc D Real: 94.609% 
Loss D Fake: 0.7724 (0.7757) Acc D Fake: 13.333% 
Loss D: 1.038 
Loss G: 0.6252 (0.6225) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,132 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.1772 (0.2443) Acc D Real: 95.156% 
Loss D Fake: 0.7660 (0.7725) Acc D Fake: 13.333% 
Loss D: 0.943 
Loss G: 0.6307 (0.6252) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,140 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3202 (0.2633) Acc D Real: 95.143% 
Loss D Fake: 0.7597 (0.7693) Acc D Fake: 13.333% 
Loss D: 1.080 
Loss G: 0.6358 (0.6279) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,157 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2092 (0.2525) Acc D Real: 95.167% 
Loss D Fake: 0.7539 (0.7662) Acc D Fake: 13.333% 
Loss D: 0.963 
Loss G: 0.6409 (0.6305) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,164 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2253 (0.2479) Acc D Real: 95.217% 
Loss D Fake: 0.7480 (0.7632) Acc D Fake: 13.333% 
Loss D: 0.973 
Loss G: 0.6460 (0.6331) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,171 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2774 (0.2522) Acc D Real: 95.246% 
Loss D Fake: 0.7423 (0.7602) Acc D Fake: 13.333% 
Loss D: 1.020 
Loss G: 0.6508 (0.6356) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,177 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2624 (0.2534) Acc D Real: 95.247% 
Loss D Fake: 0.7370 (0.7573) Acc D Fake: 13.333% 
Loss D: 0.999 
Loss G: 0.6555 (0.6381) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,184 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2805 (0.2564) Acc D Real: 95.208% 
Loss D Fake: 0.7320 (0.7545) Acc D Fake: 13.333% 
Loss D: 1.012 
Loss G: 0.6599 (0.6405) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,191 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1516 (0.2460) Acc D Real: 95.354% 
Loss D Fake: 0.7270 (0.7517) Acc D Fake: 13.333% 
Loss D: 0.879 
Loss G: 0.6647 (0.6429) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,198 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1370 (0.2361) Acc D Real: 95.521% 
Loss D Fake: 0.7216 (0.7490) Acc D Fake: 13.333% 
Loss D: 0.859 
Loss G: 0.6700 (0.6454) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,205 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2626 (0.2383) Acc D Real: 95.490% 
Loss D Fake: 0.7158 (0.7462) Acc D Fake: 13.333% 
Loss D: 0.978 
Loss G: 0.6753 (0.6479) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,213 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2317 (0.2378) Acc D Real: 95.489% 
Loss D Fake: 0.7103 (0.7435) Acc D Fake: 13.333% 
Loss D: 0.942 
Loss G: 0.6804 (0.6504) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,220 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2171 (0.2363) Acc D Real: 95.499% 
Loss D Fake: 0.7049 (0.7407) Acc D Fake: 13.333% 
Loss D: 0.922 
Loss G: 0.6856 (0.6529) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,227 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2947 (0.2402) Acc D Real: 95.424% 
Loss D Fake: 0.6997 (0.7380) Acc D Fake: 13.333% 
Loss D: 0.994 
Loss G: 0.6904 (0.6554) Acc G: 86.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,234 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2984 (0.2438) Acc D Real: 95.348% 
Loss D Fake: 0.6950 (0.7353) Acc D Fake: 13.333% 
Loss D: 0.993 
Loss G: 0.6949 (0.6579) Acc G: 82.708% 
LR: 2.000e-04 

2023-03-02 01:45:57,242 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3584 (0.2506) Acc D Real: 93.695% 
Loss D Fake: 0.6908 (0.7327) Acc D Fake: 17.255% 
Loss D: 1.049 
Loss G: 0.6987 (0.6603) Acc G: 78.922% 
LR: 2.000e-04 

2023-03-02 01:45:57,249 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2172 (0.2487) Acc D Real: 92.925% 
Loss D Fake: 0.6871 (0.7301) Acc D Fake: 20.926% 
Loss D: 0.904 
Loss G: 0.7025 (0.6626) Acc G: 75.370% 
LR: 2.000e-04 

2023-03-02 01:45:57,257 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.2165 (0.2470) Acc D Real: 92.146% 
Loss D Fake: 0.6831 (0.7277) Acc D Fake: 24.298% 
Loss D: 0.900 
Loss G: 0.7066 (0.6649) Acc G: 72.105% 
LR: 2.000e-04 

2023-03-02 01:45:57,263 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.2365 (0.2465) Acc D Real: 91.370% 
Loss D Fake: 0.6790 (0.7252) Acc D Fake: 27.417% 
Loss D: 0.916 
Loss G: 0.7108 (0.6672) Acc G: 69.167% 
LR: 2.000e-04 

2023-03-02 01:45:57,270 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.2041 (0.2445) Acc D Real: 90.836% 
Loss D Fake: 0.6748 (0.7228) Acc D Fake: 30.238% 
Loss D: 0.879 
Loss G: 0.7153 (0.6695) Acc G: 66.429% 
LR: 2.000e-04 

2023-03-02 01:45:57,277 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.2276 (0.2437) Acc D Real: 90.215% 
Loss D Fake: 0.6704 (0.7204) Acc D Fake: 32.879% 
Loss D: 0.898 
Loss G: 0.7199 (0.6718) Acc G: 63.939% 
LR: 2.000e-04 

2023-03-02 01:45:57,285 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.2272 (0.2430) Acc D Real: 89.642% 
Loss D Fake: 0.6659 (0.7181) Acc D Fake: 35.290% 
Loss D: 0.893 
Loss G: 0.7247 (0.6741) Acc G: 61.594% 
LR: 2.000e-04 

2023-03-02 01:45:57,292 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3181 (0.2461) Acc D Real: 88.609% 
Loss D Fake: 0.6616 (0.7157) Acc D Fake: 37.639% 
Loss D: 0.980 
Loss G: 0.7291 (0.6764) Acc G: 59.375% 
LR: 2.000e-04 

2023-03-02 01:45:57,299 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2052 (0.2445) Acc D Real: 88.233% 
Loss D Fake: 0.6575 (0.7134) Acc D Fake: 39.800% 
Loss D: 0.863 
Loss G: 0.7335 (0.6787) Acc G: 57.333% 
LR: 2.000e-04 

2023-03-02 01:45:57,306 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.1951 (0.2426) Acc D Real: 87.975% 
Loss D Fake: 0.6532 (0.7111) Acc D Fake: 41.795% 
Loss D: 0.848 
Loss G: 0.7383 (0.6810) Acc G: 55.449% 
LR: 2.000e-04 

2023-03-02 01:45:57,313 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.2545 (0.2430) Acc D Real: 87.434% 
Loss D Fake: 0.6488 (0.7088) Acc D Fake: 43.642% 
Loss D: 0.903 
Loss G: 0.7431 (0.6833) Acc G: 53.704% 
LR: 2.000e-04 

2023-03-02 01:45:57,320 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2254 (0.2424) Acc D Real: 87.052% 
Loss D Fake: 0.6445 (0.7065) Acc D Fake: 45.417% 
Loss D: 0.870 
Loss G: 0.7478 (0.6856) Acc G: 52.024% 
LR: 2.000e-04 

2023-03-02 01:45:57,327 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1820 (0.2403) Acc D Real: 86.945% 
Loss D Fake: 0.6401 (0.7042) Acc D Fake: 47.069% 
Loss D: 0.822 
Loss G: 0.7529 (0.6879) Acc G: 50.460% 
LR: 2.000e-04 

2023-03-02 01:45:57,337 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.2995 (0.2423) Acc D Real: 86.300% 
Loss D Fake: 0.6356 (0.7019) Acc D Fake: 48.611% 
Loss D: 0.935 
Loss G: 0.7578 (0.6902) Acc G: 49.000% 
LR: 2.000e-04 

2023-03-02 01:45:57,344 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3223 (0.2449) Acc D Real: 85.586% 
Loss D Fake: 0.6316 (0.6996) Acc D Fake: 50.054% 
Loss D: 0.954 
Loss G: 0.7620 (0.6925) Acc G: 47.634% 
LR: 2.000e-04 

2023-03-02 01:45:57,352 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3154 (0.2471) Acc D Real: 84.964% 
Loss D Fake: 0.6281 (0.6974) Acc D Fake: 51.406% 
Loss D: 0.944 
Loss G: 0.7657 (0.6948) Acc G: 46.354% 
LR: 2.000e-04 

2023-03-02 01:45:57,360 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3471 (0.2501) Acc D Real: 84.242% 
Loss D Fake: 0.6251 (0.6952) Acc D Fake: 52.677% 
Loss D: 0.972 
Loss G: 0.7689 (0.6971) Acc G: 45.152% 
LR: 2.000e-04 

2023-03-02 01:45:57,368 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.2746 (0.2508) Acc D Real: 83.842% 
Loss D Fake: 0.6225 (0.6931) Acc D Fake: 53.873% 
Loss D: 0.897 
Loss G: 0.7720 (0.6993) Acc G: 44.020% 
LR: 2.000e-04 

2023-03-02 01:45:57,375 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3526 (0.2537) Acc D Real: 83.188% 
Loss D Fake: 0.6200 (0.6910) Acc D Fake: 55.000% 
Loss D: 0.973 
Loss G: 0.7746 (0.7014) Acc G: 42.905% 
LR: 2.000e-04 

2023-03-02 01:45:57,383 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3948 (0.2576) Acc D Real: 82.415% 
Loss D Fake: 0.6181 (0.6890) Acc D Fake: 56.111% 
Loss D: 1.013 
Loss G: 0.7766 (0.7035) Acc G: 41.852% 
LR: 2.000e-04 

2023-03-02 01:45:57,391 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3231 (0.2594) Acc D Real: 81.919% 
Loss D Fake: 0.6166 (0.6870) Acc D Fake: 57.162% 
Loss D: 0.940 
Loss G: 0.7783 (0.7055) Acc G: 40.856% 
LR: 2.000e-04 

2023-03-02 01:45:57,399 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.1977 (0.2578) Acc D Real: 81.891% 
Loss D Fake: 0.6150 (0.6851) Acc D Fake: 58.158% 
Loss D: 0.813 
Loss G: 0.7805 (0.7075) Acc G: 39.912% 
LR: 2.000e-04 

2023-03-02 01:45:57,406 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3542 (0.2603) Acc D Real: 81.376% 
Loss D Fake: 0.6130 (0.6833) Acc D Fake: 59.103% 
Loss D: 0.967 
Loss G: 0.7827 (0.7094) Acc G: 39.017% 
LR: 2.000e-04 

2023-03-02 01:45:57,414 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3041 (0.2614) Acc D Real: 81.008% 
Loss D Fake: 0.6113 (0.6815) Acc D Fake: 60.000% 
Loss D: 0.915 
Loss G: 0.7848 (0.7113) Acc G: 38.167% 
LR: 2.000e-04 

2023-03-02 01:45:57,421 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3646 (0.2639) Acc D Real: 80.495% 
Loss D Fake: 0.6096 (0.6797) Acc D Fake: 60.854% 
Loss D: 0.974 
Loss G: 0.7866 (0.7132) Acc G: 37.358% 
LR: 2.000e-04 

2023-03-02 01:45:57,429 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.2412 (0.2633) Acc D Real: 80.392% 
Loss D Fake: 0.6081 (0.6780) Acc D Fake: 61.667% 
Loss D: 0.849 
Loss G: 0.7887 (0.7150) Acc G: 36.587% 
LR: 2.000e-04 

2023-03-02 01:45:57,436 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2814 (0.2638) Acc D Real: 80.148% 
Loss D Fake: 0.6063 (0.6763) Acc D Fake: 62.442% 
Loss D: 0.888 
Loss G: 0.7909 (0.7167) Acc G: 35.853% 
LR: 2.000e-04 

2023-03-02 01:45:57,444 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.2628 (0.2637) Acc D Real: 80.007% 
Loss D Fake: 0.6043 (0.6747) Acc D Fake: 63.182% 
Loss D: 0.867 
Loss G: 0.7934 (0.7185) Acc G: 35.152% 
LR: 2.000e-04 

2023-03-02 01:45:57,451 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2950 (0.2644) Acc D Real: 79.760% 
Loss D Fake: 0.6023 (0.6731) Acc D Fake: 63.889% 
Loss D: 0.897 
Loss G: 0.7959 (0.7202) Acc G: 34.481% 
LR: 2.000e-04 

2023-03-02 01:45:57,459 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2858 (0.2649) Acc D Real: 79.549% 
Loss D Fake: 0.6003 (0.6715) Acc D Fake: 64.565% 
Loss D: 0.886 
Loss G: 0.7984 (0.7219) Acc G: 33.841% 
LR: 2.000e-04 

2023-03-02 01:45:57,467 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3367 (0.2664) Acc D Real: 79.207% 
Loss D Fake: 0.5984 (0.6699) Acc D Fake: 65.213% 
Loss D: 0.935 
Loss G: 0.8006 (0.7236) Acc G: 33.227% 
LR: 2.000e-04 

2023-03-02 01:45:57,474 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.2644 (0.2664) Acc D Real: 79.067% 
Loss D Fake: 0.5966 (0.6684) Acc D Fake: 65.833% 
Loss D: 0.861 
Loss G: 0.8028 (0.7252) Acc G: 32.639% 
LR: 2.000e-04 

2023-03-02 01:45:57,481 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2143 (0.2653) Acc D Real: 79.054% 
Loss D Fake: 0.5947 (0.6669) Acc D Fake: 66.429% 
Loss D: 0.809 
Loss G: 0.8055 (0.7269) Acc G: 32.075% 
LR: 2.000e-04 

2023-03-02 01:45:57,489 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2707 (0.2654) Acc D Real: 78.907% 
Loss D Fake: 0.5924 (0.6654) Acc D Fake: 67.000% 
Loss D: 0.863 
Loss G: 0.8084 (0.7285) Acc G: 31.500% 
LR: 2.000e-04 

2023-03-02 01:45:57,496 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3387 (0.2669) Acc D Real: 78.597% 
Loss D Fake: 0.5903 (0.6640) Acc D Fake: 67.582% 
Loss D: 0.929 
Loss G: 0.8108 (0.7301) Acc G: 30.915% 
LR: 2.000e-04 

2023-03-02 01:45:57,503 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2460 (0.2665) Acc D Real: 78.528% 
Loss D Fake: 0.5883 (0.6625) Acc D Fake: 68.173% 
Loss D: 0.834 
Loss G: 0.8135 (0.7317) Acc G: 30.353% 
LR: 2.000e-04 

2023-03-02 01:45:57,510 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4106 (0.2692) Acc D Real: 78.075% 
Loss D Fake: 0.5864 (0.6611) Acc D Fake: 68.742% 
Loss D: 0.997 
Loss G: 0.8155 (0.7333) Acc G: 29.811% 
LR: 2.000e-04 

2023-03-02 01:45:57,518 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2490 (0.2688) Acc D Real: 78.009% 
Loss D Fake: 0.5849 (0.6597) Acc D Fake: 69.290% 
Loss D: 0.834 
Loss G: 0.8176 (0.7348) Acc G: 29.290% 
LR: 2.000e-04 

2023-03-02 01:45:57,525 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2969 (0.2693) Acc D Real: 77.846% 
Loss D Fake: 0.5832 (0.6583) Acc D Fake: 69.818% 
Loss D: 0.880 
Loss G: 0.8198 (0.7364) Acc G: 28.788% 
LR: 2.000e-04 

2023-03-02 01:45:57,533 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.1799 (0.2677) Acc D Real: 77.951% 
Loss D Fake: 0.5813 (0.6569) Acc D Fake: 70.327% 
Loss D: 0.761 
Loss G: 0.8227 (0.7379) Acc G: 28.304% 
LR: 2.000e-04 

2023-03-02 01:45:57,541 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2452 (0.2673) Acc D Real: 77.905% 
Loss D Fake: 0.5789 (0.6555) Acc D Fake: 70.819% 
Loss D: 0.824 
Loss G: 0.8259 (0.7395) Acc G: 27.836% 
LR: 2.000e-04 

2023-03-02 01:45:57,548 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2628 (0.2672) Acc D Real: 77.832% 
Loss D Fake: 0.5764 (0.6542) Acc D Fake: 71.293% 
Loss D: 0.839 
Loss G: 0.8291 (0.7410) Acc G: 27.385% 
LR: 2.000e-04 

2023-03-02 01:45:57,557 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3527 (0.2687) Acc D Real: 77.576% 
Loss D Fake: 0.5741 (0.6528) Acc D Fake: 71.751% 
Loss D: 0.927 
Loss G: 0.8320 (0.7426) Acc G: 26.949% 
LR: 2.000e-04 

2023-03-02 01:45:57,564 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3056 (0.2693) Acc D Real: 77.412% 
Loss D Fake: 0.5721 (0.6515) Acc D Fake: 72.222% 
Loss D: 0.878 
Loss G: 0.8345 (0.7441) Acc G: 26.500% 
LR: 2.000e-04 

2023-03-02 01:45:57,572 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.2649 (0.2692) Acc D Real: 77.342% 
Loss D Fake: 0.5701 (0.6501) Acc D Fake: 72.678% 
Loss D: 0.835 
Loss G: 0.8372 (0.7456) Acc G: 26.066% 
LR: 2.000e-04 

2023-03-02 01:45:57,580 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.1862 (0.2679) Acc D Real: 77.444% 
Loss D Fake: 0.5680 (0.6488) Acc D Fake: 73.118% 
Loss D: 0.754 
Loss G: 0.8404 (0.7471) Acc G: 25.645% 
LR: 2.000e-04 

2023-03-02 01:45:57,587 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2812 (0.2681) Acc D Real: 77.343% 
Loss D Fake: 0.5655 (0.6475) Acc D Fake: 73.545% 
Loss D: 0.847 
Loss G: 0.8437 (0.7487) Acc G: 25.238% 
LR: 2.000e-04 

2023-03-02 01:45:57,595 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.2327 (0.2676) Acc D Real: 77.340% 
Loss D Fake: 0.5631 (0.6462) Acc D Fake: 73.958% 
Loss D: 0.796 
Loss G: 0.8471 (0.7502) Acc G: 24.844% 
LR: 2.000e-04 

2023-03-02 01:45:57,602 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3001 (0.2681) Acc D Real: 77.206% 
Loss D Fake: 0.5606 (0.6448) Acc D Fake: 74.359% 
Loss D: 0.861 
Loss G: 0.8503 (0.7518) Acc G: 24.462% 
LR: 2.000e-04 

2023-03-02 01:45:57,610 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1845 (0.2668) Acc D Real: 77.295% 
Loss D Fake: 0.5582 (0.6435) Acc D Fake: 74.747% 
Loss D: 0.743 
Loss G: 0.8539 (0.7533) Acc G: 24.091% 
LR: 2.000e-04 

2023-03-02 01:45:57,618 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.2081 (0.2659) Acc D Real: 77.338% 
Loss D Fake: 0.5555 (0.6422) Acc D Fake: 75.124% 
Loss D: 0.764 
Loss G: 0.8579 (0.7549) Acc G: 23.731% 
LR: 2.000e-04 

2023-03-02 01:45:57,625 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4305 (0.2683) Acc D Real: 76.988% 
Loss D Fake: 0.5528 (0.6409) Acc D Fake: 75.490% 
Loss D: 0.983 
Loss G: 0.8609 (0.7564) Acc G: 23.382% 
LR: 2.000e-04 

2023-03-02 01:45:57,633 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3005 (0.2688) Acc D Real: 76.875% 
Loss D Fake: 0.5510 (0.6396) Acc D Fake: 75.845% 
Loss D: 0.851 
Loss G: 0.8633 (0.7580) Acc G: 23.043% 
LR: 2.000e-04 

2023-03-02 01:45:57,641 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1586 (0.2672) Acc D Real: 77.009% 
Loss D Fake: 0.5491 (0.6383) Acc D Fake: 76.190% 
Loss D: 0.708 
Loss G: 0.8664 (0.7595) Acc G: 22.714% 
LR: 2.000e-04 

2023-03-02 01:45:57,648 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2763 (0.2674) Acc D Real: 76.943% 
Loss D Fake: 0.5468 (0.6370) Acc D Fake: 76.526% 
Loss D: 0.823 
Loss G: 0.8696 (0.7611) Acc G: 22.394% 
LR: 2.000e-04 

2023-03-02 01:45:57,655 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.2725 (0.2674) Acc D Real: 76.897% 
Loss D Fake: 0.5446 (0.6357) Acc D Fake: 76.852% 
Loss D: 0.817 
Loss G: 0.8727 (0.7626) Acc G: 22.083% 
LR: 2.000e-04 

2023-03-02 01:45:57,663 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.2362 (0.2670) Acc D Real: 76.906% 
Loss D Fake: 0.5424 (0.6345) Acc D Fake: 77.169% 
Loss D: 0.779 
Loss G: 0.8759 (0.7642) Acc G: 21.781% 
LR: 2.000e-04 

2023-03-02 01:45:57,670 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3916 (0.2687) Acc D Real: 76.658% 
Loss D Fake: 0.5404 (0.6332) Acc D Fake: 77.477% 
Loss D: 0.932 
Loss G: 0.8784 (0.7657) Acc G: 21.486% 
LR: 2.000e-04 

2023-03-02 01:45:57,678 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3076 (0.2692) Acc D Real: 76.552% 
Loss D Fake: 0.5388 (0.6319) Acc D Fake: 77.778% 
Loss D: 0.846 
Loss G: 0.8805 (0.7673) Acc G: 21.200% 
LR: 2.000e-04 

2023-03-02 01:45:57,685 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.2891 (0.2695) Acc D Real: 76.479% 
Loss D Fake: 0.5374 (0.6307) Acc D Fake: 78.070% 
Loss D: 0.827 
Loss G: 0.8826 (0.7688) Acc G: 20.921% 
LR: 2.000e-04 

2023-03-02 01:45:57,693 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2418 (0.2691) Acc D Real: 76.478% 
Loss D Fake: 0.5360 (0.6294) Acc D Fake: 78.355% 
Loss D: 0.778 
Loss G: 0.8849 (0.7703) Acc G: 20.649% 
LR: 2.000e-04 

2023-03-02 01:45:57,701 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1940 (0.2681) Acc D Real: 76.559% 
Loss D Fake: 0.5342 (0.6282) Acc D Fake: 78.632% 
Loss D: 0.728 
Loss G: 0.8877 (0.7718) Acc G: 20.385% 
LR: 2.000e-04 

2023-03-02 01:45:57,710 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.2606 (0.2680) Acc D Real: 76.532% 
Loss D Fake: 0.5322 (0.6270) Acc D Fake: 78.903% 
Loss D: 0.793 
Loss G: 0.8907 (0.7733) Acc G: 20.127% 
LR: 2.000e-04 

2023-03-02 01:45:57,718 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1853 (0.2670) Acc D Real: 76.617% 
Loss D Fake: 0.5301 (0.6258) Acc D Fake: 79.167% 
Loss D: 0.715 
Loss G: 0.8942 (0.7748) Acc G: 19.875% 
LR: 2.000e-04 

2023-03-02 01:45:57,726 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.1841 (0.2660) Acc D Real: 76.702% 
Loss D Fake: 0.5276 (0.6246) Acc D Fake: 79.424% 
Loss D: 0.712 
Loss G: 0.8981 (0.7763) Acc G: 19.630% 
LR: 2.000e-04 

2023-03-02 01:45:57,735 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2182 (0.2654) Acc D Real: 76.741% 
Loss D Fake: 0.5249 (0.6234) Acc D Fake: 79.675% 
Loss D: 0.743 
Loss G: 0.9022 (0.7779) Acc G: 19.390% 
LR: 2.000e-04 

2023-03-02 01:45:57,743 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.2197 (0.2649) Acc D Real: 76.782% 
Loss D Fake: 0.5222 (0.6222) Acc D Fake: 79.920% 
Loss D: 0.742 
Loss G: 0.9065 (0.7794) Acc G: 19.157% 
LR: 2.000e-04 

2023-03-02 01:45:57,752 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4263 (0.2668) Acc D Real: 76.546% 
Loss D Fake: 0.5196 (0.6209) Acc D Fake: 80.159% 
Loss D: 0.946 
Loss G: 0.9096 (0.7810) Acc G: 18.929% 
LR: 2.000e-04 

2023-03-02 01:45:57,760 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.2965 (0.2671) Acc D Real: 76.486% 
Loss D Fake: 0.5178 (0.6197) Acc D Fake: 80.392% 
Loss D: 0.814 
Loss G: 0.9122 (0.7825) Acc G: 18.706% 
LR: 2.000e-04 

2023-03-02 01:45:57,768 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1678 (0.2660) Acc D Real: 76.598% 
Loss D Fake: 0.5161 (0.6185) Acc D Fake: 80.620% 
Loss D: 0.684 
Loss G: 0.9154 (0.7840) Acc G: 18.488% 
LR: 2.000e-04 

2023-03-02 01:45:57,777 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.2988 (0.2663) Acc D Real: 76.544% 
Loss D Fake: 0.5140 (0.6173) Acc D Fake: 80.843% 
Loss D: 0.813 
Loss G: 0.9185 (0.7856) Acc G: 18.276% 
LR: 2.000e-04 

2023-03-02 01:45:57,785 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.2042 (0.2656) Acc D Real: 76.612% 
Loss D Fake: 0.5119 (0.6161) Acc D Fake: 81.061% 
Loss D: 0.716 
Loss G: 0.9219 (0.7871) Acc G: 18.068% 
LR: 2.000e-04 

2023-03-02 01:45:57,793 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3798 (0.2669) Acc D Real: 76.444% 
Loss D Fake: 0.5098 (0.6149) Acc D Fake: 81.273% 
Loss D: 0.890 
Loss G: 0.9248 (0.7887) Acc G: 17.865% 
LR: 2.000e-04 

2023-03-02 01:45:57,801 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.2807 (0.2671) Acc D Real: 76.409% 
Loss D Fake: 0.5082 (0.6137) Acc D Fake: 81.481% 
Loss D: 0.789 
Loss G: 0.9273 (0.7902) Acc G: 17.667% 
LR: 2.000e-04 

2023-03-02 01:45:57,808 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3620 (0.2681) Acc D Real: 76.280% 
Loss D Fake: 0.5067 (0.6126) Acc D Fake: 81.685% 
Loss D: 0.869 
Loss G: 0.9294 (0.7918) Acc G: 17.473% 
LR: 2.000e-04 

2023-03-02 01:45:57,816 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3760 (0.2693) Acc D Real: 76.135% 
Loss D Fake: 0.5057 (0.6114) Acc D Fake: 81.884% 
Loss D: 0.882 
Loss G: 0.9307 (0.7933) Acc G: 17.283% 
LR: 2.000e-04 

2023-03-02 01:45:57,824 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4052 (0.2708) Acc D Real: 75.962% 
Loss D Fake: 0.5051 (0.6103) Acc D Fake: 82.079% 
Loss D: 0.910 
Loss G: 0.9312 (0.7948) Acc G: 17.097% 
LR: 2.000e-04 

2023-03-02 01:45:57,831 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3323 (0.2714) Acc D Real: 75.878% 
Loss D Fake: 0.5049 (0.6091) Acc D Fake: 82.270% 
Loss D: 0.837 
Loss G: 0.9315 (0.7962) Acc G: 16.915% 
LR: 2.000e-04 

2023-03-02 01:45:57,838 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3865 (0.2726) Acc D Real: 75.727% 
Loss D Fake: 0.5048 (0.6080) Acc D Fake: 82.456% 
Loss D: 0.891 
Loss G: 0.9313 (0.7976) Acc G: 16.737% 
LR: 2.000e-04 

2023-03-02 01:45:57,846 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.3382 (0.2733) Acc D Real: 75.656% 
Loss D Fake: 0.5050 (0.6070) Acc D Fake: 82.639% 
Loss D: 0.843 
Loss G: 0.9312 (0.7990) Acc G: 16.562% 
LR: 2.000e-04 

2023-03-02 01:45:57,853 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3026 (0.2736) Acc D Real: 75.619% 
Loss D Fake: 0.5050 (0.6059) Acc D Fake: 82.818% 
Loss D: 0.808 
Loss G: 0.9313 (0.8004) Acc G: 16.392% 
LR: 2.000e-04 

2023-03-02 01:45:57,860 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2932 (0.2738) Acc D Real: 75.577% 
Loss D Fake: 0.5049 (0.6049) Acc D Fake: 82.993% 
Loss D: 0.798 
Loss G: 0.9316 (0.8017) Acc G: 16.224% 
LR: 2.000e-04 

2023-03-02 01:45:57,868 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3315 (0.2744) Acc D Real: 75.497% 
Loss D Fake: 0.5047 (0.6039) Acc D Fake: 83.165% 
Loss D: 0.836 
Loss G: 0.9319 (0.8030) Acc G: 16.061% 
LR: 2.000e-04 

2023-03-02 01:45:57,875 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2239 (0.2739) Acc D Real: 75.544% 
Loss D Fake: 0.5044 (0.6029) Acc D Fake: 83.333% 
Loss D: 0.728 
Loss G: 0.9328 (0.8043) Acc G: 15.900% 
LR: 2.000e-04 

2023-03-02 01:45:57,883 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2191 (0.2733) Acc D Real: 75.588% 
Loss D Fake: 0.5036 (0.6019) Acc D Fake: 83.498% 
Loss D: 0.723 
Loss G: 0.9344 (0.8056) Acc G: 15.743% 
LR: 2.000e-04 

2023-03-02 01:45:57,890 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3719 (0.2743) Acc D Real: 75.476% 
Loss D Fake: 0.5026 (0.6009) Acc D Fake: 83.660% 
Loss D: 0.875 
Loss G: 0.9356 (0.8069) Acc G: 15.588% 
LR: 2.000e-04 

2023-03-02 01:45:57,898 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4204 (0.2757) Acc D Real: 75.309% 
Loss D Fake: 0.5021 (0.6000) Acc D Fake: 83.819% 
Loss D: 0.923 
Loss G: 0.9361 (0.8081) Acc G: 15.437% 
LR: 2.000e-04 

2023-03-02 01:45:57,905 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2801 (0.2758) Acc D Real: 75.291% 
Loss D Fake: 0.5019 (0.5990) Acc D Fake: 83.974% 
Loss D: 0.782 
Loss G: 0.9366 (0.8094) Acc G: 15.288% 
LR: 2.000e-04 

2023-03-02 01:45:57,913 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2190 (0.2752) Acc D Real: 75.340% 
Loss D Fake: 0.5014 (0.5981) Acc D Fake: 84.127% 
Loss D: 0.720 
Loss G: 0.9378 (0.8106) Acc G: 15.143% 
LR: 2.000e-04 

2023-03-02 01:45:57,920 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3265 (0.2757) Acc D Real: 75.273% 
Loss D Fake: 0.5006 (0.5972) Acc D Fake: 84.277% 
Loss D: 0.827 
Loss G: 0.9389 (0.8118) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:45:57,927 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2514 (0.2755) Acc D Real: 75.290% 
Loss D Fake: 0.4999 (0.5963) Acc D Fake: 84.424% 
Loss D: 0.751 
Loss G: 0.9403 (0.8130) Acc G: 14.860% 
LR: 2.000e-04 

2023-03-02 01:45:57,934 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3299 (0.2760) Acc D Real: 75.230% 
Loss D Fake: 0.4990 (0.5954) Acc D Fake: 84.568% 
Loss D: 0.829 
Loss G: 0.9416 (0.8142) Acc G: 14.722% 
LR: 2.000e-04 

2023-03-02 01:45:57,942 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3054 (0.2763) Acc D Real: 75.192% 
Loss D Fake: 0.4983 (0.5945) Acc D Fake: 84.709% 
Loss D: 0.804 
Loss G: 0.9427 (0.8154) Acc G: 14.587% 
LR: 2.000e-04 

2023-03-02 01:45:57,949 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3345 (0.2768) Acc D Real: 75.125% 
Loss D Fake: 0.4977 (0.5936) Acc D Fake: 84.848% 
Loss D: 0.832 
Loss G: 0.9435 (0.8166) Acc G: 14.455% 
LR: 2.000e-04 

2023-03-02 01:45:57,956 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.2195 (0.2763) Acc D Real: 75.172% 
Loss D Fake: 0.4971 (0.5927) Acc D Fake: 84.985% 
Loss D: 0.717 
Loss G: 0.9448 (0.8177) Acc G: 14.324% 
LR: 2.000e-04 

2023-03-02 01:45:57,963 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3379 (0.2768) Acc D Real: 75.103% 
Loss D Fake: 0.4963 (0.5919) Acc D Fake: 85.119% 
Loss D: 0.834 
Loss G: 0.9460 (0.8189) Acc G: 14.196% 
LR: 2.000e-04 

2023-03-02 01:45:57,971 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.2339 (0.2764) Acc D Real: 75.144% 
Loss D Fake: 0.4955 (0.5910) Acc D Fake: 85.251% 
Loss D: 0.729 
Loss G: 0.9476 (0.8200) Acc G: 14.071% 
LR: 2.000e-04 

2023-03-02 01:45:57,978 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3161 (0.2768) Acc D Real: 75.103% 
Loss D Fake: 0.4945 (0.5902) Acc D Fake: 85.380% 
Loss D: 0.811 
Loss G: 0.9491 (0.8211) Acc G: 13.947% 
LR: 2.000e-04 

2023-03-02 01:45:57,985 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.2531 (0.2766) Acc D Real: 75.123% 
Loss D Fake: 0.4936 (0.5893) Acc D Fake: 85.507% 
Loss D: 0.747 
Loss G: 0.9508 (0.8223) Acc G: 13.826% 
LR: 2.000e-04 

2023-03-02 01:45:57,992 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3319 (0.2771) Acc D Real: 75.069% 
Loss D Fake: 0.4926 (0.5885) Acc D Fake: 85.632% 
Loss D: 0.824 
Loss G: 0.9523 (0.8234) Acc G: 13.707% 
LR: 2.000e-04 

2023-03-02 01:45:58,001 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4688 (0.2787) Acc D Real: 74.888% 
Loss D Fake: 0.4920 (0.5877) Acc D Fake: 85.755% 
Loss D: 0.961 
Loss G: 0.9526 (0.8245) Acc G: 13.590% 
LR: 2.000e-04 

2023-03-02 01:45:58,008 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3197 (0.2790) Acc D Real: 74.846% 
Loss D Fake: 0.4920 (0.5868) Acc D Fake: 85.876% 
Loss D: 0.812 
Loss G: 0.9523 (0.8256) Acc G: 13.475% 
LR: 2.000e-04 

2023-03-02 01:45:58,015 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.2791 (0.2790) Acc D Real: 74.838% 
Loss D Fake: 0.4923 (0.5861) Acc D Fake: 85.994% 
Loss D: 0.771 
Loss G: 0.9522 (0.8266) Acc G: 13.361% 
LR: 2.000e-04 

2023-03-02 01:45:58,022 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2820 (0.2791) Acc D Real: 74.830% 
Loss D Fake: 0.4923 (0.5853) Acc D Fake: 86.111% 
Loss D: 0.774 
Loss G: 0.9524 (0.8277) Acc G: 13.250% 
LR: 2.000e-04 

2023-03-02 01:45:58,030 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2466 (0.2788) Acc D Real: 74.853% 
Loss D Fake: 0.4920 (0.5845) Acc D Fake: 86.226% 
Loss D: 0.739 
Loss G: 0.9531 (0.8287) Acc G: 13.140% 
LR: 2.000e-04 

2023-03-02 01:45:58,038 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.2730 (0.2788) Acc D Real: 74.855% 
Loss D Fake: 0.4915 (0.5837) Acc D Fake: 86.339% 
Loss D: 0.765 
Loss G: 0.9541 (0.8297) Acc G: 13.033% 
LR: 2.000e-04 

2023-03-02 01:45:58,046 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2869 (0.2788) Acc D Real: 74.848% 
Loss D Fake: 0.4909 (0.5830) Acc D Fake: 86.450% 
Loss D: 0.778 
Loss G: 0.9554 (0.8308) Acc G: 12.927% 
LR: 2.000e-04 

2023-03-02 01:45:58,055 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.2375 (0.2785) Acc D Real: 74.883% 
Loss D Fake: 0.4901 (0.5822) Acc D Fake: 86.559% 
Loss D: 0.728 
Loss G: 0.9570 (0.8318) Acc G: 12.823% 
LR: 2.000e-04 

2023-03-02 01:45:58,064 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3700 (0.2792) Acc D Real: 74.802% 
Loss D Fake: 0.4892 (0.5815) Acc D Fake: 86.667% 
Loss D: 0.859 
Loss G: 0.9582 (0.8328) Acc G: 12.720% 
LR: 2.000e-04 

2023-03-02 01:45:58,073 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3421 (0.2797) Acc D Real: 74.743% 
Loss D Fake: 0.4886 (0.5808) Acc D Fake: 86.772% 
Loss D: 0.831 
Loss G: 0.9590 (0.8338) Acc G: 12.619% 
LR: 2.000e-04 

2023-03-02 01:45:58,082 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.2653 (0.2796) Acc D Real: 74.758% 
Loss D Fake: 0.4882 (0.5800) Acc D Fake: 86.877% 
Loss D: 0.753 
Loss G: 0.9599 (0.8348) Acc G: 12.520% 
LR: 2.000e-04 

2023-03-02 01:45:58,090 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4989 (0.2813) Acc D Real: 74.577% 
Loss D Fake: 0.4878 (0.5793) Acc D Fake: 86.979% 
Loss D: 0.987 
Loss G: 0.9597 (0.8358) Acc G: 12.422% 
LR: 2.000e-04 

2023-03-02 01:45:58,097 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3130 (0.2816) Acc D Real: 74.548% 
Loss D Fake: 0.4882 (0.5786) Acc D Fake: 87.080% 
Loss D: 0.801 
Loss G: 0.9593 (0.8367) Acc G: 12.326% 
LR: 2.000e-04 

2023-03-02 01:45:58,104 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.1872 (0.2808) Acc D Real: 74.621% 
Loss D Fake: 0.4882 (0.5779) Acc D Fake: 87.179% 
Loss D: 0.675 
Loss G: 0.9597 (0.8377) Acc G: 12.231% 
LR: 2.000e-04 

2023-03-02 01:45:58,112 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3103 (0.2811) Acc D Real: 74.595% 
Loss D Fake: 0.4878 (0.5772) Acc D Fake: 87.277% 
Loss D: 0.798 
Loss G: 0.9603 (0.8386) Acc G: 12.137% 
LR: 2.000e-04 

2023-03-02 01:45:58,119 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1676 (0.2802) Acc D Real: 74.686% 
Loss D Fake: 0.4873 (0.5765) Acc D Fake: 87.374% 
Loss D: 0.655 
Loss G: 0.9618 (0.8395) Acc G: 12.045% 
LR: 2.000e-04 

2023-03-02 01:45:58,127 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3387 (0.2806) Acc D Real: 74.635% 
Loss D Fake: 0.4864 (0.5759) Acc D Fake: 87.469% 
Loss D: 0.825 
Loss G: 0.9632 (0.8405) Acc G: 11.955% 
LR: 2.000e-04 

2023-03-02 01:45:58,134 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2885 (0.2807) Acc D Real: 74.628% 
Loss D Fake: 0.4856 (0.5752) Acc D Fake: 87.562% 
Loss D: 0.774 
Loss G: 0.9645 (0.8414) Acc G: 11.866% 
LR: 2.000e-04 

2023-03-02 01:45:58,142 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3128 (0.2809) Acc D Real: 74.600% 
Loss D Fake: 0.4849 (0.5745) Acc D Fake: 87.654% 
Loss D: 0.798 
Loss G: 0.9656 (0.8423) Acc G: 11.778% 
LR: 2.000e-04 

2023-03-02 01:45:58,149 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.2192 (0.2805) Acc D Real: 74.647% 
Loss D Fake: 0.4842 (0.5739) Acc D Fake: 87.745% 
Loss D: 0.703 
Loss G: 0.9671 (0.8432) Acc G: 11.691% 
LR: 2.000e-04 

2023-03-02 01:45:58,156 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3188 (0.2808) Acc D Real: 74.624% 
Loss D Fake: 0.4833 (0.5732) Acc D Fake: 87.835% 
Loss D: 0.802 
Loss G: 0.9685 (0.8441) Acc G: 11.606% 
LR: 2.000e-04 

2023-03-02 01:45:58,164 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.2552 (0.2806) Acc D Real: 74.642% 
Loss D Fake: 0.4825 (0.5725) Acc D Fake: 87.923% 
Loss D: 0.738 
Loss G: 0.9701 (0.8451) Acc G: 11.522% 
LR: 2.000e-04 

2023-03-02 01:45:58,171 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3119 (0.2808) Acc D Real: 74.617% 
Loss D Fake: 0.4816 (0.5719) Acc D Fake: 88.010% 
Loss D: 0.793 
Loss G: 0.9715 (0.8460) Acc G: 11.439% 
LR: 2.000e-04 

2023-03-02 01:45:58,179 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3218 (0.2811) Acc D Real: 74.586% 
Loss D Fake: 0.4808 (0.5712) Acc D Fake: 88.095% 
Loss D: 0.803 
Loss G: 0.9728 (0.8469) Acc G: 11.357% 
LR: 2.000e-04 

2023-03-02 01:45:58,187 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.2846 (0.2811) Acc D Real: 74.590% 
Loss D Fake: 0.4801 (0.5706) Acc D Fake: 88.180% 
Loss D: 0.765 
Loss G: 0.9740 (0.8478) Acc G: 11.277% 
LR: 2.000e-04 

2023-03-02 01:45:58,194 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1651 (0.2803) Acc D Real: 74.681% 
Loss D Fake: 0.4793 (0.5699) Acc D Fake: 88.263% 
Loss D: 0.644 
Loss G: 0.9760 (0.8487) Acc G: 11.197% 
LR: 2.000e-04 

2023-03-02 01:45:58,202 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3757 (0.2810) Acc D Real: 74.610% 
Loss D Fake: 0.4781 (0.5693) Acc D Fake: 88.345% 
Loss D: 0.854 
Loss G: 0.9775 (0.8496) Acc G: 11.119% 
LR: 2.000e-04 

2023-03-02 01:45:58,209 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3151 (0.2812) Acc D Real: 74.592% 
Loss D Fake: 0.4774 (0.5687) Acc D Fake: 88.426% 
Loss D: 0.793 
Loss G: 0.9787 (0.8505) Acc G: 11.042% 
LR: 2.000e-04 

2023-03-02 01:45:58,216 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3187 (0.2815) Acc D Real: 74.567% 
Loss D Fake: 0.4768 (0.5680) Acc D Fake: 88.506% 
Loss D: 0.796 
Loss G: 0.9797 (0.8514) Acc G: 10.966% 
LR: 2.000e-04 

2023-03-02 01:45:58,224 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.1517 (0.2806) Acc D Real: 74.664% 
Loss D Fake: 0.4761 (0.5674) Acc D Fake: 88.584% 
Loss D: 0.628 
Loss G: 0.9815 (0.8523) Acc G: 10.890% 
LR: 2.000e-04 

2023-03-02 01:45:58,231 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.2857 (0.2806) Acc D Real: 74.663% 
Loss D Fake: 0.4749 (0.5668) Acc D Fake: 88.662% 
Loss D: 0.761 
Loss G: 0.9835 (0.8531) Acc G: 10.816% 
LR: 2.000e-04 

2023-03-02 01:45:58,238 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3133 (0.2808) Acc D Real: 74.641% 
Loss D Fake: 0.4739 (0.5661) Acc D Fake: 88.739% 
Loss D: 0.787 
Loss G: 0.9852 (0.8540) Acc G: 10.743% 
LR: 2.000e-04 

2023-03-02 01:45:58,246 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4274 (0.2818) Acc D Real: 74.538% 
Loss D Fake: 0.4732 (0.5655) Acc D Fake: 88.814% 
Loss D: 0.901 
Loss G: 0.9858 (0.8549) Acc G: 10.671% 
LR: 2.000e-04 

2023-03-02 01:45:58,253 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4671 (0.2831) Acc D Real: 74.413% 
Loss D Fake: 0.4732 (0.5649) Acc D Fake: 88.889% 
Loss D: 0.940 
Loss G: 0.9853 (0.8558) Acc G: 10.600% 
LR: 2.000e-04 

2023-03-02 01:45:58,260 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4365 (0.2841) Acc D Real: 74.310% 
Loss D Fake: 0.4737 (0.5643) Acc D Fake: 88.962% 
Loss D: 0.910 
Loss G: 0.9839 (0.8566) Acc G: 10.530% 
LR: 2.000e-04 

2023-03-02 01:45:58,268 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2076 (0.2836) Acc D Real: 74.364% 
Loss D Fake: 0.4745 (0.5637) Acc D Fake: 89.035% 
Loss D: 0.682 
Loss G: 0.9832 (0.8575) Acc G: 10.461% 
LR: 2.000e-04 

2023-03-02 01:45:58,277 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2021 (0.2830) Acc D Real: 74.421% 
Loss D Fake: 0.4745 (0.5631) Acc D Fake: 89.107% 
Loss D: 0.677 
Loss G: 0.9836 (0.8583) Acc G: 10.392% 
LR: 2.000e-04 

2023-03-02 01:45:58,285 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.2590 (0.2829) Acc D Real: 74.438% 
Loss D Fake: 0.4742 (0.5625) Acc D Fake: 89.177% 
Loss D: 0.733 
Loss G: 0.9844 (0.8591) Acc G: 10.325% 
LR: 2.000e-04 

2023-03-02 01:45:58,293 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.2945 (0.2830) Acc D Real: 74.431% 
Loss D Fake: 0.4736 (0.5620) Acc D Fake: 89.247% 
Loss D: 0.768 
Loss G: 0.9854 (0.8599) Acc G: 10.258% 
LR: 2.000e-04 

2023-03-02 01:45:58,301 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.2257 (0.2826) Acc D Real: 74.472% 
Loss D Fake: 0.4730 (0.5614) Acc D Fake: 89.316% 
Loss D: 0.699 
Loss G: 0.9867 (0.8607) Acc G: 10.192% 
LR: 2.000e-04 

2023-03-02 01:45:58,309 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.2393 (0.2823) Acc D Real: 74.503% 
Loss D Fake: 0.4721 (0.5608) Acc D Fake: 89.384% 
Loss D: 0.711 
Loss G: 0.9885 (0.8616) Acc G: 10.127% 
LR: 2.000e-04 

2023-03-02 01:45:58,318 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3502 (0.2827) Acc D Real: 74.459% 
Loss D Fake: 0.4712 (0.5603) Acc D Fake: 89.451% 
Loss D: 0.821 
Loss G: 0.9899 (0.8624) Acc G: 10.063% 
LR: 2.000e-04 

2023-03-02 01:45:58,325 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2250 (0.2824) Acc D Real: 74.498% 
Loss D Fake: 0.4704 (0.5597) Acc D Fake: 89.518% 
Loss D: 0.695 
Loss G: 0.9915 (0.8632) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:45:58,333 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.1757 (0.2817) Acc D Real: 74.570% 
Loss D Fake: 0.4693 (0.5591) Acc D Fake: 89.583% 
Loss D: 0.645 
Loss G: 0.9939 (0.8640) Acc G: 9.938% 
LR: 2.000e-04 

2023-03-02 01:45:58,341 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2912 (0.2818) Acc D Real: 74.565% 
Loss D Fake: 0.4680 (0.5586) Acc D Fake: 89.648% 
Loss D: 0.759 
Loss G: 0.9962 (0.8648) Acc G: 9.876% 
LR: 2.000e-04 

2023-03-02 01:45:58,349 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3540 (0.2822) Acc D Real: 74.523% 
Loss D Fake: 0.4669 (0.5580) Acc D Fake: 89.712% 
Loss D: 0.821 
Loss G: 0.9978 (0.8656) Acc G: 9.815% 
LR: 2.000e-04 

2023-03-02 01:45:58,356 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3846 (0.2828) Acc D Real: 74.462% 
Loss D Fake: 0.4662 (0.5574) Acc D Fake: 89.775% 
Loss D: 0.851 
Loss G: 0.9986 (0.8665) Acc G: 9.755% 
LR: 2.000e-04 

2023-03-02 01:45:58,364 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2987 (0.2829) Acc D Real: 74.454% 
Loss D Fake: 0.4659 (0.5569) Acc D Fake: 89.837% 
Loss D: 0.765 
Loss G: 0.9992 (0.8673) Acc G: 9.695% 
LR: 2.000e-04 

2023-03-02 01:45:58,372 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2351 (0.2827) Acc D Real: 74.488% 
Loss D Fake: 0.4654 (0.5563) Acc D Fake: 89.899% 
Loss D: 0.701 
Loss G: 1.0002 (0.8681) Acc G: 9.636% 
LR: 2.000e-04 

2023-03-02 01:45:58,379 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4128 (0.2834) Acc D Real: 74.410% 
Loss D Fake: 0.4650 (0.5558) Acc D Fake: 89.960% 
Loss D: 0.878 
Loss G: 1.0006 (0.8689) Acc G: 9.578% 
LR: 2.000e-04 

2023-03-02 01:45:58,387 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2682 (0.2833) Acc D Real: 74.428% 
Loss D Fake: 0.4649 (0.5552) Acc D Fake: 90.020% 
Loss D: 0.733 
Loss G: 1.0009 (0.8697) Acc G: 9.521% 
LR: 2.000e-04 

2023-03-02 01:45:58,394 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.2335 (0.2830) Acc D Real: 74.461% 
Loss D Fake: 0.4646 (0.5547) Acc D Fake: 90.079% 
Loss D: 0.698 
Loss G: 1.0017 (0.8704) Acc G: 9.464% 
LR: 2.000e-04 

2023-03-02 01:45:58,401 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3188 (0.2833) Acc D Real: 74.444% 
Loss D Fake: 0.4641 (0.5542) Acc D Fake: 90.138% 
Loss D: 0.783 
Loss G: 1.0025 (0.8712) Acc G: 9.408% 
LR: 2.000e-04 

2023-03-02 01:45:58,410 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4126 (0.2840) Acc D Real: 74.368% 
Loss D Fake: 0.4643 (0.5536) Acc D Fake: 90.196% 
Loss D: 0.877 
Loss G: 1.0006 (0.8720) Acc G: 9.353% 
LR: 2.000e-04 

2023-03-02 01:45:58,417 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3509 (0.2844) Acc D Real: 74.331% 
Loss D Fake: 0.4662 (0.5531) Acc D Fake: 90.253% 
Loss D: 0.817 
Loss G: 0.9978 (0.8727) Acc G: 9.298% 
LR: 2.000e-04 

2023-03-02 01:45:58,424 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3525 (0.2848) Acc D Real: 74.297% 
Loss D Fake: 0.4681 (0.5526) Acc D Fake: 90.310% 
Loss D: 0.821 
Loss G: 0.9948 (0.8734) Acc G: 9.244% 
LR: 2.000e-04 

2023-03-02 01:45:58,432 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.2720 (0.2847) Acc D Real: 74.308% 
Loss D Fake: 0.4700 (0.5521) Acc D Fake: 90.366% 
Loss D: 0.742 
Loss G: 0.9923 (0.8741) Acc G: 9.191% 
LR: 2.000e-04 

2023-03-02 01:45:58,439 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3045 (0.2848) Acc D Real: 74.301% 
Loss D Fake: 0.4716 (0.5517) Acc D Fake: 90.421% 
Loss D: 0.776 
Loss G: 0.9902 (0.8748) Acc G: 9.138% 
LR: 2.000e-04 

2023-03-02 01:45:58,446 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.2831 (0.2848) Acc D Real: 74.305% 
Loss D Fake: 0.4729 (0.5512) Acc D Fake: 90.476% 
Loss D: 0.756 
Loss G: 0.9884 (0.8754) Acc G: 9.086% 
LR: 2.000e-04 

2023-03-02 01:45:58,454 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3533 (0.2852) Acc D Real: 74.269% 
Loss D Fake: 0.4742 (0.5508) Acc D Fake: 90.530% 
Loss D: 0.828 
Loss G: 0.9863 (0.8761) Acc G: 9.034% 
LR: 2.000e-04 

2023-03-02 01:45:58,461 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2093 (0.2848) Acc D Real: 74.317% 
Loss D Fake: 0.4756 (0.5504) Acc D Fake: 90.584% 
Loss D: 0.685 
Loss G: 0.9848 (0.8767) Acc G: 8.983% 
LR: 2.000e-04 

2023-03-02 01:45:58,468 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2342 (0.2845) Acc D Real: 74.350% 
Loss D Fake: 0.4765 (0.5500) Acc D Fake: 90.637% 
Loss D: 0.711 
Loss G: 0.9841 (0.8773) Acc G: 8.933% 
LR: 2.000e-04 

2023-03-02 01:45:58,475 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.1482 (0.2838) Acc D Real: 74.433% 
Loss D Fake: 0.4769 (0.5495) Acc D Fake: 90.689% 
Loss D: 0.625 
Loss G: 0.9844 (0.8779) Acc G: 8.883% 
LR: 2.000e-04 

2023-03-02 01:45:58,482 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3256 (0.2840) Acc D Real: 74.412% 
Loss D Fake: 0.4768 (0.5491) Acc D Fake: 90.741% 
Loss D: 0.802 
Loss G: 0.9847 (0.8785) Acc G: 8.833% 
LR: 2.000e-04 

2023-03-02 01:45:58,490 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.1486 (0.2832) Acc D Real: 74.490% 
Loss D Fake: 0.4767 (0.5487) Acc D Fake: 90.792% 
Loss D: 0.625 
Loss G: 0.9857 (0.8791) Acc G: 8.785% 
LR: 2.000e-04 

2023-03-02 01:45:58,497 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.2981 (0.2833) Acc D Real: 74.485% 
Loss D Fake: 0.4763 (0.5483) Acc D Fake: 90.842% 
Loss D: 0.774 
Loss G: 0.9867 (0.8797) Acc G: 8.736% 
LR: 2.000e-04 

2023-03-02 01:45:58,504 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2666 (0.2832) Acc D Real: 74.496% 
Loss D Fake: 0.4759 (0.5479) Acc D Fake: 90.893% 
Loss D: 0.743 
Loss G: 0.9876 (0.8803) Acc G: 8.689% 
LR: 2.000e-04 

2023-03-02 01:45:58,511 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3573 (0.2836) Acc D Real: 74.455% 
Loss D Fake: 0.4756 (0.5476) Acc D Fake: 90.942% 
Loss D: 0.833 
Loss G: 0.9880 (0.8808) Acc G: 8.641% 
LR: 2.000e-04 

2023-03-02 01:45:58,519 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2993 (0.2837) Acc D Real: 74.450% 
Loss D Fake: 0.4756 (0.5472) Acc D Fake: 90.991% 
Loss D: 0.775 
Loss G: 0.9883 (0.8814) Acc G: 8.595% 
LR: 2.000e-04 

2023-03-02 01:45:58,526 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1791 (0.2832) Acc D Real: 74.509% 
Loss D Fake: 0.4755 (0.5468) Acc D Fake: 91.039% 
Loss D: 0.655 
Loss G: 0.9891 (0.8820) Acc G: 8.548% 
LR: 2.000e-04 

2023-03-02 01:45:58,533 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.2283 (0.2829) Acc D Real: 74.542% 
Loss D Fake: 0.4750 (0.5464) Acc D Fake: 91.087% 
Loss D: 0.703 
Loss G: 0.9903 (0.8826) Acc G: 8.503% 
LR: 2.000e-04 

2023-03-02 01:45:58,542 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.1863 (0.2823) Acc D Real: 74.599% 
Loss D Fake: 0.4743 (0.5460) Acc D Fake: 91.135% 
Loss D: 0.661 
Loss G: 0.9921 (0.8832) Acc G: 8.457% 
LR: 2.000e-04 

2023-03-02 01:45:58,549 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.2720 (0.2823) Acc D Real: 74.608% 
Loss D Fake: 0.4733 (0.5456) Acc D Fake: 91.182% 
Loss D: 0.745 
Loss G: 0.9938 (0.8837) Acc G: 8.413% 
LR: 2.000e-04 

2023-03-02 01:45:58,556 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.2688 (0.2822) Acc D Real: 74.618% 
Loss D Fake: 0.4725 (0.5452) Acc D Fake: 91.228% 
Loss D: 0.741 
Loss G: 0.9954 (0.8843) Acc G: 8.368% 
LR: 2.000e-04 

2023-03-02 01:45:58,564 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.2773 (0.2822) Acc D Real: 74.627% 
Loss D Fake: 0.4718 (0.5449) Acc D Fake: 91.274% 
Loss D: 0.749 
Loss G: 0.9968 (0.8849) Acc G: 8.325% 
LR: 2.000e-04 

2023-03-02 01:45:58,571 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3234 (0.2824) Acc D Real: 74.606% 
Loss D Fake: 0.4712 (0.5445) Acc D Fake: 91.319% 
Loss D: 0.795 
Loss G: 0.9977 (0.8855) Acc G: 8.281% 
LR: 2.000e-04 

2023-03-02 01:45:58,578 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3653 (0.2828) Acc D Real: 74.569% 
Loss D Fake: 0.4710 (0.5441) Acc D Fake: 91.364% 
Loss D: 0.836 
Loss G: 0.9979 (0.8861) Acc G: 8.238% 
LR: 2.000e-04 

2023-03-02 01:45:58,587 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3103 (0.2830) Acc D Real: 74.560% 
Loss D Fake: 0.4711 (0.5437) Acc D Fake: 91.409% 
Loss D: 0.781 
Loss G: 0.9978 (0.8867) Acc G: 8.196% 
LR: 2.000e-04 

2023-03-02 01:45:58,596 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.2730 (0.2829) Acc D Real: 74.569% 
Loss D Fake: 0.4711 (0.5433) Acc D Fake: 91.453% 
Loss D: 0.744 
Loss G: 0.9978 (0.8872) Acc G: 8.154% 
LR: 2.000e-04 

2023-03-02 01:45:58,605 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3546 (0.2833) Acc D Real: 74.536% 
Loss D Fake: 0.4712 (0.5430) Acc D Fake: 91.497% 
Loss D: 0.826 
Loss G: 0.9975 (0.8878) Acc G: 8.112% 
LR: 2.000e-04 

2023-03-02 01:45:58,613 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3817 (0.2838) Acc D Real: 74.487% 
Loss D Fake: 0.4715 (0.5426) Acc D Fake: 91.540% 
Loss D: 0.853 
Loss G: 0.9966 (0.8884) Acc G: 8.071% 
LR: 2.000e-04 

2023-03-02 01:45:58,622 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2378 (0.2836) Acc D Real: 74.514% 
Loss D Fake: 0.4721 (0.5423) Acc D Fake: 91.582% 
Loss D: 0.710 
Loss G: 0.9960 (0.8889) Acc G: 8.030% 
LR: 2.000e-04 

2023-03-02 01:45:58,629 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2907 (0.2836) Acc D Real: 74.514% 
Loss D Fake: 0.4723 (0.5419) Acc D Fake: 91.625% 
Loss D: 0.763 
Loss G: 0.9956 (0.8894) Acc G: 7.990% 
LR: 2.000e-04 

2023-03-02 01:45:58,637 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2791 (0.2836) Acc D Real: 74.520% 
Loss D Fake: 0.4726 (0.5416) Acc D Fake: 91.667% 
Loss D: 0.752 
Loss G: 0.9953 (0.8900) Acc G: 7.950% 
LR: 2.000e-04 

2023-03-02 01:45:58,644 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2393 (0.2834) Acc D Real: 74.548% 
Loss D Fake: 0.4727 (0.5412) Acc D Fake: 91.708% 
Loss D: 0.712 
Loss G: 0.9953 (0.8905) Acc G: 7.910% 
LR: 2.000e-04 

2023-03-02 01:45:58,652 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1502 (0.2827) Acc D Real: 74.618% 
Loss D Fake: 0.4725 (0.5409) Acc D Fake: 91.749% 
Loss D: 0.623 
Loss G: 0.9963 (0.8910) Acc G: 7.871% 
LR: 2.000e-04 

2023-03-02 01:45:58,659 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2137 (0.2824) Acc D Real: 74.657% 
Loss D Fake: 0.4718 (0.5405) Acc D Fake: 91.790% 
Loss D: 0.686 
Loss G: 0.9977 (0.8915) Acc G: 7.833% 
LR: 2.000e-04 

2023-03-02 01:45:58,667 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.1769 (0.2818) Acc D Real: 74.711% 
Loss D Fake: 0.4710 (0.5402) Acc D Fake: 91.830% 
Loss D: 0.648 
Loss G: 0.9997 (0.8921) Acc G: 7.794% 
LR: 2.000e-04 

2023-03-02 01:45:58,674 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2478 (0.2817) Acc D Real: 74.733% 
Loss D Fake: 0.4699 (0.5399) Acc D Fake: 91.870% 
Loss D: 0.718 
Loss G: 1.0018 (0.8926) Acc G: 7.756% 
LR: 2.000e-04 

2023-03-02 01:45:58,682 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3370 (0.2819) Acc D Real: 74.710% 
Loss D Fake: 0.4689 (0.5395) Acc D Fake: 91.909% 
Loss D: 0.806 
Loss G: 1.0034 (0.8931) Acc G: 7.718% 
LR: 2.000e-04 

2023-03-02 01:45:58,689 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.1800 (0.2814) Acc D Real: 74.764% 
Loss D Fake: 0.4681 (0.5392) Acc D Fake: 91.948% 
Loss D: 0.648 
Loss G: 1.0051 (0.8937) Acc G: 7.681% 
LR: 2.000e-04 

2023-03-02 01:45:58,697 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.1245 (0.2807) Acc D Real: 74.845% 
Loss D Fake: 0.4671 (0.5388) Acc D Fake: 91.987% 
Loss D: 0.592 
Loss G: 1.0077 (0.8942) Acc G: 7.644% 
LR: 2.000e-04 

2023-03-02 01:45:58,704 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1778 (0.2802) Acc D Real: 74.899% 
Loss D Fake: 0.4656 (0.5385) Acc D Fake: 92.026% 
Loss D: 0.643 
Loss G: 1.0108 (0.8948) Acc G: 7.608% 
LR: 2.000e-04 

2023-03-02 01:45:58,712 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.1934 (0.2798) Acc D Real: 74.946% 
Loss D Fake: 0.4640 (0.5381) Acc D Fake: 92.063% 
Loss D: 0.657 
Loss G: 1.0139 (0.8954) Acc G: 7.571% 
LR: 2.000e-04 

2023-03-02 01:45:58,720 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2152 (0.2795) Acc D Real: 74.984% 
Loss D Fake: 0.4624 (0.5378) Acc D Fake: 92.101% 
Loss D: 0.678 
Loss G: 1.0171 (0.8959) Acc G: 7.536% 
LR: 2.000e-04 

2023-03-02 01:45:58,729 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3160 (0.2797) Acc D Real: 74.973% 
Loss D Fake: 0.4610 (0.5374) Acc D Fake: 92.138% 
Loss D: 0.777 
Loss G: 1.0197 (0.8965) Acc G: 7.500% 
LR: 2.000e-04 

2023-03-02 01:45:58,736 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2131 (0.2793) Acc D Real: 75.010% 
Loss D Fake: 0.4598 (0.5370) Acc D Fake: 92.175% 
Loss D: 0.673 
Loss G: 1.0221 (0.8971) Acc G: 7.465% 
LR: 2.000e-04 

2023-03-02 01:45:58,744 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.2398 (0.2792) Acc D Real: 75.036% 
Loss D Fake: 0.4586 (0.5367) Acc D Fake: 92.212% 
Loss D: 0.698 
Loss G: 1.0243 (0.8977) Acc G: 7.430% 
LR: 2.000e-04 

2023-03-02 01:45:58,751 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.2188 (0.2789) Acc D Real: 75.071% 
Loss D Fake: 0.4576 (0.5363) Acc D Fake: 92.248% 
Loss D: 0.676 
Loss G: 1.0265 (0.8983) Acc G: 7.395% 
LR: 2.000e-04 

2023-03-02 01:45:58,758 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2240 (0.2786) Acc D Real: 75.102% 
Loss D Fake: 0.4565 (0.5359) Acc D Fake: 92.284% 
Loss D: 0.681 
Loss G: 1.0288 (0.8989) Acc G: 7.361% 
LR: 2.000e-04 

2023-03-02 01:45:58,766 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3424 (0.2789) Acc D Real: 75.081% 
Loss D Fake: 0.4556 (0.5356) Acc D Fake: 92.320% 
Loss D: 0.798 
Loss G: 1.0303 (0.8995) Acc G: 7.327% 
LR: 2.000e-04 

2023-03-02 01:45:58,773 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3095 (0.2791) Acc D Real: 75.075% 
Loss D Fake: 0.4550 (0.5352) Acc D Fake: 92.355% 
Loss D: 0.765 
Loss G: 1.0313 (0.9001) Acc G: 7.294% 
LR: 2.000e-04 

2023-03-02 01:45:58,781 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2483 (0.2789) Acc D Real: 75.096% 
Loss D Fake: 0.4546 (0.5348) Acc D Fake: 92.390% 
Loss D: 0.703 
Loss G: 1.0323 (0.9007) Acc G: 7.260% 
LR: 2.000e-04 

2023-03-02 01:45:58,788 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2183 (0.2786) Acc D Real: 75.131% 
Loss D Fake: 0.4541 (0.5344) Acc D Fake: 92.424% 
Loss D: 0.672 
Loss G: 1.0335 (0.9013) Acc G: 7.227% 
LR: 2.000e-04 

2023-03-02 01:45:58,796 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2552 (0.2785) Acc D Real: 75.149% 
Loss D Fake: 0.4535 (0.5341) Acc D Fake: 92.459% 
Loss D: 0.709 
Loss G: 1.0347 (0.9019) Acc G: 7.195% 
LR: 2.000e-04 

2023-03-02 01:45:58,803 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2184 (0.2783) Acc D Real: 75.183% 
Loss D Fake: 0.4528 (0.5337) Acc D Fake: 92.492% 
Loss D: 0.671 
Loss G: 1.0362 (0.9025) Acc G: 7.162% 
LR: 2.000e-04 

2023-03-02 01:45:58,810 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.1014 (0.2775) Acc D Real: 75.267% 
Loss D Fake: 0.4520 (0.5333) Acc D Fake: 92.526% 
Loss D: 0.553 
Loss G: 1.0386 (0.9031) Acc G: 7.130% 
LR: 2.000e-04 

2023-03-02 01:45:58,818 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2575 (0.2774) Acc D Real: 75.283% 
Loss D Fake: 0.4507 (0.5330) Acc D Fake: 92.560% 
Loss D: 0.708 
Loss G: 1.0411 (0.9038) Acc G: 7.098% 
LR: 2.000e-04 

2023-03-02 01:45:58,825 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3649 (0.2778) Acc D Real: 75.254% 
Loss D Fake: 0.4497 (0.5326) Acc D Fake: 92.593% 
Loss D: 0.815 
Loss G: 1.0426 (0.9044) Acc G: 7.067% 
LR: 2.000e-04 

2023-03-02 01:45:58,832 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2962 (0.2779) Acc D Real: 75.254% 
Loss D Fake: 0.4492 (0.5322) Acc D Fake: 92.601% 
Loss D: 0.745 
Loss G: 1.0436 (0.9050) Acc G: 7.059% 
LR: 2.000e-04 

2023-03-02 01:45:58,842 -                train: [    INFO] - 
Epoch: 6/20
2023-03-02 01:45:59,007 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2853 (0.2803) Acc D Real: 77.161% 
Loss D Fake: 0.4486 (0.4488) Acc D Fake: 100.000% 
Loss D: 0.734 
Loss G: 1.0448 (1.0445) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,016 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.2222 (0.2609) Acc D Real: 78.924% 
Loss D Fake: 0.4484 (0.4486) Acc D Fake: 100.000% 
Loss D: 0.671 
Loss G: 1.0455 (1.0448) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,023 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2248 (0.2519) Acc D Real: 79.596% 
Loss D Fake: 0.4481 (0.4485) Acc D Fake: 100.000% 
Loss D: 0.673 
Loss G: 1.0463 (1.0452) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,040 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2539 (0.2523) Acc D Real: 79.531% 
Loss D Fake: 0.4478 (0.4484) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 1.0471 (1.0456) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,047 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3115 (0.2622) Acc D Real: 78.602% 
Loss D Fake: 0.4476 (0.4482) Acc D Fake: 100.000% 
Loss D: 0.759 
Loss G: 1.0475 (1.0459) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,054 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2560 (0.2613) Acc D Real: 78.795% 
Loss D Fake: 0.4475 (0.4481) Acc D Fake: 100.000% 
Loss D: 0.703 
Loss G: 1.0477 (1.0462) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,062 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3234 (0.2690) Acc D Real: 78.034% 
Loss D Fake: 0.4477 (0.4481) Acc D Fake: 100.000% 
Loss D: 0.771 
Loss G: 1.0473 (1.0463) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,068 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2503 (0.2670) Acc D Real: 78.264% 
Loss D Fake: 0.4481 (0.4481) Acc D Fake: 100.000% 
Loss D: 0.698 
Loss G: 1.0470 (1.0464) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,075 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1782 (0.2581) Acc D Real: 79.078% 
Loss D Fake: 0.4482 (0.4481) Acc D Fake: 100.000% 
Loss D: 0.626 
Loss G: 1.0474 (1.0465) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,082 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1653 (0.2497) Acc D Real: 79.867% 
Loss D Fake: 0.4480 (0.4481) Acc D Fake: 100.000% 
Loss D: 0.613 
Loss G: 1.0484 (1.0467) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,089 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1543 (0.2417) Acc D Real: 80.690% 
Loss D Fake: 0.4475 (0.4480) Acc D Fake: 100.000% 
Loss D: 0.602 
Loss G: 1.0500 (1.0469) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,096 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.1189 (0.2323) Acc D Real: 81.571% 
Loss D Fake: 0.4468 (0.4479) Acc D Fake: 100.000% 
Loss D: 0.566 
Loss G: 1.0523 (1.0474) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,103 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2261 (0.2318) Acc D Real: 81.637% 
Loss D Fake: 0.4461 (0.4478) Acc D Fake: 100.000% 
Loss D: 0.672 
Loss G: 1.0532 (1.0478) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,110 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3862 (0.2421) Acc D Real: 80.681% 
Loss D Fake: 0.4466 (0.4477) Acc D Fake: 100.000% 
Loss D: 0.833 
Loss G: 1.0525 (1.0481) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,117 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3889 (0.2513) Acc D Real: 79.837% 
Loss D Fake: 0.4477 (0.4477) Acc D Fake: 100.000% 
Loss D: 0.837 
Loss G: 1.0507 (1.0482) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,124 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.2260 (0.2498) Acc D Real: 79.985% 
Loss D Fake: 0.4490 (0.4478) Acc D Fake: 100.000% 
Loss D: 0.675 
Loss G: 1.0492 (1.0483) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,131 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1907 (0.2465) Acc D Real: 80.312% 
Loss D Fake: 0.4498 (0.4479) Acc D Fake: 100.000% 
Loss D: 0.641 
Loss G: 1.0485 (1.0483) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:45:59,138 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4147 (0.2554) Acc D Real: 79.534% 
Loss D Fake: 0.4624 (0.4487) Acc D Fake: 100.000% 
Loss D: 0.877 
Loss G: 0.2622 (1.0069) Acc G: 4.123% 
LR: 2.000e-04 

2023-03-02 01:45:59,145 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1597 (0.2506) Acc D Real: 79.969% 
Loss D Fake: 3.9349 (0.6230) Acc D Fake: 95.833% 
Loss D: 4.095 
Loss G: 0.1812 (0.9657) Acc G: 8.167% 
LR: 2.000e-04 

2023-03-02 01:45:59,152 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3189 (0.2538) Acc D Real: 79.653% 
Loss D Fake: 4.1284 (0.7899) Acc D Fake: 91.905% 
Loss D: 4.447 
Loss G: 0.1629 (0.9274) Acc G: 11.984% 
LR: 2.000e-04 

2023-03-02 01:45:59,159 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1869 (0.2508) Acc D Real: 79.910% 
Loss D Fake: 4.1828 (0.9441) Acc D Fake: 88.258% 
Loss D: 4.370 
Loss G: 0.1546 (0.8923) Acc G: 15.455% 
LR: 2.000e-04 

2023-03-02 01:45:59,166 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1888 (0.2481) Acc D Real: 80.170% 
Loss D Fake: 4.1846 (1.0850) Acc D Fake: 84.928% 
Loss D: 4.373 
Loss G: 0.1505 (0.8600) Acc G: 18.623% 
LR: 2.000e-04 

2023-03-02 01:45:59,173 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.2320 (0.2474) Acc D Real: 80.189% 
Loss D Fake: 4.1574 (1.2130) Acc D Fake: 81.875% 
Loss D: 4.389 
Loss G: 0.1487 (0.8304) Acc G: 21.528% 
LR: 2.000e-04 

2023-03-02 01:45:59,179 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2012 (0.2456) Acc D Real: 80.344% 
Loss D Fake: 4.1124 (1.3290) Acc D Fake: 79.067% 
Loss D: 4.314 
Loss G: 0.1485 (0.8031) Acc G: 24.200% 
LR: 2.000e-04 

2023-03-02 01:45:59,186 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.1393 (0.2415) Acc D Real: 80.697% 
Loss D Fake: 4.0552 (1.4339) Acc D Fake: 76.474% 
Loss D: 4.194 
Loss G: 0.1493 (0.7780) Acc G: 26.667% 
LR: 2.000e-04 

2023-03-02 01:45:59,194 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3167 (0.2443) Acc D Real: 80.411% 
Loss D Fake: 3.9901 (1.5285) Acc D Fake: 74.074% 
Loss D: 4.307 
Loss G: 0.1508 (0.7548) Acc G: 28.951% 
LR: 2.000e-04 

2023-03-02 01:45:59,201 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1689 (0.2416) Acc D Real: 80.645% 
Loss D Fake: 3.9187 (1.6139) Acc D Fake: 71.845% 
Loss D: 4.088 
Loss G: 0.1531 (0.7333) Acc G: 31.071% 
LR: 2.000e-04 

2023-03-02 01:45:59,209 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.2629 (0.2423) Acc D Real: 80.544% 
Loss D Fake: 3.8435 (1.6908) Acc D Fake: 69.770% 
Loss D: 4.106 
Loss G: 0.1558 (0.7134) Acc G: 33.046% 
LR: 2.000e-04 

2023-03-02 01:45:59,216 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.1910 (0.2406) Acc D Real: 80.688% 
Loss D Fake: 3.7658 (1.7600) Acc D Fake: 67.835% 
Loss D: 3.957 
Loss G: 0.1591 (0.6949) Acc G: 34.833% 
LR: 2.000e-04 

2023-03-02 01:45:59,224 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.1895 (0.2390) Acc D Real: 80.822% 
Loss D Fake: 3.6861 (1.8221) Acc D Fake: 66.077% 
Loss D: 3.876 
Loss G: 0.1628 (0.6777) Acc G: 36.505% 
LR: 2.000e-04 

2023-03-02 01:45:59,231 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1837 (0.2372) Acc D Real: 80.975% 
Loss D Fake: 3.6051 (1.8778) Acc D Fake: 64.429% 
Loss D: 3.789 
Loss G: 0.1671 (0.6618) Acc G: 38.073% 
LR: 2.000e-04 

2023-03-02 01:45:59,239 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.1257 (0.2339) Acc D Real: 81.291% 
Loss D Fake: 3.5234 (1.9277) Acc D Fake: 62.880% 
Loss D: 3.649 
Loss G: 0.1719 (0.6469) Acc G: 39.495% 
LR: 2.000e-04 

2023-03-02 01:45:59,246 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1114 (0.2303) Acc D Real: 81.621% 
Loss D Fake: 3.4411 (1.9722) Acc D Fake: 61.472% 
Loss D: 3.553 
Loss G: 0.1772 (0.6331) Acc G: 40.833% 
LR: 2.000e-04 

2023-03-02 01:45:59,254 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1312 (0.2274) Acc D Real: 81.879% 
Loss D Fake: 3.3589 (2.0118) Acc D Fake: 60.144% 
Loss D: 3.490 
Loss G: 0.1830 (0.6202) Acc G: 42.057% 
LR: 2.000e-04 

2023-03-02 01:45:59,261 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1804 (0.2261) Acc D Real: 81.985% 
Loss D Fake: 3.2765 (2.0469) Acc D Fake: 58.937% 
Loss D: 3.457 
Loss G: 0.1894 (0.6083) Acc G: 43.203% 
LR: 2.000e-04 

2023-03-02 01:45:59,268 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3141 (0.2285) Acc D Real: 81.722% 
Loss D Fake: 3.1933 (2.0779) Acc D Fake: 57.794% 
Loss D: 3.507 
Loss G: 0.1967 (0.5971) Acc G: 44.243% 
LR: 2.000e-04 

2023-03-02 01:45:59,276 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2162 (0.2282) Acc D Real: 81.705% 
Loss D Fake: 3.1104 (2.1051) Acc D Fake: 56.756% 
Loss D: 3.327 
Loss G: 0.2040 (0.5868) Acc G: 45.228% 
LR: 2.000e-04 

2023-03-02 01:45:59,283 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1578 (0.2264) Acc D Real: 81.867% 
Loss D Fake: 3.0298 (2.1288) Acc D Fake: 55.771% 
Loss D: 3.188 
Loss G: 0.2121 (0.5772) Acc G: 46.119% 
LR: 2.000e-04 

2023-03-02 01:45:59,291 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2736 (0.2275) Acc D Real: 81.715% 
Loss D Fake: 2.9471 (2.1493) Acc D Fake: 54.876% 
Loss D: 3.221 
Loss G: 0.2214 (0.5683) Acc G: 46.966% 
LR: 2.000e-04 

2023-03-02 01:45:59,298 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.2228 (0.2274) Acc D Real: 81.709% 
Loss D Fake: 2.8616 (2.1666) Acc D Fake: 54.066% 
Loss D: 3.084 
Loss G: 0.2325 (0.5601) Acc G: 47.731% 
LR: 2.000e-04 

2023-03-02 01:45:59,305 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1321 (0.2252) Acc D Real: 81.930% 
Loss D Fake: 2.7722 (2.1811) Acc D Fake: 53.335% 
Loss D: 2.904 
Loss G: 0.2455 (0.5526) Acc G: 48.420% 
LR: 2.000e-04 

2023-03-02 01:45:59,313 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1744 (0.2240) Acc D Real: 82.045% 
Loss D Fake: 2.6777 (2.1926) Acc D Fake: 52.676% 
Loss D: 2.852 
Loss G: 0.2613 (0.5458) Acc G: 49.038% 
LR: 2.000e-04 

2023-03-02 01:45:59,320 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.1969 (0.2234) Acc D Real: 82.095% 
Loss D Fake: 2.5739 (2.2013) Acc D Fake: 52.085% 
Loss D: 2.771 
Loss G: 0.2816 (0.5398) Acc G: 49.553% 
LR: 2.000e-04 

2023-03-02 01:45:59,328 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2556 (0.2241) Acc D Real: 81.997% 
Loss D Fake: 2.4555 (2.2069) Acc D Fake: 51.594% 
Loss D: 2.711 
Loss G: 0.3088 (0.5347) Acc G: 49.970% 
LR: 2.000e-04 

2023-03-02 01:45:59,335 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2495 (0.2246) Acc D Real: 81.927% 
Loss D Fake: 2.3143 (2.2093) Acc D Fake: 51.197% 
Loss D: 2.564 
Loss G: 0.3459 (0.5306) Acc G: 50.286% 
LR: 2.000e-04 

2023-03-02 01:45:59,342 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.2447 (0.2251) Acc D Real: 81.863% 
Loss D Fake: 2.1308 (2.2076) Acc D Fake: 50.923% 
Loss D: 2.375 
Loss G: 0.4151 (0.5281) Acc G: 50.422% 
LR: 2.000e-04 

2023-03-02 01:45:59,350 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.2413 (0.2254) Acc D Real: 81.812% 
Loss D Fake: 1.7634 (2.1983) Acc D Fake: 50.904% 
Loss D: 2.005 
Loss G: 0.9925 (0.5378) Acc G: 49.372% 
LR: 2.000e-04 

2023-03-02 01:45:59,357 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2628 (0.2262) Acc D Real: 81.737% 
Loss D Fake: 0.4409 (2.1625) Acc D Fake: 51.906% 
Loss D: 0.704 
Loss G: 1.0795 (0.5489) Acc G: 48.364% 
LR: 2.000e-04 

2023-03-02 01:45:59,365 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2605 (0.2269) Acc D Real: 81.652% 
Loss D Fake: 0.4223 (2.1277) Acc D Fake: 52.868% 
Loss D: 0.683 
Loss G: 1.1020 (0.5599) Acc G: 47.397% 
LR: 2.000e-04 

2023-03-02 01:45:59,372 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.1511 (0.2254) Acc D Real: 81.819% 
Loss D Fake: 0.4144 (2.0941) Acc D Fake: 53.792% 
Loss D: 0.565 
Loss G: 1.1141 (0.5708) Acc G: 46.468% 
LR: 2.000e-04 

2023-03-02 01:45:59,379 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2223 (0.2253) Acc D Real: 81.837% 
Loss D Fake: 0.4097 (2.0617) Acc D Fake: 54.680% 
Loss D: 0.632 
Loss G: 1.1218 (0.5814) Acc G: 45.574% 
LR: 2.000e-04 

2023-03-02 01:45:59,387 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2684 (0.2261) Acc D Real: 81.733% 
Loss D Fake: 0.4067 (2.0304) Acc D Fake: 55.536% 
Loss D: 0.675 
Loss G: 1.1272 (0.5917) Acc G: 44.714% 
LR: 2.000e-04 

2023-03-02 01:45:59,395 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2931 (0.2274) Acc D Real: 81.584% 
Loss D Fake: 0.4045 (2.0003) Acc D Fake: 56.359% 
Loss D: 0.698 
Loss G: 1.1318 (0.6017) Acc G: 43.886% 
LR: 2.000e-04 

2023-03-02 01:45:59,403 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2541 (0.2278) Acc D Real: 81.538% 
Loss D Fake: 0.4025 (1.9713) Acc D Fake: 57.152% 
Loss D: 0.657 
Loss G: 1.1363 (0.6114) Acc G: 43.088% 
LR: 2.000e-04 

2023-03-02 01:45:59,410 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3228 (0.2295) Acc D Real: 81.324% 
Loss D Fake: 0.4006 (1.9432) Acc D Fake: 57.918% 
Loss D: 0.723 
Loss G: 1.1406 (0.6209) Acc G: 42.319% 
LR: 2.000e-04 

2023-03-02 01:45:59,417 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2206 (0.2294) Acc D Real: 81.354% 
Loss D Fake: 0.3988 (1.9161) Acc D Fake: 58.656% 
Loss D: 0.619 
Loss G: 1.1449 (0.6301) Acc G: 41.576% 
LR: 2.000e-04 

2023-03-02 01:45:59,425 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3303 (0.2311) Acc D Real: 81.158% 
Loss D Fake: 0.3971 (1.8900) Acc D Fake: 59.369% 
Loss D: 0.727 
Loss G: 1.1483 (0.6390) Acc G: 40.859% 
LR: 2.000e-04 

2023-03-02 01:45:59,432 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.2101 (0.2308) Acc D Real: 81.188% 
Loss D Fake: 0.3958 (1.8646) Acc D Fake: 60.057% 
Loss D: 0.606 
Loss G: 1.1519 (0.6477) Acc G: 40.167% 
LR: 2.000e-04 

2023-03-02 01:45:59,440 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4624 (0.2346) Acc D Real: 80.730% 
Loss D Fake: 0.3943 (1.8401) Acc D Fake: 60.723% 
Loss D: 0.857 
Loss G: 1.1555 (0.6561) Acc G: 39.497% 
LR: 2.000e-04 

2023-03-02 01:45:59,447 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.2803 (0.2354) Acc D Real: 80.633% 
Loss D Fake: 0.3929 (1.8164) Acc D Fake: 61.367% 
Loss D: 0.673 
Loss G: 1.1592 (0.6644) Acc G: 38.850% 
LR: 2.000e-04 

2023-03-02 01:45:59,454 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2704 (0.2359) Acc D Real: 80.575% 
Loss D Fake: 0.3915 (1.7934) Acc D Fake: 61.990% 
Loss D: 0.662 
Loss G: 1.1624 (0.6724) Acc G: 38.223% 
LR: 2.000e-04 

2023-03-02 01:45:59,461 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2995 (0.2370) Acc D Real: 80.430% 
Loss D Fake: 0.3903 (1.7711) Acc D Fake: 62.593% 
Loss D: 0.690 
Loss G: 1.1657 (0.6803) Acc G: 37.617% 
LR: 2.000e-04 

2023-03-02 01:45:59,469 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.2914 (0.2378) Acc D Real: 80.349% 
Loss D Fake: 0.3890 (1.7495) Acc D Fake: 63.178% 
Loss D: 0.680 
Loss G: 1.1690 (0.6879) Acc G: 37.029% 
LR: 2.000e-04 

2023-03-02 01:45:59,476 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.2333 (0.2377) Acc D Real: 80.333% 
Loss D Fake: 0.3877 (1.7286) Acc D Fake: 63.744% 
Loss D: 0.621 
Loss G: 1.1725 (0.6953) Acc G: 36.459% 
LR: 2.000e-04 

2023-03-02 01:45:59,483 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3211 (0.2390) Acc D Real: 80.186% 
Loss D Fake: 0.3864 (1.7083) Acc D Fake: 64.294% 
Loss D: 0.707 
Loss G: 1.1756 (0.7026) Acc G: 35.907% 
LR: 2.000e-04 

2023-03-02 01:45:59,491 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1996 (0.2384) Acc D Real: 80.232% 
Loss D Fake: 0.3853 (1.6885) Acc D Fake: 64.827% 
Loss D: 0.585 
Loss G: 1.1787 (0.7097) Acc G: 35.371% 
LR: 2.000e-04 

2023-03-02 01:45:59,499 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.1955 (0.2378) Acc D Real: 80.290% 
Loss D Fake: 0.3840 (1.6693) Acc D Fake: 65.344% 
Loss D: 0.580 
Loss G: 1.1821 (0.7167) Acc G: 34.851% 
LR: 2.000e-04 

2023-03-02 01:45:59,506 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.2632 (0.2381) Acc D Real: 80.215% 
Loss D Fake: 0.3827 (1.6507) Acc D Fake: 65.846% 
Loss D: 0.646 
Loss G: 1.1859 (0.7235) Acc G: 34.346% 
LR: 2.000e-04 

2023-03-02 01:45:59,514 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.2928 (0.2389) Acc D Real: 80.160% 
Loss D Fake: 0.3813 (1.6325) Acc D Fake: 66.334% 
Loss D: 0.674 
Loss G: 1.1894 (0.7301) Acc G: 33.855% 
LR: 2.000e-04 

2023-03-02 01:45:59,521 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2876 (0.2396) Acc D Real: 80.050% 
Loss D Fake: 0.3800 (1.6149) Acc D Fake: 66.808% 
Loss D: 0.668 
Loss G: 1.1931 (0.7367) Acc G: 33.378% 
LR: 2.000e-04 

2023-03-02 01:45:59,528 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3220 (0.2408) Acc D Real: 79.917% 
Loss D Fake: 0.3786 (1.5977) Acc D Fake: 67.269% 
Loss D: 0.701 
Loss G: 1.1968 (0.7430) Acc G: 32.914% 
LR: 2.000e-04 

2023-03-02 01:45:59,536 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3262 (0.2419) Acc D Real: 79.759% 
Loss D Fake: 0.3773 (1.5810) Acc D Fake: 67.718% 
Loss D: 0.703 
Loss G: 1.2005 (0.7493) Acc G: 32.464% 
LR: 2.000e-04 

2023-03-02 01:45:59,543 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.2376 (0.2419) Acc D Real: 79.761% 
Loss D Fake: 0.3759 (1.5647) Acc D Fake: 68.154% 
Loss D: 0.613 
Loss G: 1.2043 (0.7555) Acc G: 32.025% 
LR: 2.000e-04 

2023-03-02 01:45:59,550 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3842 (0.2438) Acc D Real: 79.533% 
Loss D Fake: 0.3813 (1.5489) Acc D Fake: 68.578% 
Loss D: 0.765 
Loss G: 0.8222 (0.7563) Acc G: 31.665% 
LR: 2.000e-04 

2023-03-02 01:45:59,557 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.2312 (0.2436) Acc D Real: 79.539% 
Loss D Fake: 0.5950 (1.5364) Acc D Fake: 68.948% 
Loss D: 0.826 
Loss G: 0.7987 (0.7569) Acc G: 31.314% 
LR: 2.000e-04 

2023-03-02 01:45:59,565 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2537 (0.2437) Acc D Real: 79.491% 
Loss D Fake: 0.6017 (1.5243) Acc D Fake: 69.286% 
Loss D: 0.855 
Loss G: 0.7935 (0.7574) Acc G: 30.972% 
LR: 2.000e-04 

2023-03-02 01:45:59,572 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3611 (0.2452) Acc D Real: 79.261% 
Loss D Fake: 0.6045 (1.5125) Acc D Fake: 69.616% 
Loss D: 0.966 
Loss G: 0.7909 (0.7578) Acc G: 30.660% 
LR: 2.000e-04 

2023-03-02 01:45:59,579 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.1777 (0.2444) Acc D Real: 79.345% 
Loss D Fake: 0.6060 (1.5010) Acc D Fake: 69.937% 
Loss D: 0.784 
Loss G: 0.7901 (0.7582) Acc G: 30.336% 
LR: 2.000e-04 

2023-03-02 01:45:59,586 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3502 (0.2457) Acc D Real: 79.148% 
Loss D Fake: 0.6061 (1.4898) Acc D Fake: 70.230% 
Loss D: 0.956 
Loss G: 0.7901 (0.7586) Acc G: 30.019% 
LR: 2.000e-04 

2023-03-02 01:45:59,593 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.2095 (0.2453) Acc D Real: 79.185% 
Loss D Fake: 0.6058 (1.4789) Acc D Fake: 70.536% 
Loss D: 0.815 
Loss G: 0.7911 (0.7590) Acc G: 29.710% 
LR: 2.000e-04 

2023-03-02 01:45:59,601 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2706 (0.2456) Acc D Real: 79.122% 
Loss D Fake: 0.6046 (1.4682) Acc D Fake: 70.834% 
Loss D: 0.875 
Loss G: 0.7929 (0.7594) Acc G: 29.388% 
LR: 2.000e-04 

2023-03-02 01:45:59,608 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.1767 (0.2447) Acc D Real: 79.212% 
Loss D Fake: 0.6028 (1.4578) Acc D Fake: 71.125% 
Loss D: 0.780 
Loss G: 0.7956 (0.7599) Acc G: 29.095% 
LR: 2.000e-04 

2023-03-02 01:45:59,615 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3994 (0.2466) Acc D Real: 78.946% 
Loss D Fake: 0.6005 (1.4476) Acc D Fake: 71.409% 
Loss D: 1.000 
Loss G: 0.7980 (0.7603) Acc G: 28.808% 
LR: 2.000e-04 

2023-03-02 01:45:59,623 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3190 (0.2474) Acc D Real: 78.811% 
Loss D Fake: 0.5987 (1.4376) Acc D Fake: 71.687% 
Loss D: 0.918 
Loss G: 0.8002 (0.7608) Acc G: 28.528% 
LR: 2.000e-04 

2023-03-02 01:45:59,630 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3768 (0.2489) Acc D Real: 78.587% 
Loss D Fake: 0.5971 (1.4278) Acc D Fake: 71.958% 
Loss D: 0.974 
Loss G: 0.8020 (0.7613) Acc G: 28.254% 
LR: 2.000e-04 

2023-03-02 01:45:59,637 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3417 (0.2500) Acc D Real: 78.439% 
Loss D Fake: 0.5957 (1.4183) Acc D Fake: 72.223% 
Loss D: 0.937 
Loss G: 0.8036 (0.7618) Acc G: 27.968% 
LR: 2.000e-04 

2023-03-02 01:45:59,645 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3508 (0.2511) Acc D Real: 78.275% 
Loss D Fake: 0.5945 (1.4089) Acc D Fake: 72.501% 
Loss D: 0.945 
Loss G: 0.8051 (0.7622) Acc G: 27.688% 
LR: 2.000e-04 

2023-03-02 01:45:59,652 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3947 (0.2528) Acc D Real: 78.040% 
Loss D Fake: 0.5934 (1.3997) Acc D Fake: 72.772% 
Loss D: 0.988 
Loss G: 0.8062 (0.7627) Acc G: 27.414% 
LR: 2.000e-04 

2023-03-02 01:45:59,660 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.2769 (0.2530) Acc D Real: 77.987% 
Loss D Fake: 0.5926 (1.3908) Acc D Fake: 73.038% 
Loss D: 0.869 
Loss G: 0.8075 (0.7632) Acc G: 27.146% 
LR: 2.000e-04 

2023-03-02 01:45:59,667 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.2978 (0.2535) Acc D Real: 77.906% 
Loss D Fake: 0.5913 (1.3820) Acc D Fake: 73.297% 
Loss D: 0.889 
Loss G: 0.8092 (0.7637) Acc G: 26.885% 
LR: 2.000e-04 

2023-03-02 01:45:59,674 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.2693 (0.2537) Acc D Real: 77.864% 
Loss D Fake: 0.5899 (1.3734) Acc D Fake: 73.551% 
Loss D: 0.859 
Loss G: 0.8113 (0.7643) Acc G: 26.629% 
LR: 2.000e-04 

2023-03-02 01:45:59,681 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2926 (0.2541) Acc D Real: 77.788% 
Loss D Fake: 0.5882 (1.3649) Acc D Fake: 73.800% 
Loss D: 0.881 
Loss G: 0.8135 (0.7648) Acc G: 26.378% 
LR: 2.000e-04 

2023-03-02 01:45:59,689 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4062 (0.2557) Acc D Real: 77.567% 
Loss D Fake: 0.5866 (1.3567) Acc D Fake: 74.043% 
Loss D: 0.993 
Loss G: 0.8152 (0.7653) Acc G: 26.133% 
LR: 2.000e-04 

2023-03-02 01:45:59,696 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3081 (0.2563) Acc D Real: 77.480% 
Loss D Fake: 0.5853 (1.3485) Acc D Fake: 74.281% 
Loss D: 0.893 
Loss G: 0.8170 (0.7659) Acc G: 25.893% 
LR: 2.000e-04 

2023-03-02 01:45:59,704 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2740 (0.2565) Acc D Real: 77.442% 
Loss D Fake: 0.5838 (1.3406) Acc D Fake: 74.514% 
Loss D: 0.858 
Loss G: 0.8190 (0.7664) Acc G: 25.658% 
LR: 2.000e-04 

2023-03-02 01:45:59,713 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.2728 (0.2566) Acc D Real: 77.403% 
Loss D Fake: 0.5821 (1.3328) Acc D Fake: 74.743% 
Loss D: 0.855 
Loss G: 0.8214 (0.7670) Acc G: 25.428% 
LR: 2.000e-04 

2023-03-02 01:45:59,722 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2919 (0.2570) Acc D Real: 77.337% 
Loss D Fake: 0.5803 (1.3251) Acc D Fake: 74.967% 
Loss D: 0.872 
Loss G: 0.8238 (0.7676) Acc G: 25.202% 
LR: 2.000e-04 

2023-03-02 01:45:59,730 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3276 (0.2577) Acc D Real: 77.234% 
Loss D Fake: 0.5784 (1.3175) Acc D Fake: 75.186% 
Loss D: 0.906 
Loss G: 0.8261 (0.7682) Acc G: 24.982% 
LR: 2.000e-04 

2023-03-02 01:45:59,739 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2790 (0.2579) Acc D Real: 77.191% 
Loss D Fake: 0.5766 (1.3101) Acc D Fake: 75.401% 
Loss D: 0.856 
Loss G: 0.8286 (0.7688) Acc G: 24.748% 
LR: 2.000e-04 

2023-03-02 01:45:59,747 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3038 (0.2584) Acc D Real: 77.119% 
Loss D Fake: 0.5747 (1.3028) Acc D Fake: 75.644% 
Loss D: 0.878 
Loss G: 0.8312 (0.7694) Acc G: 24.503% 
LR: 2.000e-04 

2023-03-02 01:45:59,756 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2278 (0.2581) Acc D Real: 77.143% 
Loss D Fake: 0.5725 (1.2957) Acc D Fake: 75.883% 
Loss D: 0.800 
Loss G: 0.8343 (0.7700) Acc G: 24.263% 
LR: 2.000e-04 

2023-03-02 01:45:59,765 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3511 (0.2590) Acc D Real: 77.012% 
Loss D Fake: 0.5702 (1.2886) Acc D Fake: 76.117% 
Loss D: 0.921 
Loss G: 0.8372 (0.7707) Acc G: 24.028% 
LR: 2.000e-04 

2023-03-02 01:45:59,773 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3335 (0.2597) Acc D Real: 76.904% 
Loss D Fake: 0.5683 (1.2817) Acc D Fake: 76.347% 
Loss D: 0.902 
Loss G: 0.8397 (0.7713) Acc G: 23.797% 
LR: 2.000e-04 

2023-03-02 01:45:59,781 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4093 (0.2611) Acc D Real: 76.710% 
Loss D Fake: 0.5665 (1.2749) Acc D Fake: 76.572% 
Loss D: 0.976 
Loss G: 0.8416 (0.7720) Acc G: 23.570% 
LR: 2.000e-04 

2023-03-02 01:45:59,788 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3064 (0.2615) Acc D Real: 76.644% 
Loss D Fake: 0.5653 (1.2682) Acc D Fake: 76.793% 
Loss D: 0.872 
Loss G: 0.8434 (0.7727) Acc G: 23.348% 
LR: 2.000e-04 

2023-03-02 01:45:59,795 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2994 (0.2619) Acc D Real: 76.584% 
Loss D Fake: 0.5639 (1.2616) Acc D Fake: 77.010% 
Loss D: 0.863 
Loss G: 0.8453 (0.7734) Acc G: 23.129% 
LR: 2.000e-04 

2023-03-02 01:45:59,803 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2682 (0.2620) Acc D Real: 76.565% 
Loss D Fake: 0.5624 (1.2552) Acc D Fake: 77.223% 
Loss D: 0.831 
Loss G: 0.8475 (0.7740) Acc G: 22.915% 
LR: 2.000e-04 

2023-03-02 01:45:59,810 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3061 (0.2624) Acc D Real: 76.502% 
Loss D Fake: 0.5607 (1.2488) Acc D Fake: 77.432% 
Loss D: 0.867 
Loss G: 0.8498 (0.7747) Acc G: 22.705% 
LR: 2.000e-04 

2023-03-02 01:45:59,817 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4024 (0.2636) Acc D Real: 76.330% 
Loss D Fake: 0.5592 (1.2425) Acc D Fake: 77.637% 
Loss D: 0.962 
Loss G: 0.8515 (0.7754) Acc G: 22.499% 
LR: 2.000e-04 

2023-03-02 01:45:59,824 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3001 (0.2640) Acc D Real: 76.274% 
Loss D Fake: 0.5581 (1.2363) Acc D Fake: 77.838% 
Loss D: 0.858 
Loss G: 0.8531 (0.7761) Acc G: 22.296% 
LR: 2.000e-04 

2023-03-02 01:45:59,831 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3729 (0.2649) Acc D Real: 76.146% 
Loss D Fake: 0.5570 (1.2303) Acc D Fake: 78.036% 
Loss D: 0.930 
Loss G: 0.8545 (0.7768) Acc G: 22.097% 
LR: 2.000e-04 

2023-03-02 01:45:59,840 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3314 (0.2655) Acc D Real: 76.061% 
Loss D Fake: 0.5561 (1.2243) Acc D Fake: 78.231% 
Loss D: 0.887 
Loss G: 0.8558 (0.7775) Acc G: 21.901% 
LR: 2.000e-04 

2023-03-02 01:45:59,849 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2571 (0.2655) Acc D Real: 76.060% 
Loss D Fake: 0.5551 (1.2184) Acc D Fake: 78.422% 
Loss D: 0.812 
Loss G: 0.8574 (0.7782) Acc G: 21.709% 
LR: 2.000e-04 

2023-03-02 01:45:59,859 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3013 (0.2658) Acc D Real: 76.015% 
Loss D Fake: 0.5538 (1.2127) Acc D Fake: 78.609% 
Loss D: 0.855 
Loss G: 0.8592 (0.7789) Acc G: 21.520% 
LR: 2.000e-04 

2023-03-02 01:45:59,869 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3709 (0.2667) Acc D Real: 75.893% 
Loss D Fake: 0.5526 (1.2070) Acc D Fake: 78.794% 
Loss D: 0.924 
Loss G: 0.8607 (0.7796) Acc G: 21.335% 
LR: 2.000e-04 

2023-03-02 01:45:59,879 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3620 (0.2675) Acc D Real: 75.782% 
Loss D Fake: 0.5517 (1.2014) Acc D Fake: 78.975% 
Loss D: 0.914 
Loss G: 0.8618 (0.7804) Acc G: 21.153% 
LR: 2.000e-04 

2023-03-02 01:45:59,889 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3312 (0.2680) Acc D Real: 75.705% 
Loss D Fake: 0.5510 (1.1959) Acc D Fake: 79.153% 
Loss D: 0.882 
Loss G: 0.8629 (0.7811) Acc G: 20.973% 
LR: 2.000e-04 

2023-03-02 01:45:59,898 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3137 (0.2684) Acc D Real: 75.653% 
Loss D Fake: 0.5502 (1.1904) Acc D Fake: 79.328% 
Loss D: 0.864 
Loss G: 0.8640 (0.7817) Acc G: 20.797% 
LR: 2.000e-04 

2023-03-02 01:45:59,905 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3249 (0.2689) Acc D Real: 75.586% 
Loss D Fake: 0.5493 (1.1851) Acc D Fake: 79.500% 
Loss D: 0.874 
Loss G: 0.8652 (0.7824) Acc G: 20.624% 
LR: 2.000e-04 

2023-03-02 01:45:59,913 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2150 (0.2684) Acc D Real: 75.636% 
Loss D Fake: 0.5483 (1.1798) Acc D Fake: 79.670% 
Loss D: 0.763 
Loss G: 0.8671 (0.7831) Acc G: 20.453% 
LR: 2.000e-04 

2023-03-02 01:45:59,920 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.2897 (0.2686) Acc D Real: 75.607% 
Loss D Fake: 0.5469 (1.1746) Acc D Fake: 79.836% 
Loss D: 0.837 
Loss G: 0.8693 (0.7838) Acc G: 20.286% 
LR: 2.000e-04 

2023-03-02 01:45:59,927 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3040 (0.2689) Acc D Real: 75.567% 
Loss D Fake: 0.5453 (1.1695) Acc D Fake: 80.000% 
Loss D: 0.849 
Loss G: 0.8715 (0.7846) Acc G: 20.121% 
LR: 2.000e-04 

2023-03-02 01:45:59,935 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3888 (0.2699) Acc D Real: 75.443% 
Loss D Fake: 0.5439 (1.1645) Acc D Fake: 80.162% 
Loss D: 0.933 
Loss G: 0.8732 (0.7853) Acc G: 19.958% 
LR: 2.000e-04 

2023-03-02 01:45:59,942 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3659 (0.2706) Acc D Real: 75.344% 
Loss D Fake: 0.5428 (1.1595) Acc D Fake: 80.320% 
Loss D: 0.909 
Loss G: 0.8746 (0.7860) Acc G: 19.799% 
LR: 2.000e-04 

2023-03-02 01:45:59,949 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.2289 (0.2703) Acc D Real: 75.377% 
Loss D Fake: 0.5418 (1.1546) Acc D Fake: 80.477% 
Loss D: 0.771 
Loss G: 0.8764 (0.7867) Acc G: 19.642% 
LR: 2.000e-04 

2023-03-02 01:45:59,956 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.2229 (0.2699) Acc D Real: 75.424% 
Loss D Fake: 0.5404 (1.1498) Acc D Fake: 80.630% 
Loss D: 0.763 
Loss G: 0.8789 (0.7874) Acc G: 19.487% 
LR: 2.000e-04 

2023-03-02 01:45:59,964 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3509 (0.2706) Acc D Real: 75.337% 
Loss D Fake: 0.5386 (1.1450) Acc D Fake: 80.782% 
Loss D: 0.889 
Loss G: 0.8812 (0.7882) Acc G: 19.335% 
LR: 2.000e-04 

2023-03-02 01:45:59,971 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.2870 (0.2707) Acc D Real: 75.320% 
Loss D Fake: 0.5371 (1.1403) Acc D Fake: 80.931% 
Loss D: 0.824 
Loss G: 0.8835 (0.7889) Acc G: 19.185% 
LR: 2.000e-04 

2023-03-02 01:45:59,978 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.2554 (0.2706) Acc D Real: 75.332% 
Loss D Fake: 0.5355 (1.1356) Acc D Fake: 81.077% 
Loss D: 0.791 
Loss G: 0.8859 (0.7897) Acc G: 19.037% 
LR: 2.000e-04 

2023-03-02 01:45:59,986 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.2673 (0.2705) Acc D Real: 75.335% 
Loss D Fake: 0.5338 (1.1310) Acc D Fake: 81.222% 
Loss D: 0.801 
Loss G: 0.8886 (0.7904) Acc G: 18.892% 
LR: 2.000e-04 

2023-03-02 01:45:59,993 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3023 (0.2708) Acc D Real: 75.301% 
Loss D Fake: 0.5320 (1.1265) Acc D Fake: 81.364% 
Loss D: 0.834 
Loss G: 0.8911 (0.7912) Acc G: 18.749% 
LR: 2.000e-04 

2023-03-02 01:46:00,001 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.2963 (0.2710) Acc D Real: 75.276% 
Loss D Fake: 0.5304 (1.1220) Acc D Fake: 81.504% 
Loss D: 0.827 
Loss G: 0.8937 (0.7919) Acc G: 18.608% 
LR: 2.000e-04 

2023-03-02 01:46:00,008 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3447 (0.2715) Acc D Real: 75.212% 
Loss D Fake: 0.5287 (1.1176) Acc D Fake: 81.642% 
Loss D: 0.873 
Loss G: 0.8959 (0.7927) Acc G: 18.469% 
LR: 2.000e-04 

2023-03-02 01:46:00,016 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.2836 (0.2716) Acc D Real: 75.201% 
Loss D Fake: 0.5273 (1.1132) Acc D Fake: 81.778% 
Loss D: 0.811 
Loss G: 0.8981 (0.7935) Acc G: 18.332% 
LR: 2.000e-04 

2023-03-02 01:46:00,023 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3188 (0.2720) Acc D Real: 75.160% 
Loss D Fake: 0.5259 (1.1089) Acc D Fake: 81.912% 
Loss D: 0.845 
Loss G: 0.9001 (0.7943) Acc G: 18.197% 
LR: 2.000e-04 

2023-03-02 01:46:00,030 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3284 (0.2724) Acc D Real: 75.118% 
Loss D Fake: 0.5246 (1.1046) Acc D Fake: 82.044% 
Loss D: 0.853 
Loss G: 0.9021 (0.7951) Acc G: 18.065% 
LR: 2.000e-04 

2023-03-02 01:46:00,037 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.2813 (0.2724) Acc D Real: 75.120% 
Loss D Fake: 0.5233 (1.1004) Acc D Fake: 82.174% 
Loss D: 0.805 
Loss G: 0.9041 (0.7959) Acc G: 17.934% 
LR: 2.000e-04 

2023-03-02 01:46:00,045 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.2998 (0.2726) Acc D Real: 75.097% 
Loss D Fake: 0.5220 (1.0963) Acc D Fake: 82.303% 
Loss D: 0.822 
Loss G: 0.9062 (0.7967) Acc G: 17.805% 
LR: 2.000e-04 

2023-03-02 01:46:00,052 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2970 (0.2728) Acc D Real: 75.076% 
Loss D Fake: 0.5206 (1.0921) Acc D Fake: 82.429% 
Loss D: 0.818 
Loss G: 0.9083 (0.7974) Acc G: 17.677% 
LR: 2.000e-04 

2023-03-02 01:46:00,059 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3190 (0.2731) Acc D Real: 75.040% 
Loss D Fake: 0.5193 (1.0881) Acc D Fake: 82.554% 
Loss D: 0.838 
Loss G: 0.9103 (0.7982) Acc G: 17.552% 
LR: 2.000e-04 

2023-03-02 01:46:00,067 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3713 (0.2738) Acc D Real: 74.961% 
Loss D Fake: 0.5181 (1.0841) Acc D Fake: 82.676% 
Loss D: 0.889 
Loss G: 0.9118 (0.7990) Acc G: 17.428% 
LR: 2.000e-04 

2023-03-02 01:46:00,074 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2975 (0.2740) Acc D Real: 74.945% 
Loss D Fake: 0.5172 (1.0801) Acc D Fake: 82.798% 
Loss D: 0.815 
Loss G: 0.9134 (0.7998) Acc G: 17.307% 
LR: 2.000e-04 

2023-03-02 01:46:00,081 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.2404 (0.2738) Acc D Real: 74.977% 
Loss D Fake: 0.5161 (1.0762) Acc D Fake: 82.917% 
Loss D: 0.756 
Loss G: 0.9154 (0.8007) Acc G: 17.186% 
LR: 2.000e-04 

2023-03-02 01:46:00,088 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2710 (0.2737) Acc D Real: 74.989% 
Loss D Fake: 0.5147 (1.0723) Acc D Fake: 83.035% 
Loss D: 0.786 
Loss G: 0.9176 (0.8015) Acc G: 17.068% 
LR: 2.000e-04 

2023-03-02 01:46:00,096 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.2434 (0.2735) Acc D Real: 75.014% 
Loss D Fake: 0.5132 (1.0685) Acc D Fake: 83.151% 
Loss D: 0.757 
Loss G: 0.9202 (0.8023) Acc G: 16.951% 
LR: 2.000e-04 

2023-03-02 01:46:00,103 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1889 (0.2730) Acc D Real: 75.081% 
Loss D Fake: 0.5114 (1.0647) Acc D Fake: 83.266% 
Loss D: 0.700 
Loss G: 0.9234 (0.8031) Acc G: 16.836% 
LR: 2.000e-04 

2023-03-02 01:46:00,110 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.2934 (0.2731) Acc D Real: 75.067% 
Loss D Fake: 0.5094 (1.0609) Acc D Fake: 83.379% 
Loss D: 0.803 
Loss G: 0.9266 (0.8039) Acc G: 16.722% 
LR: 2.000e-04 

2023-03-02 01:46:00,118 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4748 (0.2745) Acc D Real: 74.922% 
Loss D Fake: 0.5077 (1.0572) Acc D Fake: 83.490% 
Loss D: 0.982 
Loss G: 0.9285 (0.8048) Acc G: 16.610% 
LR: 2.000e-04 

2023-03-02 01:46:00,125 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.2661 (0.2744) Acc D Real: 74.933% 
Loss D Fake: 0.5067 (1.0536) Acc D Fake: 83.600% 
Loss D: 0.773 
Loss G: 0.9302 (0.8056) Acc G: 16.499% 
LR: 2.000e-04 

2023-03-02 01:46:00,132 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3319 (0.2748) Acc D Real: 74.895% 
Loss D Fake: 0.5057 (1.0499) Acc D Fake: 83.709% 
Loss D: 0.838 
Loss G: 0.9318 (0.8064) Acc G: 16.390% 
LR: 2.000e-04 

2023-03-02 01:46:00,140 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3086 (0.2750) Acc D Real: 74.876% 
Loss D Fake: 0.5047 (1.0463) Acc D Fake: 83.816% 
Loss D: 0.813 
Loss G: 0.9333 (0.8073) Acc G: 16.282% 
LR: 2.000e-04 

2023-03-02 01:46:00,148 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2762 (0.2750) Acc D Real: 74.880% 
Loss D Fake: 0.5038 (1.0428) Acc D Fake: 83.922% 
Loss D: 0.780 
Loss G: 0.9349 (0.8081) Acc G: 16.175% 
LR: 2.000e-04 

2023-03-02 01:46:00,155 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3062 (0.2752) Acc D Real: 74.862% 
Loss D Fake: 0.5028 (1.0393) Acc D Fake: 84.026% 
Loss D: 0.809 
Loss G: 0.9366 (0.8089) Acc G: 16.070% 
LR: 2.000e-04 

2023-03-02 01:46:00,162 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.2964 (0.2753) Acc D Real: 74.852% 
Loss D Fake: 0.5017 (1.0358) Acc D Fake: 84.129% 
Loss D: 0.798 
Loss G: 0.9383 (0.8098) Acc G: 15.967% 
LR: 2.000e-04 

2023-03-02 01:46:00,170 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4065 (0.2762) Acc D Real: 74.765% 
Loss D Fake: 0.5009 (1.0324) Acc D Fake: 84.231% 
Loss D: 0.907 
Loss G: 0.9393 (0.8106) Acc G: 15.864% 
LR: 2.000e-04 

2023-03-02 01:46:00,178 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4073 (0.2770) Acc D Real: 74.675% 
Loss D Fake: 0.5005 (1.0290) Acc D Fake: 84.332% 
Loss D: 0.908 
Loss G: 0.9396 (0.8114) Acc G: 15.763% 
LR: 2.000e-04 

2023-03-02 01:46:00,186 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2973 (0.2771) Acc D Real: 74.668% 
Loss D Fake: 0.5004 (1.0257) Acc D Fake: 84.431% 
Loss D: 0.798 
Loss G: 0.9399 (0.8122) Acc G: 15.664% 
LR: 2.000e-04 

2023-03-02 01:46:00,193 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.5731 (0.2790) Acc D Real: 74.462% 
Loss D Fake: 0.5005 (1.0224) Acc D Fake: 84.529% 
Loss D: 1.074 
Loss G: 0.9389 (0.8130) Acc G: 15.565% 
LR: 2.000e-04 

2023-03-02 01:46:00,201 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.2578 (0.2789) Acc D Real: 74.481% 
Loss D Fake: 0.5012 (1.0191) Acc D Fake: 84.625% 
Loss D: 0.759 
Loss G: 0.9381 (0.8138) Acc G: 15.468% 
LR: 2.000e-04 

2023-03-02 01:46:00,208 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4324 (0.2798) Acc D Real: 74.377% 
Loss D Fake: 0.5016 (1.0159) Acc D Fake: 84.721% 
Loss D: 0.934 
Loss G: 0.9371 (0.8146) Acc G: 15.372% 
LR: 2.000e-04 

2023-03-02 01:46:00,215 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.2753 (0.2798) Acc D Real: 74.390% 
Loss D Fake: 0.5023 (1.0127) Acc D Fake: 84.815% 
Loss D: 0.778 
Loss G: 0.9364 (0.8153) Acc G: 15.277% 
LR: 2.000e-04 

2023-03-02 01:46:00,222 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.2991 (0.2799) Acc D Real: 74.383% 
Loss D Fake: 0.5026 (1.0096) Acc D Fake: 84.908% 
Loss D: 0.802 
Loss G: 0.9360 (0.8161) Acc G: 15.183% 
LR: 2.000e-04 

2023-03-02 01:46:00,230 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4379 (0.2809) Acc D Real: 74.282% 
Loss D Fake: 0.5030 (1.0065) Acc D Fake: 85.000% 
Loss D: 0.941 
Loss G: 0.9352 (0.8168) Acc G: 15.091% 
LR: 2.000e-04 

2023-03-02 01:46:00,238 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2672 (0.2808) Acc D Real: 74.295% 
Loss D Fake: 0.5035 (1.0035) Acc D Fake: 85.091% 
Loss D: 0.771 
Loss G: 0.9347 (0.8175) Acc G: 14.999% 
LR: 2.000e-04 

2023-03-02 01:46:00,246 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3617 (0.2813) Acc D Real: 74.245% 
Loss D Fake: 0.5037 (1.0004) Acc D Fake: 85.181% 
Loss D: 0.865 
Loss G: 0.9344 (0.8182) Acc G: 14.909% 
LR: 2.000e-04 

2023-03-02 01:46:00,253 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3881 (0.2819) Acc D Real: 74.178% 
Loss D Fake: 0.5039 (0.9975) Acc D Fake: 85.270% 
Loss D: 0.892 
Loss G: 0.9338 (0.8189) Acc G: 14.819% 
LR: 2.000e-04 

2023-03-02 01:46:00,260 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4534 (0.2829) Acc D Real: 74.064% 
Loss D Fake: 0.5045 (0.9945) Acc D Fake: 85.357% 
Loss D: 0.958 
Loss G: 0.9326 (0.8196) Acc G: 14.731% 
LR: 2.000e-04 

2023-03-02 01:46:00,268 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3281 (0.2832) Acc D Real: 74.037% 
Loss D Fake: 0.5053 (0.9916) Acc D Fake: 85.444% 
Loss D: 0.833 
Loss G: 0.9316 (0.8203) Acc G: 14.644% 
LR: 2.000e-04 

2023-03-02 01:46:00,275 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3979 (0.2839) Acc D Real: 73.963% 
Loss D Fake: 0.5059 (0.9888) Acc D Fake: 85.530% 
Loss D: 0.904 
Loss G: 0.9304 (0.8209) Acc G: 14.558% 
LR: 2.000e-04 

2023-03-02 01:46:00,283 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3832 (0.2845) Acc D Real: 73.899% 
Loss D Fake: 0.5067 (0.9860) Acc D Fake: 85.614% 
Loss D: 0.890 
Loss G: 0.9291 (0.8215) Acc G: 14.473% 
LR: 2.000e-04 

2023-03-02 01:46:00,291 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.2730 (0.2844) Acc D Real: 73.910% 
Loss D Fake: 0.5074 (0.9832) Acc D Fake: 85.698% 
Loss D: 0.780 
Loss G: 0.9285 (0.8222) Acc G: 14.389% 
LR: 2.000e-04 

2023-03-02 01:46:00,298 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4017 (0.2851) Acc D Real: 73.834% 
Loss D Fake: 0.5077 (0.9804) Acc D Fake: 85.781% 
Loss D: 0.909 
Loss G: 0.9277 (0.8228) Acc G: 14.305% 
LR: 2.000e-04 

2023-03-02 01:46:00,305 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3538 (0.2855) Acc D Real: 73.793% 
Loss D Fake: 0.5082 (0.9777) Acc D Fake: 85.862% 
Loss D: 0.862 
Loss G: 0.9269 (0.8234) Acc G: 14.223% 
LR: 2.000e-04 

2023-03-02 01:46:00,313 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.1956 (0.2850) Acc D Real: 73.854% 
Loss D Fake: 0.5085 (0.9750) Acc D Fake: 85.943% 
Loss D: 0.704 
Loss G: 0.9271 (0.8240) Acc G: 14.142% 
LR: 2.000e-04 

2023-03-02 01:46:00,320 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3042 (0.2851) Acc D Real: 73.846% 
Loss D Fake: 0.5082 (0.9724) Acc D Fake: 86.023% 
Loss D: 0.812 
Loss G: 0.9277 (0.8245) Acc G: 14.062% 
LR: 2.000e-04 

2023-03-02 01:46:00,328 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2435 (0.2848) Acc D Real: 73.874% 
Loss D Fake: 0.5077 (0.9698) Acc D Fake: 86.102% 
Loss D: 0.751 
Loss G: 0.9288 (0.8251) Acc G: 13.982% 
LR: 2.000e-04 

2023-03-02 01:46:00,335 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4332 (0.2857) Acc D Real: 73.783% 
Loss D Fake: 0.5071 (0.9672) Acc D Fake: 86.180% 
Loss D: 0.940 
Loss G: 0.9293 (0.8257) Acc G: 13.904% 
LR: 2.000e-04 

2023-03-02 01:46:00,343 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3203 (0.2859) Acc D Real: 73.764% 
Loss D Fake: 0.5070 (0.9646) Acc D Fake: 86.257% 
Loss D: 0.827 
Loss G: 0.9296 (0.8263) Acc G: 13.826% 
LR: 2.000e-04 

2023-03-02 01:46:00,350 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3555 (0.2863) Acc D Real: 73.725% 
Loss D Fake: 0.5068 (0.9620) Acc D Fake: 86.334% 
Loss D: 0.862 
Loss G: 0.9298 (0.8269) Acc G: 13.749% 
LR: 2.000e-04 

2023-03-02 01:46:00,357 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.2875 (0.2863) Acc D Real: 73.724% 
Loss D Fake: 0.5067 (0.9595) Acc D Fake: 86.409% 
Loss D: 0.794 
Loss G: 0.9301 (0.8274) Acc G: 13.673% 
LR: 2.000e-04 

2023-03-02 01:46:00,366 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3181 (0.2864) Acc D Real: 73.704% 
Loss D Fake: 0.5065 (0.9570) Acc D Fake: 86.484% 
Loss D: 0.825 
Loss G: 0.9305 (0.8280) Acc G: 13.598% 
LR: 2.000e-04 

2023-03-02 01:46:00,373 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2550 (0.2863) Acc D Real: 73.726% 
Loss D Fake: 0.5061 (0.9546) Acc D Fake: 86.558% 
Loss D: 0.761 
Loss G: 0.9315 (0.8286) Acc G: 13.524% 
LR: 2.000e-04 

2023-03-02 01:46:00,381 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3821 (0.2868) Acc D Real: 73.669% 
Loss D Fake: 0.5055 (0.9521) Acc D Fake: 86.631% 
Loss D: 0.888 
Loss G: 0.9321 (0.8291) Acc G: 13.450% 
LR: 2.000e-04 

2023-03-02 01:46:00,388 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2411 (0.2865) Acc D Real: 73.699% 
Loss D Fake: 0.5051 (0.9497) Acc D Fake: 86.703% 
Loss D: 0.746 
Loss G: 0.9332 (0.8297) Acc G: 13.378% 
LR: 2.000e-04 

2023-03-02 01:46:00,395 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3764 (0.2870) Acc D Real: 73.644% 
Loss D Fake: 0.5044 (0.9473) Acc D Fake: 86.774% 
Loss D: 0.881 
Loss G: 0.9340 (0.8303) Acc G: 13.306% 
LR: 2.000e-04 

2023-03-02 01:46:00,402 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.2181 (0.2867) Acc D Real: 73.688% 
Loss D Fake: 0.5039 (0.9450) Acc D Fake: 86.845% 
Loss D: 0.722 
Loss G: 0.9353 (0.8308) Acc G: 13.234% 
LR: 2.000e-04 

2023-03-02 01:46:00,410 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3321 (0.2869) Acc D Real: 73.663% 
Loss D Fake: 0.5031 (0.9426) Acc D Fake: 86.915% 
Loss D: 0.835 
Loss G: 0.9365 (0.8314) Acc G: 13.164% 
LR: 2.000e-04 

2023-03-02 01:46:00,417 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.2495 (0.2867) Acc D Real: 73.689% 
Loss D Fake: 0.5023 (0.9403) Acc D Fake: 86.984% 
Loss D: 0.752 
Loss G: 0.9380 (0.8320) Acc G: 13.094% 
LR: 2.000e-04 

2023-03-02 01:46:00,424 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3490 (0.2870) Acc D Real: 73.656% 
Loss D Fake: 0.5014 (0.9380) Acc D Fake: 87.053% 
Loss D: 0.850 
Loss G: 0.9393 (0.8325) Acc G: 13.025% 
LR: 2.000e-04 

2023-03-02 01:46:00,432 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3269 (0.2872) Acc D Real: 73.635% 
Loss D Fake: 0.5008 (0.9357) Acc D Fake: 87.121% 
Loss D: 0.828 
Loss G: 0.9403 (0.8331) Acc G: 12.957% 
LR: 2.000e-04 

2023-03-02 01:46:00,439 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4659 (0.2882) Acc D Real: 73.535% 
Loss D Fake: 0.5004 (0.9334) Acc D Fake: 87.188% 
Loss D: 0.966 
Loss G: 0.9402 (0.8336) Acc G: 12.890% 
LR: 2.000e-04 

2023-03-02 01:46:00,446 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.2710 (0.2881) Acc D Real: 73.547% 
Loss D Fake: 0.5005 (0.9312) Acc D Fake: 87.254% 
Loss D: 0.772 
Loss G: 0.9404 (0.8342) Acc G: 12.823% 
LR: 2.000e-04 

2023-03-02 01:46:00,454 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3497 (0.2884) Acc D Real: 73.516% 
Loss D Fake: 0.5004 (0.9289) Acc D Fake: 87.320% 
Loss D: 0.850 
Loss G: 0.9404 (0.8347) Acc G: 12.757% 
LR: 2.000e-04 

2023-03-02 01:46:00,461 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3001 (0.2885) Acc D Real: 73.513% 
Loss D Fake: 0.5004 (0.9267) Acc D Fake: 87.385% 
Loss D: 0.800 
Loss G: 0.9405 (0.8353) Acc G: 12.692% 
LR: 2.000e-04 

2023-03-02 01:46:00,468 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.2917 (0.2885) Acc D Real: 73.513% 
Loss D Fake: 0.5003 (0.9246) Acc D Fake: 87.449% 
Loss D: 0.792 
Loss G: 0.9409 (0.8358) Acc G: 12.627% 
LR: 2.000e-04 

2023-03-02 01:46:00,475 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.2420 (0.2882) Acc D Real: 73.543% 
Loss D Fake: 0.5000 (0.9224) Acc D Fake: 87.513% 
Loss D: 0.742 
Loss G: 0.9417 (0.8364) Acc G: 12.563% 
LR: 2.000e-04 

2023-03-02 01:46:00,482 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3169 (0.2884) Acc D Real: 73.529% 
Loss D Fake: 0.4994 (0.9203) Acc D Fake: 87.576% 
Loss D: 0.816 
Loss G: 0.9426 (0.8369) Acc G: 12.499% 
LR: 2.000e-04 

2023-03-02 01:46:00,489 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3222 (0.2885) Acc D Real: 73.513% 
Loss D Fake: 0.4989 (0.9182) Acc D Fake: 87.638% 
Loss D: 0.821 
Loss G: 0.9433 (0.8374) Acc G: 12.436% 
LR: 2.000e-04 

2023-03-02 01:46:00,496 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2975 (0.2886) Acc D Real: 73.512% 
Loss D Fake: 0.4985 (0.9161) Acc D Fake: 87.700% 
Loss D: 0.796 
Loss G: 0.9441 (0.8380) Acc G: 12.374% 
LR: 2.000e-04 

2023-03-02 01:46:00,503 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3162 (0.2887) Acc D Real: 73.499% 
Loss D Fake: 0.4981 (0.9140) Acc D Fake: 87.761% 
Loss D: 0.814 
Loss G: 0.9448 (0.8385) Acc G: 12.313% 
LR: 2.000e-04 

2023-03-02 01:46:00,511 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.2080 (0.2883) Acc D Real: 73.551% 
Loss D Fake: 0.4975 (0.9119) Acc D Fake: 87.822% 
Loss D: 0.706 
Loss G: 0.9461 (0.8390) Acc G: 12.252% 
LR: 2.000e-04 

2023-03-02 01:46:00,518 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2794 (0.2883) Acc D Real: 73.560% 
Loss D Fake: 0.4966 (0.9099) Acc D Fake: 87.882% 
Loss D: 0.776 
Loss G: 0.9478 (0.8396) Acc G: 12.191% 
LR: 2.000e-04 

2023-03-02 01:46:00,525 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4970 (0.2893) Acc D Real: 73.452% 
Loss D Fake: 0.4958 (0.9078) Acc D Fake: 87.941% 
Loss D: 0.993 
Loss G: 0.9483 (0.8401) Acc G: 12.132% 
LR: 2.000e-04 

2023-03-02 01:46:00,532 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2661 (0.2892) Acc D Real: 73.466% 
Loss D Fake: 0.4957 (0.9058) Acc D Fake: 88.000% 
Loss D: 0.762 
Loss G: 0.9488 (0.8406) Acc G: 12.072% 
LR: 2.000e-04 

2023-03-02 01:46:00,539 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3163 (0.2893) Acc D Real: 73.455% 
Loss D Fake: 0.4954 (0.9038) Acc D Fake: 88.059% 
Loss D: 0.812 
Loss G: 0.9493 (0.8412) Acc G: 12.014% 
LR: 2.000e-04 

2023-03-02 01:46:00,546 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4430 (0.2901) Acc D Real: 73.377% 
Loss D Fake: 0.4953 (0.9019) Acc D Fake: 88.116% 
Loss D: 0.938 
Loss G: 0.9490 (0.8417) Acc G: 11.956% 
LR: 2.000e-04 

2023-03-02 01:46:00,553 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2695 (0.2900) Acc D Real: 73.392% 
Loss D Fake: 0.4955 (0.8999) Acc D Fake: 88.173% 
Loss D: 0.765 
Loss G: 0.9488 (0.8422) Acc G: 11.898% 
LR: 2.000e-04 

2023-03-02 01:46:00,560 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.2996 (0.2900) Acc D Real: 73.389% 
Loss D Fake: 0.4955 (0.8980) Acc D Fake: 88.230% 
Loss D: 0.795 
Loss G: 0.9490 (0.8427) Acc G: 11.841% 
LR: 2.000e-04 

2023-03-02 01:46:00,567 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3284 (0.2902) Acc D Real: 73.372% 
Loss D Fake: 0.4954 (0.8961) Acc D Fake: 88.286% 
Loss D: 0.824 
Loss G: 0.9492 (0.8432) Acc G: 11.785% 
LR: 2.000e-04 

2023-03-02 01:46:00,574 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2349 (0.2899) Acc D Real: 73.402% 
Loss D Fake: 0.4952 (0.8942) Acc D Fake: 88.341% 
Loss D: 0.730 
Loss G: 0.9498 (0.8437) Acc G: 11.729% 
LR: 2.000e-04 

2023-03-02 01:46:00,581 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.2779 (0.2899) Acc D Real: 73.415% 
Loss D Fake: 0.4947 (0.8923) Acc D Fake: 88.396% 
Loss D: 0.773 
Loss G: 0.9508 (0.8442) Acc G: 11.674% 
LR: 2.000e-04 

2023-03-02 01:46:00,589 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2974 (0.2899) Acc D Real: 73.415% 
Loss D Fake: 0.4941 (0.8904) Acc D Fake: 88.451% 
Loss D: 0.792 
Loss G: 0.9519 (0.8447) Acc G: 11.619% 
LR: 2.000e-04 

2023-03-02 01:46:00,597 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3362 (0.2901) Acc D Real: 73.397% 
Loss D Fake: 0.4936 (0.8886) Acc D Fake: 88.505% 
Loss D: 0.830 
Loss G: 0.9527 (0.8452) Acc G: 11.565% 
LR: 2.000e-04 

2023-03-02 01:46:00,604 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3011 (0.2902) Acc D Real: 73.395% 
Loss D Fake: 0.4931 (0.8867) Acc D Fake: 88.558% 
Loss D: 0.794 
Loss G: 0.9536 (0.8457) Acc G: 11.511% 
LR: 2.000e-04 

2023-03-02 01:46:00,611 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3184 (0.2903) Acc D Real: 73.384% 
Loss D Fake: 0.4927 (0.8849) Acc D Fake: 88.611% 
Loss D: 0.811 
Loss G: 0.9543 (0.8462) Acc G: 11.458% 
LR: 2.000e-04 

2023-03-02 01:46:00,618 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2543 (0.2901) Acc D Real: 73.407% 
Loss D Fake: 0.4922 (0.8831) Acc D Fake: 88.664% 
Loss D: 0.746 
Loss G: 0.9553 (0.8467) Acc G: 11.405% 
LR: 2.000e-04 

2023-03-02 01:46:00,625 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2446 (0.2899) Acc D Real: 73.433% 
Loss D Fake: 0.4915 (0.8813) Acc D Fake: 88.716% 
Loss D: 0.736 
Loss G: 0.9568 (0.8472) Acc G: 11.352% 
LR: 2.000e-04 

2023-03-02 01:46:00,632 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2664 (0.2898) Acc D Real: 73.450% 
Loss D Fake: 0.4906 (0.8795) Acc D Fake: 88.767% 
Loss D: 0.757 
Loss G: 0.9584 (0.8477) Acc G: 11.301% 
LR: 2.000e-04 

2023-03-02 01:46:00,639 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2718 (0.2897) Acc D Real: 73.463% 
Loss D Fake: 0.4896 (0.8777) Acc D Fake: 88.818% 
Loss D: 0.761 
Loss G: 0.9602 (0.8483) Acc G: 11.249% 
LR: 2.000e-04 

2023-03-02 01:46:00,646 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2398 (0.2895) Acc D Real: 73.493% 
Loss D Fake: 0.4885 (0.8760) Acc D Fake: 88.869% 
Loss D: 0.728 
Loss G: 0.9622 (0.8488) Acc G: 11.198% 
LR: 2.000e-04 

2023-03-02 01:46:00,654 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2400 (0.2893) Acc D Real: 73.522% 
Loss D Fake: 0.4873 (0.8742) Acc D Fake: 88.919% 
Loss D: 0.727 
Loss G: 0.9645 (0.8493) Acc G: 11.148% 
LR: 2.000e-04 

2023-03-02 01:46:00,661 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3015 (0.2894) Acc D Real: 73.521% 
Loss D Fake: 0.4860 (0.8725) Acc D Fake: 88.969% 
Loss D: 0.788 
Loss G: 0.9667 (0.8498) Acc G: 11.098% 
LR: 2.000e-04 

2023-03-02 01:46:00,668 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3273 (0.2895) Acc D Real: 73.507% 
Loss D Fake: 0.4849 (0.8707) Acc D Fake: 89.018% 
Loss D: 0.812 
Loss G: 0.9684 (0.8504) Acc G: 11.048% 
LR: 2.000e-04 

2023-03-02 01:46:00,675 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3823 (0.2899) Acc D Real: 73.466% 
Loss D Fake: 0.4842 (0.8690) Acc D Fake: 89.067% 
Loss D: 0.866 
Loss G: 0.9694 (0.8509) Acc G: 10.999% 
LR: 2.000e-04 

2023-03-02 01:46:00,682 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.5218 (0.2910) Acc D Real: 73.440% 
Loss D Fake: 0.4840 (0.8673) Acc D Fake: 89.079% 
Loss D: 1.006 
Loss G: 0.9688 (0.8514) Acc G: 10.987% 
LR: 2.000e-04 

2023-03-02 01:46:00,693 -                train: [    INFO] - 
Epoch: 7/20
2023-03-02 01:46:00,866 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3155 (0.3111) Acc D Real: 72.057% 
Loss D Fake: 0.4850 (0.4848) Acc D Fake: 100.000% 
Loss D: 0.800 
Loss G: 0.9673 (0.9676) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,874 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3252 (0.3158) Acc D Real: 71.667% 
Loss D Fake: 0.4854 (0.4850) Acc D Fake: 100.000% 
Loss D: 0.811 
Loss G: 0.9667 (0.9673) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,882 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3015 (0.3122) Acc D Real: 72.044% 
Loss D Fake: 0.4857 (0.4851) Acc D Fake: 100.000% 
Loss D: 0.787 
Loss G: 0.9662 (0.9671) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,903 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3880 (0.3274) Acc D Real: 70.417% 
Loss D Fake: 0.4860 (0.4853) Acc D Fake: 100.000% 
Loss D: 0.874 
Loss G: 0.9655 (0.9667) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,911 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2142 (0.3085) Acc D Real: 72.465% 
Loss D Fake: 0.4864 (0.4855) Acc D Fake: 100.000% 
Loss D: 0.701 
Loss G: 0.9653 (0.9665) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,919 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2174 (0.2955) Acc D Real: 73.847% 
Loss D Fake: 0.4862 (0.4856) Acc D Fake: 100.000% 
Loss D: 0.704 
Loss G: 0.9660 (0.9664) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,926 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4102 (0.3098) Acc D Real: 72.240% 
Loss D Fake: 0.4859 (0.4856) Acc D Fake: 100.000% 
Loss D: 0.896 
Loss G: 0.9662 (0.9664) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,933 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3847 (0.3182) Acc D Real: 71.354% 
Loss D Fake: 0.4860 (0.4857) Acc D Fake: 100.000% 
Loss D: 0.871 
Loss G: 0.9658 (0.9663) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,940 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3568 (0.3220) Acc D Real: 70.979% 
Loss D Fake: 0.4864 (0.4857) Acc D Fake: 100.000% 
Loss D: 0.843 
Loss G: 0.9650 (0.9662) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,947 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3240 (0.3222) Acc D Real: 70.971% 
Loss D Fake: 0.4869 (0.4858) Acc D Fake: 100.000% 
Loss D: 0.811 
Loss G: 0.9642 (0.9660) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,953 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4400 (0.3320) Acc D Real: 69.900% 
Loss D Fake: 0.4875 (0.4860) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.9628 (0.9658) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,960 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4422 (0.3405) Acc D Real: 68.970% 
Loss D Fake: 0.4885 (0.4862) Acc D Fake: 100.000% 
Loss D: 0.931 
Loss G: 0.9607 (0.9654) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,967 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3803 (0.3433) Acc D Real: 68.650% 
Loss D Fake: 0.4899 (0.4864) Acc D Fake: 100.000% 
Loss D: 0.870 
Loss G: 0.9583 (0.9649) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,974 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3479 (0.3436) Acc D Real: 68.587% 
Loss D Fake: 0.4912 (0.4868) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 0.9562 (0.9643) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,981 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2101 (0.3353) Acc D Real: 69.482% 
Loss D Fake: 0.4923 (0.4871) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9550 (0.9637) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,989 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4729 (0.3434) Acc D Real: 68.597% 
Loss D Fake: 0.4930 (0.4875) Acc D Fake: 100.000% 
Loss D: 0.966 
Loss G: 0.9533 (0.9631) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:00,996 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2981 (0.3409) Acc D Real: 68.825% 
Loss D Fake: 0.4941 (0.4878) Acc D Fake: 100.000% 
Loss D: 0.792 
Loss G: 0.9518 (0.9625) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,003 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3543 (0.3416) Acc D Real: 68.739% 
Loss D Fake: 0.4949 (0.4882) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 0.9504 (0.9618) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,010 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3430 (0.3417) Acc D Real: 68.721% 
Loss D Fake: 0.4957 (0.4886) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 0.9491 (0.9612) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,017 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3481 (0.3420) Acc D Real: 68.653% 
Loss D Fake: 0.4965 (0.4890) Acc D Fake: 100.000% 
Loss D: 0.845 
Loss G: 0.9477 (0.9605) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,024 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.2703 (0.3387) Acc D Real: 68.991% 
Loss D Fake: 0.4972 (0.4893) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 0.9469 (0.9599) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,031 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3032 (0.3372) Acc D Real: 69.160% 
Loss D Fake: 0.4976 (0.4897) Acc D Fake: 100.000% 
Loss D: 0.801 
Loss G: 0.9464 (0.9593) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,038 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3774 (0.3388) Acc D Real: 68.956% 
Loss D Fake: 0.4980 (0.4900) Acc D Fake: 100.000% 
Loss D: 0.875 
Loss G: 0.9456 (0.9588) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,045 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2396 (0.3349) Acc D Real: 69.369% 
Loss D Fake: 0.4985 (0.4904) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9453 (0.9582) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,052 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2519 (0.3317) Acc D Real: 69.718% 
Loss D Fake: 0.4985 (0.4907) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 0.9456 (0.9577) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,060 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.2422 (0.3284) Acc D Real: 70.068% 
Loss D Fake: 0.4982 (0.4910) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 0.9464 (0.9573) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,067 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2949 (0.3272) Acc D Real: 70.197% 
Loss D Fake: 0.4977 (0.4912) Acc D Fake: 100.000% 
Loss D: 0.793 
Loss G: 0.9473 (0.9570) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,075 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3593 (0.3283) Acc D Real: 70.075% 
Loss D Fake: 0.4973 (0.4914) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 0.9478 (0.9566) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,082 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4618 (0.3327) Acc D Real: 69.575% 
Loss D Fake: 0.4973 (0.4916) Acc D Fake: 100.000% 
Loss D: 0.959 
Loss G: 0.9472 (0.9563) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,090 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4308 (0.3359) Acc D Real: 69.236% 
Loss D Fake: 0.4980 (0.4918) Acc D Fake: 100.000% 
Loss D: 0.929 
Loss G: 0.9458 (0.9560) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,097 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.2740 (0.3340) Acc D Real: 69.451% 
Loss D Fake: 0.4989 (0.4920) Acc D Fake: 100.000% 
Loss D: 0.773 
Loss G: 0.9446 (0.9556) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,104 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4133 (0.3364) Acc D Real: 69.176% 
Loss D Fake: 0.4996 (0.4923) Acc D Fake: 100.000% 
Loss D: 0.913 
Loss G: 0.9431 (0.9553) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,113 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3907 (0.3380) Acc D Real: 68.992% 
Loss D Fake: 0.5006 (0.4925) Acc D Fake: 100.000% 
Loss D: 0.891 
Loss G: 0.9413 (0.9548) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,120 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3646 (0.3387) Acc D Real: 68.897% 
Loss D Fake: 0.5018 (0.4928) Acc D Fake: 100.000% 
Loss D: 0.866 
Loss G: 0.9395 (0.9544) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,127 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3051 (0.3378) Acc D Real: 68.993% 
Loss D Fake: 0.5028 (0.4931) Acc D Fake: 100.000% 
Loss D: 0.808 
Loss G: 0.9380 (0.9540) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,135 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4601 (0.3411) Acc D Real: 68.619% 
Loss D Fake: 0.5038 (0.4933) Acc D Fake: 100.000% 
Loss D: 0.964 
Loss G: 0.9360 (0.9535) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,142 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3417 (0.3411) Acc D Real: 68.606% 
Loss D Fake: 0.5052 (0.4937) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 0.9338 (0.9530) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,149 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4153 (0.3430) Acc D Real: 68.369% 
Loss D Fake: 0.5066 (0.4940) Acc D Fake: 100.000% 
Loss D: 0.922 
Loss G: 0.9315 (0.9524) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,157 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3753 (0.3438) Acc D Real: 68.268% 
Loss D Fake: 0.5081 (0.4943) Acc D Fake: 100.000% 
Loss D: 0.883 
Loss G: 0.9291 (0.9518) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,164 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.1916 (0.3401) Acc D Real: 68.667% 
Loss D Fake: 0.5094 (0.4947) Acc D Fake: 100.000% 
Loss D: 0.701 
Loss G: 0.9277 (0.9512) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,171 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.2681 (0.3384) Acc D Real: 68.843% 
Loss D Fake: 0.5100 (0.4951) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 0.9270 (0.9507) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,180 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2624 (0.3366) Acc D Real: 69.021% 
Loss D Fake: 0.5103 (0.4954) Acc D Fake: 100.000% 
Loss D: 0.773 
Loss G: 0.9269 (0.9501) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,187 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.2680 (0.3351) Acc D Real: 69.177% 
Loss D Fake: 0.5104 (0.4958) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 0.9271 (0.9496) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,195 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3680 (0.3358) Acc D Real: 69.080% 
Loss D Fake: 0.5104 (0.4961) Acc D Fake: 100.000% 
Loss D: 0.878 
Loss G: 0.9269 (0.9491) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,202 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2967 (0.3349) Acc D Real: 69.154% 
Loss D Fake: 0.5106 (0.4964) Acc D Fake: 100.000% 
Loss D: 0.807 
Loss G: 0.9268 (0.9486) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,209 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.2408 (0.3329) Acc D Real: 69.359% 
Loss D Fake: 0.5106 (0.4967) Acc D Fake: 100.000% 
Loss D: 0.751 
Loss G: 0.9271 (0.9481) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,217 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4078 (0.3345) Acc D Real: 69.178% 
Loss D Fake: 0.5105 (0.4970) Acc D Fake: 100.000% 
Loss D: 0.918 
Loss G: 0.9269 (0.9477) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,226 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2617 (0.3330) Acc D Real: 69.330% 
Loss D Fake: 0.5108 (0.4973) Acc D Fake: 100.000% 
Loss D: 0.772 
Loss G: 0.9268 (0.9473) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,234 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2404 (0.3312) Acc D Real: 69.531% 
Loss D Fake: 0.5107 (0.4975) Acc D Fake: 100.000% 
Loss D: 0.751 
Loss G: 0.9272 (0.9469) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,241 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3492 (0.3315) Acc D Real: 69.484% 
Loss D Fake: 0.5106 (0.4978) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 0.9273 (0.9465) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,249 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3033 (0.3310) Acc D Real: 69.540% 
Loss D Fake: 0.5107 (0.4981) Acc D Fake: 100.000% 
Loss D: 0.814 
Loss G: 0.9273 (0.9461) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,256 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2167 (0.3288) Acc D Real: 69.774% 
Loss D Fake: 0.5106 (0.4983) Acc D Fake: 100.000% 
Loss D: 0.727 
Loss G: 0.9278 (0.9458) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,263 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2959 (0.3282) Acc D Real: 69.837% 
Loss D Fake: 0.5103 (0.4985) Acc D Fake: 100.000% 
Loss D: 0.806 
Loss G: 0.9284 (0.9454) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,271 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4063 (0.3296) Acc D Real: 69.668% 
Loss D Fake: 0.5102 (0.4987) Acc D Fake: 100.000% 
Loss D: 0.917 
Loss G: 0.9283 (0.9451) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,278 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2293 (0.3278) Acc D Real: 69.867% 
Loss D Fake: 0.5104 (0.4989) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 0.9284 (0.9448) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,285 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2672 (0.3268) Acc D Real: 69.989% 
Loss D Fake: 0.5104 (0.4991) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 0.9287 (0.9446) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,293 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4223 (0.3284) Acc D Real: 69.802% 
Loss D Fake: 0.5104 (0.4993) Acc D Fake: 100.000% 
Loss D: 0.933 
Loss G: 0.9284 (0.9443) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,300 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3845 (0.3294) Acc D Real: 69.699% 
Loss D Fake: 0.5110 (0.4995) Acc D Fake: 100.000% 
Loss D: 0.895 
Loss G: 0.9274 (0.9440) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,307 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.2300 (0.3277) Acc D Real: 69.874% 
Loss D Fake: 0.5117 (0.4997) Acc D Fake: 100.000% 
Loss D: 0.742 
Loss G: 0.9267 (0.9437) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,315 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.2978 (0.3272) Acc D Real: 69.924% 
Loss D Fake: 0.5121 (0.4999) Acc D Fake: 100.000% 
Loss D: 0.810 
Loss G: 0.9262 (0.9434) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,322 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2468 (0.3259) Acc D Real: 70.064% 
Loss D Fake: 0.5125 (0.5001) Acc D Fake: 100.000% 
Loss D: 0.759 
Loss G: 0.9260 (0.9431) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,330 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2078 (0.3241) Acc D Real: 70.265% 
Loss D Fake: 0.5127 (0.5003) Acc D Fake: 100.000% 
Loss D: 0.720 
Loss G: 0.9262 (0.9429) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,337 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3216 (0.3240) Acc D Real: 70.267% 
Loss D Fake: 0.5127 (0.5005) Acc D Fake: 100.000% 
Loss D: 0.834 
Loss G: 0.9263 (0.9426) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,344 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.2381 (0.3227) Acc D Real: 70.407% 
Loss D Fake: 0.5129 (0.5007) Acc D Fake: 100.000% 
Loss D: 0.751 
Loss G: 0.9264 (0.9424) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,352 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.2372 (0.3214) Acc D Real: 70.554% 
Loss D Fake: 0.5129 (0.5009) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 0.9268 (0.9421) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,359 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3220 (0.3214) Acc D Real: 70.554% 
Loss D Fake: 0.5130 (0.5011) Acc D Fake: 100.000% 
Loss D: 0.835 
Loss G: 0.9268 (0.9419) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,367 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.2413 (0.3202) Acc D Real: 70.677% 
Loss D Fake: 0.5134 (0.5013) Acc D Fake: 100.000% 
Loss D: 0.755 
Loss G: 0.9267 (0.9417) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,374 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4554 (0.3222) Acc D Real: 70.463% 
Loss D Fake: 0.5141 (0.5015) Acc D Fake: 100.000% 
Loss D: 0.969 
Loss G: 0.9254 (0.9414) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,382 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3014 (0.3219) Acc D Real: 70.501% 
Loss D Fake: 0.5155 (0.5017) Acc D Fake: 100.000% 
Loss D: 0.817 
Loss G: 0.9236 (0.9412) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,389 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2700 (0.3212) Acc D Real: 70.581% 
Loss D Fake: 0.5170 (0.5019) Acc D Fake: 100.000% 
Loss D: 0.787 
Loss G: 0.9218 (0.9409) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,396 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.2900 (0.3207) Acc D Real: 70.629% 
Loss D Fake: 0.5187 (0.5021) Acc D Fake: 100.000% 
Loss D: 0.809 
Loss G: 0.9196 (0.9406) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,404 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3195 (0.3207) Acc D Real: 70.625% 
Loss D Fake: 0.5208 (0.5024) Acc D Fake: 100.000% 
Loss D: 0.840 
Loss G: 0.9169 (0.9403) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:01,412 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.2734 (0.3201) Acc D Real: 70.702% 
Loss D Fake: 0.5234 (0.5026) Acc D Fake: 99.970% 
Loss D: 0.797 
Loss G: 0.9136 (0.9399) Acc G: 0.032% 
LR: 2.000e-04 

2023-03-02 01:46:01,419 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.2516 (0.3192) Acc D Real: 70.803% 
Loss D Fake: 0.5268 (0.5030) Acc D Fake: 99.881% 
Loss D: 0.778 
Loss G: 0.9096 (0.9395) Acc G: 0.121% 
LR: 2.000e-04 

2023-03-02 01:46:01,427 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.2636 (0.3184) Acc D Real: 70.882% 
Loss D Fake: 0.5313 (0.5033) Acc D Fake: 99.751% 
Loss D: 0.795 
Loss G: 0.9041 (0.9391) Acc G: 0.251% 
LR: 2.000e-04 

2023-03-02 01:46:01,434 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2585 (0.3177) Acc D Real: 70.959% 
Loss D Fake: 0.5383 (0.5038) Acc D Fake: 99.544% 
Loss D: 0.797 
Loss G: 0.8959 (0.9385) Acc G: 0.444% 
LR: 2.000e-04 

2023-03-02 01:46:01,442 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.2664 (0.3170) Acc D Real: 71.028% 
Loss D Fake: 0.5522 (0.5044) Acc D Fake: 99.255% 
Loss D: 0.819 
Loss G: 0.8806 (0.9377) Acc G: 0.716% 
LR: 2.000e-04 

2023-03-02 01:46:01,449 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.2465 (0.3161) Acc D Real: 71.122% 
Loss D Fake: 2.6876 (0.5320) Acc D Fake: 98.041% 
Loss D: 2.934 
Loss G: 0.8978 (0.9372) Acc G: 0.897% 
LR: 2.000e-04 

2023-03-02 01:46:01,456 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.2949 (0.3158) Acc D Real: 71.151% 
Loss D Fake: 0.5322 (0.5320) Acc D Fake: 97.899% 
Loss D: 0.827 
Loss G: 0.9151 (0.9370) Acc G: 0.990% 
LR: 2.000e-04 

2023-03-02 01:46:01,463 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3023 (0.3157) Acc D Real: 71.170% 
Loss D Fake: 0.5225 (0.5319) Acc D Fake: 97.802% 
Loss D: 0.825 
Loss G: 0.9209 (0.9368) Acc G: 1.060% 
LR: 2.000e-04 

2023-03-02 01:46:01,471 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2985 (0.3155) Acc D Real: 71.193% 
Loss D Fake: 0.5190 (0.5318) Acc D Fake: 97.727% 
Loss D: 0.817 
Loss G: 0.9231 (0.9366) Acc G: 1.128% 
LR: 2.000e-04 

2023-03-02 01:46:01,479 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.4105 (0.3166) Acc D Real: 71.068% 
Loss D Fake: 0.5179 (0.5316) Acc D Fake: 97.654% 
Loss D: 0.928 
Loss G: 0.9227 (0.9364) Acc G: 1.195% 
LR: 2.000e-04 

2023-03-02 01:46:01,487 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.2715 (0.3161) Acc D Real: 71.127% 
Loss D Fake: 0.5184 (0.5314) Acc D Fake: 97.582% 
Loss D: 0.790 
Loss G: 0.9214 (0.9363) Acc G: 1.260% 
LR: 2.000e-04 

2023-03-02 01:46:01,496 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.2764 (0.3156) Acc D Real: 71.176% 
Loss D Fake: 0.5198 (0.5313) Acc D Fake: 97.513% 
Loss D: 0.796 
Loss G: 0.9191 (0.9361) Acc G: 1.343% 
LR: 2.000e-04 

2023-03-02 01:46:01,504 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4118 (0.3167) Acc D Real: 71.055% 
Loss D Fake: 0.5222 (0.5312) Acc D Fake: 97.426% 
Loss D: 0.934 
Loss G: 0.9155 (0.9358) Acc G: 1.444% 
LR: 2.000e-04 

2023-03-02 01:46:01,512 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.2731 (0.3162) Acc D Real: 71.119% 
Loss D Fake: 0.5259 (0.5311) Acc D Fake: 97.321% 
Loss D: 0.799 
Loss G: 0.9108 (0.9355) Acc G: 1.544% 
LR: 2.000e-04 

2023-03-02 01:46:01,520 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3016 (0.3161) Acc D Real: 71.140% 
Loss D Fake: 0.5308 (0.5311) Acc D Fake: 97.200% 
Loss D: 0.832 
Loss G: 0.9046 (0.9352) Acc G: 1.678% 
LR: 2.000e-04 

2023-03-02 01:46:01,528 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3732 (0.3167) Acc D Real: 71.066% 
Loss D Fake: 0.5384 (0.5312) Acc D Fake: 97.058% 
Loss D: 0.912 
Loss G: 0.8955 (0.9347) Acc G: 1.828% 
LR: 2.000e-04 

2023-03-02 01:46:01,535 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4002 (0.3176) Acc D Real: 70.978% 
Loss D Fake: 0.5523 (0.5315) Acc D Fake: 96.868% 
Loss D: 0.953 
Loss G: 0.8796 (0.9341) Acc G: 2.030% 
LR: 2.000e-04 

2023-03-02 01:46:01,542 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.2238 (0.3166) Acc D Real: 71.085% 
Loss D Fake: 0.6047 (0.5323) Acc D Fake: 96.536% 
Loss D: 0.828 
Loss G: 0.8376 (0.9331) Acc G: 2.337% 
LR: 2.000e-04 

2023-03-02 01:46:01,549 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3138 (0.3166) Acc D Real: 71.081% 
Loss D Fake: 3.2759 (0.5621) Acc D Fake: 95.523% 
Loss D: 3.590 
Loss G: 0.1497 (0.9245) Acc G: 3.362% 
LR: 2.000e-04 

2023-03-02 01:46:01,557 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4261 (0.3177) Acc D Real: 70.954% 
Loss D Fake: 3.4030 (0.5926) Acc D Fake: 94.532% 
Loss D: 3.829 
Loss G: 0.1264 (0.9160) Acc G: 4.365% 
LR: 2.000e-04 

2023-03-02 01:46:01,564 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3486 (0.3181) Acc D Real: 70.910% 
Loss D Fake: 3.4504 (0.6230) Acc D Fake: 93.562% 
Loss D: 3.799 
Loss G: 0.1157 (0.9074) Acc G: 5.347% 
LR: 2.000e-04 

2023-03-02 01:46:01,572 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3367 (0.3183) Acc D Real: 70.889% 
Loss D Fake: 3.4609 (0.6529) Acc D Fake: 92.612% 
Loss D: 3.798 
Loss G: 0.1097 (0.8991) Acc G: 6.309% 
LR: 2.000e-04 

2023-03-02 01:46:01,579 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.3302 (0.3184) Acc D Real: 70.873% 
Loss D Fake: 3.4493 (0.6820) Acc D Fake: 91.682% 
Loss D: 3.780 
Loss G: 0.1063 (0.8908) Acc G: 7.250% 
LR: 2.000e-04 

2023-03-02 01:46:01,586 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3809 (0.3190) Acc D Real: 70.796% 
Loss D Fake: 3.4238 (0.7103) Acc D Fake: 90.771% 
Loss D: 3.805 
Loss G: 0.1042 (0.8827) Acc G: 8.172% 
LR: 2.000e-04 

2023-03-02 01:46:01,593 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3438 (0.3193) Acc D Real: 70.760% 
Loss D Fake: 3.3889 (0.7376) Acc D Fake: 89.879% 
Loss D: 3.733 
Loss G: 0.1031 (0.8747) Acc G: 9.075% 
LR: 2.000e-04 

2023-03-02 01:46:01,600 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3501 (0.3196) Acc D Real: 70.712% 
Loss D Fake: 3.3475 (0.7640) Acc D Fake: 89.005% 
Loss D: 3.698 
Loss G: 0.1027 (0.8669) Acc G: 9.959% 
LR: 2.000e-04 

2023-03-02 01:46:01,609 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2682 (0.3191) Acc D Real: 70.758% 
Loss D Fake: 3.3013 (0.7894) Acc D Fake: 88.148% 
Loss D: 3.569 
Loss G: 0.1029 (0.8593) Acc G: 10.827% 
LR: 2.000e-04 

2023-03-02 01:46:01,617 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4742 (0.3206) Acc D Real: 70.552% 
Loss D Fake: 3.2516 (0.8137) Acc D Fake: 87.308% 
Loss D: 3.726 
Loss G: 0.1035 (0.8518) Acc G: 11.676% 
LR: 2.000e-04 

2023-03-02 01:46:01,624 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2777 (0.3202) Acc D Real: 70.584% 
Loss D Fake: 3.1993 (0.8371) Acc D Fake: 86.485% 
Loss D: 3.477 
Loss G: 0.1044 (0.8445) Acc G: 12.510% 
LR: 2.000e-04 

2023-03-02 01:46:01,631 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.2952 (0.3200) Acc D Real: 70.594% 
Loss D Fake: 3.1452 (0.8595) Acc D Fake: 85.678% 
Loss D: 3.440 
Loss G: 0.1056 (0.8373) Acc G: 13.327% 
LR: 2.000e-04 

2023-03-02 01:46:01,640 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2396 (0.3192) Acc D Real: 70.672% 
Loss D Fake: 3.0901 (0.8810) Acc D Fake: 84.886% 
Loss D: 3.330 
Loss G: 0.1071 (0.8303) Acc G: 14.128% 
LR: 2.000e-04 

2023-03-02 01:46:01,647 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4316 (0.3203) Acc D Real: 70.511% 
Loss D Fake: 3.0345 (0.9015) Acc D Fake: 84.109% 
Loss D: 3.466 
Loss G: 0.1089 (0.8234) Acc G: 14.914% 
LR: 2.000e-04 

2023-03-02 01:46:01,655 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3058 (0.3201) Acc D Real: 70.505% 
Loss D Fake: 2.9785 (0.9211) Acc D Fake: 83.347% 
Loss D: 3.284 
Loss G: 0.1108 (0.8167) Acc G: 15.685% 
LR: 2.000e-04 

2023-03-02 01:46:01,663 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4112 (0.3210) Acc D Real: 70.368% 
Loss D Fake: 2.9226 (0.9398) Acc D Fake: 82.599% 
Loss D: 3.334 
Loss G: 0.1130 (0.8101) Acc G: 16.442% 
LR: 2.000e-04 

2023-03-02 01:46:01,670 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3690 (0.3214) Acc D Real: 70.286% 
Loss D Fake: 2.8671 (0.9576) Acc D Fake: 81.865% 
Loss D: 3.236 
Loss G: 0.1153 (0.8037) Acc G: 17.185% 
LR: 2.000e-04 

2023-03-02 01:46:01,677 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4125 (0.3222) Acc D Real: 70.155% 
Loss D Fake: 2.8120 (0.9747) Acc D Fake: 81.145% 
Loss D: 3.224 
Loss G: 0.1178 (0.7974) Acc G: 17.914% 
LR: 2.000e-04 

2023-03-02 01:46:01,685 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3440 (0.3224) Acc D Real: 70.109% 
Loss D Fake: 2.7575 (0.9909) Acc D Fake: 80.438% 
Loss D: 3.101 
Loss G: 0.1205 (0.7912) Acc G: 18.630% 
LR: 2.000e-04 

2023-03-02 01:46:01,692 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3820 (0.3230) Acc D Real: 70.012% 
Loss D Fake: 2.7037 (1.0063) Acc D Fake: 79.743% 
Loss D: 3.086 
Loss G: 0.1234 (0.7852) Acc G: 19.333% 
LR: 2.000e-04 

2023-03-02 01:46:01,700 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3892 (0.3236) Acc D Real: 69.915% 
Loss D Fake: 2.6508 (1.0210) Acc D Fake: 79.061% 
Loss D: 3.040 
Loss G: 0.1264 (0.7793) Acc G: 20.024% 
LR: 2.000e-04 

2023-03-02 01:46:01,707 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3773 (0.3240) Acc D Real: 69.837% 
Loss D Fake: 2.5987 (1.0349) Acc D Fake: 78.390% 
Loss D: 2.976 
Loss G: 0.1296 (0.7736) Acc G: 20.702% 
LR: 2.000e-04 

2023-03-02 01:46:01,715 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3651 (0.3244) Acc D Real: 69.770% 
Loss D Fake: 2.5476 (1.0482) Acc D Fake: 77.732% 
Loss D: 2.913 
Loss G: 0.1330 (0.7680) Acc G: 21.368% 
LR: 2.000e-04 

2023-03-02 01:46:01,723 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.1745 (0.3231) Acc D Real: 69.942% 
Loss D Fake: 2.4976 (1.0608) Acc D Fake: 77.085% 
Loss D: 2.672 
Loss G: 0.1364 (0.7625) Acc G: 22.023% 
LR: 2.000e-04 

2023-03-02 01:46:01,730 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.2790 (0.3227) Acc D Real: 69.981% 
Loss D Fake: 2.4489 (1.0728) Acc D Fake: 76.449% 
Loss D: 2.728 
Loss G: 0.1400 (0.7571) Acc G: 22.667% 
LR: 2.000e-04 

2023-03-02 01:46:01,738 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3524 (0.3230) Acc D Real: 69.934% 
Loss D Fake: 2.4013 (1.0841) Acc D Fake: 75.824% 
Loss D: 2.754 
Loss G: 0.1438 (0.7519) Acc G: 23.299% 
LR: 2.000e-04 

2023-03-02 01:46:01,746 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3265 (0.3230) Acc D Real: 69.926% 
Loss D Fake: 2.3546 (1.0949) Acc D Fake: 75.210% 
Loss D: 2.681 
Loss G: 0.1476 (0.7467) Acc G: 23.921% 
LR: 2.000e-04 

2023-03-02 01:46:01,755 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3508 (0.3232) Acc D Real: 69.889% 
Loss D Fake: 2.3089 (1.1051) Acc D Fake: 74.606% 
Loss D: 2.660 
Loss G: 0.1517 (0.7417) Acc G: 24.532% 
LR: 2.000e-04 

2023-03-02 01:46:01,763 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3575 (0.3235) Acc D Real: 69.854% 
Loss D Fake: 2.2642 (1.1148) Acc D Fake: 74.012% 
Loss D: 2.622 
Loss G: 0.1558 (0.7369) Acc G: 25.133% 
LR: 2.000e-04 

2023-03-02 01:46:01,771 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4566 (0.3246) Acc D Real: 69.724% 
Loss D Fake: 2.2203 (1.1239) Acc D Fake: 73.428% 
Loss D: 2.677 
Loss G: 0.1602 (0.7321) Acc G: 25.724% 
LR: 2.000e-04 

2023-03-02 01:46:01,779 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.2421 (0.3240) Acc D Real: 69.921% 
Loss D Fake: 2.1773 (1.1325) Acc D Fake: 72.853% 
Loss D: 2.419 
Loss G: 0.1646 (0.7274) Acc G: 26.306% 
LR: 2.000e-04 

2023-03-02 01:46:01,788 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2838 (0.3236) Acc D Real: 70.113% 
Loss D Fake: 2.1357 (1.1407) Acc D Fake: 72.288% 
Loss D: 2.419 
Loss G: 0.1691 (0.7229) Acc G: 26.864% 
LR: 2.000e-04 

2023-03-02 01:46:01,797 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3772 (0.3241) Acc D Real: 70.284% 
Loss D Fake: 2.0952 (1.1484) Acc D Fake: 71.746% 
Loss D: 2.472 
Loss G: 0.1738 (0.7185) Acc G: 27.414% 
LR: 2.000e-04 

2023-03-02 01:46:01,805 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4036 (0.3247) Acc D Real: 70.437% 
Loss D Fake: 2.0554 (1.1556) Acc D Fake: 71.212% 
Loss D: 2.459 
Loss G: 0.1786 (0.7142) Acc G: 27.955% 
LR: 2.000e-04 

2023-03-02 01:46:01,814 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3810 (0.3251) Acc D Real: 70.591% 
Loss D Fake: 2.0164 (1.1625) Acc D Fake: 70.686% 
Loss D: 2.397 
Loss G: 0.1836 (0.7099) Acc G: 28.487% 
LR: 2.000e-04 

2023-03-02 01:46:01,822 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3368 (0.3252) Acc D Real: 70.755% 
Loss D Fake: 1.9784 (1.1689) Acc D Fake: 70.169% 
Loss D: 2.315 
Loss G: 0.1886 (0.7058) Acc G: 29.010% 
LR: 2.000e-04 

2023-03-02 01:46:01,830 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3535 (0.3255) Acc D Real: 70.918% 
Loss D Fake: 1.9414 (1.1749) Acc D Fake: 69.660% 
Loss D: 2.295 
Loss G: 0.1938 (0.7018) Acc G: 29.526% 
LR: 2.000e-04 

2023-03-02 01:46:01,837 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3265 (0.3255) Acc D Real: 71.095% 
Loss D Fake: 1.9053 (1.1806) Acc D Fake: 69.159% 
Loss D: 2.232 
Loss G: 0.1991 (0.6979) Acc G: 30.034% 
LR: 2.000e-04 

2023-03-02 01:46:01,845 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3194 (0.3254) Acc D Real: 71.267% 
Loss D Fake: 1.8704 (1.1859) Acc D Fake: 68.665% 
Loss D: 2.190 
Loss G: 0.2044 (0.6941) Acc G: 30.533% 
LR: 2.000e-04 

2023-03-02 01:46:01,853 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3570 (0.3257) Acc D Real: 71.419% 
Loss D Fake: 1.8364 (1.1909) Acc D Fake: 68.179% 
Loss D: 2.193 
Loss G: 0.2098 (0.6905) Acc G: 31.025% 
LR: 2.000e-04 

2023-03-02 01:46:01,860 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3353 (0.3257) Acc D Real: 71.570% 
Loss D Fake: 1.8034 (1.1955) Acc D Fake: 67.700% 
Loss D: 2.139 
Loss G: 0.2153 (0.6869) Acc G: 31.510% 
LR: 2.000e-04 

2023-03-02 01:46:01,868 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3449 (0.3259) Acc D Real: 71.727% 
Loss D Fake: 1.7714 (1.1998) Acc D Fake: 67.229% 
Loss D: 2.116 
Loss G: 0.2209 (0.6833) Acc G: 31.987% 
LR: 2.000e-04 

2023-03-02 01:46:01,876 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3538 (0.3261) Acc D Real: 71.875% 
Loss D Fake: 1.7403 (1.2039) Acc D Fake: 66.765% 
Loss D: 2.094 
Loss G: 0.2265 (0.6799) Acc G: 32.458% 
LR: 2.000e-04 

2023-03-02 01:46:01,883 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3601 (0.3263) Acc D Real: 72.018% 
Loss D Fake: 1.7100 (1.2076) Acc D Fake: 66.307% 
Loss D: 2.070 
Loss G: 0.2322 (0.6766) Acc G: 32.921% 
LR: 2.000e-04 

2023-03-02 01:46:01,891 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3250 (0.3263) Acc D Real: 72.168% 
Loss D Fake: 1.6806 (1.2111) Acc D Fake: 65.856% 
Loss D: 2.006 
Loss G: 0.2380 (0.6734) Acc G: 33.377% 
LR: 2.000e-04 

2023-03-02 01:46:01,899 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3276 (0.3263) Acc D Real: 72.313% 
Loss D Fake: 1.6523 (1.2143) Acc D Fake: 65.412% 
Loss D: 1.980 
Loss G: 0.2437 (0.6703) Acc G: 33.827% 
LR: 2.000e-04 

2023-03-02 01:46:01,906 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3740 (0.3267) Acc D Real: 72.449% 
Loss D Fake: 1.6249 (1.2173) Acc D Fake: 64.974% 
Loss D: 1.999 
Loss G: 0.2495 (0.6672) Acc G: 34.270% 
LR: 2.000e-04 

2023-03-02 01:46:01,913 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3702 (0.3270) Acc D Real: 72.581% 
Loss D Fake: 1.5981 (1.2200) Acc D Fake: 64.543% 
Loss D: 1.968 
Loss G: 0.2554 (0.6643) Acc G: 34.707% 
LR: 2.000e-04 

2023-03-02 01:46:01,921 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3623 (0.3272) Acc D Real: 72.721% 
Loss D Fake: 1.5720 (1.2225) Acc D Fake: 64.118% 
Loss D: 1.934 
Loss G: 0.2614 (0.6614) Acc G: 35.138% 
LR: 2.000e-04 

2023-03-02 01:46:01,928 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3847 (0.3276) Acc D Real: 72.846% 
Loss D Fake: 1.5466 (1.2248) Acc D Fake: 63.698% 
Loss D: 1.931 
Loss G: 0.2674 (0.6586) Acc G: 35.563% 
LR: 2.000e-04 

2023-03-02 01:46:01,936 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3508 (0.3278) Acc D Real: 72.973% 
Loss D Fake: 1.5219 (1.2269) Acc D Fake: 63.285% 
Loss D: 1.873 
Loss G: 0.2734 (0.6559) Acc G: 35.981% 
LR: 2.000e-04 

2023-03-02 01:46:01,943 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4661 (0.3288) Acc D Real: 73.095% 
Loss D Fake: 1.4979 (1.2288) Acc D Fake: 62.877% 
Loss D: 1.964 
Loss G: 0.2796 (0.6532) Acc G: 36.394% 
LR: 2.000e-04 

2023-03-02 01:46:01,950 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3423 (0.3289) Acc D Real: 73.222% 
Loss D Fake: 1.4739 (1.2305) Acc D Fake: 62.475% 
Loss D: 1.816 
Loss G: 0.2857 (0.6507) Acc G: 36.801% 
LR: 2.000e-04 

2023-03-02 01:46:01,957 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4192 (0.3295) Acc D Real: 73.330% 
Loss D Fake: 1.4509 (1.2321) Acc D Fake: 62.079% 
Loss D: 1.870 
Loss G: 0.2920 (0.6482) Acc G: 37.202% 
LR: 2.000e-04 

2023-03-02 01:46:01,964 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3721 (0.3298) Acc D Real: 73.436% 
Loss D Fake: 1.4284 (1.2334) Acc D Fake: 61.688% 
Loss D: 1.801 
Loss G: 0.2982 (0.6458) Acc G: 37.598% 
LR: 2.000e-04 

2023-03-02 01:46:01,972 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3704 (0.3301) Acc D Real: 73.558% 
Loss D Fake: 1.4070 (1.2346) Acc D Fake: 61.302% 
Loss D: 1.777 
Loss G: 0.3044 (0.6435) Acc G: 37.989% 
LR: 2.000e-04 

2023-03-02 01:46:01,979 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3711 (0.3303) Acc D Real: 73.675% 
Loss D Fake: 1.3864 (1.2356) Acc D Fake: 60.922% 
Loss D: 1.757 
Loss G: 0.3104 (0.6412) Acc G: 38.374% 
LR: 2.000e-04 

2023-03-02 01:46:01,987 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3396 (0.3304) Acc D Real: 73.803% 
Loss D Fake: 1.3669 (1.2365) Acc D Fake: 60.547% 
Loss D: 1.706 
Loss G: 0.3163 (0.6391) Acc G: 38.754% 
LR: 2.000e-04 

2023-03-02 01:46:01,994 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4120 (0.3309) Acc D Real: 73.916% 
Loss D Fake: 1.3483 (1.2372) Acc D Fake: 60.176% 
Loss D: 1.760 
Loss G: 0.3221 (0.6369) Acc G: 39.129% 
LR: 2.000e-04 

2023-03-02 01:46:02,001 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3565 (0.3311) Acc D Real: 74.045% 
Loss D Fake: 1.3301 (1.2378) Acc D Fake: 59.811% 
Loss D: 1.687 
Loss G: 0.3279 (0.6349) Acc G: 39.499% 
LR: 2.000e-04 

2023-03-02 01:46:02,009 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3870 (0.3315) Acc D Real: 74.158% 
Loss D Fake: 1.3129 (1.2383) Acc D Fake: 59.450% 
Loss D: 1.700 
Loss G: 0.3336 (0.6329) Acc G: 39.864% 
LR: 2.000e-04 

2023-03-02 01:46:02,016 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3800 (0.3318) Acc D Real: 74.274% 
Loss D Fake: 1.2963 (1.2387) Acc D Fake: 59.094% 
Loss D: 1.676 
Loss G: 0.3392 (0.6310) Acc G: 40.224% 
LR: 2.000e-04 

2023-03-02 01:46:02,023 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3733 (0.3321) Acc D Real: 74.393% 
Loss D Fake: 1.2804 (1.2390) Acc D Fake: 58.743% 
Loss D: 1.654 
Loss G: 0.3446 (0.6291) Acc G: 40.580% 
LR: 2.000e-04 

2023-03-02 01:46:02,031 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4147 (0.3326) Acc D Real: 74.484% 
Loss D Fake: 1.2652 (1.2392) Acc D Fake: 58.397% 
Loss D: 1.680 
Loss G: 0.3501 (0.6273) Acc G: 40.931% 
LR: 2.000e-04 

2023-03-02 01:46:02,038 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4168 (0.3331) Acc D Real: 74.588% 
Loss D Fake: 1.2500 (1.2392) Acc D Fake: 58.054% 
Loss D: 1.667 
Loss G: 0.3557 (0.6256) Acc G: 41.278% 
LR: 2.000e-04 

2023-03-02 01:46:02,045 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3923 (0.3335) Acc D Real: 74.702% 
Loss D Fake: 1.2350 (1.2392) Acc D Fake: 57.716% 
Loss D: 1.627 
Loss G: 0.3612 (0.6239) Acc G: 41.620% 
LR: 2.000e-04 

2023-03-02 01:46:02,053 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4293 (0.3341) Acc D Real: 74.800% 
Loss D Fake: 1.2204 (1.2391) Acc D Fake: 57.383% 
Loss D: 1.650 
Loss G: 0.3669 (0.6223) Acc G: 41.947% 
LR: 2.000e-04 

2023-03-02 01:46:02,062 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4122 (0.3346) Acc D Real: 74.901% 
Loss D Fake: 1.2058 (1.2389) Acc D Fake: 57.064% 
Loss D: 1.618 
Loss G: 0.3726 (0.6207) Acc G: 42.270% 
LR: 2.000e-04 

2023-03-02 01:46:02,069 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4050 (0.3351) Acc D Real: 75.007% 
Loss D Fake: 1.1917 (1.2386) Acc D Fake: 56.749% 
Loss D: 1.597 
Loss G: 0.3781 (0.6192) Acc G: 42.590% 
LR: 2.000e-04 

2023-03-02 01:46:02,077 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4257 (0.3356) Acc D Real: 75.093% 
Loss D Fake: 1.1781 (1.2382) Acc D Fake: 56.438% 
Loss D: 1.604 
Loss G: 0.3837 (0.6177) Acc G: 42.905% 
LR: 2.000e-04 

2023-03-02 01:46:02,084 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4177 (0.3361) Acc D Real: 75.200% 
Loss D Fake: 1.1648 (1.2377) Acc D Fake: 56.130% 
Loss D: 1.582 
Loss G: 0.3893 (0.6163) Acc G: 43.216% 
LR: 2.000e-04 

2023-03-02 01:46:02,092 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4149 (0.3366) Acc D Real: 75.300% 
Loss D Fake: 1.1517 (1.2372) Acc D Fake: 55.827% 
Loss D: 1.567 
Loss G: 0.3948 (0.6150) Acc G: 43.523% 
LR: 2.000e-04 

2023-03-02 01:46:02,099 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4113 (0.3371) Acc D Real: 75.401% 
Loss D Fake: 1.1393 (1.2366) Acc D Fake: 55.527% 
Loss D: 1.551 
Loss G: 0.4000 (0.6137) Acc G: 43.827% 
LR: 2.000e-04 

2023-03-02 01:46:02,106 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4124 (0.3375) Acc D Real: 75.508% 
Loss D Fake: 1.1278 (1.2360) Acc D Fake: 55.231% 
Loss D: 1.540 
Loss G: 0.4050 (0.6124) Acc G: 44.127% 
LR: 2.000e-04 

2023-03-02 01:46:02,114 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4223 (0.3380) Acc D Real: 75.602% 
Loss D Fake: 1.1168 (1.2352) Acc D Fake: 54.939% 
Loss D: 1.539 
Loss G: 0.4099 (0.6112) Acc G: 44.424% 
LR: 2.000e-04 

2023-03-02 01:46:02,121 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4371 (0.3386) Acc D Real: 75.679% 
Loss D Fake: 1.1062 (1.2345) Acc D Fake: 54.649% 
Loss D: 1.543 
Loss G: 0.4148 (0.6100) Acc G: 44.717% 
LR: 2.000e-04 

2023-03-02 01:46:02,128 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4334 (0.3392) Acc D Real: 75.763% 
Loss D Fake: 1.0960 (1.2336) Acc D Fake: 54.364% 
Loss D: 1.529 
Loss G: 0.4195 (0.6089) Acc G: 45.006% 
LR: 2.000e-04 

2023-03-02 01:46:02,136 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4311 (0.3397) Acc D Real: 75.854% 
Loss D Fake: 1.0864 (1.2328) Acc D Fake: 54.082% 
Loss D: 1.517 
Loss G: 0.4239 (0.6078) Acc G: 45.292% 
LR: 2.000e-04 

2023-03-02 01:46:02,143 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4333 (0.3403) Acc D Real: 75.941% 
Loss D Fake: 1.0771 (1.2319) Acc D Fake: 53.803% 
Loss D: 1.510 
Loss G: 0.4285 (0.6067) Acc G: 45.574% 
LR: 2.000e-04 

2023-03-02 01:46:02,151 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4318 (0.3408) Acc D Real: 76.029% 
Loss D Fake: 1.0678 (1.2309) Acc D Fake: 53.527% 
Loss D: 1.500 
Loss G: 0.4330 (0.6057) Acc G: 45.854% 
LR: 2.000e-04 

2023-03-02 01:46:02,158 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4285 (0.3413) Acc D Real: 76.129% 
Loss D Fake: 1.0589 (1.2299) Acc D Fake: 53.255% 
Loss D: 1.487 
Loss G: 0.4373 (0.6047) Acc G: 46.130% 
LR: 2.000e-04 

2023-03-02 01:46:02,165 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4458 (0.3419) Acc D Real: 76.206% 
Loss D Fake: 1.0506 (1.2289) Acc D Fake: 52.985% 
Loss D: 1.496 
Loss G: 0.4414 (0.6038) Acc G: 46.403% 
LR: 2.000e-04 

2023-03-02 01:46:02,173 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4333 (0.3425) Acc D Real: 76.300% 
Loss D Fake: 1.0428 (1.2278) Acc D Fake: 52.719% 
Loss D: 1.476 
Loss G: 0.4453 (0.6029) Acc G: 46.672% 
LR: 2.000e-04 

2023-03-02 01:46:02,180 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4505 (0.3431) Acc D Real: 76.372% 
Loss D Fake: 1.0352 (1.2267) Acc D Fake: 52.456% 
Loss D: 1.486 
Loss G: 0.4493 (0.6020) Acc G: 46.939% 
LR: 2.000e-04 

2023-03-02 01:46:02,187 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4476 (0.3437) Acc D Real: 76.458% 
Loss D Fake: 1.0280 (1.2256) Acc D Fake: 52.196% 
Loss D: 1.476 
Loss G: 0.4528 (0.6011) Acc G: 47.203% 
LR: 2.000e-04 

2023-03-02 01:46:02,195 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4431 (0.3442) Acc D Real: 76.538% 
Loss D Fake: 1.0214 (1.2244) Acc D Fake: 51.939% 
Loss D: 1.464 
Loss G: 0.4563 (0.6003) Acc G: 47.463% 
LR: 2.000e-04 

2023-03-02 01:46:02,202 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4599 (0.3449) Acc D Real: 76.621% 
Loss D Fake: 1.0151 (1.2232) Acc D Fake: 51.684% 
Loss D: 1.475 
Loss G: 0.4595 (0.5995) Acc G: 47.721% 
LR: 2.000e-04 

2023-03-02 01:46:02,209 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4472 (0.3455) Acc D Real: 76.700% 
Loss D Fake: 1.0095 (1.2220) Acc D Fake: 51.433% 
Loss D: 1.457 
Loss G: 0.4625 (0.5988) Acc G: 47.976% 
LR: 2.000e-04 

2023-03-02 01:46:02,217 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4603 (0.3461) Acc D Real: 76.763% 
Loss D Fake: 1.0038 (1.2208) Acc D Fake: 51.184% 
Loss D: 1.464 
Loss G: 0.4657 (0.5980) Acc G: 48.228% 
LR: 2.000e-04 

2023-03-02 01:46:02,224 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4295 (0.3466) Acc D Real: 76.849% 
Loss D Fake: 0.9980 (1.2196) Acc D Fake: 50.938% 
Loss D: 1.428 
Loss G: 0.4691 (0.5973) Acc G: 48.477% 
LR: 2.000e-04 

2023-03-02 01:46:02,232 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4544 (0.3471) Acc D Real: 76.928% 
Loss D Fake: 0.9919 (1.2183) Acc D Fake: 50.695% 
Loss D: 1.446 
Loss G: 0.4724 (0.5966) Acc G: 48.723% 
LR: 2.000e-04 

2023-03-02 01:46:02,239 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4615 (0.3478) Acc D Real: 77.000% 
Loss D Fake: 0.9862 (1.2171) Acc D Fake: 50.454% 
Loss D: 1.448 
Loss G: 0.4755 (0.5960) Acc G: 48.967% 
LR: 2.000e-04 

2023-03-02 01:46:02,247 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4522 (0.3483) Acc D Real: 77.079% 
Loss D Fake: 0.9809 (1.2158) Acc D Fake: 50.216% 
Loss D: 1.433 
Loss G: 0.4785 (0.5953) Acc G: 49.208% 
LR: 2.000e-04 

2023-03-02 01:46:02,254 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4590 (0.3489) Acc D Real: 77.163% 
Loss D Fake: 0.9758 (1.2145) Acc D Fake: 49.981% 
Loss D: 1.435 
Loss G: 0.4813 (0.5947) Acc G: 49.447% 
LR: 2.000e-04 

2023-03-02 01:46:02,261 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4651 (0.3496) Acc D Real: 77.227% 
Loss D Fake: 0.9712 (1.2132) Acc D Fake: 49.748% 
Loss D: 1.436 
Loss G: 0.4839 (0.5941) Acc G: 49.683% 
LR: 2.000e-04 

2023-03-02 01:46:02,268 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4743 (0.3502) Acc D Real: 77.296% 
Loss D Fake: 0.9669 (1.2119) Acc D Fake: 49.518% 
Loss D: 1.441 
Loss G: 0.4863 (0.5935) Acc G: 49.916% 
LR: 2.000e-04 

2023-03-02 01:46:02,276 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4731 (0.3509) Acc D Real: 77.374% 
Loss D Fake: 0.9631 (1.2106) Acc D Fake: 49.290% 
Loss D: 1.436 
Loss G: 0.4883 (0.5930) Acc G: 50.147% 
LR: 2.000e-04 

2023-03-02 01:46:02,283 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4637 (0.3515) Acc D Real: 77.451% 
Loss D Fake: 0.9600 (1.2092) Acc D Fake: 49.064% 
Loss D: 1.424 
Loss G: 0.4901 (0.5924) Acc G: 50.376% 
LR: 2.000e-04 

2023-03-02 01:46:02,291 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4631 (0.3521) Acc D Real: 77.513% 
Loss D Fake: 0.9569 (1.2079) Acc D Fake: 48.841% 
Loss D: 1.420 
Loss G: 0.4921 (0.5919) Acc G: 50.602% 
LR: 2.000e-04 

2023-03-02 01:46:02,298 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4652 (0.3527) Acc D Real: 77.577% 
Loss D Fake: 0.9535 (1.2066) Acc D Fake: 48.620% 
Loss D: 1.419 
Loss G: 0.4941 (0.5914) Acc G: 50.825% 
LR: 2.000e-04 

2023-03-02 01:46:02,306 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4579 (0.3532) Acc D Real: 77.662% 
Loss D Fake: 0.9502 (1.2052) Acc D Fake: 48.402% 
Loss D: 1.408 
Loss G: 0.4960 (0.5909) Acc G: 51.047% 
LR: 2.000e-04 

2023-03-02 01:46:02,313 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4461 (0.3537) Acc D Real: 77.716% 
Loss D Fake: 0.9469 (1.2039) Acc D Fake: 48.185% 
Loss D: 1.393 
Loss G: 0.4984 (0.5904) Acc G: 51.266% 
LR: 2.000e-04 

2023-03-02 01:46:02,321 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4681 (0.3543) Acc D Real: 77.784% 
Loss D Fake: 0.9429 (1.2025) Acc D Fake: 47.971% 
Loss D: 1.411 
Loss G: 0.5008 (0.5900) Acc G: 51.483% 
LR: 2.000e-04 

2023-03-02 01:46:02,328 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4595 (0.3548) Acc D Real: 77.846% 
Loss D Fake: 0.9390 (1.2012) Acc D Fake: 47.760% 
Loss D: 1.399 
Loss G: 0.5032 (0.5895) Acc G: 51.697% 
LR: 2.000e-04 

2023-03-02 01:46:02,336 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4681 (0.3554) Acc D Real: 77.912% 
Loss D Fake: 0.9351 (1.1998) Acc D Fake: 47.550% 
Loss D: 1.403 
Loss G: 0.5056 (0.5891) Acc G: 51.910% 
LR: 2.000e-04 

2023-03-02 01:46:02,344 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4782 (0.3560) Acc D Real: 77.988% 
Loss D Fake: 0.9315 (1.1985) Acc D Fake: 47.342% 
Loss D: 1.410 
Loss G: 0.5076 (0.5887) Acc G: 52.120% 
LR: 2.000e-04 

2023-03-02 01:46:02,351 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4614 (0.3565) Acc D Real: 78.050% 
Loss D Fake: 0.9285 (1.1971) Acc D Fake: 47.137% 
Loss D: 1.390 
Loss G: 0.5096 (0.5883) Acc G: 52.328% 
LR: 2.000e-04 

2023-03-02 01:46:02,359 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4911 (0.3572) Acc D Real: 78.113% 
Loss D Fake: 0.9255 (1.1957) Acc D Fake: 46.934% 
Loss D: 1.417 
Loss G: 0.5112 (0.5879) Acc G: 52.534% 
LR: 2.000e-04 

2023-03-02 01:46:02,366 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4759 (0.3578) Acc D Real: 78.172% 
Loss D Fake: 0.9231 (1.1944) Acc D Fake: 46.732% 
Loss D: 1.399 
Loss G: 0.5127 (0.5875) Acc G: 52.738% 
LR: 2.000e-04 

2023-03-02 01:46:02,374 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4817 (0.3584) Acc D Real: 78.243% 
Loss D Fake: 0.9209 (1.1930) Acc D Fake: 46.533% 
Loss D: 1.403 
Loss G: 0.5139 (0.5871) Acc G: 52.940% 
LR: 2.000e-04 

2023-03-02 01:46:02,381 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4457 (0.3589) Acc D Real: 78.313% 
Loss D Fake: 0.9191 (1.1917) Acc D Fake: 46.336% 
Loss D: 1.365 
Loss G: 0.5153 (0.5868) Acc G: 53.140% 
LR: 2.000e-04 

2023-03-02 01:46:02,389 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4974 (0.3595) Acc D Real: 78.371% 
Loss D Fake: 0.9170 (1.1903) Acc D Fake: 46.140% 
Loss D: 1.414 
Loss G: 0.5164 (0.5864) Acc G: 53.338% 
LR: 2.000e-04 

2023-03-02 01:46:02,396 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.4771 (0.3601) Acc D Real: 78.436% 
Loss D Fake: 0.9153 (1.1890) Acc D Fake: 45.947% 
Loss D: 1.392 
Loss G: 0.5174 (0.5861) Acc G: 53.534% 
LR: 2.000e-04 

2023-03-02 01:46:02,404 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4665 (0.3606) Acc D Real: 78.493% 
Loss D Fake: 0.9137 (1.1876) Acc D Fake: 45.755% 
Loss D: 1.380 
Loss G: 0.5186 (0.5858) Acc G: 53.728% 
LR: 2.000e-04 

2023-03-02 01:46:02,411 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4669 (0.3612) Acc D Real: 78.548% 
Loss D Fake: 0.9118 (1.1863) Acc D Fake: 45.565% 
Loss D: 1.379 
Loss G: 0.5199 (0.5854) Acc G: 53.921% 
LR: 2.000e-04 

2023-03-02 01:46:02,419 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4579 (0.3616) Acc D Real: 78.614% 
Loss D Fake: 0.9097 (1.1849) Acc D Fake: 45.377% 
Loss D: 1.368 
Loss G: 0.5213 (0.5851) Acc G: 54.111% 
LR: 2.000e-04 

2023-03-02 01:46:02,426 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4820 (0.3622) Acc D Real: 78.678% 
Loss D Fake: 0.9076 (1.1836) Acc D Fake: 45.191% 
Loss D: 1.390 
Loss G: 0.5226 (0.5848) Acc G: 54.300% 
LR: 2.000e-04 

2023-03-02 01:46:02,433 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.5015 (0.3629) Acc D Real: 78.727% 
Loss D Fake: 0.9060 (1.1823) Acc D Fake: 45.007% 
Loss D: 1.407 
Loss G: 0.5235 (0.5845) Acc G: 54.486% 
LR: 2.000e-04 

2023-03-02 01:46:02,441 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4877 (0.3635) Acc D Real: 78.793% 
Loss D Fake: 0.9048 (1.1810) Acc D Fake: 44.824% 
Loss D: 1.392 
Loss G: 0.5241 (0.5843) Acc G: 54.671% 
LR: 2.000e-04 

2023-03-02 01:46:02,448 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4714 (0.3640) Acc D Real: 78.848% 
Loss D Fake: 0.9039 (1.1797) Acc D Fake: 44.644% 
Loss D: 1.375 
Loss G: 0.5248 (0.5840) Acc G: 54.855% 
LR: 2.000e-04 

2023-03-02 01:46:02,456 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4717 (0.3645) Acc D Real: 78.917% 
Loss D Fake: 0.9029 (1.1783) Acc D Fake: 44.464% 
Loss D: 1.375 
Loss G: 0.5254 (0.5837) Acc G: 55.036% 
LR: 2.000e-04 

2023-03-02 01:46:02,463 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4640 (0.3650) Acc D Real: 78.972% 
Loss D Fake: 0.9019 (1.1770) Acc D Fake: 44.287% 
Loss D: 1.366 
Loss G: 0.5261 (0.5834) Acc G: 55.216% 
LR: 2.000e-04 

2023-03-02 01:46:02,470 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4754 (0.3655) Acc D Real: 79.035% 
Loss D Fake: 0.9007 (1.1758) Acc D Fake: 44.111% 
Loss D: 1.376 
Loss G: 0.5268 (0.5832) Acc G: 55.394% 
LR: 2.000e-04 

2023-03-02 01:46:02,478 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4900 (0.3660) Acc D Real: 79.097% 
Loss D Fake: 0.8998 (1.1745) Acc D Fake: 43.937% 
Loss D: 1.390 
Loss G: 0.5273 (0.5829) Acc G: 55.570% 
LR: 2.000e-04 

2023-03-02 01:46:02,485 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4524 (0.3664) Acc D Real: 79.135% 
Loss D Fake: 0.8990 (1.1732) Acc D Fake: 43.764% 
Loss D: 1.351 
Loss G: 0.5282 (0.5826) Acc G: 55.745% 
LR: 2.000e-04 

2023-03-02 01:46:02,493 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4649 (0.3669) Acc D Real: 79.188% 
Loss D Fake: 0.8974 (1.1719) Acc D Fake: 43.594% 
Loss D: 1.362 
Loss G: 0.5292 (0.5824) Acc G: 55.919% 
LR: 2.000e-04 

2023-03-02 01:46:02,500 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4850 (0.3674) Acc D Real: 79.250% 
Loss D Fake: 0.8959 (1.1707) Acc D Fake: 43.424% 
Loss D: 1.381 
Loss G: 0.5301 (0.5822) Acc G: 56.090% 
LR: 2.000e-04 

2023-03-02 01:46:02,508 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4685 (0.3679) Acc D Real: 79.293% 
Loss D Fake: 0.8947 (1.1694) Acc D Fake: 43.256% 
Loss D: 1.363 
Loss G: 0.5310 (0.5819) Acc G: 56.260% 
LR: 2.000e-04 

2023-03-02 01:46:02,515 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4354 (0.3682) Acc D Real: 79.346% 
Loss D Fake: 0.8930 (1.1681) Acc D Fake: 43.090% 
Loss D: 1.328 
Loss G: 0.5324 (0.5817) Acc G: 56.429% 
LR: 2.000e-04 

2023-03-02 01:46:02,522 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4622 (0.3686) Acc D Real: 79.397% 
Loss D Fake: 0.8908 (1.1669) Acc D Fake: 42.925% 
Loss D: 1.353 
Loss G: 0.5338 (0.5815) Acc G: 56.596% 
LR: 2.000e-04 

2023-03-02 01:46:02,530 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.4685 (0.3691) Acc D Real: 79.447% 
Loss D Fake: 0.8887 (1.1656) Acc D Fake: 42.762% 
Loss D: 1.357 
Loss G: 0.5353 (0.5813) Acc G: 56.761% 
LR: 2.000e-04 

2023-03-02 01:46:02,537 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4327 (0.3694) Acc D Real: 79.494% 
Loss D Fake: 0.8864 (1.1644) Acc D Fake: 42.600% 
Loss D: 1.319 
Loss G: 0.5371 (0.5811) Acc G: 56.925% 
LR: 2.000e-04 

2023-03-02 01:46:02,545 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4376 (0.3697) Acc D Real: 79.550% 
Loss D Fake: 0.8835 (1.1631) Acc D Fake: 42.440% 
Loss D: 1.321 
Loss G: 0.5391 (0.5809) Acc G: 57.088% 
LR: 2.000e-04 

2023-03-02 01:46:02,552 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4754 (0.3701) Acc D Real: 79.603% 
Loss D Fake: 0.8806 (1.1619) Acc D Fake: 42.281% 
Loss D: 1.356 
Loss G: 0.5409 (0.5807) Acc G: 57.249% 
LR: 2.000e-04 

2023-03-02 01:46:02,560 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4457 (0.3705) Acc D Real: 79.617% 
Loss D Fake: 0.8781 (1.1606) Acc D Fake: 42.241% 
Loss D: 1.324 
Loss G: 0.5427 (0.5805) Acc G: 57.289% 
LR: 2.000e-04 

2023-03-02 01:46:02,572 -                train: [    INFO] - 
Epoch: 8/20
2023-03-02 01:46:02,746 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4689 (0.4838) Acc D Real: 92.214% 
Loss D Fake: 0.8741 (0.8749) Acc D Fake: 6.667% 
Loss D: 1.343 
Loss G: 0.5452 (0.5446) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,754 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.5015 (0.4897) Acc D Real: 91.372% 
Loss D Fake: 0.8727 (0.8742) Acc D Fake: 6.667% 
Loss D: 1.374 
Loss G: 0.5460 (0.5451) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,761 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4914 (0.4901) Acc D Real: 91.328% 
Loss D Fake: 0.8717 (0.8735) Acc D Fake: 6.667% 
Loss D: 1.363 
Loss G: 0.5465 (0.5454) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,779 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4636 (0.4848) Acc D Real: 91.302% 
Loss D Fake: 0.8709 (0.8730) Acc D Fake: 6.667% 
Loss D: 1.334 
Loss G: 0.5471 (0.5458) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,786 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4523 (0.4794) Acc D Real: 91.554% 
Loss D Fake: 0.8700 (0.8725) Acc D Fake: 6.667% 
Loss D: 1.322 
Loss G: 0.5478 (0.5461) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,793 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4568 (0.4761) Acc D Real: 91.592% 
Loss D Fake: 0.8689 (0.8720) Acc D Fake: 6.667% 
Loss D: 1.326 
Loss G: 0.5486 (0.5465) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,800 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.5149 (0.4810) Acc D Real: 91.120% 
Loss D Fake: 0.8679 (0.8715) Acc D Fake: 6.667% 
Loss D: 1.383 
Loss G: 0.5492 (0.5468) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,807 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.5010 (0.4832) Acc D Real: 91.007% 
Loss D Fake: 0.8672 (0.8710) Acc D Fake: 6.667% 
Loss D: 1.368 
Loss G: 0.5495 (0.5471) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,814 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.5084 (0.4857) Acc D Real: 90.964% 
Loss D Fake: 0.8670 (0.8706) Acc D Fake: 6.667% 
Loss D: 1.375 
Loss G: 0.5496 (0.5474) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,821 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4835 (0.4855) Acc D Real: 91.075% 
Loss D Fake: 0.8670 (0.8703) Acc D Fake: 6.667% 
Loss D: 1.351 
Loss G: 0.5495 (0.5476) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,828 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4854 (0.4855) Acc D Real: 91.328% 
Loss D Fake: 0.8673 (0.8700) Acc D Fake: 6.667% 
Loss D: 1.353 
Loss G: 0.5492 (0.5477) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,836 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.4938 (0.4862) Acc D Real: 91.342% 
Loss D Fake: 0.8678 (0.8699) Acc D Fake: 6.667% 
Loss D: 1.362 
Loss G: 0.5488 (0.5478) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,843 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4268 (0.4819) Acc D Real: 91.332% 
Loss D Fake: 0.8682 (0.8697) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5488 (0.5479) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,850 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4789 (0.4817) Acc D Real: 91.285% 
Loss D Fake: 0.8678 (0.8696) Acc D Fake: 6.667% 
Loss D: 1.347 
Loss G: 0.5490 (0.5479) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,857 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4259 (0.4782) Acc D Real: 91.260% 
Loss D Fake: 0.8673 (0.8695) Acc D Fake: 6.667% 
Loss D: 1.293 
Loss G: 0.5497 (0.5480) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,864 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4576 (0.4770) Acc D Real: 91.351% 
Loss D Fake: 0.8662 (0.8693) Acc D Fake: 6.667% 
Loss D: 1.324 
Loss G: 0.5505 (0.5482) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,871 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4930 (0.4779) Acc D Real: 91.270% 
Loss D Fake: 0.8652 (0.8691) Acc D Fake: 6.667% 
Loss D: 1.358 
Loss G: 0.5510 (0.5483) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,878 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4572 (0.4768) Acc D Real: 91.118% 
Loss D Fake: 0.8644 (0.8688) Acc D Fake: 6.667% 
Loss D: 1.322 
Loss G: 0.5518 (0.5485) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,885 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4983 (0.4779) Acc D Real: 91.156% 
Loss D Fake: 0.8633 (0.8685) Acc D Fake: 6.667% 
Loss D: 1.362 
Loss G: 0.5524 (0.5487) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,892 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.5017 (0.4790) Acc D Real: 91.094% 
Loss D Fake: 0.8628 (0.8683) Acc D Fake: 6.667% 
Loss D: 1.364 
Loss G: 0.5526 (0.5489) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,899 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4453 (0.4775) Acc D Real: 91.153% 
Loss D Fake: 0.8624 (0.8680) Acc D Fake: 6.667% 
Loss D: 1.308 
Loss G: 0.5530 (0.5491) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,906 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4696 (0.4771) Acc D Real: 91.107% 
Loss D Fake: 0.8617 (0.8677) Acc D Fake: 6.667% 
Loss D: 1.331 
Loss G: 0.5535 (0.5493) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,913 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4389 (0.4756) Acc D Real: 91.115% 
Loss D Fake: 0.8609 (0.8674) Acc D Fake: 6.667% 
Loss D: 1.300 
Loss G: 0.5543 (0.5495) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,920 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4351 (0.4739) Acc D Real: 91.133% 
Loss D Fake: 0.8595 (0.8671) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5554 (0.5497) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,927 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4498 (0.4730) Acc D Real: 91.120% 
Loss D Fake: 0.8579 (0.8668) Acc D Fake: 6.667% 
Loss D: 1.308 
Loss G: 0.5567 (0.5500) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,934 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.5027 (0.4741) Acc D Real: 91.202% 
Loss D Fake: 0.8563 (0.8664) Acc D Fake: 6.667% 
Loss D: 1.359 
Loss G: 0.5575 (0.5503) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,941 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4487 (0.4732) Acc D Real: 91.265% 
Loss D Fake: 0.8554 (0.8660) Acc D Fake: 6.667% 
Loss D: 1.304 
Loss G: 0.5582 (0.5506) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,949 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4700 (0.4731) Acc D Real: 91.180% 
Loss D Fake: 0.8544 (0.8656) Acc D Fake: 6.667% 
Loss D: 1.324 
Loss G: 0.5589 (0.5508) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,956 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.5268 (0.4749) Acc D Real: 91.160% 
Loss D Fake: 0.8536 (0.8652) Acc D Fake: 6.667% 
Loss D: 1.380 
Loss G: 0.5592 (0.5511) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,964 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4468 (0.4740) Acc D Real: 91.178% 
Loss D Fake: 0.8534 (0.8648) Acc D Fake: 6.667% 
Loss D: 1.300 
Loss G: 0.5595 (0.5514) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,971 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.5068 (0.4750) Acc D Real: 91.154% 
Loss D Fake: 0.8530 (0.8644) Acc D Fake: 6.667% 
Loss D: 1.360 
Loss G: 0.5596 (0.5517) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,979 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4886 (0.4754) Acc D Real: 91.097% 
Loss D Fake: 0.8529 (0.8641) Acc D Fake: 6.667% 
Loss D: 1.342 
Loss G: 0.5596 (0.5519) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,987 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4556 (0.4748) Acc D Real: 91.131% 
Loss D Fake: 0.8528 (0.8638) Acc D Fake: 6.667% 
Loss D: 1.308 
Loss G: 0.5598 (0.5521) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:02,994 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4832 (0.4751) Acc D Real: 91.042% 
Loss D Fake: 0.8525 (0.8634) Acc D Fake: 6.667% 
Loss D: 1.336 
Loss G: 0.5601 (0.5524) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,002 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4924 (0.4755) Acc D Real: 91.056% 
Loss D Fake: 0.8522 (0.8631) Acc D Fake: 6.667% 
Loss D: 1.345 
Loss G: 0.5602 (0.5526) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,009 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4570 (0.4750) Acc D Real: 90.999% 
Loss D Fake: 0.8520 (0.8628) Acc D Fake: 6.667% 
Loss D: 1.309 
Loss G: 0.5604 (0.5528) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,017 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4420 (0.4742) Acc D Real: 91.039% 
Loss D Fake: 0.8515 (0.8625) Acc D Fake: 6.667% 
Loss D: 1.293 
Loss G: 0.5609 (0.5530) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,025 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4774 (0.4743) Acc D Real: 91.126% 
Loss D Fake: 0.8509 (0.8622) Acc D Fake: 6.667% 
Loss D: 1.328 
Loss G: 0.5612 (0.5532) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,032 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4336 (0.4732) Acc D Real: 91.112% 
Loss D Fake: 0.8504 (0.8619) Acc D Fake: 6.667% 
Loss D: 1.284 
Loss G: 0.5618 (0.5534) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,040 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.5340 (0.4747) Acc D Real: 91.033% 
Loss D Fake: 0.8497 (0.8616) Acc D Fake: 6.667% 
Loss D: 1.384 
Loss G: 0.5620 (0.5536) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,048 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4907 (0.4751) Acc D Real: 91.049% 
Loss D Fake: 0.8497 (0.8613) Acc D Fake: 6.667% 
Loss D: 1.340 
Loss G: 0.5619 (0.5538) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,055 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4788 (0.4752) Acc D Real: 91.047% 
Loss D Fake: 0.8499 (0.8611) Acc D Fake: 6.667% 
Loss D: 1.329 
Loss G: 0.5618 (0.5540) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,063 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4801 (0.4753) Acc D Real: 90.997% 
Loss D Fake: 0.8500 (0.8608) Acc D Fake: 6.667% 
Loss D: 1.330 
Loss G: 0.5617 (0.5542) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,070 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.4786 (0.4754) Acc D Real: 90.985% 
Loss D Fake: 0.8500 (0.8606) Acc D Fake: 6.667% 
Loss D: 1.329 
Loss G: 0.5617 (0.5544) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,078 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4139 (0.4740) Acc D Real: 90.966% 
Loss D Fake: 0.8498 (0.8604) Acc D Fake: 6.667% 
Loss D: 1.264 
Loss G: 0.5622 (0.5545) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,085 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4798 (0.4742) Acc D Real: 90.957% 
Loss D Fake: 0.8489 (0.8601) Acc D Fake: 6.667% 
Loss D: 1.329 
Loss G: 0.5628 (0.5547) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,092 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4789 (0.4743) Acc D Real: 90.984% 
Loss D Fake: 0.8482 (0.8599) Acc D Fake: 6.667% 
Loss D: 1.327 
Loss G: 0.5632 (0.5549) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,100 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4630 (0.4740) Acc D Real: 90.960% 
Loss D Fake: 0.8477 (0.8596) Acc D Fake: 6.667% 
Loss D: 1.311 
Loss G: 0.5637 (0.5551) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,107 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4864 (0.4743) Acc D Real: 91.004% 
Loss D Fake: 0.8471 (0.8594) Acc D Fake: 6.667% 
Loss D: 1.334 
Loss G: 0.5639 (0.5552) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,114 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4885 (0.4746) Acc D Real: 91.056% 
Loss D Fake: 0.8471 (0.8591) Acc D Fake: 6.667% 
Loss D: 1.336 
Loss G: 0.5638 (0.5554) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,122 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4422 (0.4739) Acc D Real: 91.061% 
Loss D Fake: 0.8472 (0.8589) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.5638 (0.5556) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,129 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4376 (0.4732) Acc D Real: 91.069% 
Loss D Fake: 0.8470 (0.8587) Acc D Fake: 6.667% 
Loss D: 1.285 
Loss G: 0.5641 (0.5557) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,137 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4652 (0.4731) Acc D Real: 91.073% 
Loss D Fake: 0.8464 (0.8584) Acc D Fake: 6.667% 
Loss D: 1.312 
Loss G: 0.5645 (0.5559) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,144 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4432 (0.4726) Acc D Real: 91.061% 
Loss D Fake: 0.8458 (0.8582) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.5651 (0.5561) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,151 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4356 (0.4719) Acc D Real: 91.081% 
Loss D Fake: 0.8449 (0.8580) Acc D Fake: 6.667% 
Loss D: 1.280 
Loss G: 0.5659 (0.5562) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,159 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4604 (0.4717) Acc D Real: 91.100% 
Loss D Fake: 0.8438 (0.8577) Acc D Fake: 6.667% 
Loss D: 1.304 
Loss G: 0.5666 (0.5564) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,166 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4300 (0.4710) Acc D Real: 91.116% 
Loss D Fake: 0.8428 (0.8575) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.5675 (0.5566) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,173 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4385 (0.4704) Acc D Real: 91.110% 
Loss D Fake: 0.8415 (0.8572) Acc D Fake: 6.667% 
Loss D: 1.280 
Loss G: 0.5686 (0.5568) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,181 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4693 (0.4704) Acc D Real: 91.101% 
Loss D Fake: 0.8400 (0.8569) Acc D Fake: 6.667% 
Loss D: 1.309 
Loss G: 0.5696 (0.5570) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,188 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4499 (0.4701) Acc D Real: 91.124% 
Loss D Fake: 0.8388 (0.8566) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.5705 (0.5572) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,195 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4587 (0.4699) Acc D Real: 91.135% 
Loss D Fake: 0.8376 (0.8563) Acc D Fake: 6.667% 
Loss D: 1.296 
Loss G: 0.5714 (0.5575) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,203 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4781 (0.4700) Acc D Real: 91.149% 
Loss D Fake: 0.8366 (0.8560) Acc D Fake: 6.667% 
Loss D: 1.315 
Loss G: 0.5720 (0.5577) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,210 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4692 (0.4700) Acc D Real: 91.129% 
Loss D Fake: 0.8358 (0.8557) Acc D Fake: 6.667% 
Loss D: 1.305 
Loss G: 0.5725 (0.5579) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,217 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.5379 (0.4710) Acc D Real: 91.101% 
Loss D Fake: 0.8353 (0.8554) Acc D Fake: 6.665% 
Loss D: 1.373 
Loss G: 0.5726 (0.5582) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:03,225 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4444 (0.4706) Acc D Real: 91.103% 
Loss D Fake: 0.8354 (0.8551) Acc D Fake: 6.640% 
Loss D: 1.280 
Loss G: 0.5727 (0.5584) Acc G: 93.359% 
LR: 2.000e-04 

2023-03-02 01:46:03,232 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4219 (0.4699) Acc D Real: 91.122% 
Loss D Fake: 0.8350 (0.8548) Acc D Fake: 6.615% 
Loss D: 1.257 
Loss G: 0.5731 (0.5586) Acc G: 93.383% 
LR: 2.000e-04 

2023-03-02 01:46:03,240 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4466 (0.4696) Acc D Real: 91.121% 
Loss D Fake: 0.8343 (0.8545) Acc D Fake: 6.592% 
Loss D: 1.281 
Loss G: 0.5737 (0.5588) Acc G: 93.407% 
LR: 2.000e-04 

2023-03-02 01:46:03,248 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4520 (0.4693) Acc D Real: 91.137% 
Loss D Fake: 0.8334 (0.8542) Acc D Fake: 6.569% 
Loss D: 1.285 
Loss G: 0.5744 (0.5590) Acc G: 93.430% 
LR: 2.000e-04 

2023-03-02 01:46:03,255 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4949 (0.4697) Acc D Real: 91.144% 
Loss D Fake: 0.8326 (0.8539) Acc D Fake: 6.546% 
Loss D: 1.328 
Loss G: 0.5748 (0.5593) Acc G: 93.452% 
LR: 2.000e-04 

2023-03-02 01:46:03,263 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.5279 (0.4705) Acc D Real: 91.144% 
Loss D Fake: 0.8325 (0.8536) Acc D Fake: 6.524% 
Loss D: 1.360 
Loss G: 0.5746 (0.5595) Acc G: 93.474% 
LR: 2.000e-04 

2023-03-02 01:46:03,270 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4930 (0.4708) Acc D Real: 91.148% 
Loss D Fake: 0.8331 (0.8533) Acc D Fake: 6.503% 
Loss D: 1.326 
Loss G: 0.5741 (0.5597) Acc G: 93.495% 
LR: 2.000e-04 

2023-03-02 01:46:03,278 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4685 (0.4708) Acc D Real: 91.149% 
Loss D Fake: 0.8337 (0.8530) Acc D Fake: 6.483% 
Loss D: 1.302 
Loss G: 0.5736 (0.5599) Acc G: 93.516% 
LR: 2.000e-04 

2023-03-02 01:46:03,285 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4316 (0.4703) Acc D Real: 91.147% 
Loss D Fake: 0.8341 (0.8527) Acc D Fake: 6.463% 
Loss D: 1.266 
Loss G: 0.5736 (0.5601) Acc G: 93.536% 
LR: 2.000e-04 

2023-03-02 01:46:03,293 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4322 (0.4697) Acc D Real: 91.165% 
Loss D Fake: 0.8339 (0.8525) Acc D Fake: 6.443% 
Loss D: 1.266 
Loss G: 0.5738 (0.5603) Acc G: 93.556% 
LR: 2.000e-04 

2023-03-02 01:46:03,300 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4945 (0.4701) Acc D Real: 91.165% 
Loss D Fake: 0.8336 (0.8522) Acc D Fake: 6.424% 
Loss D: 1.328 
Loss G: 0.5739 (0.5604) Acc G: 93.575% 
LR: 2.000e-04 

2023-03-02 01:46:03,308 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4331 (0.4696) Acc D Real: 91.198% 
Loss D Fake: 0.8335 (0.8520) Acc D Fake: 6.406% 
Loss D: 1.267 
Loss G: 0.5741 (0.5606) Acc G: 93.593% 
LR: 2.000e-04 

2023-03-02 01:46:03,316 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4962 (0.4699) Acc D Real: 91.174% 
Loss D Fake: 0.8334 (0.8518) Acc D Fake: 6.388% 
Loss D: 1.330 
Loss G: 0.5741 (0.5608) Acc G: 93.611% 
LR: 2.000e-04 

2023-03-02 01:46:03,323 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4188 (0.4693) Acc D Real: 91.205% 
Loss D Fake: 0.8333 (0.8515) Acc D Fake: 6.370% 
Loss D: 1.252 
Loss G: 0.5743 (0.5610) Acc G: 93.629% 
LR: 2.000e-04 

2023-03-02 01:46:03,331 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4838 (0.4695) Acc D Real: 91.223% 
Loss D Fake: 0.8330 (0.8513) Acc D Fake: 6.353% 
Loss D: 1.317 
Loss G: 0.5744 (0.5611) Acc G: 93.646% 
LR: 2.000e-04 

2023-03-02 01:46:03,339 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4861 (0.4697) Acc D Real: 91.245% 
Loss D Fake: 0.8331 (0.8511) Acc D Fake: 6.336% 
Loss D: 1.319 
Loss G: 0.5742 (0.5613) Acc G: 93.663% 
LR: 2.000e-04 

2023-03-02 01:46:03,346 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.5009 (0.4701) Acc D Real: 91.254% 
Loss D Fake: 0.8335 (0.8509) Acc D Fake: 6.320% 
Loss D: 1.334 
Loss G: 0.5737 (0.5614) Acc G: 93.679% 
LR: 2.000e-04 

2023-03-02 01:46:03,353 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3925 (0.4691) Acc D Real: 91.251% 
Loss D Fake: 0.8339 (0.8507) Acc D Fake: 6.304% 
Loss D: 1.226 
Loss G: 0.5738 (0.5616) Acc G: 93.695% 
LR: 2.000e-04 

2023-03-02 01:46:03,361 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4814 (0.4693) Acc D Real: 91.229% 
Loss D Fake: 0.8335 (0.8504) Acc D Fake: 6.288% 
Loss D: 1.315 
Loss G: 0.5741 (0.5617) Acc G: 93.710% 
LR: 2.000e-04 

2023-03-02 01:46:03,368 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4731 (0.4693) Acc D Real: 91.239% 
Loss D Fake: 0.8332 (0.8502) Acc D Fake: 6.273% 
Loss D: 1.306 
Loss G: 0.5742 (0.5619) Acc G: 93.725% 
LR: 2.000e-04 

2023-03-02 01:46:03,376 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4432 (0.4690) Acc D Real: 91.272% 
Loss D Fake: 0.8331 (0.8500) Acc D Fake: 6.258% 
Loss D: 1.276 
Loss G: 0.5743 (0.5620) Acc G: 93.740% 
LR: 2.000e-04 

2023-03-02 01:46:03,384 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.5271 (0.4697) Acc D Real: 91.306% 
Loss D Fake: 0.8332 (0.8499) Acc D Fake: 6.244% 
Loss D: 1.360 
Loss G: 0.5738 (0.5622) Acc G: 93.755% 
LR: 2.000e-04 

2023-03-02 01:46:03,391 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.5013 (0.4700) Acc D Real: 91.336% 
Loss D Fake: 0.8342 (0.8497) Acc D Fake: 6.230% 
Loss D: 1.336 
Loss G: 0.5729 (0.5623) Acc G: 93.769% 
LR: 2.000e-04 

2023-03-02 01:46:03,399 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4335 (0.4696) Acc D Real: 91.316% 
Loss D Fake: 0.8354 (0.8495) Acc D Fake: 6.216% 
Loss D: 1.269 
Loss G: 0.5723 (0.5624) Acc G: 93.783% 
LR: 2.000e-04 

2023-03-02 01:46:03,408 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4646 (0.4696) Acc D Real: 91.319% 
Loss D Fake: 0.8358 (0.8494) Acc D Fake: 6.203% 
Loss D: 1.300 
Loss G: 0.5721 (0.5625) Acc G: 93.796% 
LR: 2.000e-04 

2023-03-02 01:46:03,417 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4583 (0.4694) Acc D Real: 91.323% 
Loss D Fake: 0.8361 (0.8492) Acc D Fake: 6.189% 
Loss D: 1.294 
Loss G: 0.5719 (0.5626) Acc G: 93.810% 
LR: 2.000e-04 

2023-03-02 01:46:03,426 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4553 (0.4693) Acc D Real: 91.343% 
Loss D Fake: 0.8363 (0.8491) Acc D Fake: 6.176% 
Loss D: 1.292 
Loss G: 0.5718 (0.5627) Acc G: 93.822% 
LR: 2.000e-04 

2023-03-02 01:46:03,434 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.4351 (0.4689) Acc D Real: 91.363% 
Loss D Fake: 0.8364 (0.8489) Acc D Fake: 6.164% 
Loss D: 1.271 
Loss G: 0.5718 (0.5628) Acc G: 93.835% 
LR: 2.000e-04 

2023-03-02 01:46:03,442 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3810 (0.4680) Acc D Real: 91.382% 
Loss D Fake: 0.8360 (0.8488) Acc D Fake: 6.151% 
Loss D: 1.217 
Loss G: 0.5724 (0.5629) Acc G: 93.848% 
LR: 2.000e-04 

2023-03-02 01:46:03,449 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.4843 (0.4682) Acc D Real: 91.388% 
Loss D Fake: 0.8351 (0.8487) Acc D Fake: 6.139% 
Loss D: 1.319 
Loss G: 0.5729 (0.5630) Acc G: 93.860% 
LR: 2.000e-04 

2023-03-02 01:46:03,457 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4252 (0.4677) Acc D Real: 91.395% 
Loss D Fake: 0.8345 (0.8485) Acc D Fake: 6.127% 
Loss D: 1.260 
Loss G: 0.5735 (0.5631) Acc G: 93.872% 
LR: 2.000e-04 

2023-03-02 01:46:03,464 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4003 (0.4670) Acc D Real: 91.387% 
Loss D Fake: 0.8334 (0.8484) Acc D Fake: 6.116% 
Loss D: 1.234 
Loss G: 0.5747 (0.5632) Acc G: 93.883% 
LR: 2.000e-04 

2023-03-02 01:46:03,471 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4534 (0.4669) Acc D Real: 91.400% 
Loss D Fake: 0.8318 (0.8482) Acc D Fake: 6.104% 
Loss D: 1.285 
Loss G: 0.5758 (0.5634) Acc G: 93.895% 
LR: 2.000e-04 

2023-03-02 01:46:03,479 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4287 (0.4665) Acc D Real: 91.391% 
Loss D Fake: 0.8302 (0.8480) Acc D Fake: 6.093% 
Loss D: 1.259 
Loss G: 0.5771 (0.5635) Acc G: 93.906% 
LR: 2.000e-04 

2023-03-02 01:46:03,486 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4362 (0.4662) Acc D Real: 91.400% 
Loss D Fake: 0.8285 (0.8478) Acc D Fake: 6.082% 
Loss D: 1.265 
Loss G: 0.5784 (0.5637) Acc G: 93.917% 
LR: 2.000e-04 

2023-03-02 01:46:03,494 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.5131 (0.4666) Acc D Real: 91.408% 
Loss D Fake: 0.8270 (0.8476) Acc D Fake: 6.072% 
Loss D: 1.340 
Loss G: 0.5792 (0.5638) Acc G: 93.927% 
LR: 2.000e-04 

2023-03-02 01:46:03,501 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.5160 (0.4671) Acc D Real: 91.410% 
Loss D Fake: 0.8265 (0.8474) Acc D Fake: 6.061% 
Loss D: 1.343 
Loss G: 0.5793 (0.5640) Acc G: 93.938% 
LR: 2.000e-04 

2023-03-02 01:46:03,508 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3981 (0.4665) Acc D Real: 91.403% 
Loss D Fake: 0.8263 (0.8472) Acc D Fake: 6.051% 
Loss D: 1.224 
Loss G: 0.5798 (0.5641) Acc G: 93.948% 
LR: 2.000e-04 

2023-03-02 01:46:03,516 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3400 (0.4652) Acc D Real: 91.399% 
Loss D Fake: 0.8251 (0.8470) Acc D Fake: 6.041% 
Loss D: 1.165 
Loss G: 0.5813 (0.5643) Acc G: 93.958% 
LR: 2.000e-04 

2023-03-02 01:46:03,523 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4496 (0.4651) Acc D Real: 91.405% 
Loss D Fake: 0.8229 (0.8468) Acc D Fake: 6.031% 
Loss D: 1.272 
Loss G: 0.5829 (0.5645) Acc G: 93.968% 
LR: 2.000e-04 

2023-03-02 01:46:03,530 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.5074 (0.4655) Acc D Real: 91.403% 
Loss D Fake: 0.8211 (0.8465) Acc D Fake: 6.021% 
Loss D: 1.329 
Loss G: 0.5839 (0.5646) Acc G: 93.978% 
LR: 2.000e-04 

2023-03-02 01:46:03,537 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.5031 (0.4658) Acc D Real: 91.386% 
Loss D Fake: 0.8201 (0.8463) Acc D Fake: 6.011% 
Loss D: 1.323 
Loss G: 0.5845 (0.5648) Acc G: 93.988% 
LR: 2.000e-04 

2023-03-02 01:46:03,545 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.5352 (0.4665) Acc D Real: 91.370% 
Loss D Fake: 0.8197 (0.8460) Acc D Fake: 6.002% 
Loss D: 1.355 
Loss G: 0.5845 (0.5650) Acc G: 93.997% 
LR: 2.000e-04 

2023-03-02 01:46:03,552 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4783 (0.4666) Acc D Real: 91.379% 
Loss D Fake: 0.8199 (0.8458) Acc D Fake: 5.993% 
Loss D: 1.298 
Loss G: 0.5842 (0.5652) Acc G: 94.006% 
LR: 2.000e-04 

2023-03-02 01:46:03,559 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.5130 (0.4670) Acc D Real: 91.370% 
Loss D Fake: 0.8204 (0.8455) Acc D Fake: 5.984% 
Loss D: 1.333 
Loss G: 0.5837 (0.5654) Acc G: 94.015% 
LR: 2.000e-04 

2023-03-02 01:46:03,566 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4648 (0.4670) Acc D Real: 91.380% 
Loss D Fake: 0.8212 (0.8453) Acc D Fake: 5.975% 
Loss D: 1.286 
Loss G: 0.5831 (0.5655) Acc G: 94.024% 
LR: 2.000e-04 

2023-03-02 01:46:03,576 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4927 (0.4672) Acc D Real: 91.384% 
Loss D Fake: 0.8220 (0.8451) Acc D Fake: 5.966% 
Loss D: 1.315 
Loss G: 0.5823 (0.5657) Acc G: 94.033% 
LR: 2.000e-04 

2023-03-02 01:46:03,584 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4089 (0.4667) Acc D Real: 91.375% 
Loss D Fake: 0.8228 (0.8449) Acc D Fake: 5.958% 
Loss D: 1.232 
Loss G: 0.5821 (0.5658) Acc G: 94.041% 
LR: 2.000e-04 

2023-03-02 01:46:03,591 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4183 (0.4663) Acc D Real: 91.391% 
Loss D Fake: 0.8228 (0.8447) Acc D Fake: 5.949% 
Loss D: 1.241 
Loss G: 0.5823 (0.5660) Acc G: 94.050% 
LR: 2.000e-04 

2023-03-02 01:46:03,598 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4026 (0.4657) Acc D Real: 91.413% 
Loss D Fake: 0.8223 (0.8445) Acc D Fake: 5.941% 
Loss D: 1.225 
Loss G: 0.5828 (0.5661) Acc G: 94.058% 
LR: 2.000e-04 

2023-03-02 01:46:03,606 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4545 (0.4656) Acc D Real: 91.442% 
Loss D Fake: 0.8216 (0.8443) Acc D Fake: 5.933% 
Loss D: 1.276 
Loss G: 0.5832 (0.5662) Acc G: 94.066% 
LR: 2.000e-04 

2023-03-02 01:46:03,613 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4221 (0.4653) Acc D Real: 91.455% 
Loss D Fake: 0.8211 (0.8441) Acc D Fake: 5.925% 
Loss D: 1.243 
Loss G: 0.5837 (0.5664) Acc G: 94.074% 
LR: 2.000e-04 

2023-03-02 01:46:03,620 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4527 (0.4652) Acc D Real: 91.456% 
Loss D Fake: 0.8204 (0.8439) Acc D Fake: 5.917% 
Loss D: 1.273 
Loss G: 0.5843 (0.5665) Acc G: 94.082% 
LR: 2.000e-04 

2023-03-02 01:46:03,628 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4082 (0.4647) Acc D Real: 91.470% 
Loss D Fake: 0.8196 (0.8437) Acc D Fake: 5.909% 
Loss D: 1.228 
Loss G: 0.5850 (0.5667) Acc G: 94.090% 
LR: 2.000e-04 

2023-03-02 01:46:03,635 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4476 (0.4645) Acc D Real: 91.476% 
Loss D Fake: 0.8186 (0.8435) Acc D Fake: 5.902% 
Loss D: 1.266 
Loss G: 0.5858 (0.5669) Acc G: 94.097% 
LR: 2.000e-04 

2023-03-02 01:46:03,642 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4676 (0.4646) Acc D Real: 91.482% 
Loss D Fake: 0.8177 (0.8433) Acc D Fake: 5.894% 
Loss D: 1.285 
Loss G: 0.5864 (0.5670) Acc G: 94.105% 
LR: 2.000e-04 

2023-03-02 01:46:03,650 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4836 (0.4647) Acc D Real: 91.492% 
Loss D Fake: 0.8171 (0.8431) Acc D Fake: 5.887% 
Loss D: 1.301 
Loss G: 0.5866 (0.5672) Acc G: 94.112% 
LR: 2.000e-04 

2023-03-02 01:46:03,657 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4481 (0.4646) Acc D Real: 91.490% 
Loss D Fake: 0.8170 (0.8429) Acc D Fake: 5.880% 
Loss D: 1.265 
Loss G: 0.5868 (0.5673) Acc G: 94.119% 
LR: 2.000e-04 

2023-03-02 01:46:03,664 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4460 (0.4644) Acc D Real: 91.509% 
Loss D Fake: 0.8167 (0.8427) Acc D Fake: 5.873% 
Loss D: 1.263 
Loss G: 0.5870 (0.5675) Acc G: 94.126% 
LR: 2.000e-04 

2023-03-02 01:46:03,672 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.5064 (0.4648) Acc D Real: 91.514% 
Loss D Fake: 0.8166 (0.8425) Acc D Fake: 5.866% 
Loss D: 1.323 
Loss G: 0.5868 (0.5677) Acc G: 94.133% 
LR: 2.000e-04 

2023-03-02 01:46:03,679 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4136 (0.4644) Acc D Real: 91.523% 
Loss D Fake: 0.8169 (0.8423) Acc D Fake: 5.859% 
Loss D: 1.230 
Loss G: 0.5868 (0.5678) Acc G: 94.140% 
LR: 2.000e-04 

2023-03-02 01:46:03,687 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4066 (0.4639) Acc D Real: 91.524% 
Loss D Fake: 0.8166 (0.8421) Acc D Fake: 5.852% 
Loss D: 1.223 
Loss G: 0.5873 (0.5680) Acc G: 94.147% 
LR: 2.000e-04 

2023-03-02 01:46:03,695 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4042 (0.4634) Acc D Real: 91.527% 
Loss D Fake: 0.8157 (0.8418) Acc D Fake: 5.846% 
Loss D: 1.220 
Loss G: 0.5882 (0.5681) Acc G: 94.154% 
LR: 2.000e-04 

2023-03-02 01:46:03,702 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3839 (0.4628) Acc D Real: 91.537% 
Loss D Fake: 0.8142 (0.8416) Acc D Fake: 5.839% 
Loss D: 1.198 
Loss G: 0.5896 (0.5683) Acc G: 94.160% 
LR: 2.000e-04 

2023-03-02 01:46:03,710 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4722 (0.4629) Acc D Real: 91.535% 
Loss D Fake: 0.8125 (0.8414) Acc D Fake: 5.833% 
Loss D: 1.285 
Loss G: 0.5907 (0.5685) Acc G: 94.167% 
LR: 2.000e-04 

2023-03-02 01:46:03,717 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4181 (0.4626) Acc D Real: 91.538% 
Loss D Fake: 0.8110 (0.8412) Acc D Fake: 5.826% 
Loss D: 1.229 
Loss G: 0.5920 (0.5686) Acc G: 94.173% 
LR: 2.000e-04 

2023-03-02 01:46:03,725 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4092 (0.4622) Acc D Real: 91.545% 
Loss D Fake: 0.8094 (0.8409) Acc D Fake: 5.820% 
Loss D: 1.219 
Loss G: 0.5934 (0.5688) Acc G: 94.179% 
LR: 2.000e-04 

2023-03-02 01:46:03,733 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.5053 (0.4625) Acc D Real: 91.551% 
Loss D Fake: 0.8078 (0.8407) Acc D Fake: 5.814% 
Loss D: 1.313 
Loss G: 0.5943 (0.5690) Acc G: 94.185% 
LR: 2.000e-04 

2023-03-02 01:46:03,740 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4738 (0.4626) Acc D Real: 91.569% 
Loss D Fake: 0.8070 (0.8404) Acc D Fake: 5.808% 
Loss D: 1.281 
Loss G: 0.5947 (0.5692) Acc G: 94.192% 
LR: 2.000e-04 

2023-03-02 01:46:03,747 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3897 (0.4620) Acc D Real: 91.585% 
Loss D Fake: 0.8066 (0.8402) Acc D Fake: 5.802% 
Loss D: 1.196 
Loss G: 0.5953 (0.5694) Acc G: 94.198% 
LR: 2.000e-04 

2023-03-02 01:46:03,755 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.4302 (0.4618) Acc D Real: 91.603% 
Loss D Fake: 0.8057 (0.8399) Acc D Fake: 5.796% 
Loss D: 1.236 
Loss G: 0.5960 (0.5696) Acc G: 94.203% 
LR: 2.000e-04 

2023-03-02 01:46:03,764 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4438 (0.4617) Acc D Real: 91.613% 
Loss D Fake: 0.8048 (0.8397) Acc D Fake: 5.790% 
Loss D: 1.249 
Loss G: 0.5966 (0.5698) Acc G: 94.209% 
LR: 2.000e-04 

2023-03-02 01:46:03,772 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4922 (0.4619) Acc D Real: 91.631% 
Loss D Fake: 0.8042 (0.8394) Acc D Fake: 5.784% 
Loss D: 1.296 
Loss G: 0.5968 (0.5700) Acc G: 94.215% 
LR: 2.000e-04 

2023-03-02 01:46:03,780 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4936 (0.4621) Acc D Real: 91.648% 
Loss D Fake: 0.8044 (0.8392) Acc D Fake: 5.779% 
Loss D: 1.298 
Loss G: 0.5965 (0.5702) Acc G: 94.221% 
LR: 2.000e-04 

2023-03-02 01:46:03,787 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4639 (0.4621) Acc D Real: 91.643% 
Loss D Fake: 0.8049 (0.8389) Acc D Fake: 5.773% 
Loss D: 1.269 
Loss G: 0.5961 (0.5704) Acc G: 94.226% 
LR: 2.000e-04 

2023-03-02 01:46:03,794 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.5130 (0.4625) Acc D Real: 91.635% 
Loss D Fake: 0.8055 (0.8387) Acc D Fake: 5.768% 
Loss D: 1.318 
Loss G: 0.5954 (0.5705) Acc G: 94.232% 
LR: 2.000e-04 

2023-03-02 01:46:03,802 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.4771 (0.4626) Acc D Real: 91.643% 
Loss D Fake: 0.8064 (0.8385) Acc D Fake: 5.762% 
Loss D: 1.284 
Loss G: 0.5946 (0.5707) Acc G: 94.237% 
LR: 2.000e-04 

2023-03-02 01:46:03,809 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4117 (0.4622) Acc D Real: 91.660% 
Loss D Fake: 0.8073 (0.8382) Acc D Fake: 5.757% 
Loss D: 1.219 
Loss G: 0.5941 (0.5709) Acc G: 94.242% 
LR: 2.000e-04 

2023-03-02 01:46:03,816 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.5024 (0.4625) Acc D Real: 91.667% 
Loss D Fake: 0.8079 (0.8380) Acc D Fake: 5.752% 
Loss D: 1.310 
Loss G: 0.5934 (0.5710) Acc G: 94.248% 
LR: 2.000e-04 

2023-03-02 01:46:03,824 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4069 (0.4621) Acc D Real: 91.671% 
Loss D Fake: 0.8087 (0.8378) Acc D Fake: 5.746% 
Loss D: 1.216 
Loss G: 0.5931 (0.5712) Acc G: 94.253% 
LR: 2.000e-04 

2023-03-02 01:46:03,831 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4616 (0.4621) Acc D Real: 91.686% 
Loss D Fake: 0.8090 (0.8376) Acc D Fake: 5.741% 
Loss D: 1.271 
Loss G: 0.5928 (0.5713) Acc G: 94.258% 
LR: 2.000e-04 

2023-03-02 01:46:03,839 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4336 (0.4619) Acc D Real: 91.703% 
Loss D Fake: 0.8094 (0.8374) Acc D Fake: 5.736% 
Loss D: 1.243 
Loss G: 0.5925 (0.5715) Acc G: 94.263% 
LR: 2.000e-04 

2023-03-02 01:46:03,847 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4840 (0.4621) Acc D Real: 91.716% 
Loss D Fake: 0.8098 (0.8373) Acc D Fake: 5.731% 
Loss D: 1.294 
Loss G: 0.5920 (0.5716) Acc G: 94.268% 
LR: 2.000e-04 

2023-03-02 01:46:03,855 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.5013 (0.4623) Acc D Real: 91.723% 
Loss D Fake: 0.8106 (0.8371) Acc D Fake: 5.726% 
Loss D: 1.312 
Loss G: 0.5912 (0.5717) Acc G: 94.273% 
LR: 2.000e-04 

2023-03-02 01:46:03,863 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.5379 (0.4628) Acc D Real: 91.731% 
Loss D Fake: 0.8120 (0.8369) Acc D Fake: 5.722% 
Loss D: 1.350 
Loss G: 0.5898 (0.5719) Acc G: 94.278% 
LR: 2.000e-04 

2023-03-02 01:46:03,871 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.4648 (0.4629) Acc D Real: 91.738% 
Loss D Fake: 0.8139 (0.8368) Acc D Fake: 5.717% 
Loss D: 1.279 
Loss G: 0.5883 (0.5720) Acc G: 94.283% 
LR: 2.000e-04 

2023-03-02 01:46:03,878 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.5101 (0.4632) Acc D Real: 91.742% 
Loss D Fake: 0.8158 (0.8366) Acc D Fake: 5.712% 
Loss D: 1.326 
Loss G: 0.5867 (0.5721) Acc G: 94.287% 
LR: 2.000e-04 

2023-03-02 01:46:03,886 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.5015 (0.4634) Acc D Real: 91.743% 
Loss D Fake: 0.8180 (0.8365) Acc D Fake: 5.707% 
Loss D: 1.319 
Loss G: 0.5849 (0.5722) Acc G: 94.292% 
LR: 2.000e-04 

2023-03-02 01:46:03,894 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4548 (0.4634) Acc D Real: 91.749% 
Loss D Fake: 0.8202 (0.8364) Acc D Fake: 5.703% 
Loss D: 1.275 
Loss G: 0.5833 (0.5722) Acc G: 94.297% 
LR: 2.000e-04 

2023-03-02 01:46:03,901 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4920 (0.4635) Acc D Real: 91.765% 
Loss D Fake: 0.8222 (0.8363) Acc D Fake: 5.698% 
Loss D: 1.314 
Loss G: 0.5817 (0.5723) Acc G: 94.301% 
LR: 2.000e-04 

2023-03-02 01:46:03,908 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4293 (0.4633) Acc D Real: 91.770% 
Loss D Fake: 0.8242 (0.8362) Acc D Fake: 5.694% 
Loss D: 1.253 
Loss G: 0.5804 (0.5723) Acc G: 94.306% 
LR: 2.000e-04 

2023-03-02 01:46:03,915 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4641 (0.4633) Acc D Real: 91.773% 
Loss D Fake: 0.8257 (0.8362) Acc D Fake: 5.689% 
Loss D: 1.290 
Loss G: 0.5792 (0.5724) Acc G: 94.310% 
LR: 2.000e-04 

2023-03-02 01:46:03,923 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4128 (0.4630) Acc D Real: 91.781% 
Loss D Fake: 0.8270 (0.8361) Acc D Fake: 5.685% 
Loss D: 1.240 
Loss G: 0.5785 (0.5724) Acc G: 94.314% 
LR: 2.000e-04 

2023-03-02 01:46:03,930 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4243 (0.4628) Acc D Real: 91.791% 
Loss D Fake: 0.8277 (0.8360) Acc D Fake: 5.681% 
Loss D: 1.252 
Loss G: 0.5781 (0.5725) Acc G: 94.319% 
LR: 2.000e-04 

2023-03-02 01:46:03,937 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4216 (0.4625) Acc D Real: 91.800% 
Loss D Fake: 0.8281 (0.8360) Acc D Fake: 5.676% 
Loss D: 1.250 
Loss G: 0.5780 (0.5725) Acc G: 94.323% 
LR: 2.000e-04 

2023-03-02 01:46:03,944 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3862 (0.4620) Acc D Real: 91.806% 
Loss D Fake: 0.8279 (0.8359) Acc D Fake: 5.672% 
Loss D: 1.214 
Loss G: 0.5784 (0.5725) Acc G: 94.327% 
LR: 2.000e-04 

2023-03-02 01:46:03,952 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3750 (0.4615) Acc D Real: 91.805% 
Loss D Fake: 0.8270 (0.8359) Acc D Fake: 5.668% 
Loss D: 1.202 
Loss G: 0.5794 (0.5726) Acc G: 94.331% 
LR: 2.000e-04 

2023-03-02 01:46:03,959 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4489 (0.4614) Acc D Real: 91.807% 
Loss D Fake: 0.8255 (0.8358) Acc D Fake: 5.664% 
Loss D: 1.274 
Loss G: 0.5805 (0.5726) Acc G: 94.335% 
LR: 2.000e-04 

2023-03-02 01:46:03,967 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4421 (0.4613) Acc D Real: 91.799% 
Loss D Fake: 0.8241 (0.8358) Acc D Fake: 5.660% 
Loss D: 1.266 
Loss G: 0.5816 (0.5727) Acc G: 94.339% 
LR: 2.000e-04 

2023-03-02 01:46:03,975 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4212 (0.4611) Acc D Real: 91.798% 
Loss D Fake: 0.8226 (0.8357) Acc D Fake: 5.656% 
Loss D: 1.244 
Loss G: 0.5829 (0.5727) Acc G: 94.343% 
LR: 2.000e-04 

2023-03-02 01:46:03,982 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3877 (0.4606) Acc D Real: 91.802% 
Loss D Fake: 0.8208 (0.8356) Acc D Fake: 5.652% 
Loss D: 1.209 
Loss G: 0.5844 (0.5728) Acc G: 94.347% 
LR: 2.000e-04 

2023-03-02 01:46:03,989 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4160 (0.4604) Acc D Real: 91.809% 
Loss D Fake: 0.8187 (0.8355) Acc D Fake: 5.648% 
Loss D: 1.235 
Loss G: 0.5861 (0.5729) Acc G: 94.351% 
LR: 2.000e-04 

2023-03-02 01:46:03,997 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4520 (0.4603) Acc D Real: 91.800% 
Loss D Fake: 0.8167 (0.8354) Acc D Fake: 5.644% 
Loss D: 1.269 
Loss G: 0.5876 (0.5730) Acc G: 94.355% 
LR: 2.000e-04 

2023-03-02 01:46:04,004 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4217 (0.4601) Acc D Real: 91.814% 
Loss D Fake: 0.8148 (0.8352) Acc D Fake: 5.640% 
Loss D: 1.236 
Loss G: 0.5891 (0.5731) Acc G: 94.359% 
LR: 2.000e-04 

2023-03-02 01:46:04,012 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4234 (0.4599) Acc D Real: 91.812% 
Loss D Fake: 0.8129 (0.8351) Acc D Fake: 5.637% 
Loss D: 1.236 
Loss G: 0.5905 (0.5732) Acc G: 94.363% 
LR: 2.000e-04 

2023-03-02 01:46:04,020 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4188 (0.4596) Acc D Real: 91.821% 
Loss D Fake: 0.8111 (0.8350) Acc D Fake: 5.633% 
Loss D: 1.230 
Loss G: 0.5920 (0.5733) Acc G: 94.366% 
LR: 2.000e-04 

2023-03-02 01:46:04,027 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3345 (0.4589) Acc D Real: 91.828% 
Loss D Fake: 0.8090 (0.8348) Acc D Fake: 5.629% 
Loss D: 1.144 
Loss G: 0.5940 (0.5734) Acc G: 94.370% 
LR: 2.000e-04 

2023-03-02 01:46:04,036 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3726 (0.4584) Acc D Real: 91.847% 
Loss D Fake: 0.8063 (0.8347) Acc D Fake: 5.626% 
Loss D: 1.179 
Loss G: 0.5962 (0.5735) Acc G: 94.374% 
LR: 2.000e-04 

2023-03-02 01:46:04,043 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4761 (0.4585) Acc D Real: 91.853% 
Loss D Fake: 0.8037 (0.8345) Acc D Fake: 5.622% 
Loss D: 1.280 
Loss G: 0.5979 (0.5737) Acc G: 94.377% 
LR: 2.000e-04 

2023-03-02 01:46:04,050 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4294 (0.4583) Acc D Real: 91.869% 
Loss D Fake: 0.8019 (0.8343) Acc D Fake: 5.618% 
Loss D: 1.231 
Loss G: 0.5993 (0.5738) Acc G: 94.381% 
LR: 2.000e-04 

2023-03-02 01:46:04,059 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.5449 (0.4588) Acc D Real: 91.876% 
Loss D Fake: 0.8007 (0.8341) Acc D Fake: 5.615% 
Loss D: 1.346 
Loss G: 0.5996 (0.5740) Acc G: 94.384% 
LR: 2.000e-04 

2023-03-02 01:46:04,066 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4764 (0.4589) Acc D Real: 91.887% 
Loss D Fake: 0.8008 (0.8339) Acc D Fake: 5.611% 
Loss D: 1.277 
Loss G: 0.5994 (0.5741) Acc G: 94.388% 
LR: 2.000e-04 

2023-03-02 01:46:04,073 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4357 (0.4588) Acc D Real: 91.898% 
Loss D Fake: 0.8011 (0.8337) Acc D Fake: 5.608% 
Loss D: 1.237 
Loss G: 0.5991 (0.5743) Acc G: 94.391% 
LR: 2.000e-04 

2023-03-02 01:46:04,080 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4472 (0.4587) Acc D Real: 91.898% 
Loss D Fake: 0.8014 (0.8336) Acc D Fake: 5.605% 
Loss D: 1.249 
Loss G: 0.5989 (0.5744) Acc G: 94.395% 
LR: 2.000e-04 

2023-03-02 01:46:04,088 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4554 (0.4587) Acc D Real: 91.905% 
Loss D Fake: 0.8016 (0.8334) Acc D Fake: 5.601% 
Loss D: 1.257 
Loss G: 0.5987 (0.5745) Acc G: 94.398% 
LR: 2.000e-04 

2023-03-02 01:46:04,095 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.4727 (0.4588) Acc D Real: 91.913% 
Loss D Fake: 0.8020 (0.8332) Acc D Fake: 5.598% 
Loss D: 1.275 
Loss G: 0.5983 (0.5747) Acc G: 94.401% 
LR: 2.000e-04 

2023-03-02 01:46:04,102 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3818 (0.4584) Acc D Real: 91.921% 
Loss D Fake: 0.8024 (0.8330) Acc D Fake: 5.595% 
Loss D: 1.184 
Loss G: 0.5983 (0.5748) Acc G: 94.405% 
LR: 2.000e-04 

2023-03-02 01:46:04,110 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4738 (0.4584) Acc D Real: 91.929% 
Loss D Fake: 0.8023 (0.8329) Acc D Fake: 5.591% 
Loss D: 1.276 
Loss G: 0.5982 (0.5749) Acc G: 94.408% 
LR: 2.000e-04 

2023-03-02 01:46:04,117 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4208 (0.4582) Acc D Real: 91.930% 
Loss D Fake: 0.8024 (0.8327) Acc D Fake: 5.588% 
Loss D: 1.223 
Loss G: 0.5983 (0.5750) Acc G: 94.411% 
LR: 2.000e-04 

2023-03-02 01:46:04,125 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4903 (0.4584) Acc D Real: 91.935% 
Loss D Fake: 0.8023 (0.8325) Acc D Fake: 5.585% 
Loss D: 1.293 
Loss G: 0.5981 (0.5752) Acc G: 94.414% 
LR: 2.000e-04 

2023-03-02 01:46:04,132 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4651 (0.4584) Acc D Real: 91.940% 
Loss D Fake: 0.8027 (0.8324) Acc D Fake: 5.582% 
Loss D: 1.268 
Loss G: 0.5977 (0.5753) Acc G: 94.418% 
LR: 2.000e-04 

2023-03-02 01:46:04,141 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4637 (0.4585) Acc D Real: 91.948% 
Loss D Fake: 0.8033 (0.8322) Acc D Fake: 5.579% 
Loss D: 1.267 
Loss G: 0.5972 (0.5754) Acc G: 94.421% 
LR: 2.000e-04 

2023-03-02 01:46:04,148 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4294 (0.4583) Acc D Real: 91.958% 
Loss D Fake: 0.8038 (0.8321) Acc D Fake: 5.576% 
Loss D: 1.233 
Loss G: 0.5969 (0.5755) Acc G: 94.424% 
LR: 2.000e-04 

2023-03-02 01:46:04,156 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4473 (0.4583) Acc D Real: 91.960% 
Loss D Fake: 0.8042 (0.8319) Acc D Fake: 5.573% 
Loss D: 1.252 
Loss G: 0.5966 (0.5756) Acc G: 94.427% 
LR: 2.000e-04 

2023-03-02 01:46:04,163 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.5145 (0.4586) Acc D Real: 91.962% 
Loss D Fake: 0.8048 (0.8318) Acc D Fake: 5.570% 
Loss D: 1.319 
Loss G: 0.5958 (0.5757) Acc G: 94.430% 
LR: 2.000e-04 

2023-03-02 01:46:04,170 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4202 (0.4584) Acc D Real: 91.972% 
Loss D Fake: 0.8058 (0.8316) Acc D Fake: 5.567% 
Loss D: 1.226 
Loss G: 0.5952 (0.5758) Acc G: 94.433% 
LR: 2.000e-04 

2023-03-02 01:46:04,178 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4381 (0.4583) Acc D Real: 91.980% 
Loss D Fake: 0.8063 (0.8315) Acc D Fake: 5.564% 
Loss D: 1.244 
Loss G: 0.5948 (0.5759) Acc G: 94.436% 
LR: 2.000e-04 

2023-03-02 01:46:04,185 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4442 (0.4582) Acc D Real: 91.992% 
Loss D Fake: 0.8068 (0.8314) Acc D Fake: 5.561% 
Loss D: 1.251 
Loss G: 0.5945 (0.5760) Acc G: 94.439% 
LR: 2.000e-04 

2023-03-02 01:46:04,192 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4889 (0.4583) Acc D Real: 92.001% 
Loss D Fake: 0.8074 (0.8313) Acc D Fake: 5.558% 
Loss D: 1.296 
Loss G: 0.5938 (0.5761) Acc G: 94.442% 
LR: 2.000e-04 

2023-03-02 01:46:04,199 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3938 (0.4580) Acc D Real: 92.013% 
Loss D Fake: 0.8082 (0.8311) Acc D Fake: 5.555% 
Loss D: 1.202 
Loss G: 0.5934 (0.5762) Acc G: 94.444% 
LR: 2.000e-04 

2023-03-02 01:46:04,207 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3913 (0.4577) Acc D Real: 92.020% 
Loss D Fake: 0.8083 (0.8310) Acc D Fake: 5.552% 
Loss D: 1.200 
Loss G: 0.5936 (0.5763) Acc G: 94.447% 
LR: 2.000e-04 

2023-03-02 01:46:04,215 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4239 (0.4575) Acc D Real: 92.023% 
Loss D Fake: 0.8079 (0.8309) Acc D Fake: 5.549% 
Loss D: 1.232 
Loss G: 0.5940 (0.5764) Acc G: 94.450% 
LR: 2.000e-04 

2023-03-02 01:46:04,222 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.5276 (0.4578) Acc D Real: 92.030% 
Loss D Fake: 0.8075 (0.8308) Acc D Fake: 5.547% 
Loss D: 1.335 
Loss G: 0.5939 (0.5765) Acc G: 94.453% 
LR: 2.000e-04 

2023-03-02 01:46:04,230 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3961 (0.4575) Acc D Real: 92.032% 
Loss D Fake: 0.8079 (0.8307) Acc D Fake: 5.544% 
Loss D: 1.204 
Loss G: 0.5939 (0.5766) Acc G: 94.456% 
LR: 2.000e-04 

2023-03-02 01:46:04,237 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4444 (0.4575) Acc D Real: 92.042% 
Loss D Fake: 0.8077 (0.8306) Acc D Fake: 5.541% 
Loss D: 1.252 
Loss G: 0.5939 (0.5767) Acc G: 94.458% 
LR: 2.000e-04 

2023-03-02 01:46:04,245 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4130 (0.4573) Acc D Real: 92.056% 
Loss D Fake: 0.8076 (0.8304) Acc D Fake: 5.538% 
Loss D: 1.221 
Loss G: 0.5941 (0.5767) Acc G: 94.461% 
LR: 2.000e-04 

2023-03-02 01:46:04,253 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4464 (0.4572) Acc D Real: 92.063% 
Loss D Fake: 0.8074 (0.8303) Acc D Fake: 5.536% 
Loss D: 1.254 
Loss G: 0.5942 (0.5768) Acc G: 94.464% 
LR: 2.000e-04 

2023-03-02 01:46:04,260 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.4324 (0.4571) Acc D Real: 92.070% 
Loss D Fake: 0.8072 (0.8302) Acc D Fake: 5.533% 
Loss D: 1.240 
Loss G: 0.5944 (0.5769) Acc G: 94.466% 
LR: 2.000e-04 

2023-03-02 01:46:04,267 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3927 (0.4568) Acc D Real: 92.075% 
Loss D Fake: 0.8069 (0.8301) Acc D Fake: 5.531% 
Loss D: 1.200 
Loss G: 0.5949 (0.5770) Acc G: 94.469% 
LR: 2.000e-04 

2023-03-02 01:46:04,275 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4199 (0.4566) Acc D Real: 92.085% 
Loss D Fake: 0.8061 (0.8300) Acc D Fake: 5.528% 
Loss D: 1.226 
Loss G: 0.5955 (0.5771) Acc G: 94.472% 
LR: 2.000e-04 

2023-03-02 01:46:04,282 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4186 (0.4564) Acc D Real: 92.097% 
Loss D Fake: 0.8053 (0.8299) Acc D Fake: 5.525% 
Loss D: 1.224 
Loss G: 0.5962 (0.5772) Acc G: 94.474% 
LR: 2.000e-04 

2023-03-02 01:46:04,290 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4722 (0.4565) Acc D Real: 92.104% 
Loss D Fake: 0.8046 (0.8297) Acc D Fake: 5.523% 
Loss D: 1.277 
Loss G: 0.5965 (0.5773) Acc G: 94.477% 
LR: 2.000e-04 

2023-03-02 01:46:04,297 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4634 (0.4565) Acc D Real: 92.113% 
Loss D Fake: 0.8044 (0.8296) Acc D Fake: 5.520% 
Loss D: 1.268 
Loss G: 0.5965 (0.5774) Acc G: 94.479% 
LR: 2.000e-04 

2023-03-02 01:46:04,305 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4314 (0.4564) Acc D Real: 92.120% 
Loss D Fake: 0.8045 (0.8295) Acc D Fake: 5.518% 
Loss D: 1.236 
Loss G: 0.5964 (0.5775) Acc G: 94.482% 
LR: 2.000e-04 

2023-03-02 01:46:04,312 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3901 (0.4561) Acc D Real: 92.126% 
Loss D Fake: 0.8044 (0.8294) Acc D Fake: 5.515% 
Loss D: 1.194 
Loss G: 0.5968 (0.5776) Acc G: 94.484% 
LR: 2.000e-04 

2023-03-02 01:46:04,320 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4321 (0.4560) Acc D Real: 92.131% 
Loss D Fake: 0.8038 (0.8293) Acc D Fake: 5.513% 
Loss D: 1.236 
Loss G: 0.5973 (0.5776) Acc G: 94.487% 
LR: 2.000e-04 

2023-03-02 01:46:04,327 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4259 (0.4558) Acc D Real: 92.134% 
Loss D Fake: 0.8031 (0.8291) Acc D Fake: 5.511% 
Loss D: 1.229 
Loss G: 0.5979 (0.5777) Acc G: 94.489% 
LR: 2.000e-04 

2023-03-02 01:46:04,335 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4774 (0.4559) Acc D Real: 92.131% 
Loss D Fake: 0.8024 (0.8290) Acc D Fake: 5.508% 
Loss D: 1.280 
Loss G: 0.5983 (0.5778) Acc G: 94.491% 
LR: 2.000e-04 

2023-03-02 01:46:04,342 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4048 (0.4557) Acc D Real: 92.131% 
Loss D Fake: 0.8019 (0.8289) Acc D Fake: 5.506% 
Loss D: 1.207 
Loss G: 0.5990 (0.5779) Acc G: 94.494% 
LR: 2.000e-04 

2023-03-02 01:46:04,351 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4129 (0.4555) Acc D Real: 92.135% 
Loss D Fake: 0.8010 (0.8288) Acc D Fake: 5.503% 
Loss D: 1.214 
Loss G: 0.5997 (0.5780) Acc G: 94.496% 
LR: 2.000e-04 

2023-03-02 01:46:04,358 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4903 (0.4556) Acc D Real: 92.138% 
Loss D Fake: 0.8001 (0.8286) Acc D Fake: 5.501% 
Loss D: 1.290 
Loss G: 0.6001 (0.5781) Acc G: 94.498% 
LR: 2.000e-04 

2023-03-02 01:46:04,365 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4490 (0.4556) Acc D Real: 92.151% 
Loss D Fake: 0.7999 (0.8285) Acc D Fake: 5.499% 
Loss D: 1.249 
Loss G: 0.6002 (0.5782) Acc G: 94.501% 
LR: 2.000e-04 

2023-03-02 01:46:04,373 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4827 (0.4557) Acc D Real: 92.165% 
Loss D Fake: 0.8001 (0.8284) Acc D Fake: 5.496% 
Loss D: 1.283 
Loss G: 0.5998 (0.5783) Acc G: 94.503% 
LR: 2.000e-04 

2023-03-02 01:46:04,380 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4389 (0.4557) Acc D Real: 92.169% 
Loss D Fake: 0.8007 (0.8282) Acc D Fake: 5.494% 
Loss D: 1.240 
Loss G: 0.5993 (0.5784) Acc G: 94.505% 
LR: 2.000e-04 

2023-03-02 01:46:04,388 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4553 (0.4557) Acc D Real: 92.180% 
Loss D Fake: 0.8013 (0.8281) Acc D Fake: 5.492% 
Loss D: 1.257 
Loss G: 0.5988 (0.5785) Acc G: 94.508% 
LR: 2.000e-04 

2023-03-02 01:46:04,395 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4166 (0.4555) Acc D Real: 92.186% 
Loss D Fake: 0.8019 (0.8280) Acc D Fake: 5.490% 
Loss D: 1.218 
Loss G: 0.5984 (0.5786) Acc G: 94.510% 
LR: 2.000e-04 

2023-03-02 01:46:04,403 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.5150 (0.4557) Acc D Real: 92.190% 
Loss D Fake: 0.8025 (0.8279) Acc D Fake: 5.488% 
Loss D: 1.317 
Loss G: 0.5977 (0.5787) Acc G: 94.512% 
LR: 2.000e-04 

2023-03-02 01:46:04,411 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4825 (0.4559) Acc D Real: 92.193% 
Loss D Fake: 0.8037 (0.8278) Acc D Fake: 5.485% 
Loss D: 1.286 
Loss G: 0.5966 (0.5788) Acc G: 94.514% 
LR: 2.000e-04 

2023-03-02 01:46:04,419 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4510 (0.4558) Acc D Real: 92.195% 
Loss D Fake: 0.8050 (0.8277) Acc D Fake: 5.483% 
Loss D: 1.256 
Loss G: 0.5956 (0.5789) Acc G: 94.516% 
LR: 2.000e-04 

2023-03-02 01:46:04,426 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4178 (0.4557) Acc D Real: 92.200% 
Loss D Fake: 0.8060 (0.8276) Acc D Fake: 5.481% 
Loss D: 1.224 
Loss G: 0.5950 (0.5789) Acc G: 94.519% 
LR: 2.000e-04 

2023-03-02 01:46:04,433 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4876 (0.4558) Acc D Real: 92.204% 
Loss D Fake: 0.8069 (0.8275) Acc D Fake: 5.480% 
Loss D: 1.294 
Loss G: 0.5941 (0.5790) Acc G: 94.519% 
LR: 2.000e-04 

2023-03-02 01:46:04,443 -                train: [    INFO] - 
Epoch: 9/20
2023-03-02 01:46:04,618 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3994 (0.4316) Acc D Real: 93.750% 
Loss D Fake: 0.8093 (0.8087) Acc D Fake: 5.000% 
Loss D: 1.209 
Loss G: 0.5923 (0.5927) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,626 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.4408 (0.4347) Acc D Real: 93.542% 
Loss D Fake: 0.8099 (0.8091) Acc D Fake: 5.000% 
Loss D: 1.251 
Loss G: 0.5919 (0.5924) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,633 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3675 (0.4179) Acc D Real: 93.880% 
Loss D Fake: 0.8103 (0.8094) Acc D Fake: 5.000% 
Loss D: 1.178 
Loss G: 0.5919 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,651 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4259 (0.4195) Acc D Real: 93.917% 
Loss D Fake: 0.8100 (0.8095) Acc D Fake: 5.000% 
Loss D: 1.236 
Loss G: 0.5922 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,658 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4633 (0.4268) Acc D Real: 93.984% 
Loss D Fake: 0.8097 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.273 
Loss G: 0.5922 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,665 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4288 (0.4271) Acc D Real: 94.048% 
Loss D Fake: 0.8098 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.239 
Loss G: 0.5922 (0.5922) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,672 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4198 (0.4262) Acc D Real: 94.017% 
Loss D Fake: 0.8098 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.230 
Loss G: 0.5922 (0.5922) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,679 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4369 (0.4274) Acc D Real: 94.010% 
Loss D Fake: 0.8097 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.247 
Loss G: 0.5923 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,686 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3945 (0.4241) Acc D Real: 94.052% 
Loss D Fake: 0.8095 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.204 
Loss G: 0.5926 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,693 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4744 (0.4287) Acc D Real: 94.119% 
Loss D Fake: 0.8092 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.284 
Loss G: 0.5926 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,700 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.4257 (0.4284) Acc D Real: 94.136% 
Loss D Fake: 0.8094 (0.8096) Acc D Fake: 5.000% 
Loss D: 1.235 
Loss G: 0.5925 (0.5923) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,707 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3734 (0.4242) Acc D Real: 94.107% 
Loss D Fake: 0.8092 (0.8095) Acc D Fake: 5.000% 
Loss D: 1.183 
Loss G: 0.5930 (0.5924) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,714 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.5118 (0.4304) Acc D Real: 94.040% 
Loss D Fake: 0.8087 (0.8095) Acc D Fake: 5.000% 
Loss D: 1.321 
Loss G: 0.5930 (0.5924) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,722 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3831 (0.4273) Acc D Real: 94.101% 
Loss D Fake: 0.8088 (0.8094) Acc D Fake: 5.000% 
Loss D: 1.192 
Loss G: 0.5931 (0.5925) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,729 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3745 (0.4240) Acc D Real: 94.020% 
Loss D Fake: 0.8083 (0.8094) Acc D Fake: 5.000% 
Loss D: 1.183 
Loss G: 0.5939 (0.5926) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,736 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.4040 (0.4228) Acc D Real: 93.955% 
Loss D Fake: 0.8071 (0.8092) Acc D Fake: 5.000% 
Loss D: 1.211 
Loss G: 0.5949 (0.5927) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,743 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3809 (0.4205) Acc D Real: 93.909% 
Loss D Fake: 0.8055 (0.8090) Acc D Fake: 5.000% 
Loss D: 1.186 
Loss G: 0.5964 (0.5929) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,750 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4499 (0.4220) Acc D Real: 93.936% 
Loss D Fake: 0.8037 (0.8087) Acc D Fake: 5.000% 
Loss D: 1.254 
Loss G: 0.5976 (0.5931) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,757 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.5011 (0.4260) Acc D Real: 93.935% 
Loss D Fake: 0.8027 (0.8084) Acc D Fake: 5.000% 
Loss D: 1.304 
Loss G: 0.5980 (0.5934) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,764 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4318 (0.4263) Acc D Real: 93.946% 
Loss D Fake: 0.8024 (0.8082) Acc D Fake: 5.000% 
Loss D: 1.234 
Loss G: 0.5983 (0.5936) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,771 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4045 (0.4253) Acc D Real: 93.956% 
Loss D Fake: 0.8020 (0.8079) Acc D Fake: 5.000% 
Loss D: 1.207 
Loss G: 0.5986 (0.5939) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,778 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4122 (0.4247) Acc D Real: 93.954% 
Loss D Fake: 0.8015 (0.8076) Acc D Fake: 5.000% 
Loss D: 1.214 
Loss G: 0.5991 (0.5941) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,786 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4884 (0.4274) Acc D Real: 93.961% 
Loss D Fake: 0.8010 (0.8073) Acc D Fake: 5.000% 
Loss D: 1.289 
Loss G: 0.5992 (0.5943) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,793 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4112 (0.4267) Acc D Real: 93.992% 
Loss D Fake: 0.8011 (0.8071) Acc D Fake: 5.000% 
Loss D: 1.212 
Loss G: 0.5992 (0.5945) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,800 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3757 (0.4248) Acc D Real: 94.012% 
Loss D Fake: 0.8008 (0.8068) Acc D Fake: 5.000% 
Loss D: 1.177 
Loss G: 0.5997 (0.5947) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,807 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4569 (0.4259) Acc D Real: 93.999% 
Loss D Fake: 0.8002 (0.8066) Acc D Fake: 5.000% 
Loss D: 1.257 
Loss G: 0.6001 (0.5949) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,814 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4380 (0.4264) Acc D Real: 93.992% 
Loss D Fake: 0.7998 (0.8063) Acc D Fake: 5.000% 
Loss D: 1.238 
Loss G: 0.6003 (0.5951) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,821 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4343 (0.4266) Acc D Real: 93.962% 
Loss D Fake: 0.7996 (0.8061) Acc D Fake: 5.000% 
Loss D: 1.234 
Loss G: 0.6005 (0.5953) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,829 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4728 (0.4282) Acc D Real: 93.958% 
Loss D Fake: 0.7995 (0.8059) Acc D Fake: 5.000% 
Loss D: 1.272 
Loss G: 0.6004 (0.5954) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,836 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4933 (0.4303) Acc D Real: 93.982% 
Loss D Fake: 0.7999 (0.8057) Acc D Fake: 5.000% 
Loss D: 1.293 
Loss G: 0.5997 (0.5956) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,843 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4003 (0.4293) Acc D Real: 93.981% 
Loss D Fake: 0.8008 (0.8055) Acc D Fake: 5.000% 
Loss D: 1.201 
Loss G: 0.5992 (0.5957) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,851 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4104 (0.4288) Acc D Real: 93.976% 
Loss D Fake: 0.8012 (0.8054) Acc D Fake: 5.000% 
Loss D: 1.212 
Loss G: 0.5991 (0.5958) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,858 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4285 (0.4288) Acc D Real: 93.968% 
Loss D Fake: 0.8012 (0.8053) Acc D Fake: 5.000% 
Loss D: 1.230 
Loss G: 0.5991 (0.5959) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,866 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4636 (0.4298) Acc D Real: 93.932% 
Loss D Fake: 0.8012 (0.8052) Acc D Fake: 5.000% 
Loss D: 1.265 
Loss G: 0.5991 (0.5960) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,873 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4084 (0.4292) Acc D Real: 93.955% 
Loss D Fake: 0.8012 (0.8051) Acc D Fake: 5.000% 
Loss D: 1.210 
Loss G: 0.5991 (0.5961) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,880 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4213 (0.4290) Acc D Real: 93.957% 
Loss D Fake: 0.8011 (0.8050) Acc D Fake: 5.000% 
Loss D: 1.222 
Loss G: 0.5993 (0.5962) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,888 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4479 (0.4294) Acc D Real: 93.998% 
Loss D Fake: 0.8009 (0.8048) Acc D Fake: 5.000% 
Loss D: 1.249 
Loss G: 0.5993 (0.5962) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,895 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4578 (0.4302) Acc D Real: 93.998% 
Loss D Fake: 0.8011 (0.8048) Acc D Fake: 5.000% 
Loss D: 1.259 
Loss G: 0.5990 (0.5963) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,902 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4093 (0.4297) Acc D Real: 94.020% 
Loss D Fake: 0.8015 (0.8047) Acc D Fake: 5.000% 
Loss D: 1.211 
Loss G: 0.5988 (0.5964) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,910 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4341 (0.4298) Acc D Real: 94.071% 
Loss D Fake: 0.8017 (0.8046) Acc D Fake: 5.000% 
Loss D: 1.236 
Loss G: 0.5985 (0.5964) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,917 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3920 (0.4289) Acc D Real: 94.091% 
Loss D Fake: 0.8020 (0.8045) Acc D Fake: 5.000% 
Loss D: 1.194 
Loss G: 0.5985 (0.5965) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,924 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4652 (0.4297) Acc D Real: 94.111% 
Loss D Fake: 0.8020 (0.8045) Acc D Fake: 5.000% 
Loss D: 1.267 
Loss G: 0.5983 (0.5965) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,932 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3943 (0.4289) Acc D Real: 94.132% 
Loss D Fake: 0.8023 (0.8044) Acc D Fake: 5.000% 
Loss D: 1.197 
Loss G: 0.5982 (0.5966) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,939 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3547 (0.4273) Acc D Real: 94.133% 
Loss D Fake: 0.8020 (0.8044) Acc D Fake: 5.000% 
Loss D: 1.157 
Loss G: 0.5988 (0.5966) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,947 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3738 (0.4261) Acc D Real: 94.144% 
Loss D Fake: 0.8011 (0.8043) Acc D Fake: 5.000% 
Loss D: 1.175 
Loss G: 0.5997 (0.5967) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,954 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4365 (0.4263) Acc D Real: 94.163% 
Loss D Fake: 0.7999 (0.8042) Acc D Fake: 5.000% 
Loss D: 1.236 
Loss G: 0.6005 (0.5968) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,962 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3952 (0.4257) Acc D Real: 94.156% 
Loss D Fake: 0.7990 (0.8041) Acc D Fake: 5.000% 
Loss D: 1.194 
Loss G: 0.6013 (0.5969) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,969 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4017 (0.4252) Acc D Real: 94.172% 
Loss D Fake: 0.7978 (0.8040) Acc D Fake: 5.000% 
Loss D: 1.200 
Loss G: 0.6023 (0.5970) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,976 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3974 (0.4246) Acc D Real: 94.178% 
Loss D Fake: 0.7966 (0.8038) Acc D Fake: 5.000% 
Loss D: 1.194 
Loss G: 0.6034 (0.5971) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,984 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3503 (0.4232) Acc D Real: 94.198% 
Loss D Fake: 0.7950 (0.8036) Acc D Fake: 5.000% 
Loss D: 1.145 
Loss G: 0.6050 (0.5972) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,991 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4097 (0.4229) Acc D Real: 94.214% 
Loss D Fake: 0.7930 (0.8034) Acc D Fake: 5.000% 
Loss D: 1.203 
Loss G: 0.6065 (0.5974) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:04,998 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3981 (0.4224) Acc D Real: 94.214% 
Loss D Fake: 0.7912 (0.8032) Acc D Fake: 5.000% 
Loss D: 1.189 
Loss G: 0.6080 (0.5976) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,006 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4114 (0.4222) Acc D Real: 94.228% 
Loss D Fake: 0.7894 (0.8030) Acc D Fake: 5.000% 
Loss D: 1.201 
Loss G: 0.6094 (0.5978) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,013 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3988 (0.4218) Acc D Real: 94.230% 
Loss D Fake: 0.7879 (0.8027) Acc D Fake: 5.000% 
Loss D: 1.187 
Loss G: 0.6107 (0.5981) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,020 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3995 (0.4214) Acc D Real: 94.229% 
Loss D Fake: 0.7863 (0.8024) Acc D Fake: 5.000% 
Loss D: 1.186 
Loss G: 0.6120 (0.5983) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,027 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4873 (0.4226) Acc D Real: 94.237% 
Loss D Fake: 0.7850 (0.8021) Acc D Fake: 5.000% 
Loss D: 1.272 
Loss G: 0.6127 (0.5986) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,035 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3922 (0.4220) Acc D Real: 94.255% 
Loss D Fake: 0.7844 (0.8018) Acc D Fake: 5.000% 
Loss D: 1.177 
Loss G: 0.6132 (0.5988) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,042 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4020 (0.4217) Acc D Real: 94.258% 
Loss D Fake: 0.7837 (0.8015) Acc D Fake: 5.000% 
Loss D: 1.186 
Loss G: 0.6138 (0.5991) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,050 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3886 (0.4212) Acc D Real: 94.253% 
Loss D Fake: 0.7828 (0.8012) Acc D Fake: 5.000% 
Loss D: 1.171 
Loss G: 0.6147 (0.5993) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,057 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3962 (0.4207) Acc D Real: 94.267% 
Loss D Fake: 0.7817 (0.8008) Acc D Fake: 5.000% 
Loss D: 1.178 
Loss G: 0.6157 (0.5996) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,065 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4244 (0.4208) Acc D Real: 94.290% 
Loss D Fake: 0.7806 (0.8005) Acc D Fake: 5.000% 
Loss D: 1.205 
Loss G: 0.6164 (0.5999) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,072 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4122 (0.4207) Acc D Real: 94.300% 
Loss D Fake: 0.7799 (0.8002) Acc D Fake: 5.000% 
Loss D: 1.192 
Loss G: 0.6170 (0.6002) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,079 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4295 (0.4208) Acc D Real: 94.312% 
Loss D Fake: 0.7793 (0.7999) Acc D Fake: 5.000% 
Loss D: 1.209 
Loss G: 0.6174 (0.6004) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,087 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4425 (0.4211) Acc D Real: 94.320% 
Loss D Fake: 0.7790 (0.7995) Acc D Fake: 5.000% 
Loss D: 1.222 
Loss G: 0.6175 (0.6007) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,094 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3705 (0.4204) Acc D Real: 94.339% 
Loss D Fake: 0.7789 (0.7992) Acc D Fake: 5.000% 
Loss D: 1.149 
Loss G: 0.6178 (0.6009) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,101 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.5040 (0.4216) Acc D Real: 94.332% 
Loss D Fake: 0.7786 (0.7989) Acc D Fake: 5.000% 
Loss D: 1.283 
Loss G: 0.6176 (0.6012) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,108 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4129 (0.4215) Acc D Real: 94.339% 
Loss D Fake: 0.7792 (0.7986) Acc D Fake: 5.000% 
Loss D: 1.192 
Loss G: 0.6172 (0.6014) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,116 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4019 (0.4212) Acc D Real: 94.354% 
Loss D Fake: 0.7795 (0.7984) Acc D Fake: 5.000% 
Loss D: 1.181 
Loss G: 0.6170 (0.6017) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,123 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3824 (0.4207) Acc D Real: 94.371% 
Loss D Fake: 0.7796 (0.7981) Acc D Fake: 5.000% 
Loss D: 1.162 
Loss G: 0.6171 (0.6019) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,131 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.3336 (0.4194) Acc D Real: 94.389% 
Loss D Fake: 0.7791 (0.7978) Acc D Fake: 5.000% 
Loss D: 1.113 
Loss G: 0.6179 (0.6021) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,140 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3830 (0.4189) Acc D Real: 94.394% 
Loss D Fake: 0.7778 (0.7975) Acc D Fake: 5.000% 
Loss D: 1.161 
Loss G: 0.6192 (0.6023) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,147 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3591 (0.4181) Acc D Real: 94.399% 
Loss D Fake: 0.7761 (0.7972) Acc D Fake: 5.000% 
Loss D: 1.135 
Loss G: 0.6208 (0.6026) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,155 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4122 (0.4180) Acc D Real: 94.410% 
Loss D Fake: 0.7741 (0.7969) Acc D Fake: 5.000% 
Loss D: 1.186 
Loss G: 0.6223 (0.6029) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,163 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.4299 (0.4182) Acc D Real: 94.414% 
Loss D Fake: 0.7726 (0.7966) Acc D Fake: 5.000% 
Loss D: 1.203 
Loss G: 0.6234 (0.6031) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,170 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3791 (0.4177) Acc D Real: 94.419% 
Loss D Fake: 0.7714 (0.7963) Acc D Fake: 5.000% 
Loss D: 1.151 
Loss G: 0.6246 (0.6034) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,178 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.4615 (0.4182) Acc D Real: 94.427% 
Loss D Fake: 0.7702 (0.7959) Acc D Fake: 5.000% 
Loss D: 1.232 
Loss G: 0.6252 (0.6037) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,185 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.5304 (0.4197) Acc D Real: 94.428% 
Loss D Fake: 0.7701 (0.7956) Acc D Fake: 5.000% 
Loss D: 1.301 
Loss G: 0.6247 (0.6040) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,193 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3903 (0.4193) Acc D Real: 94.438% 
Loss D Fake: 0.7710 (0.7953) Acc D Fake: 5.000% 
Loss D: 1.161 
Loss G: 0.6241 (0.6042) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,200 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4923 (0.4202) Acc D Real: 94.447% 
Loss D Fake: 0.7717 (0.7950) Acc D Fake: 5.000% 
Loss D: 1.264 
Loss G: 0.6231 (0.6045) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,208 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4573 (0.4207) Acc D Real: 94.466% 
Loss D Fake: 0.7733 (0.7947) Acc D Fake: 5.000% 
Loss D: 1.231 
Loss G: 0.6216 (0.6047) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,215 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4095 (0.4205) Acc D Real: 94.469% 
Loss D Fake: 0.7750 (0.7945) Acc D Fake: 5.000% 
Loss D: 1.185 
Loss G: 0.6202 (0.6049) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,223 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3250 (0.4194) Acc D Real: 94.481% 
Loss D Fake: 0.7761 (0.7943) Acc D Fake: 5.000% 
Loss D: 1.101 
Loss G: 0.6200 (0.6050) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,231 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.4358 (0.4196) Acc D Real: 94.473% 
Loss D Fake: 0.7762 (0.7941) Acc D Fake: 5.000% 
Loss D: 1.212 
Loss G: 0.6197 (0.6052) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,238 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.4540 (0.4200) Acc D Real: 94.471% 
Loss D Fake: 0.7767 (0.7939) Acc D Fake: 5.000% 
Loss D: 1.231 
Loss G: 0.6191 (0.6054) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,246 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4001 (0.4198) Acc D Real: 94.476% 
Loss D Fake: 0.7773 (0.7937) Acc D Fake: 5.000% 
Loss D: 1.177 
Loss G: 0.6187 (0.6055) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,253 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3850 (0.4194) Acc D Real: 94.480% 
Loss D Fake: 0.7776 (0.7935) Acc D Fake: 5.000% 
Loss D: 1.163 
Loss G: 0.6187 (0.6057) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,261 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4048 (0.4192) Acc D Real: 94.483% 
Loss D Fake: 0.7774 (0.7933) Acc D Fake: 5.000% 
Loss D: 1.182 
Loss G: 0.6190 (0.6058) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,269 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3879 (0.4188) Acc D Real: 94.494% 
Loss D Fake: 0.7769 (0.7931) Acc D Fake: 5.000% 
Loss D: 1.165 
Loss G: 0.6195 (0.6060) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,276 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4730 (0.4194) Acc D Real: 94.501% 
Loss D Fake: 0.7765 (0.7929) Acc D Fake: 5.000% 
Loss D: 1.249 
Loss G: 0.6194 (0.6061) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,284 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3667 (0.4189) Acc D Real: 94.505% 
Loss D Fake: 0.7767 (0.7927) Acc D Fake: 5.000% 
Loss D: 1.143 
Loss G: 0.6196 (0.6063) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,291 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4118 (0.4188) Acc D Real: 94.509% 
Loss D Fake: 0.7763 (0.7926) Acc D Fake: 5.000% 
Loss D: 1.188 
Loss G: 0.6199 (0.6064) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,299 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3517 (0.4181) Acc D Real: 94.521% 
Loss D Fake: 0.7758 (0.7924) Acc D Fake: 5.000% 
Loss D: 1.127 
Loss G: 0.6207 (0.6066) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,306 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3806 (0.4177) Acc D Real: 94.523% 
Loss D Fake: 0.7746 (0.7922) Acc D Fake: 5.000% 
Loss D: 1.155 
Loss G: 0.6219 (0.6068) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,314 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3955 (0.4174) Acc D Real: 94.527% 
Loss D Fake: 0.7731 (0.7920) Acc D Fake: 5.000% 
Loss D: 1.169 
Loss G: 0.6231 (0.6069) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,321 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4831 (0.4181) Acc D Real: 94.522% 
Loss D Fake: 0.7720 (0.7918) Acc D Fake: 5.000% 
Loss D: 1.255 
Loss G: 0.6235 (0.6071) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,329 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4314 (0.4182) Acc D Real: 94.518% 
Loss D Fake: 0.7719 (0.7916) Acc D Fake: 5.000% 
Loss D: 1.203 
Loss G: 0.6235 (0.6073) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,336 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3773 (0.4178) Acc D Real: 94.525% 
Loss D Fake: 0.7719 (0.7914) Acc D Fake: 5.000% 
Loss D: 1.149 
Loss G: 0.6238 (0.6074) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,344 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4008 (0.4177) Acc D Real: 94.531% 
Loss D Fake: 0.7713 (0.7912) Acc D Fake: 5.000% 
Loss D: 1.172 
Loss G: 0.6243 (0.6076) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,351 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4148 (0.4176) Acc D Real: 94.542% 
Loss D Fake: 0.7708 (0.7910) Acc D Fake: 5.000% 
Loss D: 1.186 
Loss G: 0.6247 (0.6078) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,359 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3761 (0.4172) Acc D Real: 94.553% 
Loss D Fake: 0.7703 (0.7908) Acc D Fake: 5.000% 
Loss D: 1.146 
Loss G: 0.6253 (0.6080) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,367 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4127 (0.4172) Acc D Real: 94.559% 
Loss D Fake: 0.7696 (0.7906) Acc D Fake: 5.000% 
Loss D: 1.182 
Loss G: 0.6258 (0.6081) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,374 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4563 (0.4176) Acc D Real: 94.568% 
Loss D Fake: 0.7694 (0.7904) Acc D Fake: 5.000% 
Loss D: 1.226 
Loss G: 0.6255 (0.6083) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,381 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4330 (0.4177) Acc D Real: 94.578% 
Loss D Fake: 0.7701 (0.7902) Acc D Fake: 5.000% 
Loss D: 1.203 
Loss G: 0.6248 (0.6085) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,389 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3466 (0.4170) Acc D Real: 94.584% 
Loss D Fake: 0.7706 (0.7900) Acc D Fake: 5.000% 
Loss D: 1.117 
Loss G: 0.6249 (0.6086) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,396 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4308 (0.4172) Acc D Real: 94.586% 
Loss D Fake: 0.7702 (0.7898) Acc D Fake: 5.000% 
Loss D: 1.201 
Loss G: 0.6251 (0.6088) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,404 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4216 (0.4172) Acc D Real: 94.590% 
Loss D Fake: 0.7701 (0.7896) Acc D Fake: 5.000% 
Loss D: 1.192 
Loss G: 0.6251 (0.6089) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,412 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4784 (0.4178) Acc D Real: 94.595% 
Loss D Fake: 0.7705 (0.7894) Acc D Fake: 5.000% 
Loss D: 1.249 
Loss G: 0.6242 (0.6091) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,421 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3564 (0.4172) Acc D Real: 94.598% 
Loss D Fake: 0.7715 (0.7893) Acc D Fake: 5.000% 
Loss D: 1.128 
Loss G: 0.6239 (0.6092) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,429 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4209 (0.4172) Acc D Real: 94.611% 
Loss D Fake: 0.7717 (0.7891) Acc D Fake: 5.000% 
Loss D: 1.193 
Loss G: 0.6235 (0.6093) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,436 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4401 (0.4174) Acc D Real: 94.612% 
Loss D Fake: 0.7724 (0.7890) Acc D Fake: 5.000% 
Loss D: 1.212 
Loss G: 0.6227 (0.6094) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,444 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3916 (0.4172) Acc D Real: 94.621% 
Loss D Fake: 0.7733 (0.7888) Acc D Fake: 5.000% 
Loss D: 1.165 
Loss G: 0.6222 (0.6096) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,452 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4252 (0.4173) Acc D Real: 94.626% 
Loss D Fake: 0.7738 (0.7887) Acc D Fake: 5.000% 
Loss D: 1.199 
Loss G: 0.6217 (0.6097) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,460 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3966 (0.4171) Acc D Real: 94.632% 
Loss D Fake: 0.7743 (0.7886) Acc D Fake: 5.000% 
Loss D: 1.171 
Loss G: 0.6215 (0.6098) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,467 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4075 (0.4170) Acc D Real: 94.637% 
Loss D Fake: 0.7745 (0.7884) Acc D Fake: 5.000% 
Loss D: 1.182 
Loss G: 0.6214 (0.6099) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,475 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4121 (0.4170) Acc D Real: 94.640% 
Loss D Fake: 0.7745 (0.7883) Acc D Fake: 5.000% 
Loss D: 1.187 
Loss G: 0.6213 (0.6100) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,482 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4181 (0.4170) Acc D Real: 94.641% 
Loss D Fake: 0.7746 (0.7882) Acc D Fake: 5.000% 
Loss D: 1.193 
Loss G: 0.6213 (0.6101) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,490 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4036 (0.4169) Acc D Real: 94.655% 
Loss D Fake: 0.7747 (0.7881) Acc D Fake: 5.000% 
Loss D: 1.178 
Loss G: 0.6212 (0.6102) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,497 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3640 (0.4164) Acc D Real: 94.663% 
Loss D Fake: 0.7745 (0.7880) Acc D Fake: 5.000% 
Loss D: 1.139 
Loss G: 0.6220 (0.6103) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,505 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4502 (0.4167) Acc D Real: 94.670% 
Loss D Fake: 0.7735 (0.7878) Acc D Fake: 5.000% 
Loss D: 1.224 
Loss G: 0.6224 (0.6104) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,513 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3446 (0.4161) Acc D Real: 94.678% 
Loss D Fake: 0.7729 (0.7877) Acc D Fake: 5.000% 
Loss D: 1.117 
Loss G: 0.6238 (0.6105) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,521 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4009 (0.4160) Acc D Real: 94.683% 
Loss D Fake: 0.7708 (0.7876) Acc D Fake: 5.000% 
Loss D: 1.172 
Loss G: 0.6255 (0.6106) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,528 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4524 (0.4163) Acc D Real: 94.690% 
Loss D Fake: 0.7694 (0.7874) Acc D Fake: 5.000% 
Loss D: 1.222 
Loss G: 0.6262 (0.6107) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,536 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.4816 (0.4168) Acc D Real: 94.693% 
Loss D Fake: 0.7694 (0.7873) Acc D Fake: 5.000% 
Loss D: 1.251 
Loss G: 0.6255 (0.6108) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,544 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4203 (0.4168) Acc D Real: 94.697% 
Loss D Fake: 0.7706 (0.7872) Acc D Fake: 5.000% 
Loss D: 1.191 
Loss G: 0.6247 (0.6110) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,554 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3634 (0.4164) Acc D Real: 94.708% 
Loss D Fake: 0.7711 (0.7870) Acc D Fake: 5.000% 
Loss D: 1.134 
Loss G: 0.6251 (0.6111) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,561 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3395 (0.4158) Acc D Real: 94.711% 
Loss D Fake: 0.7694 (0.7869) Acc D Fake: 5.000% 
Loss D: 1.109 
Loss G: 0.6280 (0.6112) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,569 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4156 (0.4158) Acc D Real: 94.714% 
Loss D Fake: 0.7656 (0.7867) Acc D Fake: 5.000% 
Loss D: 1.181 
Loss G: 0.6310 (0.6114) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,576 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4179 (0.4158) Acc D Real: 94.722% 
Loss D Fake: 0.7626 (0.7865) Acc D Fake: 5.000% 
Loss D: 1.181 
Loss G: 0.6333 (0.6115) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,584 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4016 (0.4157) Acc D Real: 94.729% 
Loss D Fake: 0.7602 (0.7863) Acc D Fake: 5.000% 
Loss D: 1.162 
Loss G: 0.6355 (0.6117) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,592 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4379 (0.4159) Acc D Real: 94.736% 
Loss D Fake: 0.7584 (0.7861) Acc D Fake: 5.000% 
Loss D: 1.196 
Loss G: 0.6361 (0.6119) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,599 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4153 (0.4159) Acc D Real: 94.744% 
Loss D Fake: 0.7584 (0.7859) Acc D Fake: 5.000% 
Loss D: 1.174 
Loss G: 0.6359 (0.6121) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,607 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3519 (0.4154) Acc D Real: 94.752% 
Loss D Fake: 0.7584 (0.7857) Acc D Fake: 5.000% 
Loss D: 1.110 
Loss G: 0.6370 (0.6123) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,614 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3956 (0.4152) Acc D Real: 94.755% 
Loss D Fake: 0.7574 (0.7855) Acc D Fake: 5.000% 
Loss D: 1.153 
Loss G: 0.6369 (0.6124) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,622 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.5044 (0.4159) Acc D Real: 94.731% 
Loss D Fake: 0.7601 (0.7853) Acc D Fake: 5.000% 
Loss D: 1.264 
Loss G: 0.6316 (0.6126) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,629 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3784 (0.4156) Acc D Real: 94.734% 
Loss D Fake: 0.7681 (0.7852) Acc D Fake: 5.000% 
Loss D: 1.146 
Loss G: 0.6276 (0.6127) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,637 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4061 (0.4156) Acc D Real: 94.714% 
Loss D Fake: 0.7690 (0.7851) Acc D Fake: 5.000% 
Loss D: 1.175 
Loss G: 0.6316 (0.6128) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,644 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3828 (0.4153) Acc D Real: 94.721% 
Loss D Fake: 0.7597 (0.7849) Acc D Fake: 5.000% 
Loss D: 1.142 
Loss G: 0.6403 (0.6130) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,652 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3500 (0.4149) Acc D Real: 94.723% 
Loss D Fake: 0.7495 (0.7846) Acc D Fake: 5.000% 
Loss D: 1.099 
Loss G: 0.6479 (0.6133) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,660 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4100 (0.4148) Acc D Real: 94.703% 
Loss D Fake: 0.7426 (0.7843) Acc D Fake: 5.000% 
Loss D: 1.153 
Loss G: 0.6524 (0.6136) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,667 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3960 (0.4147) Acc D Real: 94.685% 
Loss D Fake: 0.7393 (0.7840) Acc D Fake: 5.000% 
Loss D: 1.135 
Loss G: 0.6540 (0.6139) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,675 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3726 (0.4144) Acc D Real: 94.651% 
Loss D Fake: 0.7387 (0.7837) Acc D Fake: 5.000% 
Loss D: 1.111 
Loss G: 0.6540 (0.6141) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,682 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4268 (0.4145) Acc D Real: 94.573% 
Loss D Fake: 0.7393 (0.7834) Acc D Fake: 5.000% 
Loss D: 1.166 
Loss G: 0.6534 (0.6144) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,690 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3474 (0.4140) Acc D Real: 94.559% 
Loss D Fake: 0.7410 (0.7831) Acc D Fake: 5.000% 
Loss D: 1.088 
Loss G: 0.6504 (0.6147) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,697 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3865 (0.4138) Acc D Real: 94.522% 
Loss D Fake: 0.7464 (0.7828) Acc D Fake: 5.000% 
Loss D: 1.133 
Loss G: 0.6457 (0.6149) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,705 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3986 (0.4137) Acc D Real: 94.490% 
Loss D Fake: 0.7516 (0.7826) Acc D Fake: 5.000% 
Loss D: 1.150 
Loss G: 0.6465 (0.6151) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,712 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3634 (0.4134) Acc D Real: 94.467% 
Loss D Fake: 0.7449 (0.7824) Acc D Fake: 5.000% 
Loss D: 1.108 
Loss G: 0.6536 (0.6154) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,720 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.4000 (0.4133) Acc D Real: 94.427% 
Loss D Fake: 0.7379 (0.7821) Acc D Fake: 5.000% 
Loss D: 1.138 
Loss G: 0.6560 (0.6156) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,728 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3597 (0.4129) Acc D Real: 94.407% 
Loss D Fake: 0.7375 (0.7818) Acc D Fake: 5.000% 
Loss D: 1.097 
Loss G: 0.6561 (0.6159) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,736 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4376 (0.4131) Acc D Real: 94.335% 
Loss D Fake: 0.7390 (0.7815) Acc D Fake: 5.000% 
Loss D: 1.177 
Loss G: 0.6534 (0.6161) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,744 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3768 (0.4128) Acc D Real: 94.285% 
Loss D Fake: 0.7439 (0.7812) Acc D Fake: 5.000% 
Loss D: 1.121 
Loss G: 0.6506 (0.6164) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,751 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3563 (0.4125) Acc D Real: 94.271% 
Loss D Fake: 0.7444 (0.7810) Acc D Fake: 5.000% 
Loss D: 1.101 
Loss G: 0.6544 (0.6166) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,759 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3583 (0.4121) Acc D Real: 94.199% 
Loss D Fake: 0.7360 (0.7807) Acc D Fake: 5.000% 
Loss D: 1.094 
Loss G: 0.6612 (0.6169) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:05,767 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3159 (0.4115) Acc D Real: 94.172% 
Loss D Fake: 0.7292 (0.7804) Acc D Fake: 5.000% 
Loss D: 1.045 
Loss G: 0.6657 (0.6172) Acc G: 94.773% 
LR: 2.000e-04 

2023-03-02 01:46:05,774 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3307 (0.4110) Acc D Real: 94.110% 
Loss D Fake: 0.7245 (0.7800) Acc D Fake: 5.269% 
Loss D: 1.055 
Loss G: 0.6708 (0.6176) Acc G: 94.430% 
LR: 2.000e-04 

2023-03-02 01:46:05,783 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.2983 (0.4103) Acc D Real: 94.064% 
Loss D Fake: 0.7195 (0.7796) Acc D Fake: 5.620% 
Loss D: 1.018 
Loss G: 0.6746 (0.6179) Acc G: 94.060% 
LR: 2.000e-04 

2023-03-02 01:46:05,791 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3820 (0.4101) Acc D Real: 94.022% 
Loss D Fake: 0.7195 (0.7792) Acc D Fake: 5.966% 
Loss D: 1.102 
Loss G: 0.6669 (0.6183) Acc G: 93.790% 
LR: 2.000e-04 

2023-03-02 01:46:05,798 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3269 (0.4095) Acc D Real: 93.984% 
Loss D Fake: 0.7443 (0.7790) Acc D Fake: 5.960% 
Loss D: 1.071 
Loss G: 0.6662 (0.6186) Acc G: 93.534% 
LR: 2.000e-04 

2023-03-02 01:46:05,806 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3923 (0.4094) Acc D Real: 93.931% 
Loss D Fake: 0.7194 (0.7786) Acc D Fake: 6.279% 
Loss D: 1.112 
Loss G: 0.6791 (0.6189) Acc G: 93.155% 
LR: 2.000e-04 

2023-03-02 01:46:05,813 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3454 (0.4090) Acc D Real: 93.872% 
Loss D Fake: 0.7089 (0.7782) Acc D Fake: 6.667% 
Loss D: 1.054 
Loss G: 0.6870 (0.6194) Acc G: 92.750% 
LR: 2.000e-04 

2023-03-02 01:46:05,821 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3946 (0.4089) Acc D Real: 93.712% 
Loss D Fake: 0.7021 (0.7777) Acc D Fake: 7.081% 
Loss D: 1.097 
Loss G: 0.6925 (0.6198) Acc G: 92.329% 
LR: 2.000e-04 

2023-03-02 01:46:05,830 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4399 (0.4091) Acc D Real: 93.592% 
Loss D Fake: 0.7007 (0.7772) Acc D Fake: 7.490% 
Loss D: 1.141 
Loss G: 0.6862 (0.6202) Acc G: 91.944% 
LR: 2.000e-04 

2023-03-02 01:46:05,837 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4196 (0.4092) Acc D Real: 93.514% 
Loss D Fake: 0.7149 (0.7769) Acc D Fake: 7.822% 
Loss D: 1.134 
Loss G: 0.6674 (0.6205) Acc G: 91.697% 
LR: 2.000e-04 

2023-03-02 01:46:05,845 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3139 (0.4086) Acc D Real: 93.503% 
Loss D Fake: 0.7661 (0.7768) Acc D Fake: 7.805% 
Loss D: 1.080 
Loss G: 0.6928 (0.6210) Acc G: 91.291% 
LR: 2.000e-04 

2023-03-02 01:46:05,853 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3914 (0.4085) Acc D Real: 93.406% 
Loss D Fake: 0.6876 (0.7763) Acc D Fake: 8.242% 
Loss D: 1.079 
Loss G: 0.7102 (0.6215) Acc G: 90.848% 
LR: 2.000e-04 

2023-03-02 01:46:05,861 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3580 (0.4082) Acc D Real: 93.290% 
Loss D Fake: 0.6782 (0.7757) Acc D Fake: 8.695% 
Loss D: 1.036 
Loss G: 0.7167 (0.6221) Acc G: 90.402% 
LR: 2.000e-04 

2023-03-02 01:46:05,868 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4198 (0.4083) Acc D Real: 93.081% 
Loss D Fake: 0.6737 (0.7751) Acc D Fake: 9.152% 
Loss D: 1.093 
Loss G: 0.7200 (0.6227) Acc G: 89.950% 
LR: 2.000e-04 

2023-03-02 01:46:05,876 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3874 (0.4082) Acc D Real: 92.927% 
Loss D Fake: 0.6712 (0.7744) Acc D Fake: 9.603% 
Loss D: 1.059 
Loss G: 0.7219 (0.6233) Acc G: 89.504% 
LR: 2.000e-04 

2023-03-02 01:46:05,884 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4510 (0.4084) Acc D Real: 92.714% 
Loss D Fake: 0.6698 (0.7738) Acc D Fake: 10.049% 
Loss D: 1.121 
Loss G: 0.7226 (0.6238) Acc G: 89.063% 
LR: 2.000e-04 

2023-03-02 01:46:05,891 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3507 (0.4081) Acc D Real: 92.552% 
Loss D Fake: 0.6694 (0.7732) Acc D Fake: 10.490% 
Loss D: 1.020 
Loss G: 0.7232 (0.6244) Acc G: 88.627% 
LR: 2.000e-04 

2023-03-02 01:46:05,900 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3484 (0.4077) Acc D Real: 92.399% 
Loss D Fake: 0.6686 (0.7726) Acc D Fake: 10.926% 
Loss D: 1.017 
Loss G: 0.7241 (0.6250) Acc G: 88.197% 
LR: 2.000e-04 

2023-03-02 01:46:05,908 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4615 (0.4080) Acc D Real: 92.166% 
Loss D Fake: 0.6679 (0.7720) Acc D Fake: 11.357% 
Loss D: 1.129 
Loss G: 0.7242 (0.6256) Acc G: 87.762% 
LR: 2.000e-04 

2023-03-02 01:46:05,915 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.4811 (0.4085) Acc D Real: 91.913% 
Loss D Fake: 0.6684 (0.7714) Acc D Fake: 11.782% 
Loss D: 1.149 
Loss G: 0.7230 (0.6261) Acc G: 87.331% 
LR: 2.000e-04 

2023-03-02 01:46:05,923 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.5358 (0.4092) Acc D Real: 91.618% 
Loss D Fake: 0.6701 (0.7708) Acc D Fake: 12.203% 
Loss D: 1.206 
Loss G: 0.7202 (0.6267) Acc G: 86.916% 
LR: 2.000e-04 

2023-03-02 01:46:05,930 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3883 (0.4091) Acc D Real: 91.447% 
Loss D Fake: 0.6729 (0.7702) Acc D Fake: 12.619% 
Loss D: 1.061 
Loss G: 0.7175 (0.6272) Acc G: 86.505% 
LR: 2.000e-04 

2023-03-02 01:46:05,938 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4055 (0.4091) Acc D Real: 91.262% 
Loss D Fake: 0.6752 (0.7697) Acc D Fake: 13.030% 
Loss D: 1.081 
Loss G: 0.7152 (0.6277) Acc G: 86.098% 
LR: 2.000e-04 

2023-03-02 01:46:05,948 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4922 (0.4095) Acc D Real: 91.018% 
Loss D Fake: 0.6776 (0.7692) Acc D Fake: 13.437% 
Loss D: 1.170 
Loss G: 0.7121 (0.6282) Acc G: 85.697% 
LR: 2.000e-04 

2023-03-02 01:46:05,957 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4717 (0.4099) Acc D Real: 90.801% 
Loss D Fake: 0.6809 (0.7687) Acc D Fake: 13.839% 
Loss D: 1.153 
Loss G: 0.7084 (0.6286) Acc G: 85.300% 
LR: 2.000e-04 

2023-03-02 01:46:05,966 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4496 (0.4101) Acc D Real: 90.606% 
Loss D Fake: 0.6846 (0.7682) Acc D Fake: 14.236% 
Loss D: 1.134 
Loss G: 0.7044 (0.6291) Acc G: 84.907% 
LR: 2.000e-04 

2023-03-02 01:46:05,973 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3847 (0.4100) Acc D Real: 90.491% 
Loss D Fake: 0.6883 (0.7678) Acc D Fake: 14.630% 
Loss D: 1.073 
Loss G: 0.7010 (0.6295) Acc G: 84.528% 
LR: 2.000e-04 

2023-03-02 01:46:05,980 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3585 (0.4097) Acc D Real: 90.382% 
Loss D Fake: 0.6910 (0.7673) Acc D Fake: 15.009% 
Loss D: 1.050 
Loss G: 0.6987 (0.6298) Acc G: 84.153% 
LR: 2.000e-04 

2023-03-02 01:46:05,987 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4192 (0.4097) Acc D Real: 90.227% 
Loss D Fake: 0.6930 (0.7669) Acc D Fake: 15.385% 
Loss D: 1.112 
Loss G: 0.6966 (0.6302) Acc G: 83.782% 
LR: 2.000e-04 

2023-03-02 01:46:05,995 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3394 (0.4093) Acc D Real: 90.134% 
Loss D Fake: 0.6947 (0.7665) Acc D Fake: 15.756% 
Loss D: 1.034 
Loss G: 0.6955 (0.6306) Acc G: 83.415% 
LR: 2.000e-04 

2023-03-02 01:46:06,002 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3514 (0.4090) Acc D Real: 90.092% 
Loss D Fake: 0.6953 (0.7662) Acc D Fake: 16.123% 
Loss D: 1.047 
Loss G: 0.6953 (0.6309) Acc G: 83.053% 
LR: 2.000e-04 

2023-03-02 01:46:06,009 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4685 (0.4093) Acc D Real: 89.943% 
Loss D Fake: 0.6956 (0.7658) Acc D Fake: 16.486% 
Loss D: 1.164 
Loss G: 0.6945 (0.6313) Acc G: 82.694% 
LR: 2.000e-04 

2023-03-02 01:46:06,017 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4288 (0.4094) Acc D Real: 89.838% 
Loss D Fake: 0.6967 (0.7654) Acc D Fake: 16.846% 
Loss D: 1.126 
Loss G: 0.6933 (0.6316) Acc G: 82.339% 
LR: 2.000e-04 

2023-03-02 01:46:06,025 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4191 (0.4095) Acc D Real: 89.742% 
Loss D Fake: 0.6980 (0.7650) Acc D Fake: 17.201% 
Loss D: 1.117 
Loss G: 0.6919 (0.6319) Acc G: 81.988% 
LR: 2.000e-04 

2023-03-02 01:46:06,032 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4929 (0.4099) Acc D Real: 89.624% 
Loss D Fake: 0.6998 (0.7647) Acc D Fake: 17.544% 
Loss D: 1.193 
Loss G: 0.6894 (0.6322) Acc G: 81.649% 
LR: 2.000e-04 

2023-03-02 01:46:06,039 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.5085 (0.4105) Acc D Real: 89.523% 
Loss D Fake: 0.7030 (0.7644) Acc D Fake: 17.884% 
Loss D: 1.212 
Loss G: 0.6854 (0.6325) Acc G: 81.323% 
LR: 2.000e-04 

2023-03-02 01:46:06,047 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3487 (0.4101) Acc D Real: 89.488% 
Loss D Fake: 0.7070 (0.7641) Acc D Fake: 18.211% 
Loss D: 1.056 
Loss G: 0.6822 (0.6328) Acc G: 81.009% 
LR: 2.000e-04 

2023-03-02 01:46:06,055 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4639 (0.4104) Acc D Real: 89.371% 
Loss D Fake: 0.7101 (0.7638) Acc D Fake: 18.517% 
Loss D: 1.174 
Loss G: 0.6788 (0.6330) Acc G: 80.707% 
LR: 2.000e-04 

2023-03-02 01:46:06,062 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4907 (0.4108) Acc D Real: 89.301% 
Loss D Fake: 0.7140 (0.7635) Acc D Fake: 18.785% 
Loss D: 1.205 
Loss G: 0.6744 (0.6332) Acc G: 80.781% 
LR: 2.000e-04 

2023-03-02 01:46:06,069 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3389 (0.4105) Acc D Real: 89.319% 
Loss D Fake: 0.7184 (0.7633) Acc D Fake: 18.713% 
Loss D: 1.057 
Loss G: 0.6710 (0.6334) Acc G: 80.855% 
LR: 2.000e-04 

2023-03-02 01:46:06,077 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4120 (0.4105) Acc D Real: 89.301% 
Loss D Fake: 0.7215 (0.7631) Acc D Fake: 18.643% 
Loss D: 1.134 
Loss G: 0.6681 (0.6336) Acc G: 80.928% 
LR: 2.000e-04 

2023-03-02 01:46:06,084 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4291 (0.4106) Acc D Real: 89.288% 
Loss D Fake: 0.7245 (0.7629) Acc D Fake: 18.573% 
Loss D: 1.154 
Loss G: 0.6651 (0.6338) Acc G: 81.000% 
LR: 2.000e-04 

2023-03-02 01:46:06,092 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4002 (0.4105) Acc D Real: 89.304% 
Loss D Fake: 0.7282 (0.7627) Acc D Fake: 18.503% 
Loss D: 1.128 
Loss G: 0.6608 (0.6339) Acc G: 81.071% 
LR: 2.000e-04 

2023-03-02 01:46:06,099 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3041 (0.4100) Acc D Real: 89.314% 
Loss D Fake: 0.7326 (0.7625) Acc D Fake: 18.435% 
Loss D: 1.037 
Loss G: 0.6579 (0.6340) Acc G: 81.142% 
LR: 2.000e-04 

2023-03-02 01:46:06,106 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4031 (0.4099) Acc D Real: 89.305% 
Loss D Fake: 0.7349 (0.7624) Acc D Fake: 18.367% 
Loss D: 1.138 
Loss G: 0.6559 (0.6341) Acc G: 81.212% 
LR: 2.000e-04 

2023-03-02 01:46:06,114 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3790 (0.4098) Acc D Real: 89.325% 
Loss D Fake: 0.7370 (0.7623) Acc D Fake: 18.300% 
Loss D: 1.116 
Loss G: 0.6542 (0.6342) Acc G: 81.281% 
LR: 2.000e-04 

2023-03-02 01:46:06,121 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3665 (0.4096) Acc D Real: 89.339% 
Loss D Fake: 0.7385 (0.7622) Acc D Fake: 18.233% 
Loss D: 1.105 
Loss G: 0.6534 (0.6343) Acc G: 81.350% 
LR: 2.000e-04 

2023-03-02 01:46:06,128 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3220 (0.4091) Acc D Real: 89.356% 
Loss D Fake: 0.7386 (0.7620) Acc D Fake: 18.167% 
Loss D: 1.061 
Loss G: 0.6543 (0.6344) Acc G: 81.418% 
LR: 2.000e-04 

2023-03-02 01:46:06,135 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4782 (0.4095) Acc D Real: 89.361% 
Loss D Fake: 0.7379 (0.7619) Acc D Fake: 18.102% 
Loss D: 1.216 
Loss G: 0.6536 (0.6345) Acc G: 81.485% 
LR: 2.000e-04 

2023-03-02 01:46:06,143 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3713 (0.4093) Acc D Real: 89.378% 
Loss D Fake: 0.7391 (0.7618) Acc D Fake: 18.038% 
Loss D: 1.110 
Loss G: 0.6529 (0.6346) Acc G: 81.552% 
LR: 2.000e-04 

2023-03-02 01:46:06,150 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3910 (0.4092) Acc D Real: 89.368% 
Loss D Fake: 0.7396 (0.7617) Acc D Fake: 17.974% 
Loss D: 1.131 
Loss G: 0.6528 (0.6347) Acc G: 81.618% 
LR: 2.000e-04 

2023-03-02 01:46:06,157 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3381 (0.4088) Acc D Real: 89.352% 
Loss D Fake: 0.7391 (0.7616) Acc D Fake: 17.911% 
Loss D: 1.077 
Loss G: 0.6543 (0.6348) Acc G: 81.683% 
LR: 2.000e-04 

2023-03-02 01:46:06,164 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3796 (0.4087) Acc D Real: 89.367% 
Loss D Fake: 0.7371 (0.7615) Acc D Fake: 17.848% 
Loss D: 1.117 
Loss G: 0.6560 (0.6349) Acc G: 81.748% 
LR: 2.000e-04 

2023-03-02 01:46:06,171 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3388 (0.4084) Acc D Real: 89.390% 
Loss D Fake: 0.7350 (0.7613) Acc D Fake: 17.786% 
Loss D: 1.074 
Loss G: 0.6585 (0.6350) Acc G: 81.812% 
LR: 2.000e-04 

2023-03-02 01:46:06,178 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3886 (0.4083) Acc D Real: 89.386% 
Loss D Fake: 0.7326 (0.7612) Acc D Fake: 17.724% 
Loss D: 1.121 
Loss G: 0.6599 (0.6351) Acc G: 81.875% 
LR: 2.000e-04 

2023-03-02 01:46:06,185 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4132 (0.4083) Acc D Real: 89.392% 
Loss D Fake: 0.7320 (0.7611) Acc D Fake: 17.663% 
Loss D: 1.145 
Loss G: 0.6598 (0.6352) Acc G: 81.938% 
LR: 2.000e-04 

2023-03-02 01:46:06,192 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3993 (0.4083) Acc D Real: 89.380% 
Loss D Fake: 0.7327 (0.7609) Acc D Fake: 17.603% 
Loss D: 1.132 
Loss G: 0.6588 (0.6354) Acc G: 82.000% 
LR: 2.000e-04 

2023-03-02 01:46:06,200 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.4599 (0.4085) Acc D Real: 89.354% 
Loss D Fake: 0.7348 (0.7608) Acc D Fake: 17.543% 
Loss D: 1.195 
Loss G: 0.6557 (0.6355) Acc G: 82.062% 
LR: 2.000e-04 

2023-03-02 01:46:06,207 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3538 (0.4082) Acc D Real: 89.365% 
Loss D Fake: 0.7388 (0.7607) Acc D Fake: 17.484% 
Loss D: 1.093 
Loss G: 0.6523 (0.6355) Acc G: 82.123% 
LR: 2.000e-04 

2023-03-02 01:46:06,214 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3556 (0.4080) Acc D Real: 89.356% 
Loss D Fake: 0.7420 (0.7606) Acc D Fake: 17.426% 
Loss D: 1.098 
Loss G: 0.6502 (0.6356) Acc G: 82.183% 
LR: 2.000e-04 

2023-03-02 01:46:06,221 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3206 (0.4076) Acc D Real: 89.369% 
Loss D Fake: 0.7433 (0.7605) Acc D Fake: 17.368% 
Loss D: 1.064 
Loss G: 0.6500 (0.6357) Acc G: 82.243% 
LR: 2.000e-04 

2023-03-02 01:46:06,228 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3811 (0.4075) Acc D Real: 89.347% 
Loss D Fake: 0.7428 (0.7605) Acc D Fake: 17.310% 
Loss D: 1.124 
Loss G: 0.6509 (0.6357) Acc G: 82.302% 
LR: 2.000e-04 

2023-03-02 01:46:06,235 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2777 (0.4069) Acc D Real: 89.354% 
Loss D Fake: 0.7405 (0.7604) Acc D Fake: 17.253% 
Loss D: 1.018 
Loss G: 0.6554 (0.6358) Acc G: 82.361% 
LR: 2.000e-04 

2023-03-02 01:46:06,242 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2794 (0.4063) Acc D Real: 89.384% 
Loss D Fake: 0.7348 (0.7602) Acc D Fake: 17.197% 
Loss D: 1.014 
Loss G: 0.6604 (0.6359) Acc G: 82.419% 
LR: 2.000e-04 

2023-03-02 01:46:06,250 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3965 (0.4062) Acc D Real: 89.337% 
Loss D Fake: 0.7304 (0.7601) Acc D Fake: 17.141% 
Loss D: 1.127 
Loss G: 0.6634 (0.6361) Acc G: 82.477% 
LR: 2.000e-04 

2023-03-02 01:46:06,257 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3858 (0.4061) Acc D Real: 89.297% 
Loss D Fake: 0.7289 (0.7600) Acc D Fake: 17.085% 
Loss D: 1.115 
Loss G: 0.6634 (0.6362) Acc G: 82.534% 
LR: 2.000e-04 

2023-03-02 01:46:06,264 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3246 (0.4058) Acc D Real: 89.272% 
Loss D Fake: 0.7295 (0.7598) Acc D Fake: 17.030% 
Loss D: 1.054 
Loss G: 0.6639 (0.6363) Acc G: 82.545% 
LR: 2.000e-04 

2023-03-02 01:46:06,271 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3493 (0.4055) Acc D Real: 89.251% 
Loss D Fake: 0.7296 (0.7597) Acc D Fake: 16.976% 
Loss D: 1.079 
Loss G: 0.6624 (0.6364) Acc G: 82.564% 
LR: 2.000e-04 

2023-03-02 01:46:06,278 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2494 (0.4048) Acc D Real: 89.249% 
Loss D Fake: 0.7312 (0.7596) Acc D Fake: 16.922% 
Loss D: 0.981 
Loss G: 0.6637 (0.6366) Acc G: 82.508% 
LR: 2.000e-04 

2023-03-02 01:46:06,286 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3885 (0.4047) Acc D Real: 89.196% 
Loss D Fake: 0.7305 (0.7594) Acc D Fake: 16.898% 
Loss D: 1.119 
Loss G: 0.6628 (0.6367) Acc G: 82.474% 
LR: 2.000e-04 

2023-03-02 01:46:06,293 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3265 (0.4044) Acc D Real: 89.171% 
Loss D Fake: 0.7330 (0.7593) Acc D Fake: 16.845% 
Loss D: 1.060 
Loss G: 0.6648 (0.6368) Acc G: 82.411% 
LR: 2.000e-04 

2023-03-02 01:46:06,300 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3466 (0.4041) Acc D Real: 89.136% 
Loss D Fake: 0.7297 (0.7592) Acc D Fake: 16.815% 
Loss D: 1.076 
Loss G: 0.6708 (0.6370) Acc G: 82.281% 
LR: 2.000e-04 

2023-03-02 01:46:06,307 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2554 (0.4035) Acc D Real: 89.136% 
Loss D Fake: 0.7212 (0.7590) Acc D Fake: 16.844% 
Loss D: 0.977 
Loss G: 0.6814 (0.6372) Acc G: 82.233% 
LR: 2.000e-04 

2023-03-02 01:46:06,320 -                train: [    INFO] - 
Epoch: 10/20
2023-03-02 01:46:06,499 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3129 (0.3092) Acc D Real: 78.542% 
Loss D Fake: 0.6846 (0.6949) Acc D Fake: 70.000% 
Loss D: 0.998 
Loss G: 0.7184 (0.7090) Acc G: 24.167% 
LR: 2.000e-04 

2023-03-02 01:46:06,506 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.2968 (0.3050) Acc D Real: 80.000% 
Loss D Fake: 0.6713 (0.6870) Acc D Fake: 72.778% 
Loss D: 0.968 
Loss G: 0.7248 (0.7143) Acc G: 22.778% 
LR: 2.000e-04 

2023-03-02 01:46:06,513 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2927 (0.3020) Acc D Real: 81.484% 
Loss D Fake: 0.6727 (0.6834) Acc D Fake: 74.583% 
Loss D: 0.965 
Loss G: 0.7146 (0.7144) Acc G: 22.500% 
LR: 2.000e-04 

2023-03-02 01:46:06,531 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2836 (0.2983) Acc D Real: 81.958% 
Loss D Fake: 0.6955 (0.6858) Acc D Fake: 74.667% 
Loss D: 0.979 
Loss G: 0.7035 (0.7122) Acc G: 22.667% 
LR: 2.000e-04 

2023-03-02 01:46:06,537 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2981 (0.2983) Acc D Real: 82.196% 
Loss D Fake: 0.7063 (0.6892) Acc D Fake: 74.167% 
Loss D: 1.004 
Loss G: 0.7196 (0.7134) Acc G: 22.500% 
LR: 2.000e-04 

2023-03-02 01:46:06,544 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2594 (0.2927) Acc D Real: 82.939% 
Loss D Fake: 0.6618 (0.6853) Acc D Fake: 75.000% 
Loss D: 0.921 
Loss G: 0.7460 (0.7181) Acc G: 21.905% 
LR: 2.000e-04 

2023-03-02 01:46:06,551 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3250 (0.2967) Acc D Real: 82.109% 
Loss D Fake: 0.6450 (0.6803) Acc D Fake: 76.042% 
Loss D: 0.970 
Loss G: 0.7583 (0.7231) Acc G: 21.250% 
LR: 2.000e-04 

2023-03-02 01:46:06,558 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2539 (0.2920) Acc D Real: 82.245% 
Loss D Fake: 0.6373 (0.6755) Acc D Fake: 76.852% 
Loss D: 0.891 
Loss G: 0.7658 (0.7279) Acc G: 20.741% 
LR: 2.000e-04 

2023-03-02 01:46:06,565 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3206 (0.2948) Acc D Real: 81.578% 
Loss D Fake: 0.6318 (0.6711) Acc D Fake: 77.500% 
Loss D: 0.952 
Loss G: 0.7718 (0.7322) Acc G: 20.333% 
LR: 2.000e-04 

2023-03-02 01:46:06,572 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3306 (0.2981) Acc D Real: 81.373% 
Loss D Fake: 0.6320 (0.6676) Acc D Fake: 78.030% 
Loss D: 0.963 
Loss G: 0.7589 (0.7347) Acc G: 20.303% 
LR: 2.000e-04 

2023-03-02 01:46:06,578 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2917 (0.2976) Acc D Real: 81.476% 
Loss D Fake: 0.6582 (0.6668) Acc D Fake: 77.778% 
Loss D: 0.950 
Loss G: 0.7313 (0.7344) Acc G: 20.833% 
LR: 2.000e-04 

2023-03-02 01:46:06,585 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3186 (0.2992) Acc D Real: 80.978% 
Loss D Fake: 0.6940 (0.6689) Acc D Fake: 76.538% 
Loss D: 1.013 
Loss G: 0.7845 (0.7383) Acc G: 20.513% 
LR: 2.000e-04 

2023-03-02 01:46:06,592 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2309 (0.2943) Acc D Real: 81.522% 
Loss D Fake: 0.6027 (0.6642) Acc D Fake: 77.262% 
Loss D: 0.834 
Loss G: 0.8176 (0.7439) Acc G: 19.881% 
LR: 2.000e-04 

2023-03-02 01:46:06,599 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2863 (0.2938) Acc D Real: 81.389% 
Loss D Fake: 0.5849 (0.6589) Acc D Fake: 78.000% 
Loss D: 0.871 
Loss G: 0.8341 (0.7499) Acc G: 19.333% 
LR: 2.000e-04 

2023-03-02 01:46:06,606 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4901 (0.3060) Acc D Real: 79.691% 
Loss D Fake: 0.5751 (0.6536) Acc D Fake: 78.646% 
Loss D: 1.065 
Loss G: 0.8440 (0.7558) Acc G: 18.854% 
LR: 2.000e-04 

2023-03-02 01:46:06,613 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3453 (0.3083) Acc D Real: 79.317% 
Loss D Fake: 0.5693 (0.6487) Acc D Fake: 79.314% 
Loss D: 0.915 
Loss G: 0.8499 (0.7613) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 01:46:06,620 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4938 (0.3187) Acc D Real: 77.905% 
Loss D Fake: 0.5662 (0.6441) Acc D Fake: 79.907% 
Loss D: 1.060 
Loss G: 0.8529 (0.7664) Acc G: 17.870% 
LR: 2.000e-04 

2023-03-02 01:46:06,627 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3911 (0.3225) Acc D Real: 77.289% 
Loss D Fake: 0.5646 (0.6399) Acc D Fake: 80.439% 
Loss D: 0.956 
Loss G: 0.8544 (0.7711) Acc G: 17.456% 
LR: 2.000e-04 

2023-03-02 01:46:06,634 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4337 (0.3280) Acc D Real: 76.513% 
Loss D Fake: 0.5641 (0.6361) Acc D Fake: 80.917% 
Loss D: 0.998 
Loss G: 0.8541 (0.7752) Acc G: 17.083% 
LR: 2.000e-04 

2023-03-02 01:46:06,641 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3402 (0.3286) Acc D Real: 76.245% 
Loss D Fake: 0.5648 (0.6327) Acc D Fake: 81.349% 
Loss D: 0.905 
Loss G: 0.8533 (0.7789) Acc G: 16.746% 
LR: 2.000e-04 

2023-03-02 01:46:06,648 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4110 (0.3324) Acc D Real: 75.720% 
Loss D Fake: 0.5656 (0.6297) Acc D Fake: 81.742% 
Loss D: 0.977 
Loss G: 0.8519 (0.7823) Acc G: 16.439% 
LR: 2.000e-04 

2023-03-02 01:46:06,655 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4813 (0.3388) Acc D Real: 74.846% 
Loss D Fake: 0.5671 (0.6270) Acc D Fake: 82.029% 
Loss D: 1.048 
Loss G: 0.8490 (0.7852) Acc G: 16.232% 
LR: 2.000e-04 

2023-03-02 01:46:06,662 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4798 (0.3447) Acc D Real: 74.080% 
Loss D Fake: 0.5699 (0.6246) Acc D Fake: 82.292% 
Loss D: 1.050 
Loss G: 0.8445 (0.7876) Acc G: 16.042% 
LR: 2.000e-04 

2023-03-02 01:46:06,669 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3145 (0.3435) Acc D Real: 74.135% 
Loss D Fake: 0.5735 (0.6225) Acc D Fake: 82.533% 
Loss D: 0.888 
Loss G: 0.8402 (0.7897) Acc G: 15.867% 
LR: 2.000e-04 

2023-03-02 01:46:06,675 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4812 (0.3488) Acc D Real: 73.393% 
Loss D Fake: 0.5769 (0.6208) Acc D Fake: 82.756% 
Loss D: 1.058 
Loss G: 0.8352 (0.7915) Acc G: 15.705% 
LR: 2.000e-04 

2023-03-02 01:46:06,682 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.4776 (0.3536) Acc D Real: 72.699% 
Loss D Fake: 0.5811 (0.6193) Acc D Fake: 82.963% 
Loss D: 1.059 
Loss G: 0.8293 (0.7929) Acc G: 15.556% 
LR: 2.000e-04 

2023-03-02 01:46:06,689 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3670 (0.3540) Acc D Real: 72.545% 
Loss D Fake: 0.5858 (0.6181) Acc D Fake: 83.155% 
Loss D: 0.953 
Loss G: 0.8234 (0.7940) Acc G: 15.476% 
LR: 2.000e-04 

2023-03-02 01:46:06,696 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3909 (0.3553) Acc D Real: 72.277% 
Loss D Fake: 0.5904 (0.6172) Acc D Fake: 83.276% 
Loss D: 0.981 
Loss G: 0.8177 (0.7948) Acc G: 15.402% 
LR: 2.000e-04 

2023-03-02 01:46:06,704 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4343 (0.3579) Acc D Real: 71.872% 
Loss D Fake: 0.5950 (0.6164) Acc D Fake: 83.389% 
Loss D: 1.029 
Loss G: 0.8116 (0.7953) Acc G: 15.333% 
LR: 2.000e-04 

2023-03-02 01:46:06,711 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3663 (0.3582) Acc D Real: 71.736% 
Loss D Fake: 0.6000 (0.6159) Acc D Fake: 83.495% 
Loss D: 0.966 
Loss G: 0.8058 (0.7957) Acc G: 15.269% 
LR: 2.000e-04 

2023-03-02 01:46:06,718 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3959 (0.3594) Acc D Real: 71.509% 
Loss D Fake: 0.6056 (0.6156) Acc D Fake: 83.542% 
Loss D: 1.002 
Loss G: 0.7968 (0.7957) Acc G: 15.260% 
LR: 2.000e-04 

2023-03-02 01:46:06,726 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2739 (0.3568) Acc D Real: 71.739% 
Loss D Fake: 0.6142 (0.6155) Acc D Fake: 83.535% 
Loss D: 0.888 
Loss G: 0.7869 (0.7954) Acc G: 15.303% 
LR: 2.000e-04 

2023-03-02 01:46:06,733 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3186 (0.3557) Acc D Real: 71.800% 
Loss D Fake: 0.6229 (0.6157) Acc D Fake: 83.480% 
Loss D: 0.942 
Loss G: 0.7773 (0.7949) Acc G: 15.392% 
LR: 2.000e-04 

2023-03-02 01:46:06,740 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3160 (0.3545) Acc D Real: 71.908% 
Loss D Fake: 0.6319 (0.6162) Acc D Fake: 83.381% 
Loss D: 0.948 
Loss G: 0.7677 (0.7941) Acc G: 15.524% 
LR: 2.000e-04 

2023-03-02 01:46:06,748 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.2760 (0.3524) Acc D Real: 72.099% 
Loss D Fake: 0.6412 (0.6169) Acc D Fake: 83.241% 
Loss D: 0.917 
Loss G: 0.7595 (0.7932) Acc G: 15.741% 
LR: 2.000e-04 

2023-03-02 01:46:06,755 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3773 (0.3530) Acc D Real: 72.023% 
Loss D Fake: 0.6508 (0.6178) Acc D Fake: 83.018% 
Loss D: 1.028 
Loss G: 0.7504 (0.7920) Acc G: 16.036% 
LR: 2.000e-04 

2023-03-02 01:46:06,762 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3294 (0.3524) Acc D Real: 71.987% 
Loss D Fake: 0.6624 (0.6190) Acc D Fake: 82.675% 
Loss D: 0.992 
Loss G: 0.7471 (0.7908) Acc G: 16.360% 
LR: 2.000e-04 

2023-03-02 01:46:06,771 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.2561 (0.3499) Acc D Real: 72.229% 
Loss D Fake: 0.6628 (0.6201) Acc D Fake: 82.350% 
Loss D: 0.919 
Loss G: 0.7550 (0.7899) Acc G: 16.624% 
LR: 2.000e-04 

2023-03-02 01:46:06,779 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2654 (0.3478) Acc D Real: 72.396% 
Loss D Fake: 0.6493 (0.6208) Acc D Fake: 82.125% 
Loss D: 0.915 
Loss G: 0.7693 (0.7894) Acc G: 16.792% 
LR: 2.000e-04 

2023-03-02 01:46:06,788 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3669 (0.3483) Acc D Real: 72.320% 
Loss D Fake: 0.6359 (0.6212) Acc D Fake: 81.992% 
Loss D: 1.003 
Loss G: 0.7793 (0.7892) Acc G: 16.911% 
LR: 2.000e-04 

2023-03-02 01:46:06,795 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3377 (0.3480) Acc D Real: 72.273% 
Loss D Fake: 0.6283 (0.6214) Acc D Fake: 81.865% 
Loss D: 0.966 
Loss G: 0.7871 (0.7891) Acc G: 16.984% 
LR: 2.000e-04 

2023-03-02 01:46:06,803 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2896 (0.3467) Acc D Real: 72.432% 
Loss D Fake: 0.6271 (0.6215) Acc D Fake: 81.744% 
Loss D: 0.917 
Loss G: 0.7786 (0.7889) Acc G: 17.132% 
LR: 2.000e-04 

2023-03-02 01:46:06,810 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3660 (0.3471) Acc D Real: 72.347% 
Loss D Fake: 0.6578 (0.6223) Acc D Fake: 81.402% 
Loss D: 1.024 
Loss G: 0.7728 (0.7885) Acc G: 17.348% 
LR: 2.000e-04 

2023-03-02 01:46:06,819 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3253 (0.3466) Acc D Real: 72.356% 
Loss D Fake: 0.6454 (0.6229) Acc D Fake: 81.148% 
Loss D: 0.971 
Loss G: 0.7876 (0.7885) Acc G: 17.481% 
LR: 2.000e-04 

2023-03-02 01:46:06,827 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4372 (0.3486) Acc D Real: 72.086% 
Loss D Fake: 0.6234 (0.6229) Acc D Fake: 81.051% 
Loss D: 1.061 
Loss G: 0.8001 (0.7887) Acc G: 17.536% 
LR: 2.000e-04 

2023-03-02 01:46:06,835 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4803 (0.3514) Acc D Real: 71.704% 
Loss D Fake: 0.6133 (0.6227) Acc D Fake: 80.993% 
Loss D: 1.094 
Loss G: 0.8073 (0.7891) Acc G: 17.589% 
LR: 2.000e-04 

2023-03-02 01:46:06,843 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3898 (0.3522) Acc D Real: 71.539% 
Loss D Fake: 0.6081 (0.6224) Acc D Fake: 80.972% 
Loss D: 0.998 
Loss G: 0.8109 (0.7896) Acc G: 17.639% 
LR: 2.000e-04 

2023-03-02 01:46:06,850 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2750 (0.3506) Acc D Real: 71.679% 
Loss D Fake: 0.6058 (0.6220) Acc D Fake: 80.952% 
Loss D: 0.881 
Loss G: 0.8134 (0.7901) Acc G: 17.653% 
LR: 2.000e-04 

2023-03-02 01:46:06,857 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.5116 (0.3539) Acc D Real: 71.275% 
Loss D Fake: 0.6046 (0.6217) Acc D Fake: 80.933% 
Loss D: 1.116 
Loss G: 0.8135 (0.7905) Acc G: 17.700% 
LR: 2.000e-04 

2023-03-02 01:46:06,865 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3891 (0.3545) Acc D Real: 71.200% 
Loss D Fake: 0.6065 (0.6214) Acc D Fake: 80.915% 
Loss D: 0.996 
Loss G: 0.8096 (0.7909) Acc G: 17.745% 
LR: 2.000e-04 

2023-03-02 01:46:06,872 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4431 (0.3562) Acc D Real: 70.993% 
Loss D Fake: 0.6133 (0.6212) Acc D Fake: 80.865% 
Loss D: 1.056 
Loss G: 0.8005 (0.7911) Acc G: 17.821% 
LR: 2.000e-04 

2023-03-02 01:46:06,879 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4426 (0.3579) Acc D Real: 70.773% 
Loss D Fake: 0.6268 (0.6213) Acc D Fake: 80.755% 
Loss D: 1.069 
Loss G: 0.7890 (0.7911) Acc G: 17.956% 
LR: 2.000e-04 

2023-03-02 01:46:06,887 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.4074 (0.3588) Acc D Real: 70.659% 
Loss D Fake: 0.6469 (0.6218) Acc D Fake: 80.556% 
Loss D: 1.054 
Loss G: 0.7799 (0.7908) Acc G: 18.117% 
LR: 2.000e-04 

2023-03-02 01:46:06,894 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4586 (0.3606) Acc D Real: 70.421% 
Loss D Fake: 0.6514 (0.6223) Acc D Fake: 80.333% 
Loss D: 1.110 
Loss G: 0.7865 (0.7908) Acc G: 18.242% 
LR: 2.000e-04 

2023-03-02 01:46:06,902 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2834 (0.3592) Acc D Real: 70.519% 
Loss D Fake: 0.6292 (0.6225) Acc D Fake: 80.238% 
Loss D: 0.913 
Loss G: 0.7977 (0.7909) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 01:46:06,909 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3312 (0.3587) Acc D Real: 70.529% 
Loss D Fake: 0.6178 (0.6224) Acc D Fake: 80.175% 
Loss D: 0.949 
Loss G: 0.8053 (0.7911) Acc G: 18.392% 
LR: 2.000e-04 

2023-03-02 01:46:06,916 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.4940 (0.3611) Acc D Real: 70.228% 
Loss D Fake: 0.6116 (0.6222) Acc D Fake: 80.144% 
Loss D: 1.106 
Loss G: 0.8089 (0.7915) Acc G: 18.420% 
LR: 2.000e-04 

2023-03-02 01:46:06,924 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4054 (0.3618) Acc D Real: 70.094% 
Loss D Fake: 0.6089 (0.6220) Acc D Fake: 80.113% 
Loss D: 1.014 
Loss G: 0.8108 (0.7918) Acc G: 18.446% 
LR: 2.000e-04 

2023-03-02 01:46:06,931 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.2952 (0.3607) Acc D Real: 70.186% 
Loss D Fake: 0.6072 (0.6217) Acc D Fake: 80.089% 
Loss D: 0.902 
Loss G: 0.8123 (0.7921) Acc G: 18.472% 
LR: 2.000e-04 

2023-03-02 01:46:06,939 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3791 (0.3610) Acc D Real: 70.102% 
Loss D Fake: 0.6059 (0.6215) Acc D Fake: 80.088% 
Loss D: 0.985 
Loss G: 0.8134 (0.7925) Acc G: 18.497% 
LR: 2.000e-04 

2023-03-02 01:46:06,946 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3264 (0.3605) Acc D Real: 70.114% 
Loss D Fake: 0.6050 (0.6212) Acc D Fake: 80.087% 
Loss D: 0.931 
Loss G: 0.8144 (0.7928) Acc G: 18.522% 
LR: 2.000e-04 

2023-03-02 01:46:06,954 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.5314 (0.3632) Acc D Real: 69.785% 
Loss D Fake: 0.6047 (0.6209) Acc D Fake: 80.085% 
Loss D: 1.136 
Loss G: 0.8138 (0.7932) Acc G: 18.545% 
LR: 2.000e-04 

2023-03-02 01:46:06,961 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.5264 (0.3657) Acc D Real: 69.471% 
Loss D Fake: 0.6061 (0.6207) Acc D Fake: 80.071% 
Loss D: 1.132 
Loss G: 0.8112 (0.7934) Acc G: 18.568% 
LR: 2.000e-04 

2023-03-02 01:46:06,969 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3470 (0.3654) Acc D Real: 69.449% 
Loss D Fake: 0.6084 (0.6205) Acc D Fake: 80.044% 
Loss D: 0.955 
Loss G: 0.8087 (0.7937) Acc G: 18.590% 
LR: 2.000e-04 

2023-03-02 01:46:06,976 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.6377 (0.3696) Acc D Real: 68.953% 
Loss D Fake: 0.6111 (0.6204) Acc D Fake: 80.018% 
Loss D: 1.249 
Loss G: 0.8043 (0.7938) Acc G: 18.636% 
LR: 2.000e-04 

2023-03-02 01:46:06,983 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.5020 (0.3715) Acc D Real: 68.686% 
Loss D Fake: 0.6156 (0.6203) Acc D Fake: 79.993% 
Loss D: 1.118 
Loss G: 0.7988 (0.7939) Acc G: 18.682% 
LR: 2.000e-04 

2023-03-02 01:46:06,990 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4916 (0.3733) Acc D Real: 68.454% 
Loss D Fake: 0.6208 (0.6203) Acc D Fake: 79.969% 
Loss D: 1.112 
Loss G: 0.7924 (0.7939) Acc G: 18.725% 
LR: 2.000e-04 

2023-03-02 01:46:06,998 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.5641 (0.3761) Acc D Real: 68.096% 
Loss D Fake: 0.6272 (0.6204) Acc D Fake: 79.921% 
Loss D: 1.191 
Loss G: 0.7849 (0.7938) Acc G: 18.792% 
LR: 2.000e-04 

2023-03-02 01:46:07,005 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4893 (0.3777) Acc D Real: 67.868% 
Loss D Fake: 0.6345 (0.6206) Acc D Fake: 79.874% 
Loss D: 1.124 
Loss G: 0.7770 (0.7935) Acc G: 18.857% 
LR: 2.000e-04 

2023-03-02 01:46:07,013 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.5256 (0.3798) Acc D Real: 67.587% 
Loss D Fake: 0.6431 (0.6209) Acc D Fake: 79.806% 
Loss D: 1.169 
Loss G: 0.7670 (0.7931) Acc G: 18.944% 
LR: 2.000e-04 

2023-03-02 01:46:07,020 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.5976 (0.3828) Acc D Real: 67.185% 
Loss D Fake: 0.6553 (0.6214) Acc D Fake: 79.716% 
Loss D: 1.253 
Loss G: 0.7552 (0.7926) Acc G: 19.051% 
LR: 2.000e-04 

2023-03-02 01:46:07,028 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.5210 (0.3847) Acc D Real: 66.905% 
Loss D Fake: 0.6704 (0.6221) Acc D Fake: 79.605% 
Loss D: 1.191 
Loss G: 0.7423 (0.7919) Acc G: 19.178% 
LR: 2.000e-04 

2023-03-02 01:46:07,037 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4233 (0.3852) Acc D Real: 66.808% 
Loss D Fake: 0.6912 (0.6230) Acc D Fake: 79.453% 
Loss D: 1.114 
Loss G: 0.7280 (0.7911) Acc G: 19.324% 
LR: 2.000e-04 

2023-03-02 01:46:07,045 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3784 (0.3851) Acc D Real: 66.763% 
Loss D Fake: 0.7333 (0.6245) Acc D Fake: 79.216% 
Loss D: 1.112 
Loss G: 0.7204 (0.7901) Acc G: 19.489% 
LR: 2.000e-04 

2023-03-02 01:46:07,052 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4686 (0.3862) Acc D Real: 66.578% 
Loss D Fake: 0.7477 (0.6261) Acc D Fake: 78.958% 
Loss D: 1.216 
Loss G: 0.7242 (0.7893) Acc G: 19.627% 
LR: 2.000e-04 

2023-03-02 01:46:07,059 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.5092 (0.3878) Acc D Real: 66.331% 
Loss D Fake: 0.7239 (0.6274) Acc D Fake: 78.757% 
Loss D: 1.233 
Loss G: 0.7260 (0.7884) Acc G: 19.744% 
LR: 2.000e-04 

2023-03-02 01:46:07,067 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.5274 (0.3896) Acc D Real: 66.052% 
Loss D Fake: 0.7481 (0.6289) Acc D Fake: 78.517% 
Loss D: 1.276 
Loss G: 0.7219 (0.7876) Acc G: 19.875% 
LR: 2.000e-04 

2023-03-02 01:46:07,074 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.4645 (0.3905) Acc D Real: 65.876% 
Loss D Fake: 1.8774 (0.6447) Acc D Fake: 77.607% 
Loss D: 2.342 
Loss G: 0.7425 (0.7870) Acc G: 19.940% 
LR: 2.000e-04 

2023-03-02 01:46:07,081 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4817 (0.3917) Acc D Real: 65.680% 
Loss D Fake: 0.6593 (0.6449) Acc D Fake: 77.575% 
Loss D: 1.141 
Loss G: 0.7596 (0.7867) Acc G: 19.941% 
LR: 2.000e-04 

2023-03-02 01:46:07,089 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.5010 (0.3930) Acc D Real: 65.441% 
Loss D Fake: 0.6424 (0.6449) Acc D Fake: 77.605% 
Loss D: 1.143 
Loss G: 0.7652 (0.7864) Acc G: 19.921% 
LR: 2.000e-04 

2023-03-02 01:46:07,096 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.5057 (0.3944) Acc D Real: 65.208% 
Loss D Fake: 0.6368 (0.6448) Acc D Fake: 77.634% 
Loss D: 1.143 
Loss G: 0.7661 (0.7862) Acc G: 19.902% 
LR: 2.000e-04 

2023-03-02 01:46:07,103 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3887 (0.3943) Acc D Real: 65.147% 
Loss D Fake: 0.6354 (0.6447) Acc D Fake: 77.683% 
Loss D: 1.024 
Loss G: 0.7652 (0.7859) Acc G: 19.863% 
LR: 2.000e-04 

2023-03-02 01:46:07,110 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.6557 (0.3975) Acc D Real: 64.691% 
Loss D Fake: 0.6361 (0.6446) Acc D Fake: 77.750% 
Loss D: 1.292 
Loss G: 0.7618 (0.7856) Acc G: 19.825% 
LR: 2.000e-04 

2023-03-02 01:46:07,117 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.6006 (0.3998) Acc D Real: 64.320% 
Loss D Fake: 0.6392 (0.6445) Acc D Fake: 77.816% 
Loss D: 1.240 
Loss G: 0.7565 (0.7853) Acc G: 19.787% 
LR: 2.000e-04 

2023-03-02 01:46:07,125 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.5521 (0.4016) Acc D Real: 64.027% 
Loss D Fake: 0.6438 (0.6445) Acc D Fake: 77.880% 
Loss D: 1.196 
Loss G: 0.7503 (0.7849) Acc G: 19.751% 
LR: 2.000e-04 

2023-03-02 01:46:07,132 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.5110 (0.4029) Acc D Real: 63.810% 
Loss D Fake: 0.6491 (0.6445) Acc D Fake: 77.923% 
Loss D: 1.160 
Loss G: 0.7436 (0.7844) Acc G: 19.734% 
LR: 2.000e-04 

2023-03-02 01:46:07,139 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.5272 (0.4043) Acc D Real: 63.554% 
Loss D Fake: 0.6550 (0.6447) Acc D Fake: 77.966% 
Loss D: 1.182 
Loss G: 0.7367 (0.7838) Acc G: 19.718% 
LR: 2.000e-04 

2023-03-02 01:46:07,147 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4755 (0.4051) Acc D Real: 63.361% 
Loss D Fake: 0.6613 (0.6448) Acc D Fake: 77.989% 
Loss D: 1.137 
Loss G: 0.7297 (0.7832) Acc G: 19.709% 
LR: 2.000e-04 

2023-03-02 01:46:07,154 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4747 (0.4059) Acc D Real: 63.179% 
Loss D Fake: 0.6677 (0.6451) Acc D Fake: 78.011% 
Loss D: 1.142 
Loss G: 0.7227 (0.7826) Acc G: 19.712% 
LR: 2.000e-04 

2023-03-02 01:46:07,161 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.5075 (0.4070) Acc D Real: 62.941% 
Loss D Fake: 0.6745 (0.6454) Acc D Fake: 78.015% 
Loss D: 1.182 
Loss G: 0.7154 (0.7818) Acc G: 19.733% 
LR: 2.000e-04 

2023-03-02 01:46:07,168 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4825 (0.4078) Acc D Real: 62.744% 
Loss D Fake: 0.6820 (0.6458) Acc D Fake: 78.000% 
Loss D: 1.165 
Loss G: 0.7078 (0.7810) Acc G: 19.772% 
LR: 2.000e-04 

2023-03-02 01:46:07,175 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.5194 (0.4090) Acc D Real: 62.492% 
Loss D Fake: 0.6907 (0.6463) Acc D Fake: 77.968% 
Loss D: 1.210 
Loss G: 0.6992 (0.7801) Acc G: 19.829% 
LR: 2.000e-04 

2023-03-02 01:46:07,183 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4786 (0.4097) Acc D Real: 62.303% 
Loss D Fake: 0.7032 (0.6469) Acc D Fake: 77.901% 
Loss D: 1.182 
Loss G: 0.6886 (0.7792) Acc G: 19.919% 
LR: 2.000e-04 

2023-03-02 01:46:07,190 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.5100 (0.4108) Acc D Real: 62.075% 
Loss D Fake: 2.6403 (0.6679) Acc D Fake: 77.168% 
Loss D: 3.150 
Loss G: 0.6875 (0.7782) Acc G: 19.990% 
LR: 2.000e-04 

2023-03-02 01:46:07,197 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4287 (0.4110) Acc D Real: 61.962% 
Loss D Fake: 0.7045 (0.6683) Acc D Fake: 77.133% 
Loss D: 1.133 
Loss G: 0.6869 (0.7773) Acc G: 20.025% 
LR: 2.000e-04 

2023-03-02 01:46:07,204 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.5017 (0.4119) Acc D Real: 61.792% 
Loss D Fake: 0.7058 (0.6687) Acc D Fake: 77.096% 
Loss D: 1.207 
Loss G: 0.6829 (0.7763) Acc G: 20.059% 
LR: 2.000e-04 

2023-03-02 01:46:07,212 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.4583 (0.4124) Acc D Real: 61.799% 
Loss D Fake: 0.7098 (0.6691) Acc D Fake: 77.057% 
Loss D: 1.168 
Loss G: 0.6781 (0.7753) Acc G: 20.143% 
LR: 2.000e-04 

2023-03-02 01:46:07,219 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4743 (0.4130) Acc D Real: 62.025% 
Loss D Fake: 0.7145 (0.6695) Acc D Fake: 76.919% 
Loss D: 1.189 
Loss G: 0.6731 (0.7742) Acc G: 20.866% 
LR: 2.000e-04 

2023-03-02 01:46:07,226 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.5943 (0.4148) Acc D Real: 62.354% 
Loss D Fake: 0.7198 (0.6700) Acc D Fake: 76.233% 
Loss D: 1.314 
Loss G: 0.6674 (0.7732) Acc G: 21.574% 
LR: 2.000e-04 

2023-03-02 01:46:07,233 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4852 (0.4155) Acc D Real: 62.684% 
Loss D Fake: 0.7259 (0.6706) Acc D Fake: 75.561% 
Loss D: 1.211 
Loss G: 0.6616 (0.7721) Acc G: 22.268% 
LR: 2.000e-04 

2023-03-02 01:46:07,241 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4233 (0.4156) Acc D Real: 63.015% 
Loss D Fake: 0.7317 (0.6712) Acc D Fake: 74.901% 
Loss D: 1.155 
Loss G: 0.6564 (0.7709) Acc G: 22.948% 
LR: 2.000e-04 

2023-03-02 01:46:07,248 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4579 (0.4160) Acc D Real: 63.332% 
Loss D Fake: 0.7371 (0.6718) Acc D Fake: 74.255% 
Loss D: 1.195 
Loss G: 0.6515 (0.7698) Acc G: 23.615% 
LR: 2.000e-04 

2023-03-02 01:46:07,255 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4742 (0.4166) Acc D Real: 63.643% 
Loss D Fake: 0.7422 (0.6725) Acc D Fake: 73.621% 
Loss D: 1.216 
Loss G: 0.6468 (0.7686) Acc G: 24.270% 
LR: 2.000e-04 

2023-03-02 01:46:07,262 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4998 (0.4174) Acc D Real: 63.946% 
Loss D Fake: 0.7474 (0.6732) Acc D Fake: 73.000% 
Loss D: 1.247 
Loss G: 0.6421 (0.7674) Acc G: 24.912% 
LR: 2.000e-04 

2023-03-02 01:46:07,270 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.5117 (0.4182) Acc D Real: 64.245% 
Loss D Fake: 0.7527 (0.6740) Acc D Fake: 72.389% 
Loss D: 1.264 
Loss G: 0.6372 (0.7662) Acc G: 25.541% 
LR: 2.000e-04 

2023-03-02 01:46:07,277 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4593 (0.4186) Acc D Real: 64.541% 
Loss D Fake: 0.7581 (0.6748) Acc D Fake: 71.791% 
Loss D: 1.217 
Loss G: 0.6326 (0.7649) Acc G: 26.159% 
LR: 2.000e-04 

2023-03-02 01:46:07,284 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.5061 (0.4194) Acc D Real: 64.829% 
Loss D Fake: 0.7634 (0.6756) Acc D Fake: 71.203% 
Loss D: 1.269 
Loss G: 0.6279 (0.7636) Acc G: 26.766% 
LR: 2.000e-04 

2023-03-02 01:46:07,291 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3677 (0.4190) Acc D Real: 65.119% 
Loss D Fake: 0.7685 (0.6764) Acc D Fake: 70.626% 
Loss D: 1.136 
Loss G: 0.6239 (0.7624) Acc G: 27.361% 
LR: 2.000e-04 

2023-03-02 01:46:07,299 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.5275 (0.4200) Acc D Real: 65.403% 
Loss D Fake: 0.7730 (0.6773) Acc D Fake: 70.060% 
Loss D: 1.301 
Loss G: 0.6198 (0.7611) Acc G: 27.946% 
LR: 2.000e-04 

2023-03-02 01:46:07,306 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4142 (0.4199) Acc D Real: 65.678% 
Loss D Fake: 0.7779 (0.6782) Acc D Fake: 69.504% 
Loss D: 1.192 
Loss G: 0.6160 (0.7598) Acc G: 28.520% 
LR: 2.000e-04 

2023-03-02 01:46:07,313 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.5081 (0.4207) Acc D Real: 65.950% 
Loss D Fake: 0.7823 (0.6791) Acc D Fake: 68.958% 
Loss D: 1.290 
Loss G: 0.6122 (0.7584) Acc G: 29.084% 
LR: 2.000e-04 

2023-03-02 01:46:07,321 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4387 (0.4208) Acc D Real: 66.216% 
Loss D Fake: 0.7870 (0.6801) Acc D Fake: 68.421% 
Loss D: 1.226 
Loss G: 0.6084 (0.7571) Acc G: 29.638% 
LR: 2.000e-04 

2023-03-02 01:46:07,328 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4210 (0.4209) Acc D Real: 66.481% 
Loss D Fake: 0.7913 (0.6811) Acc D Fake: 67.881% 
Loss D: 1.212 
Loss G: 0.6051 (0.7558) Acc G: 30.196% 
LR: 2.000e-04 

2023-03-02 01:46:07,335 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4512 (0.4211) Acc D Real: 66.738% 
Loss D Fake: 0.7953 (0.6821) Acc D Fake: 67.349% 
Loss D: 1.246 
Loss G: 0.6019 (0.7544) Acc G: 30.745% 
LR: 2.000e-04 

2023-03-02 01:46:07,342 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4932 (0.4217) Acc D Real: 66.993% 
Loss D Fake: 0.7993 (0.6831) Acc D Fake: 66.826% 
Loss D: 1.293 
Loss G: 0.5984 (0.7531) Acc G: 31.285% 
LR: 2.000e-04 

2023-03-02 01:46:07,349 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4107 (0.4216) Acc D Real: 67.244% 
Loss D Fake: 0.8036 (0.6841) Acc D Fake: 66.311% 
Loss D: 1.214 
Loss G: 0.5953 (0.7518) Acc G: 31.815% 
LR: 2.000e-04 

2023-03-02 01:46:07,356 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4878 (0.4222) Acc D Real: 67.490% 
Loss D Fake: 0.8075 (0.6852) Acc D Fake: 65.806% 
Loss D: 1.295 
Loss G: 0.5920 (0.7504) Acc G: 32.337% 
LR: 2.000e-04 

2023-03-02 01:46:07,364 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4694 (0.4226) Acc D Real: 67.728% 
Loss D Fake: 0.8117 (0.6862) Acc D Fake: 65.309% 
Loss D: 1.281 
Loss G: 0.5888 (0.7490) Acc G: 32.849% 
LR: 2.000e-04 

2023-03-02 01:46:07,371 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.4823 (0.4231) Acc D Real: 67.964% 
Loss D Fake: 0.8160 (0.6873) Acc D Fake: 64.820% 
Loss D: 1.298 
Loss G: 0.5854 (0.7477) Acc G: 33.353% 
LR: 2.000e-04 

2023-03-02 01:46:07,378 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4492 (0.4233) Acc D Real: 68.197% 
Loss D Fake: 0.8204 (0.6884) Acc D Fake: 64.340% 
Loss D: 1.270 
Loss G: 0.5821 (0.7463) Acc G: 33.849% 
LR: 2.000e-04 

2023-03-02 01:46:07,386 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4616 (0.4236) Acc D Real: 68.421% 
Loss D Fake: 0.8246 (0.6895) Acc D Fake: 63.867% 
Loss D: 1.286 
Loss G: 0.5788 (0.7449) Acc G: 34.337% 
LR: 2.000e-04 

2023-03-02 01:46:07,393 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4106 (0.4235) Acc D Real: 68.650% 
Loss D Fake: 0.8288 (0.6906) Acc D Fake: 63.402% 
Loss D: 1.239 
Loss G: 0.5759 (0.7436) Acc G: 34.816% 
LR: 2.000e-04 

2023-03-02 01:46:07,401 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3234 (0.4227) Acc D Real: 68.881% 
Loss D Fake: 0.8323 (0.6918) Acc D Fake: 62.944% 
Loss D: 1.156 
Loss G: 0.5737 (0.7422) Acc G: 35.288% 
LR: 2.000e-04 

2023-03-02 01:46:07,408 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3772 (0.4223) Acc D Real: 69.102% 
Loss D Fake: 0.8348 (0.6929) Acc D Fake: 62.494% 
Loss D: 1.212 
Loss G: 0.5721 (0.7408) Acc G: 35.752% 
LR: 2.000e-04 

2023-03-02 01:46:07,415 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4497 (0.4226) Acc D Real: 69.320% 
Loss D Fake: 0.8369 (0.6941) Acc D Fake: 62.051% 
Loss D: 1.287 
Loss G: 0.5703 (0.7395) Acc G: 36.209% 
LR: 2.000e-04 

2023-03-02 01:46:07,422 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4221 (0.4226) Acc D Real: 69.534% 
Loss D Fake: 0.8393 (0.6952) Acc D Fake: 61.615% 
Loss D: 1.261 
Loss G: 0.5686 (0.7381) Acc G: 36.659% 
LR: 2.000e-04 

2023-03-02 01:46:07,429 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4629 (0.4229) Acc D Real: 69.741% 
Loss D Fake: 0.8419 (0.6964) Acc D Fake: 61.186% 
Loss D: 1.305 
Loss G: 0.5666 (0.7368) Acc G: 37.102% 
LR: 2.000e-04 

2023-03-02 01:46:07,436 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4482 (0.4231) Acc D Real: 69.942% 
Loss D Fake: 0.8447 (0.6975) Acc D Fake: 60.763% 
Loss D: 1.293 
Loss G: 0.5645 (0.7355) Acc G: 37.538% 
LR: 2.000e-04 

2023-03-02 01:46:07,444 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.4451 (0.4232) Acc D Real: 70.146% 
Loss D Fake: 0.8477 (0.6987) Acc D Fake: 60.347% 
Loss D: 1.293 
Loss G: 0.5623 (0.7341) Acc G: 37.967% 
LR: 2.000e-04 

2023-03-02 01:46:07,452 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3718 (0.4228) Acc D Real: 70.349% 
Loss D Fake: 0.8505 (0.6998) Acc D Fake: 59.937% 
Loss D: 1.222 
Loss G: 0.5605 (0.7328) Acc G: 38.390% 
LR: 2.000e-04 

2023-03-02 01:46:07,459 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4416 (0.4230) Acc D Real: 70.548% 
Loss D Fake: 0.8529 (0.7010) Acc D Fake: 59.534% 
Loss D: 1.294 
Loss G: 0.5587 (0.7315) Acc G: 38.806% 
LR: 2.000e-04 

2023-03-02 01:46:07,467 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3804 (0.4227) Acc D Real: 70.745% 
Loss D Fake: 0.8554 (0.7022) Acc D Fake: 59.136% 
Loss D: 1.236 
Loss G: 0.5571 (0.7302) Acc G: 39.216% 
LR: 2.000e-04 

2023-03-02 01:46:07,475 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.4807 (0.4231) Acc D Real: 70.935% 
Loss D Fake: 0.8576 (0.7033) Acc D Fake: 58.745% 
Loss D: 1.338 
Loss G: 0.5553 (0.7289) Acc G: 39.620% 
LR: 2.000e-04 

2023-03-02 01:46:07,482 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4082 (0.4230) Acc D Real: 71.124% 
Loss D Fake: 0.8602 (0.7045) Acc D Fake: 58.359% 
Loss D: 1.268 
Loss G: 0.5535 (0.7276) Acc G: 40.018% 
LR: 2.000e-04 

2023-03-02 01:46:07,489 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3928 (0.4228) Acc D Real: 71.310% 
Loss D Fake: 0.8627 (0.7056) Acc D Fake: 57.979% 
Loss D: 1.255 
Loss G: 0.5520 (0.7263) Acc G: 40.410% 
LR: 2.000e-04 

2023-03-02 01:46:07,498 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.4012 (0.4226) Acc D Real: 71.496% 
Loss D Fake: 0.8648 (0.7068) Acc D Fake: 57.604% 
Loss D: 1.266 
Loss G: 0.5505 (0.7250) Acc G: 40.796% 
LR: 2.000e-04 

2023-03-02 01:46:07,505 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4462 (0.4228) Acc D Real: 71.674% 
Loss D Fake: 0.8669 (0.7080) Acc D Fake: 57.235% 
Loss D: 1.313 
Loss G: 0.5490 (0.7237) Acc G: 41.177% 
LR: 2.000e-04 

2023-03-02 01:46:07,513 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4711 (0.4231) Acc D Real: 71.851% 
Loss D Fake: 0.8694 (0.7091) Acc D Fake: 56.871% 
Loss D: 1.340 
Loss G: 0.5471 (0.7224) Acc G: 41.552% 
LR: 2.000e-04 

2023-03-02 01:46:07,521 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3892 (0.4229) Acc D Real: 72.027% 
Loss D Fake: 0.8721 (0.7103) Acc D Fake: 56.513% 
Loss D: 1.261 
Loss G: 0.5454 (0.7212) Acc G: 41.922% 
LR: 2.000e-04 

2023-03-02 01:46:07,529 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3983 (0.4227) Acc D Real: 72.202% 
Loss D Fake: 0.8743 (0.7114) Acc D Fake: 56.159% 
Loss D: 1.273 
Loss G: 0.5440 (0.7199) Acc G: 42.286% 
LR: 2.000e-04 

2023-03-02 01:46:07,537 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3954 (0.4225) Acc D Real: 72.376% 
Loss D Fake: 0.8764 (0.7126) Acc D Fake: 55.811% 
Loss D: 1.272 
Loss G: 0.5426 (0.7187) Acc G: 42.646% 
LR: 2.000e-04 

2023-03-02 01:46:07,544 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3956 (0.4223) Acc D Real: 72.544% 
Loss D Fake: 0.8783 (0.7138) Acc D Fake: 55.467% 
Loss D: 1.274 
Loss G: 0.5414 (0.7174) Acc G: 43.000% 
LR: 2.000e-04 

2023-03-02 01:46:07,552 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4006 (0.4222) Acc D Real: 72.708% 
Loss D Fake: 0.8800 (0.7149) Acc D Fake: 55.128% 
Loss D: 1.281 
Loss G: 0.5403 (0.7162) Acc G: 43.350% 
LR: 2.000e-04 

2023-03-02 01:46:07,560 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3556 (0.4217) Acc D Real: 72.871% 
Loss D Fake: 0.8814 (0.7161) Acc D Fake: 54.794% 
Loss D: 1.237 
Loss G: 0.5396 (0.7150) Acc G: 43.695% 
LR: 2.000e-04 

2023-03-02 01:46:07,568 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3659 (0.4213) Acc D Real: 73.034% 
Loss D Fake: 0.8823 (0.7172) Acc D Fake: 54.464% 
Loss D: 1.248 
Loss G: 0.5391 (0.7138) Acc G: 44.035% 
LR: 2.000e-04 

2023-03-02 01:46:07,575 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3917 (0.4211) Acc D Real: 73.193% 
Loss D Fake: 0.8830 (0.7183) Acc D Fake: 54.139% 
Loss D: 1.275 
Loss G: 0.5386 (0.7126) Acc G: 44.370% 
LR: 2.000e-04 

2023-03-02 01:46:07,582 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3549 (0.4207) Acc D Real: 73.359% 
Loss D Fake: 0.8836 (0.7195) Acc D Fake: 53.818% 
Loss D: 1.238 
Loss G: 0.5383 (0.7114) Acc G: 44.701% 
LR: 2.000e-04 

2023-03-02 01:46:07,589 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3904 (0.4205) Acc D Real: 73.513% 
Loss D Fake: 0.8840 (0.7206) Acc D Fake: 53.502% 
Loss D: 1.274 
Loss G: 0.5380 (0.7102) Acc G: 45.027% 
LR: 2.000e-04 

2023-03-02 01:46:07,596 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3881 (0.4203) Acc D Real: 73.666% 
Loss D Fake: 0.8845 (0.7216) Acc D Fake: 53.190% 
Loss D: 1.273 
Loss G: 0.5377 (0.7091) Acc G: 45.349% 
LR: 2.000e-04 

2023-03-02 01:46:07,603 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3956 (0.4201) Acc D Real: 73.816% 
Loss D Fake: 0.8849 (0.7227) Acc D Fake: 52.881% 
Loss D: 1.280 
Loss G: 0.5375 (0.7080) Acc G: 45.667% 
LR: 2.000e-04 

2023-03-02 01:46:07,611 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3898 (0.4199) Acc D Real: 73.965% 
Loss D Fake: 0.8854 (0.7238) Acc D Fake: 52.577% 
Loss D: 1.275 
Loss G: 0.5371 (0.7068) Acc G: 45.981% 
LR: 2.000e-04 

2023-03-02 01:46:07,619 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.4170 (0.4199) Acc D Real: 74.109% 
Loss D Fake: 0.8860 (0.7249) Acc D Fake: 52.277% 
Loss D: 1.303 
Loss G: 0.5366 (0.7057) Acc G: 46.290% 
LR: 2.000e-04 

2023-03-02 01:46:07,626 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4280 (0.4199) Acc D Real: 74.257% 
Loss D Fake: 0.8869 (0.7259) Acc D Fake: 51.981% 
Loss D: 1.315 
Loss G: 0.5358 (0.7046) Acc G: 46.596% 
LR: 2.000e-04 

2023-03-02 01:46:07,633 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3848 (0.4197) Acc D Real: 74.403% 
Loss D Fake: 0.8882 (0.7270) Acc D Fake: 51.689% 
Loss D: 1.273 
Loss G: 0.5351 (0.7035) Acc G: 46.897% 
LR: 2.000e-04 

2023-03-02 01:46:07,641 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4164 (0.4197) Acc D Real: 74.542% 
Loss D Fake: 0.8893 (0.7280) Acc D Fake: 51.400% 
Loss D: 1.306 
Loss G: 0.5342 (0.7024) Acc G: 47.195% 
LR: 2.000e-04 

2023-03-02 01:46:07,650 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3895 (0.4195) Acc D Real: 74.680% 
Loss D Fake: 0.8906 (0.7290) Acc D Fake: 51.115% 
Loss D: 1.280 
Loss G: 0.5335 (0.7014) Acc G: 47.489% 
LR: 2.000e-04 

2023-03-02 01:46:07,657 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3713 (0.4192) Acc D Real: 74.817% 
Loss D Fake: 0.8917 (0.7301) Acc D Fake: 50.834% 
Loss D: 1.263 
Loss G: 0.5328 (0.7003) Acc G: 47.779% 
LR: 2.000e-04 

2023-03-02 01:46:07,664 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3744 (0.4189) Acc D Real: 74.953% 
Loss D Fake: 0.8926 (0.7311) Acc D Fake: 50.556% 
Loss D: 1.267 
Loss G: 0.5323 (0.6992) Acc G: 48.065% 
LR: 2.000e-04 

2023-03-02 01:46:07,672 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3856 (0.4187) Acc D Real: 75.089% 
Loss D Fake: 0.8932 (0.7321) Acc D Fake: 50.282% 
Loss D: 1.279 
Loss G: 0.5319 (0.6982) Acc G: 48.348% 
LR: 2.000e-04 

2023-03-02 01:46:07,679 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3868 (0.4185) Acc D Real: 75.221% 
Loss D Fake: 0.8939 (0.7331) Acc D Fake: 50.011% 
Loss D: 1.281 
Loss G: 0.5315 (0.6972) Acc G: 48.628% 
LR: 2.000e-04 

2023-03-02 01:46:07,686 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3587 (0.4181) Acc D Real: 75.355% 
Loss D Fake: 0.8944 (0.7341) Acc D Fake: 49.743% 
Loss D: 1.253 
Loss G: 0.5313 (0.6961) Acc G: 48.904% 
LR: 2.000e-04 

2023-03-02 01:46:07,695 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4137 (0.4181) Acc D Real: 75.483% 
Loss D Fake: 0.8947 (0.7351) Acc D Fake: 49.479% 
Loss D: 1.308 
Loss G: 0.5310 (0.6951) Acc G: 49.176% 
LR: 2.000e-04 

2023-03-02 01:46:07,703 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3705 (0.4178) Acc D Real: 75.613% 
Loss D Fake: 0.8952 (0.7361) Acc D Fake: 49.218% 
Loss D: 1.266 
Loss G: 0.5308 (0.6941) Acc G: 49.446% 
LR: 2.000e-04 

2023-03-02 01:46:07,711 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3721 (0.4175) Acc D Real: 75.741% 
Loss D Fake: 0.8955 (0.7370) Acc D Fake: 48.960% 
Loss D: 1.268 
Loss G: 0.5306 (0.6931) Acc G: 49.711% 
LR: 2.000e-04 

2023-03-02 01:46:07,719 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3597 (0.4172) Acc D Real: 75.868% 
Loss D Fake: 0.8956 (0.7380) Acc D Fake: 48.705% 
Loss D: 1.255 
Loss G: 0.5306 (0.6922) Acc G: 49.974% 
LR: 2.000e-04 

2023-03-02 01:46:07,728 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3842 (0.4170) Acc D Real: 75.992% 
Loss D Fake: 0.8955 (0.7389) Acc D Fake: 48.454% 
Loss D: 1.280 
Loss G: 0.5307 (0.6912) Acc G: 50.234% 
LR: 2.000e-04 

2023-03-02 01:46:07,736 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4091 (0.4170) Acc D Real: 76.114% 
Loss D Fake: 0.8956 (0.7399) Acc D Fake: 48.205% 
Loss D: 1.305 
Loss G: 0.5305 (0.6902) Acc G: 50.490% 
LR: 2.000e-04 

2023-03-02 01:46:07,744 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3107 (0.4163) Acc D Real: 76.237% 
Loss D Fake: 0.8957 (0.7408) Acc D Fake: 47.959% 
Loss D: 1.206 
Loss G: 0.5307 (0.6893) Acc G: 50.744% 
LR: 2.000e-04 

2023-03-02 01:46:07,751 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3318 (0.4158) Acc D Real: 76.358% 
Loss D Fake: 0.8951 (0.7417) Acc D Fake: 47.716% 
Loss D: 1.227 
Loss G: 0.5313 (0.6884) Acc G: 50.994% 
LR: 2.000e-04 

2023-03-02 01:46:07,759 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3732 (0.4156) Acc D Real: 76.479% 
Loss D Fake: 0.8941 (0.7426) Acc D Fake: 47.476% 
Loss D: 1.267 
Loss G: 0.5319 (0.6874) Acc G: 51.242% 
LR: 2.000e-04 

2023-03-02 01:46:07,766 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3848 (0.4154) Acc D Real: 76.597% 
Loss D Fake: 0.8933 (0.7435) Acc D Fake: 47.239% 
Loss D: 1.278 
Loss G: 0.5323 (0.6865) Acc G: 51.487% 
LR: 2.000e-04 

2023-03-02 01:46:07,773 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3996 (0.4153) Acc D Real: 76.713% 
Loss D Fake: 0.8927 (0.7443) Acc D Fake: 47.004% 
Loss D: 1.292 
Loss G: 0.5326 (0.6857) Acc G: 51.729% 
LR: 2.000e-04 

2023-03-02 01:46:07,780 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4125 (0.4153) Acc D Real: 76.824% 
Loss D Fake: 0.8925 (0.7452) Acc D Fake: 46.773% 
Loss D: 1.305 
Loss G: 0.5326 (0.6848) Acc G: 51.968% 
LR: 2.000e-04 

2023-03-02 01:46:07,787 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3838 (0.4151) Acc D Real: 76.939% 
Loss D Fake: 0.8926 (0.7460) Acc D Fake: 46.543% 
Loss D: 1.276 
Loss G: 0.5326 (0.6839) Acc G: 52.204% 
LR: 2.000e-04 

2023-03-02 01:46:07,794 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3351 (0.4147) Acc D Real: 77.053% 
Loss D Fake: 0.8925 (0.7468) Acc D Fake: 46.317% 
Loss D: 1.228 
Loss G: 0.5328 (0.6830) Acc G: 52.438% 
LR: 2.000e-04 

2023-03-02 01:46:07,801 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3755 (0.4144) Acc D Real: 77.163% 
Loss D Fake: 0.8921 (0.7477) Acc D Fake: 46.093% 
Loss D: 1.268 
Loss G: 0.5331 (0.6822) Acc G: 52.669% 
LR: 2.000e-04 

2023-03-02 01:46:07,809 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3466 (0.4141) Acc D Real: 77.272% 
Loss D Fake: 0.8916 (0.7485) Acc D Fake: 45.871% 
Loss D: 1.238 
Loss G: 0.5335 (0.6814) Acc G: 52.889% 
LR: 2.000e-04 

2023-03-02 01:46:07,816 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3908 (0.4139) Acc D Real: 77.378% 
Loss D Fake: 0.8909 (0.7493) Acc D Fake: 45.662% 
Loss D: 1.282 
Loss G: 0.5339 (0.6805) Acc G: 53.106% 
LR: 2.000e-04 

2023-03-02 01:46:07,824 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4278 (0.4140) Acc D Real: 77.483% 
Loss D Fake: 0.8905 (0.7501) Acc D Fake: 45.454% 
Loss D: 1.318 
Loss G: 0.5340 (0.6797) Acc G: 53.320% 
LR: 2.000e-04 

2023-03-02 01:46:07,831 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3818 (0.4138) Acc D Real: 77.588% 
Loss D Fake: 0.8905 (0.7508) Acc D Fake: 45.249% 
Loss D: 1.272 
Loss G: 0.5339 (0.6789) Acc G: 53.532% 
LR: 2.000e-04 

2023-03-02 01:46:07,838 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3974 (0.4137) Acc D Real: 77.693% 
Loss D Fake: 0.8907 (0.7516) Acc D Fake: 45.046% 
Loss D: 1.288 
Loss G: 0.5338 (0.6781) Acc G: 53.741% 
LR: 2.000e-04 

2023-03-02 01:46:07,846 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3787 (0.4135) Acc D Real: 77.799% 
Loss D Fake: 0.8910 (0.7524) Acc D Fake: 44.846% 
Loss D: 1.270 
Loss G: 0.5336 (0.6773) Acc G: 53.949% 
LR: 2.000e-04 

2023-03-02 01:46:07,853 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3509 (0.4132) Acc D Real: 77.902% 
Loss D Fake: 0.8912 (0.7531) Acc D Fake: 44.647% 
Loss D: 1.242 
Loss G: 0.5336 (0.6765) Acc G: 54.154% 
LR: 2.000e-04 

2023-03-02 01:46:07,860 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3450 (0.4128) Acc D Real: 78.004% 
Loss D Fake: 0.8909 (0.7539) Acc D Fake: 44.451% 
Loss D: 1.236 
Loss G: 0.5339 (0.6758) Acc G: 54.356% 
LR: 2.000e-04 

2023-03-02 01:46:07,867 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3430 (0.4125) Acc D Real: 78.106% 
Loss D Fake: 0.8903 (0.7546) Acc D Fake: 44.257% 
Loss D: 1.233 
Loss G: 0.5344 (0.6750) Acc G: 54.557% 
LR: 2.000e-04 

2023-03-02 01:46:07,876 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4614 (0.4127) Acc D Real: 78.203% 
Loss D Fake: 0.8898 (0.7553) Acc D Fake: 44.065% 
Loss D: 1.351 
Loss G: 0.5344 (0.6743) Acc G: 54.755% 
LR: 2.000e-04 

2023-03-02 01:46:07,883 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3722 (0.4125) Acc D Real: 78.302% 
Loss D Fake: 0.8900 (0.7560) Acc D Fake: 43.875% 
Loss D: 1.262 
Loss G: 0.5343 (0.6735) Acc G: 54.952% 
LR: 2.000e-04 

2023-03-02 01:46:07,890 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3878 (0.4124) Acc D Real: 78.398% 
Loss D Fake: 0.8901 (0.7567) Acc D Fake: 43.687% 
Loss D: 1.278 
Loss G: 0.5342 (0.6728) Acc G: 55.146% 
LR: 2.000e-04 

2023-03-02 01:46:07,898 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3191 (0.4119) Acc D Real: 78.495% 
Loss D Fake: 0.8901 (0.7574) Acc D Fake: 43.501% 
Loss D: 1.209 
Loss G: 0.5344 (0.6721) Acc G: 55.338% 
LR: 2.000e-04 

2023-03-02 01:46:07,905 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3729 (0.4117) Acc D Real: 78.589% 
Loss D Fake: 0.8896 (0.7581) Acc D Fake: 43.316% 
Loss D: 1.262 
Loss G: 0.5348 (0.6713) Acc G: 55.528% 
LR: 2.000e-04 

2023-03-02 01:46:07,912 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3781 (0.4115) Acc D Real: 78.679% 
Loss D Fake: 0.8890 (0.7588) Acc D Fake: 43.134% 
Loss D: 1.267 
Loss G: 0.5352 (0.6706) Acc G: 55.717% 
LR: 2.000e-04 

2023-03-02 01:46:07,920 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3778 (0.4113) Acc D Real: 78.770% 
Loss D Fake: 0.8884 (0.7595) Acc D Fake: 42.954% 
Loss D: 1.266 
Loss G: 0.5356 (0.6699) Acc G: 55.903% 
LR: 2.000e-04 

2023-03-02 01:46:07,928 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3981 (0.4113) Acc D Real: 78.859% 
Loss D Fake: 0.8878 (0.7602) Acc D Fake: 42.775% 
Loss D: 1.286 
Loss G: 0.5358 (0.6692) Acc G: 56.087% 
LR: 2.000e-04 

2023-03-02 01:46:07,935 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3312 (0.4109) Acc D Real: 78.952% 
Loss D Fake: 0.8874 (0.7608) Acc D Fake: 42.599% 
Loss D: 1.219 
Loss G: 0.5362 (0.6686) Acc G: 56.270% 
LR: 2.000e-04 

2023-03-02 01:46:07,943 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3758 (0.4107) Acc D Real: 79.042% 
Loss D Fake: 0.8867 (0.7614) Acc D Fake: 42.424% 
Loss D: 1.262 
Loss G: 0.5367 (0.6679) Acc G: 56.450% 
LR: 2.000e-04 

2023-03-02 01:46:07,950 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4051 (0.4106) Acc D Real: 79.129% 
Loss D Fake: 0.8861 (0.7621) Acc D Fake: 42.251% 
Loss D: 1.291 
Loss G: 0.5370 (0.6672) Acc G: 56.629% 
LR: 2.000e-04 

2023-03-02 01:46:07,957 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3839 (0.4105) Acc D Real: 79.215% 
Loss D Fake: 0.8857 (0.7627) Acc D Fake: 42.080% 
Loss D: 1.270 
Loss G: 0.5372 (0.6666) Acc G: 56.806% 
LR: 2.000e-04 

2023-03-02 01:46:07,965 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.4395 (0.4107) Acc D Real: 79.298% 
Loss D Fake: 0.8856 (0.7633) Acc D Fake: 41.910% 
Loss D: 1.325 
Loss G: 0.5370 (0.6659) Acc G: 56.981% 
LR: 2.000e-04 

2023-03-02 01:46:07,972 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4014 (0.4106) Acc D Real: 79.383% 
Loss D Fake: 0.8861 (0.7639) Acc D Fake: 41.742% 
Loss D: 1.287 
Loss G: 0.5367 (0.6653) Acc G: 57.155% 
LR: 2.000e-04 

2023-03-02 01:46:07,979 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3847 (0.4105) Acc D Real: 79.469% 
Loss D Fake: 0.8865 (0.7645) Acc D Fake: 41.576% 
Loss D: 1.271 
Loss G: 0.5364 (0.6646) Acc G: 57.326% 
LR: 2.000e-04 

2023-03-02 01:46:07,987 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3677 (0.4103) Acc D Real: 79.554% 
Loss D Fake: 0.8869 (0.7652) Acc D Fake: 41.411% 
Loss D: 1.255 
Loss G: 0.5362 (0.6640) Acc G: 57.496% 
LR: 2.000e-04 

2023-03-02 01:46:07,994 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3441 (0.4099) Acc D Real: 79.637% 
Loss D Fake: 0.8870 (0.7658) Acc D Fake: 41.248% 
Loss D: 1.231 
Loss G: 0.5363 (0.6634) Acc G: 57.665% 
LR: 2.000e-04 

2023-03-02 01:46:08,001 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3825 (0.4098) Acc D Real: 79.721% 
Loss D Fake: 0.8869 (0.7663) Acc D Fake: 41.087% 
Loss D: 1.269 
Loss G: 0.5364 (0.6627) Acc G: 57.831% 
LR: 2.000e-04 

2023-03-02 01:46:08,008 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.4136 (0.4098) Acc D Real: 79.800% 
Loss D Fake: 0.8868 (0.7669) Acc D Fake: 40.927% 
Loss D: 1.300 
Loss G: 0.5363 (0.6621) Acc G: 57.996% 
LR: 2.000e-04 

2023-03-02 01:46:08,015 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3333 (0.4095) Acc D Real: 79.882% 
Loss D Fake: 0.8869 (0.7675) Acc D Fake: 40.769% 
Loss D: 1.220 
Loss G: 0.5365 (0.6615) Acc G: 58.160% 
LR: 2.000e-04 

2023-03-02 01:46:08,022 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4049 (0.4094) Acc D Real: 79.962% 
Loss D Fake: 0.8865 (0.7681) Acc D Fake: 40.612% 
Loss D: 1.291 
Loss G: 0.5367 (0.6609) Acc G: 58.322% 
LR: 2.000e-04 

2023-03-02 01:46:08,029 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3461 (0.4091) Acc D Real: 80.043% 
Loss D Fake: 0.8861 (0.7687) Acc D Fake: 40.457% 
Loss D: 1.232 
Loss G: 0.5370 (0.6603) Acc G: 58.482% 
LR: 2.000e-04 

2023-03-02 01:46:08,037 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3160 (0.4087) Acc D Real: 80.126% 
Loss D Fake: 0.8853 (0.7692) Acc D Fake: 40.304% 
Loss D: 1.201 
Loss G: 0.5377 (0.6597) Acc G: 58.641% 
LR: 2.000e-04 

2023-03-02 01:46:08,044 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4161 (0.4087) Acc D Real: 80.202% 
Loss D Fake: 0.8842 (0.7698) Acc D Fake: 40.151% 
Loss D: 1.300 
Loss G: 0.5383 (0.6591) Acc G: 58.798% 
LR: 2.000e-04 

2023-03-02 01:46:08,051 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3724 (0.4085) Acc D Real: 80.278% 
Loss D Fake: 0.8836 (0.7703) Acc D Fake: 40.000% 
Loss D: 1.256 
Loss G: 0.5387 (0.6586) Acc G: 58.954% 
LR: 2.000e-04 

2023-03-02 01:46:08,058 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3431 (0.4082) Acc D Real: 80.354% 
Loss D Fake: 0.8828 (0.7708) Acc D Fake: 39.851% 
Loss D: 1.226 
Loss G: 0.5394 (0.6580) Acc G: 59.108% 
LR: 2.000e-04 

2023-03-02 01:46:08,065 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3590 (0.4080) Acc D Real: 80.430% 
Loss D Fake: 0.8816 (0.7714) Acc D Fake: 39.703% 
Loss D: 1.241 
Loss G: 0.5402 (0.6575) Acc G: 59.261% 
LR: 2.000e-04 

2023-03-02 01:46:08,072 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3853 (0.4079) Acc D Real: 80.506% 
Loss D Fake: 0.8804 (0.7719) Acc D Fake: 39.557% 
Loss D: 1.266 
Loss G: 0.5409 (0.6569) Acc G: 59.412% 
LR: 2.000e-04 

2023-03-02 01:46:08,080 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4838 (0.4083) Acc D Real: 80.573% 
Loss D Fake: 0.8799 (0.7724) Acc D Fake: 39.411% 
Loss D: 1.364 
Loss G: 0.5408 (0.6564) Acc G: 59.562% 
LR: 2.000e-04 

2023-03-02 01:46:08,087 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3720 (0.4081) Acc D Real: 80.648% 
Loss D Fake: 0.8802 (0.7729) Acc D Fake: 39.267% 
Loss D: 1.252 
Loss G: 0.5407 (0.6558) Acc G: 59.711% 
LR: 2.000e-04 

2023-03-02 01:46:08,094 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3754 (0.4079) Acc D Real: 80.719% 
Loss D Fake: 0.8804 (0.7734) Acc D Fake: 39.125% 
Loss D: 1.256 
Loss G: 0.5406 (0.6553) Acc G: 59.858% 
LR: 2.000e-04 

2023-03-02 01:46:08,101 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3965 (0.4079) Acc D Real: 80.793% 
Loss D Fake: 0.8805 (0.7739) Acc D Fake: 38.984% 
Loss D: 1.277 
Loss G: 0.5404 (0.6548) Acc G: 60.004% 
LR: 2.000e-04 

2023-03-02 01:46:08,108 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3212 (0.4075) Acc D Real: 80.866% 
Loss D Fake: 0.8805 (0.7743) Acc D Fake: 38.844% 
Loss D: 1.202 
Loss G: 0.5407 (0.6543) Acc G: 60.149% 
LR: 2.000e-04 

2023-03-02 01:46:08,115 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3699 (0.4073) Acc D Real: 80.938% 
Loss D Fake: 0.8799 (0.7748) Acc D Fake: 38.705% 
Loss D: 1.250 
Loss G: 0.5411 (0.6537) Acc G: 60.292% 
LR: 2.000e-04 

2023-03-02 01:46:08,124 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3634 (0.4071) Acc D Real: 81.009% 
Loss D Fake: 0.8793 (0.7753) Acc D Fake: 38.568% 
Loss D: 1.243 
Loss G: 0.5416 (0.6532) Acc G: 60.434% 
LR: 2.000e-04 

2023-03-02 01:46:08,131 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3971 (0.4071) Acc D Real: 81.076% 
Loss D Fake: 0.8786 (0.7758) Acc D Fake: 38.431% 
Loss D: 1.276 
Loss G: 0.5420 (0.6527) Acc G: 60.575% 
LR: 2.000e-04 

2023-03-02 01:46:08,139 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3916 (0.4070) Acc D Real: 81.142% 
Loss D Fake: 0.8780 (0.7762) Acc D Fake: 38.296% 
Loss D: 1.270 
Loss G: 0.5423 (0.6522) Acc G: 60.714% 
LR: 2.000e-04 

2023-03-02 01:46:08,147 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4301 (0.4071) Acc D Real: 81.208% 
Loss D Fake: 0.8778 (0.7767) Acc D Fake: 38.163% 
Loss D: 1.308 
Loss G: 0.5423 (0.6518) Acc G: 60.852% 
LR: 2.000e-04 

2023-03-02 01:46:08,154 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3874 (0.4070) Acc D Real: 81.275% 
Loss D Fake: 0.8779 (0.7771) Acc D Fake: 38.030% 
Loss D: 1.265 
Loss G: 0.5422 (0.6513) Acc G: 60.989% 
LR: 2.000e-04 

2023-03-02 01:46:08,161 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.3450 (0.4067) Acc D Real: 81.293% 
Loss D Fake: 0.8778 (0.7776) Acc D Fake: 37.997% 
Loss D: 1.223 
Loss G: 0.5424 (0.6508) Acc G: 61.022% 
LR: 2.000e-04 

2023-03-02 01:46:08,171 -                train: [    INFO] - 
Epoch: 11/20
2023-03-02 01:46:08,345 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3421 (0.3913) Acc D Real: 96.328% 
Loss D Fake: 0.8777 (0.8776) Acc D Fake: 10.000% 
Loss D: 1.220 
Loss G: 0.5424 (0.5424) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,352 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3942 (0.3922) Acc D Real: 96.424% 
Loss D Fake: 0.8775 (0.8776) Acc D Fake: 10.000% 
Loss D: 1.272 
Loss G: 0.5424 (0.5424) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,362 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3620 (0.3847) Acc D Real: 96.406% 
Loss D Fake: 0.8775 (0.8776) Acc D Fake: 10.000% 
Loss D: 1.239 
Loss G: 0.5426 (0.5425) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,378 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3644 (0.3806) Acc D Real: 96.469% 
Loss D Fake: 0.8771 (0.8775) Acc D Fake: 10.000% 
Loss D: 1.241 
Loss G: 0.5429 (0.5425) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,385 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3462 (0.3749) Acc D Real: 96.528% 
Loss D Fake: 0.8765 (0.8773) Acc D Fake: 10.000% 
Loss D: 1.223 
Loss G: 0.5434 (0.5427) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,391 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4567 (0.3866) Acc D Real: 96.391% 
Loss D Fake: 0.8759 (0.8771) Acc D Fake: 10.000% 
Loss D: 1.333 
Loss G: 0.5435 (0.5428) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,398 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3775 (0.3854) Acc D Real: 96.374% 
Loss D Fake: 0.8759 (0.8770) Acc D Fake: 10.000% 
Loss D: 1.253 
Loss G: 0.5435 (0.5429) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,405 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2873 (0.3745) Acc D Real: 96.400% 
Loss D Fake: 0.8755 (0.8768) Acc D Fake: 10.000% 
Loss D: 1.163 
Loss G: 0.5442 (0.5430) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,412 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3651 (0.3736) Acc D Real: 96.328% 
Loss D Fake: 0.8743 (0.8765) Acc D Fake: 10.000% 
Loss D: 1.239 
Loss G: 0.5451 (0.5432) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,419 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.4682 (0.3822) Acc D Real: 96.312% 
Loss D Fake: 0.8733 (0.8763) Acc D Fake: 10.000% 
Loss D: 1.341 
Loss G: 0.5453 (0.5434) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,426 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3195 (0.3770) Acc D Real: 96.354% 
Loss D Fake: 0.8731 (0.8760) Acc D Fake: 10.000% 
Loss D: 1.193 
Loss G: 0.5457 (0.5436) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,432 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3775 (0.3770) Acc D Real: 96.346% 
Loss D Fake: 0.8723 (0.8757) Acc D Fake: 10.000% 
Loss D: 1.250 
Loss G: 0.5462 (0.5438) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,439 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4584 (0.3828) Acc D Real: 96.302% 
Loss D Fake: 0.8718 (0.8754) Acc D Fake: 10.000% 
Loss D: 1.330 
Loss G: 0.5463 (0.5440) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,446 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3547 (0.3809) Acc D Real: 96.316% 
Loss D Fake: 0.8718 (0.8752) Acc D Fake: 10.000% 
Loss D: 1.227 
Loss G: 0.5463 (0.5442) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,453 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4042 (0.3824) Acc D Real: 96.289% 
Loss D Fake: 0.8717 (0.8750) Acc D Fake: 10.000% 
Loss D: 1.276 
Loss G: 0.5463 (0.5443) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,460 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3600 (0.3811) Acc D Real: 96.308% 
Loss D Fake: 0.8717 (0.8748) Acc D Fake: 10.000% 
Loss D: 1.232 
Loss G: 0.5464 (0.5444) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,467 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3780 (0.3809) Acc D Real: 96.319% 
Loss D Fake: 0.8715 (0.8746) Acc D Fake: 10.000% 
Loss D: 1.250 
Loss G: 0.5466 (0.5445) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,474 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4013 (0.3820) Acc D Real: 96.305% 
Loss D Fake: 0.8712 (0.8744) Acc D Fake: 10.000% 
Loss D: 1.273 
Loss G: 0.5467 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,481 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4319 (0.3845) Acc D Real: 96.302% 
Loss D Fake: 0.8713 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.303 
Loss G: 0.5464 (0.5447) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,488 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3819 (0.3844) Acc D Real: 96.285% 
Loss D Fake: 0.8718 (0.8742) Acc D Fake: 10.000% 
Loss D: 1.254 
Loss G: 0.5462 (0.5448) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,495 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.4299 (0.3864) Acc D Real: 96.262% 
Loss D Fake: 0.8721 (0.8741) Acc D Fake: 10.000% 
Loss D: 1.302 
Loss G: 0.5458 (0.5448) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,502 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3546 (0.3850) Acc D Real: 96.252% 
Loss D Fake: 0.8727 (0.8740) Acc D Fake: 10.000% 
Loss D: 1.227 
Loss G: 0.5456 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,509 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4165 (0.3864) Acc D Real: 96.235% 
Loss D Fake: 0.8729 (0.8740) Acc D Fake: 10.000% 
Loss D: 1.289 
Loss G: 0.5453 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,516 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3721 (0.3858) Acc D Real: 96.248% 
Loss D Fake: 0.8734 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.245 
Loss G: 0.5451 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,522 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4030 (0.3864) Acc D Real: 96.242% 
Loss D Fake: 0.8736 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.277 
Loss G: 0.5449 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,529 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3756 (0.3860) Acc D Real: 96.242% 
Loss D Fake: 0.8739 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.250 
Loss G: 0.5448 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,536 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3575 (0.3850) Acc D Real: 96.231% 
Loss D Fake: 0.8739 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.231 
Loss G: 0.5449 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,544 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3538 (0.3839) Acc D Real: 96.227% 
Loss D Fake: 0.8734 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.227 
Loss G: 0.5454 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,551 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3892 (0.3841) Acc D Real: 96.231% 
Loss D Fake: 0.8726 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.262 
Loss G: 0.5459 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,559 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.4418 (0.3860) Acc D Real: 96.200% 
Loss D Fake: 0.8722 (0.8738) Acc D Fake: 10.000% 
Loss D: 1.314 
Loss G: 0.5459 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,566 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4088 (0.3867) Acc D Real: 96.206% 
Loss D Fake: 0.8724 (0.8738) Acc D Fake: 10.000% 
Loss D: 1.281 
Loss G: 0.5456 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,574 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3895 (0.3868) Acc D Real: 96.215% 
Loss D Fake: 0.8728 (0.8737) Acc D Fake: 10.000% 
Loss D: 1.262 
Loss G: 0.5454 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,581 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4640 (0.3891) Acc D Real: 96.210% 
Loss D Fake: 0.8734 (0.8737) Acc D Fake: 10.000% 
Loss D: 1.337 
Loss G: 0.5447 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,589 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.4047 (0.3895) Acc D Real: 96.192% 
Loss D Fake: 0.8746 (0.8737) Acc D Fake: 10.000% 
Loss D: 1.279 
Loss G: 0.5439 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,596 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4100 (0.3901) Acc D Real: 96.188% 
Loss D Fake: 0.8758 (0.8738) Acc D Fake: 10.000% 
Loss D: 1.286 
Loss G: 0.5430 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,603 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3964 (0.3902) Acc D Real: 96.189% 
Loss D Fake: 0.8769 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.273 
Loss G: 0.5423 (0.5448) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,610 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3468 (0.3891) Acc D Real: 96.208% 
Loss D Fake: 0.8778 (0.8740) Acc D Fake: 10.000% 
Loss D: 1.225 
Loss G: 0.5419 (0.5448) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,618 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3455 (0.3880) Acc D Real: 96.217% 
Loss D Fake: 0.8780 (0.8741) Acc D Fake: 10.000% 
Loss D: 1.224 
Loss G: 0.5420 (0.5447) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,626 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3557 (0.3872) Acc D Real: 96.223% 
Loss D Fake: 0.8776 (0.8742) Acc D Fake: 10.000% 
Loss D: 1.233 
Loss G: 0.5424 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,633 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3559 (0.3864) Acc D Real: 96.239% 
Loss D Fake: 0.8768 (0.8742) Acc D Fake: 10.000% 
Loss D: 1.233 
Loss G: 0.5430 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,641 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3728 (0.3861) Acc D Real: 96.238% 
Loss D Fake: 0.8759 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.249 
Loss G: 0.5437 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,648 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4002 (0.3864) Acc D Real: 96.227% 
Loss D Fake: 0.8749 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.275 
Loss G: 0.5442 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,655 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4042 (0.3868) Acc D Real: 96.222% 
Loss D Fake: 0.8743 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.278 
Loss G: 0.5446 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,663 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3589 (0.3862) Acc D Real: 96.223% 
Loss D Fake: 0.8737 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.233 
Loss G: 0.5451 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,670 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4098 (0.3867) Acc D Real: 96.232% 
Loss D Fake: 0.8731 (0.8743) Acc D Fake: 10.000% 
Loss D: 1.283 
Loss G: 0.5454 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,677 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3677 (0.3863) Acc D Real: 96.240% 
Loss D Fake: 0.8727 (0.8742) Acc D Fake: 10.000% 
Loss D: 1.240 
Loss G: 0.5457 (0.5446) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,685 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3733 (0.3860) Acc D Real: 96.237% 
Loss D Fake: 0.8722 (0.8742) Acc D Fake: 10.000% 
Loss D: 1.245 
Loss G: 0.5461 (0.5447) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,692 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.4026 (0.3864) Acc D Real: 96.224% 
Loss D Fake: 0.8717 (0.8741) Acc D Fake: 10.000% 
Loss D: 1.274 
Loss G: 0.5463 (0.5447) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,700 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3328 (0.3853) Acc D Real: 96.233% 
Loss D Fake: 0.8712 (0.8741) Acc D Fake: 10.000% 
Loss D: 1.204 
Loss G: 0.5469 (0.5447) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,707 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3424 (0.3845) Acc D Real: 96.242% 
Loss D Fake: 0.8702 (0.8740) Acc D Fake: 10.000% 
Loss D: 1.213 
Loss G: 0.5477 (0.5448) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,715 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3706 (0.3842) Acc D Real: 96.252% 
Loss D Fake: 0.8688 (0.8739) Acc D Fake: 10.000% 
Loss D: 1.239 
Loss G: 0.5486 (0.5449) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,722 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3213 (0.3830) Acc D Real: 96.258% 
Loss D Fake: 0.8674 (0.8738) Acc D Fake: 10.000% 
Loss D: 1.189 
Loss G: 0.5499 (0.5450) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,729 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3879 (0.3831) Acc D Real: 96.255% 
Loss D Fake: 0.8655 (0.8736) Acc D Fake: 10.000% 
Loss D: 1.253 
Loss G: 0.5510 (0.5451) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,737 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4450 (0.3842) Acc D Real: 96.245% 
Loss D Fake: 0.8642 (0.8734) Acc D Fake: 10.000% 
Loss D: 1.309 
Loss G: 0.5516 (0.5452) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,745 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3483 (0.3836) Acc D Real: 96.248% 
Loss D Fake: 0.8635 (0.8733) Acc D Fake: 10.000% 
Loss D: 1.212 
Loss G: 0.5522 (0.5453) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,752 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.4026 (0.3839) Acc D Real: 96.237% 
Loss D Fake: 0.8627 (0.8731) Acc D Fake: 10.000% 
Loss D: 1.265 
Loss G: 0.5526 (0.5454) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,760 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3717 (0.3837) Acc D Real: 96.245% 
Loss D Fake: 0.8622 (0.8729) Acc D Fake: 10.000% 
Loss D: 1.234 
Loss G: 0.5530 (0.5456) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,767 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3503 (0.3831) Acc D Real: 96.249% 
Loss D Fake: 0.8615 (0.8727) Acc D Fake: 10.000% 
Loss D: 1.212 
Loss G: 0.5536 (0.5457) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,774 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3938 (0.3833) Acc D Real: 96.241% 
Loss D Fake: 0.8607 (0.8725) Acc D Fake: 10.000% 
Loss D: 1.255 
Loss G: 0.5540 (0.5458) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,781 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3784 (0.3832) Acc D Real: 96.241% 
Loss D Fake: 0.8601 (0.8723) Acc D Fake: 10.000% 
Loss D: 1.239 
Loss G: 0.5545 (0.5460) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,788 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3662 (0.3830) Acc D Real: 96.255% 
Loss D Fake: 0.8594 (0.8721) Acc D Fake: 10.000% 
Loss D: 1.226 
Loss G: 0.5550 (0.5461) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,795 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3726 (0.3828) Acc D Real: 96.257% 
Loss D Fake: 0.8587 (0.8719) Acc D Fake: 10.000% 
Loss D: 1.231 
Loss G: 0.5555 (0.5463) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,803 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3268 (0.3819) Acc D Real: 96.261% 
Loss D Fake: 0.8578 (0.8717) Acc D Fake: 10.000% 
Loss D: 1.185 
Loss G: 0.5564 (0.5464) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,810 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3821 (0.3819) Acc D Real: 96.261% 
Loss D Fake: 0.8565 (0.8714) Acc D Fake: 10.000% 
Loss D: 1.239 
Loss G: 0.5573 (0.5466) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,817 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4163 (0.3825) Acc D Real: 96.249% 
Loss D Fake: 0.8554 (0.8712) Acc D Fake: 10.000% 
Loss D: 1.272 
Loss G: 0.5578 (0.5468) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,824 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3378 (0.3818) Acc D Real: 96.254% 
Loss D Fake: 0.8547 (0.8709) Acc D Fake: 10.000% 
Loss D: 1.192 
Loss G: 0.5585 (0.5470) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,831 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4045 (0.3821) Acc D Real: 96.250% 
Loss D Fake: 0.8537 (0.8707) Acc D Fake: 10.000% 
Loss D: 1.258 
Loss G: 0.5590 (0.5471) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,839 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4180 (0.3826) Acc D Real: 96.251% 
Loss D Fake: 0.8532 (0.8704) Acc D Fake: 10.000% 
Loss D: 1.271 
Loss G: 0.5591 (0.5473) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,846 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3385 (0.3820) Acc D Real: 96.258% 
Loss D Fake: 0.8531 (0.8702) Acc D Fake: 10.000% 
Loss D: 1.192 
Loss G: 0.5595 (0.5475) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,853 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.3636 (0.3817) Acc D Real: 96.251% 
Loss D Fake: 0.8523 (0.8699) Acc D Fake: 10.000% 
Loss D: 1.216 
Loss G: 0.5601 (0.5477) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,860 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3658 (0.3815) Acc D Real: 96.254% 
Loss D Fake: 0.8514 (0.8697) Acc D Fake: 10.000% 
Loss D: 1.217 
Loss G: 0.5608 (0.5478) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,867 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3435 (0.3810) Acc D Real: 96.254% 
Loss D Fake: 0.8503 (0.8694) Acc D Fake: 10.000% 
Loss D: 1.194 
Loss G: 0.5618 (0.5480) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,875 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3505 (0.3806) Acc D Real: 96.260% 
Loss D Fake: 0.8488 (0.8691) Acc D Fake: 10.000% 
Loss D: 1.199 
Loss G: 0.5629 (0.5482) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,882 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3751 (0.3805) Acc D Real: 96.262% 
Loss D Fake: 0.8472 (0.8688) Acc D Fake: 10.000% 
Loss D: 1.222 
Loss G: 0.5640 (0.5484) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,889 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3907 (0.3807) Acc D Real: 96.262% 
Loss D Fake: 0.8459 (0.8685) Acc D Fake: 10.000% 
Loss D: 1.237 
Loss G: 0.5648 (0.5487) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,896 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3633 (0.3804) Acc D Real: 96.268% 
Loss D Fake: 0.8448 (0.8682) Acc D Fake: 10.000% 
Loss D: 1.208 
Loss G: 0.5656 (0.5489) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,903 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3900 (0.3806) Acc D Real: 96.267% 
Loss D Fake: 0.8438 (0.8679) Acc D Fake: 10.000% 
Loss D: 1.234 
Loss G: 0.5662 (0.5491) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,911 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3125 (0.3797) Acc D Real: 96.270% 
Loss D Fake: 0.8429 (0.8676) Acc D Fake: 10.000% 
Loss D: 1.155 
Loss G: 0.5672 (0.5493) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,918 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3470 (0.3793) Acc D Real: 96.274% 
Loss D Fake: 0.8413 (0.8673) Acc D Fake: 10.000% 
Loss D: 1.188 
Loss G: 0.5685 (0.5496) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,925 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3892 (0.3794) Acc D Real: 96.269% 
Loss D Fake: 0.8397 (0.8669) Acc D Fake: 10.000% 
Loss D: 1.229 
Loss G: 0.5695 (0.5498) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,932 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4365 (0.3801) Acc D Real: 96.266% 
Loss D Fake: 0.8388 (0.8666) Acc D Fake: 10.000% 
Loss D: 1.275 
Loss G: 0.5696 (0.5501) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,939 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3744 (0.3800) Acc D Real: 96.271% 
Loss D Fake: 0.8388 (0.8663) Acc D Fake: 10.000% 
Loss D: 1.213 
Loss G: 0.5696 (0.5503) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,947 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3801 (0.3800) Acc D Real: 96.270% 
Loss D Fake: 0.8389 (0.8659) Acc D Fake: 10.000% 
Loss D: 1.219 
Loss G: 0.5696 (0.5505) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,954 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3979 (0.3802) Acc D Real: 96.263% 
Loss D Fake: 0.8390 (0.8656) Acc D Fake: 10.000% 
Loss D: 1.237 
Loss G: 0.5694 (0.5507) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,961 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3595 (0.3800) Acc D Real: 96.258% 
Loss D Fake: 0.8391 (0.8653) Acc D Fake: 10.000% 
Loss D: 1.199 
Loss G: 0.5695 (0.5510) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,968 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3346 (0.3795) Acc D Real: 96.269% 
Loss D Fake: 0.8387 (0.8650) Acc D Fake: 10.000% 
Loss D: 1.173 
Loss G: 0.5701 (0.5512) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,975 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3711 (0.3794) Acc D Real: 96.266% 
Loss D Fake: 0.8378 (0.8647) Acc D Fake: 10.000% 
Loss D: 1.209 
Loss G: 0.5708 (0.5514) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,982 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.2924 (0.3784) Acc D Real: 96.276% 
Loss D Fake: 0.8365 (0.8644) Acc D Fake: 10.000% 
Loss D: 1.129 
Loss G: 0.5723 (0.5516) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,990 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3586 (0.3782) Acc D Real: 96.280% 
Loss D Fake: 0.8341 (0.8640) Acc D Fake: 10.000% 
Loss D: 1.193 
Loss G: 0.5741 (0.5519) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:08,997 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3532 (0.3779) Acc D Real: 96.285% 
Loss D Fake: 0.8317 (0.8637) Acc D Fake: 10.000% 
Loss D: 1.185 
Loss G: 0.5760 (0.5522) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,004 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3421 (0.3775) Acc D Real: 96.283% 
Loss D Fake: 0.8292 (0.8633) Acc D Fake: 10.000% 
Loss D: 1.171 
Loss G: 0.5779 (0.5524) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,011 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3340 (0.3771) Acc D Real: 96.283% 
Loss D Fake: 0.8264 (0.8629) Acc D Fake: 10.000% 
Loss D: 1.160 
Loss G: 0.5802 (0.5527) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,019 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3731 (0.3770) Acc D Real: 96.282% 
Loss D Fake: 0.8235 (0.8625) Acc D Fake: 10.000% 
Loss D: 1.197 
Loss G: 0.5821 (0.5530) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,026 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3957 (0.3772) Acc D Real: 96.280% 
Loss D Fake: 0.8214 (0.8621) Acc D Fake: 10.000% 
Loss D: 1.217 
Loss G: 0.5834 (0.5534) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,033 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4062 (0.3775) Acc D Real: 96.274% 
Loss D Fake: 0.8202 (0.8616) Acc D Fake: 10.000% 
Loss D: 1.226 
Loss G: 0.5840 (0.5537) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,040 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3700 (0.3774) Acc D Real: 96.274% 
Loss D Fake: 0.8197 (0.8612) Acc D Fake: 10.000% 
Loss D: 1.190 
Loss G: 0.5844 (0.5540) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,047 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3254 (0.3769) Acc D Real: 96.276% 
Loss D Fake: 0.8190 (0.8608) Acc D Fake: 10.000% 
Loss D: 1.144 
Loss G: 0.5853 (0.5543) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,055 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4091 (0.3772) Acc D Real: 96.272% 
Loss D Fake: 0.8179 (0.8603) Acc D Fake: 10.000% 
Loss D: 1.227 
Loss G: 0.5858 (0.5546) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,062 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3450 (0.3769) Acc D Real: 96.276% 
Loss D Fake: 0.8173 (0.8599) Acc D Fake: 10.000% 
Loss D: 1.162 
Loss G: 0.5866 (0.5550) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,069 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3842 (0.3770) Acc D Real: 96.267% 
Loss D Fake: 0.8163 (0.8595) Acc D Fake: 10.000% 
Loss D: 1.200 
Loss G: 0.5872 (0.5553) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,076 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3885 (0.3771) Acc D Real: 96.260% 
Loss D Fake: 0.8157 (0.8590) Acc D Fake: 10.000% 
Loss D: 1.204 
Loss G: 0.5876 (0.5556) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,083 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3829 (0.3771) Acc D Real: 96.256% 
Loss D Fake: 0.8155 (0.8586) Acc D Fake: 10.000% 
Loss D: 1.198 
Loss G: 0.5877 (0.5559) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,092 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3912 (0.3773) Acc D Real: 96.251% 
Loss D Fake: 0.8156 (0.8582) Acc D Fake: 10.000% 
Loss D: 1.207 
Loss G: 0.5874 (0.5562) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,099 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.4072 (0.3776) Acc D Real: 96.249% 
Loss D Fake: 0.8163 (0.8578) Acc D Fake: 10.000% 
Loss D: 1.224 
Loss G: 0.5867 (0.5565) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,107 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3662 (0.3775) Acc D Real: 96.250% 
Loss D Fake: 0.8173 (0.8574) Acc D Fake: 10.000% 
Loss D: 1.183 
Loss G: 0.5863 (0.5568) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,114 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.3558 (0.3773) Acc D Real: 96.252% 
Loss D Fake: 0.8172 (0.8570) Acc D Fake: 10.000% 
Loss D: 1.173 
Loss G: 0.5871 (0.5571) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,122 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3082 (0.3766) Acc D Real: 96.258% 
Loss D Fake: 0.8151 (0.8567) Acc D Fake: 10.000% 
Loss D: 1.123 
Loss G: 0.5898 (0.5574) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,129 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3938 (0.3768) Acc D Real: 96.254% 
Loss D Fake: 0.8115 (0.8562) Acc D Fake: 10.000% 
Loss D: 1.205 
Loss G: 0.5920 (0.5577) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,137 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4221 (0.3772) Acc D Real: 96.253% 
Loss D Fake: 0.8093 (0.8558) Acc D Fake: 10.000% 
Loss D: 1.231 
Loss G: 0.5933 (0.5580) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,146 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4006 (0.3774) Acc D Real: 96.249% 
Loss D Fake: 0.8080 (0.8554) Acc D Fake: 10.000% 
Loss D: 1.209 
Loss G: 0.5942 (0.5583) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,154 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3461 (0.3771) Acc D Real: 96.252% 
Loss D Fake: 0.8067 (0.8549) Acc D Fake: 10.000% 
Loss D: 1.153 
Loss G: 0.5955 (0.5587) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,163 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3330 (0.3767) Acc D Real: 96.253% 
Loss D Fake: 0.8046 (0.8545) Acc D Fake: 10.000% 
Loss D: 1.138 
Loss G: 0.5976 (0.5590) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,172 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3785 (0.3767) Acc D Real: 96.251% 
Loss D Fake: 0.8020 (0.8540) Acc D Fake: 10.000% 
Loss D: 1.180 
Loss G: 0.5992 (0.5594) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,179 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3322 (0.3764) Acc D Real: 96.255% 
Loss D Fake: 0.8001 (0.8536) Acc D Fake: 10.000% 
Loss D: 1.132 
Loss G: 0.6008 (0.5597) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,187 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3798 (0.3764) Acc D Real: 96.253% 
Loss D Fake: 0.7987 (0.8531) Acc D Fake: 10.000% 
Loss D: 1.178 
Loss G: 0.6013 (0.5601) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,194 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3980 (0.3766) Acc D Real: 96.251% 
Loss D Fake: 0.7990 (0.8526) Acc D Fake: 10.000% 
Loss D: 1.197 
Loss G: 0.6005 (0.5604) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,201 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4001 (0.3768) Acc D Real: 96.250% 
Loss D Fake: 0.8009 (0.8522) Acc D Fake: 10.000% 
Loss D: 1.201 
Loss G: 0.5991 (0.5608) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,208 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3221 (0.3763) Acc D Real: 96.254% 
Loss D Fake: 0.8017 (0.8518) Acc D Fake: 10.000% 
Loss D: 1.124 
Loss G: 0.6004 (0.5611) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,216 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3078 (0.3757) Acc D Real: 96.257% 
Loss D Fake: 0.7980 (0.8513) Acc D Fake: 10.000% 
Loss D: 1.106 
Loss G: 0.6041 (0.5614) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,223 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3663 (0.3757) Acc D Real: 96.254% 
Loss D Fake: 0.7931 (0.8508) Acc D Fake: 10.000% 
Loss D: 1.159 
Loss G: 0.6074 (0.5618) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,230 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3528 (0.3755) Acc D Real: 96.258% 
Loss D Fake: 0.7897 (0.8503) Acc D Fake: 10.000% 
Loss D: 1.142 
Loss G: 0.6094 (0.5622) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,237 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3654 (0.3754) Acc D Real: 96.257% 
Loss D Fake: 0.7883 (0.8498) Acc D Fake: 10.000% 
Loss D: 1.154 
Loss G: 0.6096 (0.5626) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,244 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3416 (0.3751) Acc D Real: 96.258% 
Loss D Fake: 0.7889 (0.8493) Acc D Fake: 10.000% 
Loss D: 1.131 
Loss G: 0.6093 (0.5630) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,252 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.4203 (0.3755) Acc D Real: 96.251% 
Loss D Fake: 0.7904 (0.8489) Acc D Fake: 10.000% 
Loss D: 1.211 
Loss G: 0.6070 (0.5633) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,260 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3673 (0.3754) Acc D Real: 96.248% 
Loss D Fake: 0.7946 (0.8484) Acc D Fake: 10.000% 
Loss D: 1.162 
Loss G: 0.6050 (0.5637) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,267 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4006 (0.3756) Acc D Real: 96.241% 
Loss D Fake: 0.7962 (0.8480) Acc D Fake: 10.000% 
Loss D: 1.197 
Loss G: 0.6049 (0.5640) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,274 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3889 (0.3757) Acc D Real: 96.236% 
Loss D Fake: 0.7940 (0.8476) Acc D Fake: 10.000% 
Loss D: 1.183 
Loss G: 0.6083 (0.5643) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,281 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3123 (0.3752) Acc D Real: 96.238% 
Loss D Fake: 0.7883 (0.8472) Acc D Fake: 10.000% 
Loss D: 1.101 
Loss G: 0.6123 (0.5647) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,289 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3044 (0.3747) Acc D Real: 96.238% 
Loss D Fake: 0.7834 (0.8467) Acc D Fake: 10.000% 
Loss D: 1.088 
Loss G: 0.6158 (0.5651) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,296 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3219 (0.3743) Acc D Real: 96.242% 
Loss D Fake: 0.7794 (0.8461) Acc D Fake: 10.000% 
Loss D: 1.101 
Loss G: 0.6187 (0.5655) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,303 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3685 (0.3742) Acc D Real: 96.241% 
Loss D Fake: 0.7770 (0.8456) Acc D Fake: 10.000% 
Loss D: 1.146 
Loss G: 0.6192 (0.5659) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,311 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3708 (0.3742) Acc D Real: 96.242% 
Loss D Fake: 0.7780 (0.8451) Acc D Fake: 9.989% 
Loss D: 1.149 
Loss G: 0.6178 (0.5663) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:09,318 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3614 (0.3741) Acc D Real: 96.243% 
Loss D Fake: 0.7813 (0.8446) Acc D Fake: 9.977% 
Loss D: 1.143 
Loss G: 0.6145 (0.5667) Acc G: 90.012% 
LR: 2.000e-04 

2023-03-02 01:46:09,325 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3564 (0.3740) Acc D Real: 96.243% 
Loss D Fake: 0.7872 (0.8442) Acc D Fake: 9.965% 
Loss D: 1.144 
Loss G: 0.6111 (0.5670) Acc G: 90.025% 
LR: 2.000e-04 

2023-03-02 01:46:09,332 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3963 (0.3741) Acc D Real: 96.237% 
Loss D Fake: 0.7914 (0.8438) Acc D Fake: 9.953% 
Loss D: 1.188 
Loss G: 0.6097 (0.5673) Acc G: 90.037% 
LR: 2.000e-04 

2023-03-02 01:46:09,339 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3399 (0.3739) Acc D Real: 96.237% 
Loss D Fake: 0.7906 (0.8434) Acc D Fake: 9.941% 
Loss D: 1.131 
Loss G: 0.6127 (0.5676) Acc G: 90.049% 
LR: 2.000e-04 

2023-03-02 01:46:09,347 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4086 (0.3741) Acc D Real: 96.234% 
Loss D Fake: 0.7849 (0.8430) Acc D Fake: 9.929% 
Loss D: 1.193 
Loss G: 0.6161 (0.5680) Acc G: 90.060% 
LR: 2.000e-04 

2023-03-02 01:46:09,354 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3368 (0.3739) Acc D Real: 96.236% 
Loss D Fake: 0.7812 (0.8426) Acc D Fake: 9.918% 
Loss D: 1.118 
Loss G: 0.6179 (0.5683) Acc G: 90.072% 
LR: 2.000e-04 

2023-03-02 01:46:09,361 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3855 (0.3740) Acc D Real: 96.229% 
Loss D Fake: 0.7806 (0.8421) Acc D Fake: 9.907% 
Loss D: 1.166 
Loss G: 0.6166 (0.5687) Acc G: 90.083% 
LR: 2.000e-04 

2023-03-02 01:46:09,368 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3317 (0.3737) Acc D Real: 96.235% 
Loss D Fake: 0.7835 (0.8417) Acc D Fake: 9.895% 
Loss D: 1.115 
Loss G: 0.6152 (0.5690) Acc G: 90.095% 
LR: 2.000e-04 

2023-03-02 01:46:09,375 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3287 (0.3733) Acc D Real: 96.238% 
Loss D Fake: 0.7845 (0.8413) Acc D Fake: 9.884% 
Loss D: 1.113 
Loss G: 0.6158 (0.5694) Acc G: 90.106% 
LR: 2.000e-04 

2023-03-02 01:46:09,383 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3286 (0.3730) Acc D Real: 96.237% 
Loss D Fake: 0.7835 (0.8409) Acc D Fake: 9.874% 
Loss D: 1.112 
Loss G: 0.6162 (0.5697) Acc G: 90.117% 
LR: 2.000e-04 

2023-03-02 01:46:09,390 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3923 (0.3732) Acc D Real: 96.237% 
Loss D Fake: 0.7844 (0.8405) Acc D Fake: 9.863% 
Loss D: 1.177 
Loss G: 0.6149 (0.5700) Acc G: 90.127% 
LR: 2.000e-04 

2023-03-02 01:46:09,397 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3664 (0.3731) Acc D Real: 96.236% 
Loss D Fake: 0.7869 (0.8401) Acc D Fake: 9.852% 
Loss D: 1.153 
Loss G: 0.6148 (0.5703) Acc G: 90.138% 
LR: 2.000e-04 

2023-03-02 01:46:09,404 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3674 (0.3731) Acc D Real: 96.239% 
Loss D Fake: 0.7868 (0.8398) Acc D Fake: 9.842% 
Loss D: 1.154 
Loss G: 0.6145 (0.5706) Acc G: 90.148% 
LR: 2.000e-04 

2023-03-02 01:46:09,411 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3494 (0.3729) Acc D Real: 96.238% 
Loss D Fake: 0.7872 (0.8394) Acc D Fake: 9.832% 
Loss D: 1.137 
Loss G: 0.6156 (0.5709) Acc G: 90.159% 
LR: 2.000e-04 

2023-03-02 01:46:09,419 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3629 (0.3729) Acc D Real: 96.237% 
Loss D Fake: 0.7845 (0.8390) Acc D Fake: 9.822% 
Loss D: 1.147 
Loss G: 0.6178 (0.5712) Acc G: 90.169% 
LR: 2.000e-04 

2023-03-02 01:46:09,426 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3688 (0.3728) Acc D Real: 96.234% 
Loss D Fake: 0.7813 (0.8387) Acc D Fake: 9.812% 
Loss D: 1.150 
Loss G: 0.6198 (0.5716) Acc G: 90.179% 
LR: 2.000e-04 

2023-03-02 01:46:09,433 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3887 (0.3729) Acc D Real: 96.231% 
Loss D Fake: 0.7787 (0.8383) Acc D Fake: 9.802% 
Loss D: 1.167 
Loss G: 0.6216 (0.5719) Acc G: 90.189% 
LR: 2.000e-04 

2023-03-02 01:46:09,440 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3987 (0.3731) Acc D Real: 96.228% 
Loss D Fake: 0.7772 (0.8379) Acc D Fake: 9.792% 
Loss D: 1.176 
Loss G: 0.6214 (0.5722) Acc G: 90.199% 
LR: 2.000e-04 

2023-03-02 01:46:09,447 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2929 (0.3726) Acc D Real: 96.229% 
Loss D Fake: 0.7778 (0.8375) Acc D Fake: 9.782% 
Loss D: 1.071 
Loss G: 0.6218 (0.5725) Acc G: 90.208% 
LR: 2.000e-04 

2023-03-02 01:46:09,454 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3451 (0.3724) Acc D Real: 96.222% 
Loss D Fake: 0.7789 (0.8371) Acc D Fake: 9.773% 
Loss D: 1.124 
Loss G: 0.6184 (0.5728) Acc G: 90.218% 
LR: 2.000e-04 

2023-03-02 01:46:09,462 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3985 (0.3726) Acc D Real: 96.203% 
Loss D Fake: 0.7885 (0.8368) Acc D Fake: 9.764% 
Loss D: 1.187 
Loss G: 0.6125 (0.5731) Acc G: 90.227% 
LR: 2.000e-04 

2023-03-02 01:46:09,469 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.4415 (0.3730) Acc D Real: 96.194% 
Loss D Fake: 0.7993 (0.8365) Acc D Fake: 9.754% 
Loss D: 1.241 
Loss G: 0.6114 (0.5733) Acc G: 90.237% 
LR: 2.000e-04 

2023-03-02 01:46:09,476 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3465 (0.3728) Acc D Real: 96.182% 
Loss D Fake: 0.7932 (0.8362) Acc D Fake: 9.745% 
Loss D: 1.140 
Loss G: 0.6143 (0.5736) Acc G: 90.246% 
LR: 2.000e-04 

2023-03-02 01:46:09,484 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3398 (0.3726) Acc D Real: 96.171% 
Loss D Fake: 0.7867 (0.8359) Acc D Fake: 9.736% 
Loss D: 1.127 
Loss G: 0.6184 (0.5739) Acc G: 90.255% 
LR: 2.000e-04 

2023-03-02 01:46:09,491 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2849 (0.3721) Acc D Real: 96.167% 
Loss D Fake: 0.7814 (0.8356) Acc D Fake: 9.727% 
Loss D: 1.066 
Loss G: 0.6204 (0.5742) Acc G: 90.264% 
LR: 2.000e-04 

2023-03-02 01:46:09,498 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.4476 (0.3725) Acc D Real: 96.139% 
Loss D Fake: 0.7839 (0.8353) Acc D Fake: 9.719% 
Loss D: 1.232 
Loss G: 0.6133 (0.5744) Acc G: 90.273% 
LR: 2.000e-04 

2023-03-02 01:46:09,505 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3849 (0.3726) Acc D Real: 96.105% 
Loss D Fake: 0.8037 (0.8351) Acc D Fake: 9.710% 
Loss D: 1.189 
Loss G: 0.6117 (0.5747) Acc G: 90.281% 
LR: 2.000e-04 

2023-03-02 01:46:09,512 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3819 (0.3727) Acc D Real: 96.093% 
Loss D Fake: 0.7902 (0.8348) Acc D Fake: 9.701% 
Loss D: 1.172 
Loss G: 0.6174 (0.5749) Acc G: 90.290% 
LR: 2.000e-04 

2023-03-02 01:46:09,520 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3972 (0.3728) Acc D Real: 96.060% 
Loss D Fake: 0.7850 (0.8345) Acc D Fake: 9.693% 
Loss D: 1.182 
Loss G: 0.6159 (0.5752) Acc G: 90.298% 
LR: 2.000e-04 

2023-03-02 01:46:09,527 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4453 (0.3733) Acc D Real: 95.968% 
Loss D Fake: 0.7887 (0.8342) Acc D Fake: 9.685% 
Loss D: 1.234 
Loss G: 0.6162 (0.5754) Acc G: 90.307% 
LR: 2.000e-04 

2023-03-02 01:46:09,534 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.4165 (0.3735) Acc D Real: 95.909% 
Loss D Fake: 0.7875 (0.8339) Acc D Fake: 9.676% 
Loss D: 1.204 
Loss G: 0.6138 (0.5757) Acc G: 90.315% 
LR: 2.000e-04 

2023-03-02 01:46:09,541 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4145 (0.3738) Acc D Real: 95.842% 
Loss D Fake: 0.7913 (0.8336) Acc D Fake: 9.668% 
Loss D: 1.206 
Loss G: 0.6137 (0.5759) Acc G: 90.323% 
LR: 2.000e-04 

2023-03-02 01:46:09,548 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.4372 (0.3742) Acc D Real: 95.730% 
Loss D Fake: 0.7872 (0.8334) Acc D Fake: 9.660% 
Loss D: 1.224 
Loss G: 0.6168 (0.5761) Acc G: 90.331% 
LR: 2.000e-04 

2023-03-02 01:46:09,555 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2829 (0.3736) Acc D Real: 95.695% 
Loss D Fake: 0.7812 (0.8331) Acc D Fake: 9.652% 
Loss D: 1.064 
Loss G: 0.6212 (0.5764) Acc G: 90.339% 
LR: 2.000e-04 

2023-03-02 01:46:09,563 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4502 (0.3741) Acc D Real: 95.617% 
Loss D Fake: 0.7784 (0.8327) Acc D Fake: 9.644% 
Loss D: 1.229 
Loss G: 0.6181 (0.5767) Acc G: 90.347% 
LR: 2.000e-04 

2023-03-02 01:46:09,570 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4029 (0.3743) Acc D Real: 95.581% 
Loss D Fake: 0.7866 (0.8325) Acc D Fake: 9.637% 
Loss D: 1.190 
Loss G: 0.6115 (0.5769) Acc G: 90.355% 
LR: 2.000e-04 

2023-03-02 01:46:09,577 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3890 (0.3743) Acc D Real: 95.528% 
Loss D Fake: 0.7960 (0.8322) Acc D Fake: 9.629% 
Loss D: 1.185 
Loss G: 0.6082 (0.5771) Acc G: 90.363% 
LR: 2.000e-04 

2023-03-02 01:46:09,584 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4047 (0.3745) Acc D Real: 95.458% 
Loss D Fake: 0.7945 (0.8320) Acc D Fake: 9.621% 
Loss D: 1.199 
Loss G: 0.6122 (0.5773) Acc G: 90.370% 
LR: 2.000e-04 

2023-03-02 01:46:09,591 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3694 (0.3745) Acc D Real: 95.412% 
Loss D Fake: 0.7863 (0.8318) Acc D Fake: 9.604% 
Loss D: 1.156 
Loss G: 0.6172 (0.5775) Acc G: 90.388% 
LR: 2.000e-04 

2023-03-02 01:46:09,599 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3657 (0.3744) Acc D Real: 95.362% 
Loss D Fake: 0.7800 (0.8315) Acc D Fake: 9.587% 
Loss D: 1.146 
Loss G: 0.6213 (0.5777) Acc G: 90.405% 
LR: 2.000e-04 

2023-03-02 01:46:09,607 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.4353 (0.3748) Acc D Real: 95.299% 
Loss D Fake: 0.7760 (0.8311) Acc D Fake: 9.570% 
Loss D: 1.211 
Loss G: 0.6226 (0.5780) Acc G: 90.421% 
LR: 2.000e-04 

2023-03-02 01:46:09,614 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.4699 (0.3753) Acc D Real: 95.222% 
Loss D Fake: 0.7761 (0.8308) Acc D Fake: 9.554% 
Loss D: 1.246 
Loss G: 0.6212 (0.5783) Acc G: 90.438% 
LR: 2.000e-04 

2023-03-02 01:46:09,622 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.4070 (0.3755) Acc D Real: 95.143% 
Loss D Fake: 0.7782 (0.8305) Acc D Fake: 9.537% 
Loss D: 1.185 
Loss G: 0.6200 (0.5785) Acc G: 90.455% 
LR: 2.000e-04 

2023-03-02 01:46:09,630 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4700 (0.3760) Acc D Real: 95.104% 
Loss D Fake: 0.7795 (0.8302) Acc D Fake: 9.521% 
Loss D: 1.250 
Loss G: 0.6188 (0.5787) Acc G: 90.471% 
LR: 2.000e-04 

2023-03-02 01:46:09,637 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4215 (0.3763) Acc D Real: 95.060% 
Loss D Fake: 0.7808 (0.8300) Acc D Fake: 9.505% 
Loss D: 1.202 
Loss G: 0.6176 (0.5789) Acc G: 90.487% 
LR: 2.000e-04 

2023-03-02 01:46:09,645 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4296 (0.3766) Acc D Real: 95.015% 
Loss D Fake: 0.7821 (0.8297) Acc D Fake: 9.489% 
Loss D: 1.212 
Loss G: 0.6165 (0.5791) Acc G: 90.503% 
LR: 2.000e-04 

2023-03-02 01:46:09,652 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4459 (0.3770) Acc D Real: 94.971% 
Loss D Fake: 0.7837 (0.8294) Acc D Fake: 9.474% 
Loss D: 1.230 
Loss G: 0.6148 (0.5793) Acc G: 90.519% 
LR: 2.000e-04 

2023-03-02 01:46:09,660 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.5069 (0.3777) Acc D Real: 94.897% 
Loss D Fake: 0.7864 (0.8292) Acc D Fake: 9.458% 
Loss D: 1.293 
Loss G: 0.6119 (0.5795) Acc G: 90.534% 
LR: 2.000e-04 

2023-03-02 01:46:09,667 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4834 (0.3783) Acc D Real: 94.829% 
Loss D Fake: 0.7902 (0.8290) Acc D Fake: 9.443% 
Loss D: 1.274 
Loss G: 0.6089 (0.5797) Acc G: 90.549% 
LR: 2.000e-04 

2023-03-02 01:46:09,675 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.4869 (0.3789) Acc D Real: 94.797% 
Loss D Fake: 0.7952 (0.8288) Acc D Fake: 9.428% 
Loss D: 1.282 
Loss G: 0.6027 (0.5798) Acc G: 90.565% 
LR: 2.000e-04 

2023-03-02 01:46:09,683 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4219 (0.3791) Acc D Real: 94.787% 
Loss D Fake: 0.8051 (0.8287) Acc D Fake: 9.413% 
Loss D: 1.227 
Loss G: 0.5948 (0.5799) Acc G: 90.580% 
LR: 2.000e-04 

2023-03-02 01:46:09,690 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4110 (0.3793) Acc D Real: 94.755% 
Loss D Fake: 0.8152 (0.8286) Acc D Fake: 9.398% 
Loss D: 1.226 
Loss G: 0.5899 (0.5799) Acc G: 90.595% 
LR: 2.000e-04 

2023-03-02 01:46:09,697 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3387 (0.3791) Acc D Real: 94.757% 
Loss D Fake: 0.8192 (0.8285) Acc D Fake: 9.383% 
Loss D: 1.158 
Loss G: 0.5884 (0.5800) Acc G: 90.609% 
LR: 2.000e-04 

2023-03-02 01:46:09,705 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.4483 (0.3794) Acc D Real: 94.751% 
Loss D Fake: 0.8190 (0.8285) Acc D Fake: 9.369% 
Loss D: 1.267 
Loss G: 0.5891 (0.5800) Acc G: 90.624% 
LR: 2.000e-04 

2023-03-02 01:46:09,712 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.4568 (0.3798) Acc D Real: 94.748% 
Loss D Fake: 0.8204 (0.8285) Acc D Fake: 9.354% 
Loss D: 1.277 
Loss G: 0.5833 (0.5801) Acc G: 90.638% 
LR: 2.000e-04 

2023-03-02 01:46:09,720 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3476 (0.3797) Acc D Real: 94.759% 
Loss D Fake: 0.8321 (0.8285) Acc D Fake: 9.340% 
Loss D: 1.180 
Loss G: 0.5797 (0.5801) Acc G: 90.653% 
LR: 2.000e-04 

2023-03-02 01:46:09,728 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3739 (0.3796) Acc D Real: 94.765% 
Loss D Fake: 0.8301 (0.8285) Acc D Fake: 9.326% 
Loss D: 1.204 
Loss G: 0.5838 (0.5801) Acc G: 90.667% 
LR: 2.000e-04 

2023-03-02 01:46:09,735 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4653 (0.3801) Acc D Real: 94.763% 
Loss D Fake: 0.8211 (0.8284) Acc D Fake: 9.312% 
Loss D: 1.286 
Loss G: 0.5887 (0.5801) Acc G: 90.681% 
LR: 2.000e-04 

2023-03-02 01:46:09,742 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4040 (0.3802) Acc D Real: 94.772% 
Loss D Fake: 0.8158 (0.8284) Acc D Fake: 9.298% 
Loss D: 1.220 
Loss G: 0.5906 (0.5802) Acc G: 90.694% 
LR: 2.000e-04 

2023-03-02 01:46:09,750 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4291 (0.3805) Acc D Real: 94.775% 
Loss D Fake: 0.8159 (0.8283) Acc D Fake: 9.285% 
Loss D: 1.245 
Loss G: 0.5880 (0.5802) Acc G: 90.708% 
LR: 2.000e-04 

2023-03-02 01:46:09,757 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4045 (0.3806) Acc D Real: 94.786% 
Loss D Fake: 0.8212 (0.8283) Acc D Fake: 9.271% 
Loss D: 1.226 
Loss G: 0.5845 (0.5802) Acc G: 90.722% 
LR: 2.000e-04 

2023-03-02 01:46:09,765 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3964 (0.3807) Acc D Real: 94.795% 
Loss D Fake: 0.8251 (0.8283) Acc D Fake: 9.258% 
Loss D: 1.222 
Loss G: 0.5831 (0.5803) Acc G: 90.735% 
LR: 2.000e-04 

2023-03-02 01:46:09,772 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3576 (0.3806) Acc D Real: 94.803% 
Loss D Fake: 0.8257 (0.8282) Acc D Fake: 9.245% 
Loss D: 1.183 
Loss G: 0.5832 (0.5803) Acc G: 90.748% 
LR: 2.000e-04 

2023-03-02 01:46:09,780 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3645 (0.3805) Acc D Real: 94.810% 
Loss D Fake: 0.8261 (0.8282) Acc D Fake: 9.231% 
Loss D: 1.191 
Loss G: 0.5817 (0.5803) Acc G: 90.761% 
LR: 2.000e-04 

2023-03-02 01:46:09,787 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.4093 (0.3806) Acc D Real: 94.814% 
Loss D Fake: 0.8282 (0.8282) Acc D Fake: 9.218% 
Loss D: 1.238 
Loss G: 0.5824 (0.5803) Acc G: 90.774% 
LR: 2.000e-04 

2023-03-02 01:46:09,796 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3603 (0.3805) Acc D Real: 94.823% 
Loss D Fake: 0.8243 (0.8282) Acc D Fake: 9.206% 
Loss D: 1.185 
Loss G: 0.5863 (0.5803) Acc G: 90.787% 
LR: 2.000e-04 

2023-03-02 01:46:09,803 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4887 (0.3811) Acc D Real: 94.822% 
Loss D Fake: 0.8192 (0.8282) Acc D Fake: 9.193% 
Loss D: 1.308 
Loss G: 0.5880 (0.5804) Acc G: 90.800% 
LR: 2.000e-04 

2023-03-02 01:46:09,811 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.4518 (0.3814) Acc D Real: 94.824% 
Loss D Fake: 0.8179 (0.8281) Acc D Fake: 9.180% 
Loss D: 1.270 
Loss G: 0.5888 (0.5804) Acc G: 90.813% 
LR: 2.000e-04 

2023-03-02 01:46:09,822 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3877 (0.3814) Acc D Real: 94.828% 
Loss D Fake: 0.8169 (0.8281) Acc D Fake: 9.168% 
Loss D: 1.205 
Loss G: 0.5892 (0.5804) Acc G: 90.825% 
LR: 2.000e-04 

2023-03-02 01:46:09,830 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3665 (0.3814) Acc D Real: 94.836% 
Loss D Fake: 0.8165 (0.8280) Acc D Fake: 9.156% 
Loss D: 1.183 
Loss G: 0.5895 (0.5805) Acc G: 90.837% 
LR: 2.000e-04 

2023-03-02 01:46:09,837 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3902 (0.3814) Acc D Real: 94.839% 
Loss D Fake: 0.8157 (0.8279) Acc D Fake: 9.143% 
Loss D: 1.206 
Loss G: 0.5906 (0.5805) Acc G: 90.850% 
LR: 2.000e-04 

2023-03-02 01:46:09,844 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3859 (0.3814) Acc D Real: 94.846% 
Loss D Fake: 0.8142 (0.8279) Acc D Fake: 9.131% 
Loss D: 1.200 
Loss G: 0.5913 (0.5806) Acc G: 90.862% 
LR: 2.000e-04 

2023-03-02 01:46:09,852 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4191 (0.3816) Acc D Real: 94.848% 
Loss D Fake: 0.8134 (0.8278) Acc D Fake: 9.119% 
Loss D: 1.233 
Loss G: 0.5919 (0.5806) Acc G: 90.874% 
LR: 2.000e-04 

2023-03-02 01:46:09,859 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4206 (0.3818) Acc D Real: 94.856% 
Loss D Fake: 0.8126 (0.8277) Acc D Fake: 9.108% 
Loss D: 1.233 
Loss G: 0.5923 (0.5807) Acc G: 90.886% 
LR: 2.000e-04 

2023-03-02 01:46:09,866 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3724 (0.3818) Acc D Real: 94.858% 
Loss D Fake: 0.8124 (0.8277) Acc D Fake: 9.096% 
Loss D: 1.185 
Loss G: 0.5923 (0.5808) Acc G: 90.897% 
LR: 2.000e-04 

2023-03-02 01:46:09,873 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4546 (0.3821) Acc D Real: 94.863% 
Loss D Fake: 0.8128 (0.8276) Acc D Fake: 9.084% 
Loss D: 1.267 
Loss G: 0.5918 (0.5808) Acc G: 90.909% 
LR: 2.000e-04 

2023-03-02 01:46:09,880 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2867 (0.3817) Acc D Real: 94.875% 
Loss D Fake: 0.8142 (0.8275) Acc D Fake: 9.073% 
Loss D: 1.101 
Loss G: 0.5900 (0.5808) Acc G: 90.921% 
LR: 2.000e-04 

2023-03-02 01:46:09,888 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3895 (0.3817) Acc D Real: 94.881% 
Loss D Fake: 0.8175 (0.8275) Acc D Fake: 9.061% 
Loss D: 1.207 
Loss G: 0.5883 (0.5809) Acc G: 90.932% 
LR: 2.000e-04 

2023-03-02 01:46:09,895 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3339 (0.3815) Acc D Real: 94.890% 
Loss D Fake: 0.8202 (0.8274) Acc D Fake: 9.050% 
Loss D: 1.154 
Loss G: 0.5862 (0.5809) Acc G: 90.943% 
LR: 2.000e-04 

2023-03-02 01:46:09,902 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.4368 (0.3817) Acc D Real: 94.896% 
Loss D Fake: 0.8244 (0.8274) Acc D Fake: 9.039% 
Loss D: 1.261 
Loss G: 0.5842 (0.5809) Acc G: 90.955% 
LR: 2.000e-04 

2023-03-02 01:46:09,910 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3969 (0.3818) Acc D Real: 94.903% 
Loss D Fake: 0.8244 (0.8274) Acc D Fake: 9.028% 
Loss D: 1.221 
Loss G: 0.5878 (0.5810) Acc G: 90.966% 
LR: 2.000e-04 

2023-03-02 01:46:09,917 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3557 (0.3817) Acc D Real: 94.910% 
Loss D Fake: 0.8193 (0.8274) Acc D Fake: 9.017% 
Loss D: 1.175 
Loss G: 0.5875 (0.5810) Acc G: 90.977% 
LR: 2.000e-04 

2023-03-02 01:46:09,925 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.4721 (0.3821) Acc D Real: 94.911% 
Loss D Fake: 0.8227 (0.8274) Acc D Fake: 9.006% 
Loss D: 1.295 
Loss G: 0.5866 (0.5810) Acc G: 90.988% 
LR: 2.000e-04 

2023-03-02 01:46:09,932 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.4899 (0.3826) Acc D Real: 94.912% 
Loss D Fake: 0.8215 (0.8273) Acc D Fake: 8.995% 
Loss D: 1.311 
Loss G: 0.5886 (0.5810) Acc G: 90.998% 
LR: 2.000e-04 

2023-03-02 01:46:09,940 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4172 (0.3827) Acc D Real: 94.918% 
Loss D Fake: 0.8167 (0.8273) Acc D Fake: 8.984% 
Loss D: 1.234 
Loss G: 0.5917 (0.5811) Acc G: 91.009% 
LR: 2.000e-04 

2023-03-02 01:46:09,947 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.4211 (0.3829) Acc D Real: 94.919% 
Loss D Fake: 0.8125 (0.8272) Acc D Fake: 8.974% 
Loss D: 1.234 
Loss G: 0.5937 (0.5812) Acc G: 91.020% 
LR: 2.000e-04 

2023-03-02 01:46:09,955 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.4387 (0.3832) Acc D Real: 94.925% 
Loss D Fake: 0.8100 (0.8271) Acc D Fake: 8.963% 
Loss D: 1.249 
Loss G: 0.5951 (0.5812) Acc G: 91.030% 
LR: 2.000e-04 

2023-03-02 01:46:09,962 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4955 (0.3837) Acc D Real: 94.926% 
Loss D Fake: 0.8098 (0.8271) Acc D Fake: 8.953% 
Loss D: 1.305 
Loss G: 0.5929 (0.5813) Acc G: 91.041% 
LR: 2.000e-04 

2023-03-02 01:46:09,969 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3912 (0.3837) Acc D Real: 94.930% 
Loss D Fake: 0.8138 (0.8270) Acc D Fake: 8.943% 
Loss D: 1.205 
Loss G: 0.5904 (0.5813) Acc G: 91.051% 
LR: 2.000e-04 

2023-03-02 01:46:09,977 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3175 (0.3834) Acc D Real: 94.938% 
Loss D Fake: 0.8160 (0.8269) Acc D Fake: 8.932% 
Loss D: 1.133 
Loss G: 0.5900 (0.5814) Acc G: 91.061% 
LR: 2.000e-04 

2023-03-02 01:46:09,984 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3684 (0.3834) Acc D Real: 94.947% 
Loss D Fake: 0.8168 (0.8269) Acc D Fake: 8.922% 
Loss D: 1.185 
Loss G: 0.5879 (0.5814) Acc G: 91.071% 
LR: 2.000e-04 

2023-03-02 01:46:09,991 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3998 (0.3834) Acc D Real: 94.952% 
Loss D Fake: 0.8210 (0.8269) Acc D Fake: 8.912% 
Loss D: 1.221 
Loss G: 0.5856 (0.5814) Acc G: 91.081% 
LR: 2.000e-04 

2023-03-02 01:46:09,999 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4554 (0.3837) Acc D Real: 94.954% 
Loss D Fake: 0.8228 (0.8269) Acc D Fake: 8.910% 
Loss D: 1.278 
Loss G: 0.5860 (0.5814) Acc G: 91.084% 
LR: 2.000e-04 

2023-03-02 01:46:10,010 -                train: [    INFO] - 
Epoch: 12/20
2023-03-02 01:46:10,192 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4277 (0.3905) Acc D Real: 96.380% 
Loss D Fake: 0.8157 (0.8179) Acc D Fake: 6.667% 
Loss D: 1.243 
Loss G: 0.5915 (0.5901) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,200 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3646 (0.3819) Acc D Real: 96.406% 
Loss D Fake: 0.8118 (0.8158) Acc D Fake: 6.667% 
Loss D: 1.176 
Loss G: 0.5944 (0.5916) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,209 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4550 (0.4002) Acc D Real: 96.328% 
Loss D Fake: 0.8084 (0.8140) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.5960 (0.5927) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,226 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4281 (0.4057) Acc D Real: 96.354% 
Loss D Fake: 0.8070 (0.8126) Acc D Fake: 6.667% 
Loss D: 1.235 
Loss G: 0.5964 (0.5934) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,233 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3488 (0.3962) Acc D Real: 96.354% 
Loss D Fake: 0.8068 (0.8116) Acc D Fake: 6.667% 
Loss D: 1.156 
Loss G: 0.5965 (0.5939) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,240 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3585 (0.3908) Acc D Real: 96.503% 
Loss D Fake: 0.8066 (0.8109) Acc D Fake: 6.667% 
Loss D: 1.165 
Loss G: 0.5969 (0.5944) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,247 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4388 (0.3968) Acc D Real: 96.432% 
Loss D Fake: 0.8061 (0.8103) Acc D Fake: 6.667% 
Loss D: 1.245 
Loss G: 0.5971 (0.5947) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,255 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3632 (0.3931) Acc D Real: 96.395% 
Loss D Fake: 0.8060 (0.8098) Acc D Fake: 6.667% 
Loss D: 1.169 
Loss G: 0.5973 (0.5950) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,262 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3455 (0.3883) Acc D Real: 96.443% 
Loss D Fake: 0.8054 (0.8094) Acc D Fake: 6.667% 
Loss D: 1.151 
Loss G: 0.5981 (0.5953) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,269 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3579 (0.3856) Acc D Real: 96.435% 
Loss D Fake: 0.8041 (0.8089) Acc D Fake: 6.667% 
Loss D: 1.162 
Loss G: 0.5993 (0.5957) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,276 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3500 (0.3826) Acc D Real: 96.489% 
Loss D Fake: 0.8024 (0.8084) Acc D Fake: 6.667% 
Loss D: 1.152 
Loss G: 0.6009 (0.5961) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,283 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3909 (0.3832) Acc D Real: 96.386% 
Loss D Fake: 0.8004 (0.8078) Acc D Fake: 6.667% 
Loss D: 1.191 
Loss G: 0.6024 (0.5966) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,290 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.4640 (0.3890) Acc D Real: 96.343% 
Loss D Fake: 0.7988 (0.8071) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6034 (0.5971) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,297 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.4508 (0.3931) Acc D Real: 96.243% 
Loss D Fake: 0.7982 (0.8065) Acc D Fake: 6.667% 
Loss D: 1.249 
Loss G: 0.6032 (0.5975) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,305 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3779 (0.3922) Acc D Real: 96.263% 
Loss D Fake: 0.7987 (0.8060) Acc D Fake: 6.667% 
Loss D: 1.177 
Loss G: 0.6030 (0.5978) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,314 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3539 (0.3899) Acc D Real: 96.278% 
Loss D Fake: 0.7988 (0.8056) Acc D Fake: 6.667% 
Loss D: 1.153 
Loss G: 0.6031 (0.5982) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,322 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4219 (0.3917) Acc D Real: 96.282% 
Loss D Fake: 0.7989 (0.8052) Acc D Fake: 6.667% 
Loss D: 1.221 
Loss G: 0.6028 (0.5984) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,330 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3452 (0.3893) Acc D Real: 96.291% 
Loss D Fake: 0.7993 (0.8049) Acc D Fake: 6.667% 
Loss D: 1.145 
Loss G: 0.6030 (0.5987) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,338 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.4342 (0.3915) Acc D Real: 96.292% 
Loss D Fake: 0.7992 (0.8046) Acc D Fake: 6.667% 
Loss D: 1.233 
Loss G: 0.6028 (0.5989) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,345 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3652 (0.3902) Acc D Real: 96.186% 
Loss D Fake: 0.7996 (0.8044) Acc D Fake: 6.667% 
Loss D: 1.165 
Loss G: 0.6030 (0.5991) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,353 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3729 (0.3895) Acc D Real: 96.184% 
Loss D Fake: 0.7989 (0.8041) Acc D Fake: 6.667% 
Loss D: 1.172 
Loss G: 0.6039 (0.5993) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,361 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.4556 (0.3923) Acc D Real: 96.153% 
Loss D Fake: 0.7978 (0.8039) Acc D Fake: 6.667% 
Loss D: 1.253 
Loss G: 0.6043 (0.5995) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,369 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4278 (0.3938) Acc D Real: 96.118% 
Loss D Fake: 0.7977 (0.8036) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6043 (0.5997) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,377 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4335 (0.3954) Acc D Real: 96.119% 
Loss D Fake: 0.7976 (0.8034) Acc D Fake: 6.667% 
Loss D: 1.231 
Loss G: 0.6044 (0.5999) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,385 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.4074 (0.3959) Acc D Real: 96.104% 
Loss D Fake: 0.7972 (0.8031) Acc D Fake: 6.667% 
Loss D: 1.205 
Loss G: 0.6047 (0.6001) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,392 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3791 (0.3952) Acc D Real: 96.140% 
Loss D Fake: 0.7967 (0.8029) Acc D Fake: 6.667% 
Loss D: 1.176 
Loss G: 0.6051 (0.6003) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,399 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.4132 (0.3959) Acc D Real: 96.142% 
Loss D Fake: 0.7971 (0.8027) Acc D Fake: 6.667% 
Loss D: 1.210 
Loss G: 0.6032 (0.6004) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,406 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3854 (0.3955) Acc D Real: 96.139% 
Loss D Fake: 0.8011 (0.8026) Acc D Fake: 6.667% 
Loss D: 1.187 
Loss G: 0.6002 (0.6004) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,413 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4066 (0.3959) Acc D Real: 96.146% 
Loss D Fake: 0.8056 (0.8027) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.5972 (0.6003) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,421 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3568 (0.3946) Acc D Real: 96.148% 
Loss D Fake: 0.8095 (0.8030) Acc D Fake: 6.667% 
Loss D: 1.166 
Loss G: 0.5952 (0.6001) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,428 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.4114 (0.3952) Acc D Real: 96.178% 
Loss D Fake: 0.8119 (0.8032) Acc D Fake: 6.667% 
Loss D: 1.223 
Loss G: 0.5945 (0.5999) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,435 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3783 (0.3946) Acc D Real: 96.185% 
Loss D Fake: 0.8104 (0.8034) Acc D Fake: 6.667% 
Loss D: 1.189 
Loss G: 0.5976 (0.5998) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,442 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4545 (0.3964) Acc D Real: 96.172% 
Loss D Fake: 0.8045 (0.8035) Acc D Fake: 6.667% 
Loss D: 1.259 
Loss G: 0.6013 (0.5999) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,450 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3095 (0.3939) Acc D Real: 96.204% 
Loss D Fake: 0.7997 (0.8034) Acc D Fake: 6.667% 
Loss D: 1.109 
Loss G: 0.6044 (0.6000) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,457 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4557 (0.3956) Acc D Real: 96.201% 
Loss D Fake: 0.7968 (0.8032) Acc D Fake: 6.667% 
Loss D: 1.252 
Loss G: 0.6051 (0.6002) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,464 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3582 (0.3946) Acc D Real: 96.212% 
Loss D Fake: 0.7964 (0.8030) Acc D Fake: 6.667% 
Loss D: 1.155 
Loss G: 0.6059 (0.6003) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,472 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4671 (0.3965) Acc D Real: 96.168% 
Loss D Fake: 0.7955 (0.8028) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6060 (0.6005) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,479 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4646 (0.3983) Acc D Real: 96.139% 
Loss D Fake: 0.7958 (0.8026) Acc D Fake: 6.667% 
Loss D: 1.260 
Loss G: 0.6056 (0.6006) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,486 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4152 (0.3987) Acc D Real: 96.142% 
Loss D Fake: 0.7964 (0.8025) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.6051 (0.6007) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,494 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4799 (0.4007) Acc D Real: 96.146% 
Loss D Fake: 0.7973 (0.8023) Acc D Fake: 6.667% 
Loss D: 1.277 
Loss G: 0.6040 (0.6008) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:46:10,501 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4264 (0.4013) Acc D Real: 96.127% 
Loss D Fake: 0.7993 (0.8023) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6019 (0.6008) Acc G: 93.373% 
LR: 2.000e-04 

2023-03-02 01:46:10,509 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.4796 (0.4031) Acc D Real: 96.093% 
Loss D Fake: 0.8025 (0.8023) Acc D Fake: 6.628% 
Loss D: 1.282 
Loss G: 0.5991 (0.6008) Acc G: 93.411% 
LR: 2.000e-04 

2023-03-02 01:46:10,517 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4612 (0.4044) Acc D Real: 96.085% 
Loss D Fake: 0.8064 (0.8024) Acc D Fake: 6.591% 
Loss D: 1.268 
Loss G: 0.5962 (0.6007) Acc G: 93.447% 
LR: 2.000e-04 

2023-03-02 01:46:10,525 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3463 (0.4031) Acc D Real: 96.123% 
Loss D Fake: 0.8096 (0.8025) Acc D Fake: 6.556% 
Loss D: 1.156 
Loss G: 0.5944 (0.6005) Acc G: 93.481% 
LR: 2.000e-04 

2023-03-02 01:46:10,532 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.4091 (0.4033) Acc D Real: 96.139% 
Loss D Fake: 0.8110 (0.8027) Acc D Fake: 6.522% 
Loss D: 1.220 
Loss G: 0.5939 (0.6004) Acc G: 93.514% 
LR: 2.000e-04 

2023-03-02 01:46:10,540 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3522 (0.4022) Acc D Real: 96.165% 
Loss D Fake: 0.8107 (0.8029) Acc D Fake: 6.489% 
Loss D: 1.163 
Loss G: 0.5946 (0.6003) Acc G: 93.546% 
LR: 2.000e-04 

2023-03-02 01:46:10,548 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3804 (0.4017) Acc D Real: 96.184% 
Loss D Fake: 0.8115 (0.8031) Acc D Fake: 6.458% 
Loss D: 1.192 
Loss G: 0.5906 (0.6001) Acc G: 93.576% 
LR: 2.000e-04 

2023-03-02 01:46:10,555 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3789 (0.4013) Acc D Real: 96.200% 
Loss D Fake: 0.8207 (0.8034) Acc D Fake: 6.429% 
Loss D: 1.200 
Loss G: 0.5851 (0.5998) Acc G: 93.605% 
LR: 2.000e-04 

2023-03-02 01:46:10,564 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4303 (0.4018) Acc D Real: 96.206% 
Loss D Fake: 0.8290 (0.8039) Acc D Fake: 6.400% 
Loss D: 1.259 
Loss G: 0.5826 (0.5994) Acc G: 93.633% 
LR: 2.000e-04 

2023-03-02 01:46:10,572 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4173 (0.4021) Acc D Real: 96.206% 
Loss D Fake: 0.8254 (0.8044) Acc D Fake: 6.373% 
Loss D: 1.243 
Loss G: 0.5879 (0.5992) Acc G: 93.660% 
LR: 2.000e-04 

2023-03-02 01:46:10,579 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3673 (0.4015) Acc D Real: 96.228% 
Loss D Fake: 0.8157 (0.8046) Acc D Fake: 6.346% 
Loss D: 1.183 
Loss G: 0.5931 (0.5991) Acc G: 93.686% 
LR: 2.000e-04 

2023-03-02 01:46:10,586 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3749 (0.4010) Acc D Real: 96.236% 
Loss D Fake: 0.8094 (0.8047) Acc D Fake: 6.321% 
Loss D: 1.184 
Loss G: 0.5967 (0.5990) Acc G: 93.711% 
LR: 2.000e-04 

2023-03-02 01:46:10,594 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3856 (0.4007) Acc D Real: 96.256% 
Loss D Fake: 0.8055 (0.8047) Acc D Fake: 6.296% 
Loss D: 1.191 
Loss G: 0.5987 (0.5990) Acc G: 93.735% 
LR: 2.000e-04 

2023-03-02 01:46:10,601 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4321 (0.4013) Acc D Real: 96.264% 
Loss D Fake: 0.8036 (0.8047) Acc D Fake: 6.273% 
Loss D: 1.236 
Loss G: 0.5996 (0.5990) Acc G: 93.758% 
LR: 2.000e-04 

2023-03-02 01:46:10,608 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.4147 (0.4015) Acc D Real: 96.270% 
Loss D Fake: 0.8026 (0.8046) Acc D Fake: 6.250% 
Loss D: 1.217 
Loss G: 0.6002 (0.5991) Acc G: 93.780% 
LR: 2.000e-04 

2023-03-02 01:46:10,615 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3528 (0.4006) Acc D Real: 96.282% 
Loss D Fake: 0.8017 (0.8046) Acc D Fake: 6.228% 
Loss D: 1.155 
Loss G: 0.6010 (0.5991) Acc G: 93.801% 
LR: 2.000e-04 

2023-03-02 01:46:10,622 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3896 (0.4005) Acc D Real: 96.298% 
Loss D Fake: 0.8007 (0.8045) Acc D Fake: 6.207% 
Loss D: 1.190 
Loss G: 0.6016 (0.5991) Acc G: 93.822% 
LR: 2.000e-04 

2023-03-02 01:46:10,630 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4088 (0.4006) Acc D Real: 96.309% 
Loss D Fake: 0.8000 (0.8044) Acc D Fake: 6.186% 
Loss D: 1.209 
Loss G: 0.6021 (0.5992) Acc G: 93.842% 
LR: 2.000e-04 

2023-03-02 01:46:10,637 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.4076 (0.4007) Acc D Real: 96.317% 
Loss D Fake: 0.7994 (0.8044) Acc D Fake: 6.167% 
Loss D: 1.207 
Loss G: 0.6025 (0.5992) Acc G: 93.861% 
LR: 2.000e-04 

2023-03-02 01:46:10,645 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4022 (0.4007) Acc D Real: 96.323% 
Loss D Fake: 0.7997 (0.8043) Acc D Fake: 6.148% 
Loss D: 1.202 
Loss G: 0.6011 (0.5993) Acc G: 93.880% 
LR: 2.000e-04 

2023-03-02 01:46:10,653 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3738 (0.4003) Acc D Real: 96.343% 
Loss D Fake: 0.8027 (0.8043) Acc D Fake: 6.129% 
Loss D: 1.177 
Loss G: 0.5991 (0.5993) Acc G: 93.898% 
LR: 2.000e-04 

2023-03-02 01:46:10,661 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3956 (0.4002) Acc D Real: 96.351% 
Loss D Fake: 0.8056 (0.8043) Acc D Fake: 6.111% 
Loss D: 1.201 
Loss G: 0.5970 (0.5992) Acc G: 93.915% 
LR: 2.000e-04 

2023-03-02 01:46:10,668 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3968 (0.4002) Acc D Real: 96.365% 
Loss D Fake: 0.8088 (0.8043) Acc D Fake: 6.094% 
Loss D: 1.206 
Loss G: 0.5951 (0.5992) Acc G: 93.932% 
LR: 2.000e-04 

2023-03-02 01:46:10,677 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.4053 (0.4003) Acc D Real: 96.373% 
Loss D Fake: 0.8111 (0.8044) Acc D Fake: 6.077% 
Loss D: 1.216 
Loss G: 0.5940 (0.5991) Acc G: 93.949% 
LR: 2.000e-04 

2023-03-02 01:46:10,685 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3598 (0.3996) Acc D Real: 96.384% 
Loss D Fake: 0.8121 (0.8046) Acc D Fake: 6.061% 
Loss D: 1.172 
Loss G: 0.5939 (0.5990) Acc G: 93.965% 
LR: 2.000e-04 

2023-03-02 01:46:10,692 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3239 (0.3985) Acc D Real: 96.397% 
Loss D Fake: 0.8110 (0.8047) Acc D Fake: 6.045% 
Loss D: 1.135 
Loss G: 0.5961 (0.5990) Acc G: 93.980% 
LR: 2.000e-04 

2023-03-02 01:46:10,700 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.3534 (0.3979) Acc D Real: 96.410% 
Loss D Fake: 0.8068 (0.8047) Acc D Fake: 6.029% 
Loss D: 1.160 
Loss G: 0.5993 (0.5990) Acc G: 93.995% 
LR: 2.000e-04 

2023-03-02 01:46:10,707 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4272 (0.3983) Acc D Real: 96.405% 
Loss D Fake: 0.8025 (0.8047) Acc D Fake: 6.014% 
Loss D: 1.230 
Loss G: 0.6024 (0.5990) Acc G: 94.010% 
LR: 2.000e-04 

2023-03-02 01:46:10,715 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3553 (0.3977) Acc D Real: 96.407% 
Loss D Fake: 0.7985 (0.8046) Acc D Fake: 6.000% 
Loss D: 1.154 
Loss G: 0.6054 (0.5991) Acc G: 94.024% 
LR: 2.000e-04 

2023-03-02 01:46:10,723 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4083 (0.3978) Acc D Real: 96.402% 
Loss D Fake: 0.7947 (0.8044) Acc D Fake: 5.986% 
Loss D: 1.203 
Loss G: 0.6080 (0.5992) Acc G: 94.038% 
LR: 2.000e-04 

2023-03-02 01:46:10,730 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.4752 (0.3989) Acc D Real: 96.385% 
Loss D Fake: 0.7918 (0.8043) Acc D Fake: 5.972% 
Loss D: 1.267 
Loss G: 0.6096 (0.5994) Acc G: 94.051% 
LR: 2.000e-04 

2023-03-02 01:46:10,738 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4946 (0.4002) Acc D Real: 96.374% 
Loss D Fake: 0.7903 (0.8041) Acc D Fake: 5.959% 
Loss D: 1.285 
Loss G: 0.6101 (0.5995) Acc G: 94.064% 
LR: 2.000e-04 

2023-03-02 01:46:10,745 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4009 (0.4002) Acc D Real: 96.382% 
Loss D Fake: 0.7899 (0.8039) Acc D Fake: 5.946% 
Loss D: 1.191 
Loss G: 0.6102 (0.5997) Acc G: 94.077% 
LR: 2.000e-04 

2023-03-02 01:46:10,753 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3956 (0.4001) Acc D Real: 96.385% 
Loss D Fake: 0.7896 (0.8037) Acc D Fake: 5.933% 
Loss D: 1.185 
Loss G: 0.6102 (0.5998) Acc G: 94.089% 
LR: 2.000e-04 

2023-03-02 01:46:10,760 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4390 (0.4007) Acc D Real: 96.388% 
Loss D Fake: 0.7897 (0.8035) Acc D Fake: 5.921% 
Loss D: 1.229 
Loss G: 0.6098 (0.5999) Acc G: 94.101% 
LR: 2.000e-04 

2023-03-02 01:46:10,767 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3978 (0.4006) Acc D Real: 96.394% 
Loss D Fake: 0.7904 (0.8033) Acc D Fake: 5.909% 
Loss D: 1.188 
Loss G: 0.6093 (0.6001) Acc G: 94.113% 
LR: 2.000e-04 

2023-03-02 01:46:10,775 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4682 (0.4015) Acc D Real: 96.391% 
Loss D Fake: 0.7912 (0.8032) Acc D Fake: 5.897% 
Loss D: 1.259 
Loss G: 0.6081 (0.6002) Acc G: 94.124% 
LR: 2.000e-04 

2023-03-02 01:46:10,782 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3178 (0.4004) Acc D Real: 96.410% 
Loss D Fake: 0.7931 (0.8030) Acc D Fake: 5.886% 
Loss D: 1.111 
Loss G: 0.6065 (0.6002) Acc G: 94.135% 
LR: 2.000e-04 

2023-03-02 01:46:10,789 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3976 (0.4004) Acc D Real: 96.424% 
Loss D Fake: 0.7966 (0.8030) Acc D Fake: 5.875% 
Loss D: 1.194 
Loss G: 0.6018 (0.6003) Acc G: 94.146% 
LR: 2.000e-04 

2023-03-02 01:46:10,796 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3982 (0.4004) Acc D Real: 96.431% 
Loss D Fake: 0.8063 (0.8030) Acc D Fake: 5.864% 
Loss D: 1.205 
Loss G: 0.5927 (0.6002) Acc G: 94.156% 
LR: 2.000e-04 

2023-03-02 01:46:10,804 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3891 (0.4002) Acc D Real: 96.442% 
Loss D Fake: 0.8277 (0.8033) Acc D Fake: 5.854% 
Loss D: 1.217 
Loss G: 0.5827 (0.6000) Acc G: 94.167% 
LR: 2.000e-04 

2023-03-02 01:46:10,811 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3917 (0.4001) Acc D Real: 96.448% 
Loss D Fake: 0.8279 (0.8036) Acc D Fake: 5.843% 
Loss D: 1.220 
Loss G: 0.5922 (0.5999) Acc G: 94.177% 
LR: 2.000e-04 

2023-03-02 01:46:10,819 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3874 (0.4000) Acc D Real: 96.458% 
Loss D Fake: 0.8081 (0.8037) Acc D Fake: 5.833% 
Loss D: 1.196 
Loss G: 0.5998 (0.5999) Acc G: 94.187% 
LR: 2.000e-04 

2023-03-02 01:46:10,826 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3767 (0.3997) Acc D Real: 96.464% 
Loss D Fake: 0.8005 (0.8036) Acc D Fake: 5.824% 
Loss D: 1.177 
Loss G: 0.6039 (0.5999) Acc G: 94.196% 
LR: 2.000e-04 

2023-03-02 01:46:10,833 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4300 (0.4001) Acc D Real: 96.469% 
Loss D Fake: 0.7962 (0.8035) Acc D Fake: 5.814% 
Loss D: 1.226 
Loss G: 0.6062 (0.6000) Acc G: 94.205% 
LR: 2.000e-04 

2023-03-02 01:46:10,841 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3641 (0.3996) Acc D Real: 96.478% 
Loss D Fake: 0.7936 (0.8034) Acc D Fake: 5.805% 
Loss D: 1.158 
Loss G: 0.6080 (0.6001) Acc G: 94.215% 
LR: 2.000e-04 

2023-03-02 01:46:10,848 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4338 (0.4000) Acc D Real: 96.483% 
Loss D Fake: 0.7915 (0.8033) Acc D Fake: 5.795% 
Loss D: 1.225 
Loss G: 0.6091 (0.6002) Acc G: 94.223% 
LR: 2.000e-04 

2023-03-02 01:46:10,856 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.4242 (0.4003) Acc D Real: 96.488% 
Loss D Fake: 0.7922 (0.8032) Acc D Fake: 5.787% 
Loss D: 1.216 
Loss G: 0.6055 (0.6002) Acc G: 94.232% 
LR: 2.000e-04 

2023-03-02 01:46:10,863 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3617 (0.3999) Acc D Real: 96.491% 
Loss D Fake: 0.8004 (0.8031) Acc D Fake: 5.778% 
Loss D: 1.162 
Loss G: 0.5979 (0.6002) Acc G: 94.241% 
LR: 2.000e-04 

2023-03-02 01:46:10,871 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3970 (0.3998) Acc D Real: 96.496% 
Loss D Fake: 0.8173 (0.8033) Acc D Fake: 5.769% 
Loss D: 1.214 
Loss G: 0.5809 (0.6000) Acc G: 94.249% 
LR: 2.000e-04 

2023-03-02 01:46:10,878 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3227 (0.3990) Acc D Real: 96.508% 
Loss D Fake: 1.1348 (0.8069) Acc D Fake: 5.761% 
Loss D: 1.458 
Loss G: 0.6009 (0.6000) Acc G: 94.257% 
LR: 2.000e-04 

2023-03-02 01:46:10,885 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3681 (0.3987) Acc D Real: 96.519% 
Loss D Fake: 0.7929 (0.8067) Acc D Fake: 5.753% 
Loss D: 1.161 
Loss G: 0.6118 (0.6001) Acc G: 94.265% 
LR: 2.000e-04 

2023-03-02 01:46:10,893 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3983 (0.3987) Acc D Real: 96.528% 
Loss D Fake: 0.7842 (0.8065) Acc D Fake: 5.745% 
Loss D: 1.183 
Loss G: 0.6163 (0.6003) Acc G: 94.273% 
LR: 2.000e-04 

2023-03-02 01:46:10,900 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3891 (0.3986) Acc D Real: 96.532% 
Loss D Fake: 0.7797 (0.8062) Acc D Fake: 5.737% 
Loss D: 1.169 
Loss G: 0.6190 (0.6005) Acc G: 94.281% 
LR: 2.000e-04 

2023-03-02 01:46:10,908 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4928 (0.3995) Acc D Real: 96.536% 
Loss D Fake: 0.7771 (0.8059) Acc D Fake: 5.729% 
Loss D: 1.270 
Loss G: 0.6202 (0.6007) Acc G: 94.288% 
LR: 2.000e-04 

2023-03-02 01:46:10,915 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4165 (0.3997) Acc D Real: 96.537% 
Loss D Fake: 0.7760 (0.8056) Acc D Fake: 5.722% 
Loss D: 1.193 
Loss G: 0.6207 (0.6009) Acc G: 94.296% 
LR: 2.000e-04 

2023-03-02 01:46:10,922 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.5308 (0.4011) Acc D Real: 96.525% 
Loss D Fake: 0.7758 (0.8053) Acc D Fake: 5.714% 
Loss D: 1.307 
Loss G: 0.6201 (0.6011) Acc G: 94.303% 
LR: 2.000e-04 

2023-03-02 01:46:10,929 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.4480 (0.4015) Acc D Real: 96.528% 
Loss D Fake: 0.7770 (0.8050) Acc D Fake: 5.707% 
Loss D: 1.225 
Loss G: 0.6188 (0.6013) Acc G: 94.310% 
LR: 2.000e-04 

2023-03-02 01:46:10,936 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.4232 (0.4017) Acc D Real: 96.531% 
Loss D Fake: 0.7786 (0.8047) Acc D Fake: 5.700% 
Loss D: 1.202 
Loss G: 0.6175 (0.6015) Acc G: 94.317% 
LR: 2.000e-04 

2023-03-02 01:46:10,945 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3877 (0.4016) Acc D Real: 96.533% 
Loss D Fake: 0.7799 (0.8045) Acc D Fake: 5.693% 
Loss D: 1.168 
Loss G: 0.6165 (0.6016) Acc G: 94.323% 
LR: 2.000e-04 

2023-03-02 01:46:10,952 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3789 (0.4014) Acc D Real: 96.546% 
Loss D Fake: 0.7808 (0.8043) Acc D Fake: 5.686% 
Loss D: 1.160 
Loss G: 0.6158 (0.6017) Acc G: 94.330% 
LR: 2.000e-04 

2023-03-02 01:46:10,960 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4346 (0.4017) Acc D Real: 96.549% 
Loss D Fake: 0.7815 (0.8040) Acc D Fake: 5.680% 
Loss D: 1.216 
Loss G: 0.6151 (0.6019) Acc G: 94.337% 
LR: 2.000e-04 

2023-03-02 01:46:10,967 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4599 (0.4023) Acc D Real: 96.548% 
Loss D Fake: 0.7825 (0.8038) Acc D Fake: 5.673% 
Loss D: 1.242 
Loss G: 0.6140 (0.6020) Acc G: 94.343% 
LR: 2.000e-04 

2023-03-02 01:46:10,975 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3778 (0.4020) Acc D Real: 96.554% 
Loss D Fake: 0.7837 (0.8037) Acc D Fake: 5.667% 
Loss D: 1.162 
Loss G: 0.6132 (0.6021) Acc G: 94.349% 
LR: 2.000e-04 

2023-03-02 01:46:10,982 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4815 (0.4028) Acc D Real: 96.558% 
Loss D Fake: 0.7848 (0.8035) Acc D Fake: 5.660% 
Loss D: 1.266 
Loss G: 0.6119 (0.6022) Acc G: 94.355% 
LR: 2.000e-04 

2023-03-02 01:46:10,989 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4402 (0.4031) Acc D Real: 96.565% 
Loss D Fake: 0.7867 (0.8033) Acc D Fake: 5.654% 
Loss D: 1.227 
Loss G: 0.6102 (0.6023) Acc G: 94.361% 
LR: 2.000e-04 

2023-03-02 01:46:10,998 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4090 (0.4032) Acc D Real: 96.570% 
Loss D Fake: 0.7886 (0.8032) Acc D Fake: 5.648% 
Loss D: 1.198 
Loss G: 0.6086 (0.6023) Acc G: 94.367% 
LR: 2.000e-04 

2023-03-02 01:46:11,005 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.4431 (0.4036) Acc D Real: 96.573% 
Loss D Fake: 0.7905 (0.8031) Acc D Fake: 5.642% 
Loss D: 1.234 
Loss G: 0.6070 (0.6024) Acc G: 94.373% 
LR: 2.000e-04 

2023-03-02 01:46:11,013 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4413 (0.4039) Acc D Real: 96.581% 
Loss D Fake: 0.7926 (0.8030) Acc D Fake: 5.636% 
Loss D: 1.234 
Loss G: 0.6052 (0.6024) Acc G: 94.379% 
LR: 2.000e-04 

2023-03-02 01:46:11,020 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.4077 (0.4039) Acc D Real: 96.585% 
Loss D Fake: 0.7947 (0.8029) Acc D Fake: 5.631% 
Loss D: 1.202 
Loss G: 0.6036 (0.6024) Acc G: 94.384% 
LR: 2.000e-04 

2023-03-02 01:46:11,027 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4955 (0.4048) Acc D Real: 96.589% 
Loss D Fake: 0.7968 (0.8028) Acc D Fake: 5.625% 
Loss D: 1.292 
Loss G: 0.6015 (0.6024) Acc G: 94.390% 
LR: 2.000e-04 

2023-03-02 01:46:11,035 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4225 (0.4049) Acc D Real: 96.596% 
Loss D Fake: 0.7994 (0.8028) Acc D Fake: 5.619% 
Loss D: 1.222 
Loss G: 0.5995 (0.6024) Acc G: 94.395% 
LR: 2.000e-04 

2023-03-02 01:46:11,042 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3869 (0.4048) Acc D Real: 96.604% 
Loss D Fake: 0.8017 (0.8028) Acc D Fake: 5.614% 
Loss D: 1.189 
Loss G: 0.5979 (0.6023) Acc G: 94.401% 
LR: 2.000e-04 

2023-03-02 01:46:11,049 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4518 (0.4052) Acc D Real: 96.608% 
Loss D Fake: 0.8036 (0.8028) Acc D Fake: 5.609% 
Loss D: 1.255 
Loss G: 0.5962 (0.6023) Acc G: 94.406% 
LR: 2.000e-04 

2023-03-02 01:46:11,057 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4385 (0.4054) Acc D Real: 96.614% 
Loss D Fake: 0.8058 (0.8028) Acc D Fake: 5.603% 
Loss D: 1.244 
Loss G: 0.5945 (0.6022) Acc G: 94.411% 
LR: 2.000e-04 

2023-03-02 01:46:11,064 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.4309 (0.4057) Acc D Real: 96.619% 
Loss D Fake: 0.8079 (0.8029) Acc D Fake: 5.598% 
Loss D: 1.239 
Loss G: 0.5927 (0.6021) Acc G: 94.416% 
LR: 2.000e-04 

2023-03-02 01:46:11,071 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3985 (0.4056) Acc D Real: 96.627% 
Loss D Fake: 0.8100 (0.8029) Acc D Fake: 5.593% 
Loss D: 1.209 
Loss G: 0.5912 (0.6020) Acc G: 94.421% 
LR: 2.000e-04 

2023-03-02 01:46:11,079 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3697 (0.4053) Acc D Real: 96.632% 
Loss D Fake: 0.8116 (0.8030) Acc D Fake: 5.588% 
Loss D: 1.181 
Loss G: 0.5902 (0.6019) Acc G: 94.426% 
LR: 2.000e-04 

2023-03-02 01:46:11,086 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3856 (0.4051) Acc D Real: 96.635% 
Loss D Fake: 0.8126 (0.8031) Acc D Fake: 5.583% 
Loss D: 1.198 
Loss G: 0.5895 (0.6018) Acc G: 94.431% 
LR: 2.000e-04 

2023-03-02 01:46:11,093 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2661 (0.4040) Acc D Real: 96.652% 
Loss D Fake: 0.8129 (0.8032) Acc D Fake: 5.579% 
Loss D: 1.079 
Loss G: 0.5900 (0.6017) Acc G: 94.435% 
LR: 2.000e-04 

2023-03-02 01:46:11,100 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4511 (0.4044) Acc D Real: 96.657% 
Loss D Fake: 0.8121 (0.8032) Acc D Fake: 5.574% 
Loss D: 1.263 
Loss G: 0.5903 (0.6016) Acc G: 94.440% 
LR: 2.000e-04 

2023-03-02 01:46:11,108 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3772 (0.4042) Acc D Real: 96.663% 
Loss D Fake: 0.8118 (0.8033) Acc D Fake: 5.569% 
Loss D: 1.189 
Loss G: 0.5906 (0.6015) Acc G: 94.444% 
LR: 2.000e-04 

2023-03-02 01:46:11,115 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3974 (0.4041) Acc D Real: 96.667% 
Loss D Fake: 0.8115 (0.8034) Acc D Fake: 5.565% 
Loss D: 1.209 
Loss G: 0.5909 (0.6015) Acc G: 94.449% 
LR: 2.000e-04 

2023-03-02 01:46:11,122 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3500 (0.4037) Acc D Real: 96.674% 
Loss D Fake: 0.8110 (0.8034) Acc D Fake: 5.560% 
Loss D: 1.161 
Loss G: 0.5915 (0.6014) Acc G: 94.453% 
LR: 2.000e-04 

2023-03-02 01:46:11,129 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4270 (0.4039) Acc D Real: 96.680% 
Loss D Fake: 0.8103 (0.8035) Acc D Fake: 5.556% 
Loss D: 1.237 
Loss G: 0.5918 (0.6013) Acc G: 94.458% 
LR: 2.000e-04 

2023-03-02 01:46:11,137 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3646 (0.4035) Acc D Real: 96.686% 
Loss D Fake: 0.8100 (0.8035) Acc D Fake: 5.551% 
Loss D: 1.175 
Loss G: 0.5922 (0.6012) Acc G: 94.462% 
LR: 2.000e-04 

2023-03-02 01:46:11,144 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3315 (0.4030) Acc D Real: 96.698% 
Loss D Fake: 0.8093 (0.8036) Acc D Fake: 5.547% 
Loss D: 1.141 
Loss G: 0.5931 (0.6012) Acc G: 94.466% 
LR: 2.000e-04 

2023-03-02 01:46:11,151 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.4749 (0.4035) Acc D Real: 96.704% 
Loss D Fake: 0.8084 (0.8036) Acc D Fake: 5.543% 
Loss D: 1.283 
Loss G: 0.5933 (0.6011) Acc G: 94.470% 
LR: 2.000e-04 

2023-03-02 01:46:11,159 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3810 (0.4034) Acc D Real: 96.713% 
Loss D Fake: 0.8084 (0.8037) Acc D Fake: 5.538% 
Loss D: 1.189 
Loss G: 0.5934 (0.6011) Acc G: 94.474% 
LR: 2.000e-04 

2023-03-02 01:46:11,167 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.4149 (0.4035) Acc D Real: 96.716% 
Loss D Fake: 0.8083 (0.8037) Acc D Fake: 5.534% 
Loss D: 1.223 
Loss G: 0.5934 (0.6010) Acc G: 94.478% 
LR: 2.000e-04 

2023-03-02 01:46:11,174 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.5021 (0.4042) Acc D Real: 96.722% 
Loss D Fake: 0.8087 (0.8037) Acc D Fake: 5.530% 
Loss D: 1.311 
Loss G: 0.5926 (0.6009) Acc G: 94.482% 
LR: 2.000e-04 

2023-03-02 01:46:11,182 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.4610 (0.4046) Acc D Real: 96.728% 
Loss D Fake: 0.8101 (0.8038) Acc D Fake: 5.526% 
Loss D: 1.271 
Loss G: 0.5912 (0.6009) Acc G: 94.486% 
LR: 2.000e-04 

2023-03-02 01:46:11,190 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3787 (0.4044) Acc D Real: 96.734% 
Loss D Fake: 0.8118 (0.8038) Acc D Fake: 5.522% 
Loss D: 1.191 
Loss G: 0.5901 (0.6008) Acc G: 94.490% 
LR: 2.000e-04 

2023-03-02 01:46:11,198 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4463 (0.4047) Acc D Real: 96.737% 
Loss D Fake: 0.8132 (0.8039) Acc D Fake: 5.519% 
Loss D: 1.259 
Loss G: 0.5889 (0.6007) Acc G: 94.494% 
LR: 2.000e-04 

2023-03-02 01:46:11,205 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.2976 (0.4040) Acc D Real: 96.748% 
Loss D Fake: 0.8144 (0.8040) Acc D Fake: 5.515% 
Loss D: 1.112 
Loss G: 0.5885 (0.6006) Acc G: 94.498% 
LR: 2.000e-04 

2023-03-02 01:46:11,213 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3527 (0.4036) Acc D Real: 96.757% 
Loss D Fake: 0.8143 (0.8041) Acc D Fake: 5.511% 
Loss D: 1.167 
Loss G: 0.5888 (0.6005) Acc G: 94.501% 
LR: 2.000e-04 

2023-03-02 01:46:11,220 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3498 (0.4032) Acc D Real: 96.767% 
Loss D Fake: 0.8138 (0.8041) Acc D Fake: 5.507% 
Loss D: 1.164 
Loss G: 0.5895 (0.6004) Acc G: 94.505% 
LR: 2.000e-04 

2023-03-02 01:46:11,228 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3499 (0.4028) Acc D Real: 96.774% 
Loss D Fake: 0.8127 (0.8042) Acc D Fake: 5.504% 
Loss D: 1.163 
Loss G: 0.5905 (0.6004) Acc G: 94.508% 
LR: 2.000e-04 

2023-03-02 01:46:11,235 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3652 (0.4025) Acc D Real: 96.783% 
Loss D Fake: 0.8113 (0.8043) Acc D Fake: 5.500% 
Loss D: 1.177 
Loss G: 0.5916 (0.6003) Acc G: 94.512% 
LR: 2.000e-04 

2023-03-02 01:46:11,243 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4010 (0.4025) Acc D Real: 96.787% 
Loss D Fake: 0.8099 (0.8043) Acc D Fake: 5.496% 
Loss D: 1.211 
Loss G: 0.5927 (0.6002) Acc G: 94.515% 
LR: 2.000e-04 

2023-03-02 01:46:11,251 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3844 (0.4024) Acc D Real: 96.792% 
Loss D Fake: 0.8087 (0.8043) Acc D Fake: 5.493% 
Loss D: 1.193 
Loss G: 0.5936 (0.6002) Acc G: 94.519% 
LR: 2.000e-04 

2023-03-02 01:46:11,258 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3582 (0.4021) Acc D Real: 96.801% 
Loss D Fake: 0.8075 (0.8043) Acc D Fake: 5.490% 
Loss D: 1.166 
Loss G: 0.5945 (0.6002) Acc G: 94.522% 
LR: 2.000e-04 

2023-03-02 01:46:11,265 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.4058 (0.4021) Acc D Real: 96.806% 
Loss D Fake: 0.8066 (0.8044) Acc D Fake: 5.486% 
Loss D: 1.212 
Loss G: 0.5952 (0.6001) Acc G: 94.525% 
LR: 2.000e-04 

2023-03-02 01:46:11,273 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4619 (0.4025) Acc D Real: 96.809% 
Loss D Fake: 0.8061 (0.8044) Acc D Fake: 5.483% 
Loss D: 1.268 
Loss G: 0.5953 (0.6001) Acc G: 94.529% 
LR: 2.000e-04 

2023-03-02 01:46:11,281 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.4182 (0.4026) Acc D Real: 96.814% 
Loss D Fake: 0.8063 (0.8044) Acc D Fake: 5.479% 
Loss D: 1.225 
Loss G: 0.5950 (0.6001) Acc G: 94.532% 
LR: 2.000e-04 

2023-03-02 01:46:11,288 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.4007 (0.4026) Acc D Real: 96.817% 
Loss D Fake: 0.8068 (0.8044) Acc D Fake: 5.476% 
Loss D: 1.207 
Loss G: 0.5947 (0.6000) Acc G: 94.535% 
LR: 2.000e-04 

2023-03-02 01:46:11,295 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3615 (0.4023) Acc D Real: 96.823% 
Loss D Fake: 0.8071 (0.8044) Acc D Fake: 5.473% 
Loss D: 1.169 
Loss G: 0.5946 (0.6000) Acc G: 94.538% 
LR: 2.000e-04 

2023-03-02 01:46:11,303 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.4830 (0.4029) Acc D Real: 96.825% 
Loss D Fake: 0.8075 (0.8044) Acc D Fake: 5.470% 
Loss D: 1.291 
Loss G: 0.5939 (0.5999) Acc G: 94.541% 
LR: 2.000e-04 

2023-03-02 01:46:11,310 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4008 (0.4029) Acc D Real: 96.831% 
Loss D Fake: 0.8085 (0.8045) Acc D Fake: 5.467% 
Loss D: 1.209 
Loss G: 0.5932 (0.5999) Acc G: 94.544% 
LR: 2.000e-04 

2023-03-02 01:46:11,317 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3772 (0.4027) Acc D Real: 96.838% 
Loss D Fake: 0.8093 (0.8045) Acc D Fake: 5.464% 
Loss D: 1.187 
Loss G: 0.5927 (0.5998) Acc G: 94.547% 
LR: 2.000e-04 

2023-03-02 01:46:11,325 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3615 (0.4024) Acc D Real: 96.846% 
Loss D Fake: 0.8098 (0.8045) Acc D Fake: 5.461% 
Loss D: 1.171 
Loss G: 0.5926 (0.5998) Acc G: 94.550% 
LR: 2.000e-04 

2023-03-02 01:46:11,332 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3596 (0.4022) Acc D Real: 96.852% 
Loss D Fake: 0.8097 (0.8046) Acc D Fake: 5.458% 
Loss D: 1.169 
Loss G: 0.5929 (0.5998) Acc G: 94.553% 
LR: 2.000e-04 

2023-03-02 01:46:11,339 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3908 (0.4021) Acc D Real: 96.856% 
Loss D Fake: 0.8093 (0.8046) Acc D Fake: 5.455% 
Loss D: 1.200 
Loss G: 0.5932 (0.5997) Acc G: 94.556% 
LR: 2.000e-04 

2023-03-02 01:46:11,346 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3279 (0.4016) Acc D Real: 96.862% 
Loss D Fake: 0.8086 (0.8046) Acc D Fake: 5.452% 
Loss D: 1.137 
Loss G: 0.5941 (0.5997) Acc G: 94.559% 
LR: 2.000e-04 

2023-03-02 01:46:11,353 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4400 (0.4018) Acc D Real: 96.865% 
Loss D Fake: 0.8076 (0.8046) Acc D Fake: 5.449% 
Loss D: 1.248 
Loss G: 0.5946 (0.5996) Acc G: 94.562% 
LR: 2.000e-04 

2023-03-02 01:46:11,361 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4470 (0.4021) Acc D Real: 96.866% 
Loss D Fake: 0.8073 (0.8047) Acc D Fake: 5.446% 
Loss D: 1.254 
Loss G: 0.5946 (0.5996) Acc G: 94.565% 
LR: 2.000e-04 

2023-03-02 01:46:11,368 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3595 (0.4019) Acc D Real: 96.874% 
Loss D Fake: 0.8073 (0.8047) Acc D Fake: 5.443% 
Loss D: 1.167 
Loss G: 0.5948 (0.5996) Acc G: 94.568% 
LR: 2.000e-04 

2023-03-02 01:46:11,375 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3807 (0.4017) Acc D Real: 96.880% 
Loss D Fake: 0.8069 (0.8047) Acc D Fake: 5.440% 
Loss D: 1.188 
Loss G: 0.5952 (0.5996) Acc G: 94.570% 
LR: 2.000e-04 

2023-03-02 01:46:11,383 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4047 (0.4018) Acc D Real: 96.882% 
Loss D Fake: 0.8064 (0.8047) Acc D Fake: 5.438% 
Loss D: 1.211 
Loss G: 0.5956 (0.5995) Acc G: 94.573% 
LR: 2.000e-04 

2023-03-02 01:46:11,390 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.4260 (0.4019) Acc D Real: 96.887% 
Loss D Fake: 0.8062 (0.8047) Acc D Fake: 5.435% 
Loss D: 1.232 
Loss G: 0.5956 (0.5995) Acc G: 94.576% 
LR: 2.000e-04 

2023-03-02 01:46:11,397 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3330 (0.4015) Acc D Real: 96.898% 
Loss D Fake: 0.8060 (0.8047) Acc D Fake: 5.432% 
Loss D: 1.139 
Loss G: 0.5961 (0.5995) Acc G: 94.578% 
LR: 2.000e-04 

2023-03-02 01:46:11,404 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3544 (0.4012) Acc D Real: 96.905% 
Loss D Fake: 0.8053 (0.8047) Acc D Fake: 5.429% 
Loss D: 1.160 
Loss G: 0.5968 (0.5995) Acc G: 94.581% 
LR: 2.000e-04 

2023-03-02 01:46:11,412 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3390 (0.4008) Acc D Real: 96.913% 
Loss D Fake: 0.8042 (0.8047) Acc D Fake: 5.427% 
Loss D: 1.143 
Loss G: 0.5978 (0.5995) Acc G: 94.583% 
LR: 2.000e-04 

2023-03-02 01:46:11,420 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2965 (0.4002) Acc D Real: 96.920% 
Loss D Fake: 0.8026 (0.8047) Acc D Fake: 5.424% 
Loss D: 1.099 
Loss G: 0.5995 (0.5995) Acc G: 94.586% 
LR: 2.000e-04 

2023-03-02 01:46:11,427 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3287 (0.3997) Acc D Real: 96.926% 
Loss D Fake: 0.8003 (0.8047) Acc D Fake: 5.422% 
Loss D: 1.129 
Loss G: 0.6015 (0.5995) Acc G: 94.588% 
LR: 2.000e-04 

2023-03-02 01:46:11,434 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4138 (0.3998) Acc D Real: 96.930% 
Loss D Fake: 0.7980 (0.8046) Acc D Fake: 5.419% 
Loss D: 1.212 
Loss G: 0.6032 (0.5995) Acc G: 94.591% 
LR: 2.000e-04 

2023-03-02 01:46:11,442 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3452 (0.3995) Acc D Real: 96.937% 
Loss D Fake: 0.7961 (0.8046) Acc D Fake: 5.417% 
Loss D: 1.141 
Loss G: 0.6048 (0.5995) Acc G: 94.593% 
LR: 2.000e-04 

2023-03-02 01:46:11,449 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3550 (0.3992) Acc D Real: 96.942% 
Loss D Fake: 0.7942 (0.8045) Acc D Fake: 5.414% 
Loss D: 1.149 
Loss G: 0.6064 (0.5996) Acc G: 94.596% 
LR: 2.000e-04 

2023-03-02 01:46:11,457 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3862 (0.3992) Acc D Real: 96.946% 
Loss D Fake: 0.7925 (0.8045) Acc D Fake: 5.412% 
Loss D: 1.179 
Loss G: 0.6077 (0.5996) Acc G: 94.598% 
LR: 2.000e-04 

2023-03-02 01:46:11,464 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.4053 (0.3992) Acc D Real: 96.948% 
Loss D Fake: 0.7910 (0.8044) Acc D Fake: 5.409% 
Loss D: 1.196 
Loss G: 0.6088 (0.5997) Acc G: 94.600% 
LR: 2.000e-04 

2023-03-02 01:46:11,472 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3220 (0.3988) Acc D Real: 96.955% 
Loss D Fake: 0.7897 (0.8043) Acc D Fake: 5.407% 
Loss D: 1.112 
Loss G: 0.6102 (0.5997) Acc G: 94.603% 
LR: 2.000e-04 

2023-03-02 01:46:11,480 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.2844 (0.3981) Acc D Real: 96.962% 
Loss D Fake: 0.7877 (0.8042) Acc D Fake: 5.405% 
Loss D: 1.072 
Loss G: 0.6122 (0.5998) Acc G: 94.605% 
LR: 2.000e-04 

2023-03-02 01:46:11,488 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3776 (0.3980) Acc D Real: 96.966% 
Loss D Fake: 0.7853 (0.8041) Acc D Fake: 5.402% 
Loss D: 1.163 
Loss G: 0.6140 (0.5999) Acc G: 94.607% 
LR: 2.000e-04 

2023-03-02 01:46:11,495 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3414 (0.3977) Acc D Real: 96.971% 
Loss D Fake: 0.7833 (0.8040) Acc D Fake: 5.400% 
Loss D: 1.125 
Loss G: 0.6158 (0.6000) Acc G: 94.610% 
LR: 2.000e-04 

2023-03-02 01:46:11,503 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3407 (0.3973) Acc D Real: 96.977% 
Loss D Fake: 0.7812 (0.8038) Acc D Fake: 5.398% 
Loss D: 1.122 
Loss G: 0.6177 (0.6001) Acc G: 94.612% 
LR: 2.000e-04 

2023-03-02 01:46:11,511 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4043 (0.3974) Acc D Real: 96.980% 
Loss D Fake: 0.7791 (0.8037) Acc D Fake: 5.395% 
Loss D: 1.183 
Loss G: 0.6193 (0.6002) Acc G: 94.614% 
LR: 2.000e-04 

2023-03-02 01:46:11,519 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.4974 (0.3979) Acc D Real: 96.981% 
Loss D Fake: 0.7779 (0.8036) Acc D Fake: 5.393% 
Loss D: 1.275 
Loss G: 0.6197 (0.6003) Acc G: 94.616% 
LR: 2.000e-04 

2023-03-02 01:46:11,528 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3606 (0.3977) Acc D Real: 96.984% 
Loss D Fake: 0.7777 (0.8034) Acc D Fake: 5.391% 
Loss D: 1.138 
Loss G: 0.6200 (0.6004) Acc G: 94.618% 
LR: 2.000e-04 

2023-03-02 01:46:11,536 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.4517 (0.3980) Acc D Real: 96.985% 
Loss D Fake: 0.7775 (0.8033) Acc D Fake: 5.389% 
Loss D: 1.229 
Loss G: 0.6199 (0.6005) Acc G: 94.620% 
LR: 2.000e-04 

2023-03-02 01:46:11,545 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3539 (0.3978) Acc D Real: 96.990% 
Loss D Fake: 0.7777 (0.8031) Acc D Fake: 5.387% 
Loss D: 1.132 
Loss G: 0.6200 (0.6006) Acc G: 94.622% 
LR: 2.000e-04 

2023-03-02 01:46:11,553 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3429 (0.3975) Acc D Real: 96.995% 
Loss D Fake: 0.7774 (0.8030) Acc D Fake: 5.385% 
Loss D: 1.120 
Loss G: 0.6204 (0.6007) Acc G: 94.625% 
LR: 2.000e-04 

2023-03-02 01:46:11,562 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3766 (0.3974) Acc D Real: 97.000% 
Loss D Fake: 0.7768 (0.8028) Acc D Fake: 5.383% 
Loss D: 1.153 
Loss G: 0.6209 (0.6008) Acc G: 94.627% 
LR: 2.000e-04 

2023-03-02 01:46:11,570 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.4154 (0.3975) Acc D Real: 97.001% 
Loss D Fake: 0.7765 (0.8027) Acc D Fake: 5.380% 
Loss D: 1.192 
Loss G: 0.6210 (0.6009) Acc G: 94.629% 
LR: 2.000e-04 

2023-03-02 01:46:11,577 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3432 (0.3972) Acc D Real: 97.003% 
Loss D Fake: 0.7764 (0.8026) Acc D Fake: 5.378% 
Loss D: 1.120 
Loss G: 0.6214 (0.6011) Acc G: 94.631% 
LR: 2.000e-04 

2023-03-02 01:46:11,585 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.4086 (0.3972) Acc D Real: 97.003% 
Loss D Fake: 0.7760 (0.8024) Acc D Fake: 5.376% 
Loss D: 1.185 
Loss G: 0.6217 (0.6012) Acc G: 94.633% 
LR: 2.000e-04 

2023-03-02 01:46:11,592 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3038 (0.3967) Acc D Real: 97.010% 
Loss D Fake: 0.7755 (0.8023) Acc D Fake: 5.374% 
Loss D: 1.079 
Loss G: 0.6225 (0.6013) Acc G: 94.635% 
LR: 2.000e-04 

2023-03-02 01:46:11,599 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3947 (0.3967) Acc D Real: 97.014% 
Loss D Fake: 0.7744 (0.8021) Acc D Fake: 5.372% 
Loss D: 1.169 
Loss G: 0.6234 (0.6014) Acc G: 94.637% 
LR: 2.000e-04 

2023-03-02 01:46:11,607 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4409 (0.3970) Acc D Real: 97.017% 
Loss D Fake: 0.7737 (0.8020) Acc D Fake: 5.370% 
Loss D: 1.215 
Loss G: 0.6237 (0.6015) Acc G: 94.638% 
LR: 2.000e-04 

2023-03-02 01:46:11,614 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3301 (0.3966) Acc D Real: 97.022% 
Loss D Fake: 0.7736 (0.8018) Acc D Fake: 5.368% 
Loss D: 1.104 
Loss G: 0.6240 (0.6016) Acc G: 94.640% 
LR: 2.000e-04 

2023-03-02 01:46:11,621 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.4412 (0.3968) Acc D Real: 97.022% 
Loss D Fake: 0.7734 (0.8017) Acc D Fake: 5.366% 
Loss D: 1.215 
Loss G: 0.6239 (0.6018) Acc G: 94.642% 
LR: 2.000e-04 

2023-03-02 01:46:11,629 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3221 (0.3964) Acc D Real: 97.027% 
Loss D Fake: 0.7738 (0.8015) Acc D Fake: 5.365% 
Loss D: 1.096 
Loss G: 0.6237 (0.6019) Acc G: 94.644% 
LR: 2.000e-04 

2023-03-02 01:46:11,637 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4290 (0.3966) Acc D Real: 97.029% 
Loss D Fake: 0.7742 (0.8014) Acc D Fake: 5.363% 
Loss D: 1.203 
Loss G: 0.6233 (0.6020) Acc G: 94.646% 
LR: 2.000e-04 

2023-03-02 01:46:11,645 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4197 (0.3967) Acc D Real: 97.030% 
Loss D Fake: 0.7750 (0.8013) Acc D Fake: 5.361% 
Loss D: 1.195 
Loss G: 0.6226 (0.6021) Acc G: 94.648% 
LR: 2.000e-04 

2023-03-02 01:46:11,652 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3254 (0.3964) Acc D Real: 97.035% 
Loss D Fake: 0.7758 (0.8011) Acc D Fake: 5.359% 
Loss D: 1.101 
Loss G: 0.6225 (0.6022) Acc G: 94.650% 
LR: 2.000e-04 

2023-03-02 01:46:11,659 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.3884 (0.3963) Acc D Real: 97.034% 
Loss D Fake: 0.7757 (0.8010) Acc D Fake: 5.357% 
Loss D: 1.164 
Loss G: 0.6227 (0.6023) Acc G: 94.651% 
LR: 2.000e-04 

2023-03-02 01:46:11,667 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4018 (0.3964) Acc D Real: 97.035% 
Loss D Fake: 0.7754 (0.8009) Acc D Fake: 5.355% 
Loss D: 1.177 
Loss G: 0.6230 (0.6024) Acc G: 94.653% 
LR: 2.000e-04 

2023-03-02 01:46:11,675 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2972 (0.3959) Acc D Real: 97.040% 
Loss D Fake: 0.7748 (0.8007) Acc D Fake: 5.354% 
Loss D: 1.072 
Loss G: 0.6240 (0.6025) Acc G: 94.655% 
LR: 2.000e-04 

2023-03-02 01:46:11,683 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3229 (0.3955) Acc D Real: 97.044% 
Loss D Fake: 0.7733 (0.8006) Acc D Fake: 5.352% 
Loss D: 1.096 
Loss G: 0.6257 (0.6026) Acc G: 94.657% 
LR: 2.000e-04 

2023-03-02 01:46:11,691 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3415 (0.3952) Acc D Real: 97.048% 
Loss D Fake: 0.7710 (0.8004) Acc D Fake: 5.350% 
Loss D: 1.113 
Loss G: 0.6277 (0.6027) Acc G: 94.658% 
LR: 2.000e-04 

2023-03-02 01:46:11,698 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2894 (0.3947) Acc D Real: 97.053% 
Loss D Fake: 0.7689 (0.8003) Acc D Fake: 5.348% 
Loss D: 1.058 
Loss G: 0.6292 (0.6029) Acc G: 94.660% 
LR: 2.000e-04 

2023-03-02 01:46:11,706 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3538 (0.3945) Acc D Real: 97.043% 
Loss D Fake: 0.7676 (0.8001) Acc D Fake: 5.347% 
Loss D: 1.121 
Loss G: 0.6305 (0.6030) Acc G: 94.662% 
LR: 2.000e-04 

2023-03-02 01:46:11,713 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3785 (0.3944) Acc D Real: 97.035% 
Loss D Fake: 0.7663 (0.8000) Acc D Fake: 5.345% 
Loss D: 1.145 
Loss G: 0.6318 (0.6032) Acc G: 94.663% 
LR: 2.000e-04 

2023-03-02 01:46:11,721 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3306 (0.3941) Acc D Real: 97.029% 
Loss D Fake: 0.7648 (0.7998) Acc D Fake: 5.343% 
Loss D: 1.095 
Loss G: 0.6335 (0.6033) Acc G: 94.665% 
LR: 2.000e-04 

2023-03-02 01:46:11,729 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3597 (0.3939) Acc D Real: 97.017% 
Loss D Fake: 0.7630 (0.7996) Acc D Fake: 5.341% 
Loss D: 1.123 
Loss G: 0.6347 (0.6035) Acc G: 94.667% 
LR: 2.000e-04 

2023-03-02 01:46:11,737 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.4701 (0.3943) Acc D Real: 96.978% 
Loss D Fake: 0.7621 (0.7994) Acc D Fake: 5.340% 
Loss D: 1.232 
Loss G: 0.6353 (0.6036) Acc G: 94.668% 
LR: 2.000e-04 

2023-03-02 01:46:11,744 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4516 (0.3946) Acc D Real: 96.961% 
Loss D Fake: 0.7617 (0.7992) Acc D Fake: 5.338% 
Loss D: 1.213 
Loss G: 0.6355 (0.6038) Acc G: 94.670% 
LR: 2.000e-04 

2023-03-02 01:46:11,752 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4524 (0.3949) Acc D Real: 96.912% 
Loss D Fake: 0.7617 (0.7991) Acc D Fake: 5.337% 
Loss D: 1.214 
Loss G: 0.6354 (0.6039) Acc G: 94.671% 
LR: 2.000e-04 

2023-03-02 01:46:11,759 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.4776 (0.3952) Acc D Real: 96.856% 
Loss D Fake: 0.7627 (0.7989) Acc D Fake: 5.335% 
Loss D: 1.240 
Loss G: 0.6332 (0.6041) Acc G: 94.673% 
LR: 2.000e-04 

2023-03-02 01:46:11,767 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3786 (0.3952) Acc D Real: 96.853% 
Loss D Fake: 0.7661 (0.7987) Acc D Fake: 5.333% 
Loss D: 1.145 
Loss G: 0.6309 (0.6042) Acc G: 94.675% 
LR: 2.000e-04 

2023-03-02 01:46:11,775 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3668 (0.3950) Acc D Real: 96.851% 
Loss D Fake: 0.7683 (0.7986) Acc D Fake: 5.332% 
Loss D: 1.135 
Loss G: 0.6298 (0.6043) Acc G: 94.676% 
LR: 2.000e-04 

2023-03-02 01:46:11,784 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3949 (0.3950) Acc D Real: 96.834% 
Loss D Fake: 0.7690 (0.7984) Acc D Fake: 5.330% 
Loss D: 1.164 
Loss G: 0.6296 (0.6044) Acc G: 94.678% 
LR: 2.000e-04 

2023-03-02 01:46:11,791 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3599 (0.3949) Acc D Real: 96.817% 
Loss D Fake: 0.7685 (0.7983) Acc D Fake: 5.329% 
Loss D: 1.128 
Loss G: 0.6304 (0.6046) Acc G: 94.679% 
LR: 2.000e-04 

2023-03-02 01:46:11,798 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4772 (0.3953) Acc D Real: 96.789% 
Loss D Fake: 0.7674 (0.7982) Acc D Fake: 5.327% 
Loss D: 1.245 
Loss G: 0.6307 (0.6047) Acc G: 94.681% 
LR: 2.000e-04 

2023-03-02 01:46:11,806 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3487 (0.3950) Acc D Real: 96.778% 
Loss D Fake: 0.7670 (0.7980) Acc D Fake: 5.326% 
Loss D: 1.116 
Loss G: 0.6312 (0.6048) Acc G: 94.682% 
LR: 2.000e-04 

2023-03-02 01:46:11,813 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3945 (0.3950) Acc D Real: 96.758% 
Loss D Fake: 0.7669 (0.7979) Acc D Fake: 5.324% 
Loss D: 1.161 
Loss G: 0.6300 (0.6049) Acc G: 94.684% 
LR: 2.000e-04 

2023-03-02 01:46:11,821 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3616 (0.3949) Acc D Real: 96.747% 
Loss D Fake: 0.7691 (0.7977) Acc D Fake: 5.323% 
Loss D: 1.131 
Loss G: 0.6288 (0.6050) Acc G: 94.685% 
LR: 2.000e-04 

2023-03-02 01:46:11,828 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.4282 (0.3950) Acc D Real: 96.722% 
Loss D Fake: 0.7702 (0.7976) Acc D Fake: 5.321% 
Loss D: 1.198 
Loss G: 0.6282 (0.6051) Acc G: 94.687% 
LR: 2.000e-04 

2023-03-02 01:46:11,835 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3735 (0.3949) Acc D Real: 96.712% 
Loss D Fake: 0.7704 (0.7975) Acc D Fake: 5.320% 
Loss D: 1.144 
Loss G: 0.6284 (0.6052) Acc G: 94.688% 
LR: 2.000e-04 

2023-03-02 01:46:11,843 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3538 (0.3947) Acc D Real: 96.710% 
Loss D Fake: 0.7698 (0.7974) Acc D Fake: 5.318% 
Loss D: 1.124 
Loss G: 0.6292 (0.6053) Acc G: 94.689% 
LR: 2.000e-04 

2023-03-02 01:46:11,850 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3812 (0.3947) Acc D Real: 96.711% 
Loss D Fake: 0.7684 (0.7972) Acc D Fake: 5.317% 
Loss D: 1.150 
Loss G: 0.6304 (0.6055) Acc G: 94.691% 
LR: 2.000e-04 

2023-03-02 01:46:11,858 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2719 (0.3941) Acc D Real: 96.714% 
Loss D Fake: 0.7665 (0.7971) Acc D Fake: 5.315% 
Loss D: 1.038 
Loss G: 0.6325 (0.6056) Acc G: 94.692% 
LR: 2.000e-04 

2023-03-02 01:46:11,865 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2695 (0.3936) Acc D Real: 96.705% 
Loss D Fake: 0.7633 (0.7969) Acc D Fake: 5.314% 
Loss D: 1.033 
Loss G: 0.6355 (0.6057) Acc G: 94.694% 
LR: 2.000e-04 

2023-03-02 01:46:11,873 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3556 (0.3934) Acc D Real: 96.697% 
Loss D Fake: 0.7595 (0.7968) Acc D Fake: 5.312% 
Loss D: 1.115 
Loss G: 0.6386 (0.6059) Acc G: 94.695% 
LR: 2.000e-04 

2023-03-02 01:46:11,880 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4908 (0.3938) Acc D Real: 96.672% 
Loss D Fake: 0.7565 (0.7966) Acc D Fake: 5.311% 
Loss D: 1.247 
Loss G: 0.6404 (0.6060) Acc G: 94.696% 
LR: 2.000e-04 

2023-03-02 01:46:11,888 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4743 (0.3942) Acc D Real: 96.664% 
Loss D Fake: 0.7551 (0.7964) Acc D Fake: 5.311% 
Loss D: 1.229 
Loss G: 0.6410 (0.6062) Acc G: 94.697% 
LR: 2.000e-04 

2023-03-02 01:46:11,900 -                train: [    INFO] - 
Epoch: 13/20
2023-03-02 01:46:12,063 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3519 (0.4482) Acc D Real: 89.974% 
Loss D Fake: 0.7563 (0.7557) Acc D Fake: 5.000% 
Loss D: 1.108 
Loss G: 0.6392 (0.6396) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,071 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3457 (0.4140) Acc D Real: 90.729% 
Loss D Fake: 0.7569 (0.7561) Acc D Fake: 5.000% 
Loss D: 1.103 
Loss G: 0.6390 (0.6394) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,078 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4244 (0.4166) Acc D Real: 91.419% 
Loss D Fake: 0.7570 (0.7563) Acc D Fake: 5.000% 
Loss D: 1.181 
Loss G: 0.6387 (0.6392) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,099 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3857 (0.4104) Acc D Real: 91.708% 
Loss D Fake: 0.7581 (0.7567) Acc D Fake: 5.000% 
Loss D: 1.144 
Loss G: 0.6365 (0.6387) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,106 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4146 (0.4111) Acc D Real: 92.101% 
Loss D Fake: 0.7618 (0.7575) Acc D Fake: 5.000% 
Loss D: 1.176 
Loss G: 0.6335 (0.6378) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,113 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.4018 (0.4098) Acc D Real: 92.567% 
Loss D Fake: 0.7656 (0.7587) Acc D Fake: 5.000% 
Loss D: 1.167 
Loss G: 0.6307 (0.6368) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,121 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3704 (0.4049) Acc D Real: 93.008% 
Loss D Fake: 0.7690 (0.7600) Acc D Fake: 5.000% 
Loss D: 1.139 
Loss G: 0.6285 (0.6357) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,129 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.4009 (0.4044) Acc D Real: 93.218% 
Loss D Fake: 0.7712 (0.7612) Acc D Fake: 5.000% 
Loss D: 1.172 
Loss G: 0.6272 (0.6348) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,137 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3604 (0.4000) Acc D Real: 93.302% 
Loss D Fake: 0.7722 (0.7623) Acc D Fake: 5.000% 
Loss D: 1.133 
Loss G: 0.6273 (0.6340) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,145 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3702 (0.3973) Acc D Real: 93.665% 
Loss D Fake: 0.7714 (0.7632) Acc D Fake: 5.000% 
Loss D: 1.142 
Loss G: 0.6281 (0.6335) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,153 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2942 (0.3887) Acc D Real: 93.902% 
Loss D Fake: 0.7698 (0.7637) Acc D Fake: 5.000% 
Loss D: 1.064 
Loss G: 0.6300 (0.6332) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,160 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3984 (0.3895) Acc D Real: 93.802% 
Loss D Fake: 0.7670 (0.7640) Acc D Fake: 5.000% 
Loss D: 1.165 
Loss G: 0.6320 (0.6331) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,168 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3887 (0.3894) Acc D Real: 93.813% 
Loss D Fake: 0.7647 (0.7640) Acc D Fake: 5.000% 
Loss D: 1.153 
Loss G: 0.6337 (0.6332) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,176 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3813 (0.3889) Acc D Real: 93.889% 
Loss D Fake: 0.7628 (0.7639) Acc D Fake: 5.000% 
Loss D: 1.144 
Loss G: 0.6352 (0.6333) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,183 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3079 (0.3838) Acc D Real: 93.923% 
Loss D Fake: 0.7610 (0.7637) Acc D Fake: 5.000% 
Loss D: 1.069 
Loss G: 0.6369 (0.6335) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,192 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3635 (0.3826) Acc D Real: 93.882% 
Loss D Fake: 0.7591 (0.7635) Acc D Fake: 5.000% 
Loss D: 1.123 
Loss G: 0.6380 (0.6338) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,200 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.4077 (0.3840) Acc D Real: 93.791% 
Loss D Fake: 0.7582 (0.7632) Acc D Fake: 5.000% 
Loss D: 1.166 
Loss G: 0.6387 (0.6341) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,207 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.4236 (0.3861) Acc D Real: 93.684% 
Loss D Fake: 0.7577 (0.7629) Acc D Fake: 5.000% 
Loss D: 1.181 
Loss G: 0.6391 (0.6343) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,215 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3414 (0.3839) Acc D Real: 93.612% 
Loss D Fake: 0.7579 (0.7627) Acc D Fake: 5.000% 
Loss D: 1.099 
Loss G: 0.6382 (0.6345) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,223 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3412 (0.3818) Acc D Real: 93.222% 
Loss D Fake: 0.7598 (0.7625) Acc D Fake: 5.000% 
Loss D: 1.101 
Loss G: 0.6369 (0.6346) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,230 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3392 (0.3799) Acc D Real: 93.075% 
Loss D Fake: 0.7615 (0.7625) Acc D Fake: 5.000% 
Loss D: 1.101 
Loss G: 0.6364 (0.6347) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,237 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3140 (0.3770) Acc D Real: 93.096% 
Loss D Fake: 0.7617 (0.7624) Acc D Fake: 5.000% 
Loss D: 1.076 
Loss G: 0.6371 (0.6348) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,243 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4555 (0.3803) Acc D Real: 92.702% 
Loss D Fake: 0.7605 (0.7624) Acc D Fake: 5.000% 
Loss D: 1.216 
Loss G: 0.6386 (0.6350) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,250 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3370 (0.3786) Acc D Real: 92.379% 
Loss D Fake: 0.7579 (0.7622) Acc D Fake: 5.000% 
Loss D: 1.095 
Loss G: 0.6413 (0.6352) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,256 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2766 (0.3746) Acc D Real: 92.292% 
Loss D Fake: 0.7538 (0.7619) Acc D Fake: 5.000% 
Loss D: 1.030 
Loss G: 0.6451 (0.6356) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,263 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3627 (0.3742) Acc D Real: 91.943% 
Loss D Fake: 0.7492 (0.7614) Acc D Fake: 5.000% 
Loss D: 1.112 
Loss G: 0.6487 (0.6361) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,270 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3748 (0.3742) Acc D Real: 91.501% 
Loss D Fake: 0.7453 (0.7608) Acc D Fake: 5.000% 
Loss D: 1.120 
Loss G: 0.6516 (0.6366) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,277 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.4206 (0.3758) Acc D Real: 90.894% 
Loss D Fake: 0.7423 (0.7602) Acc D Fake: 5.000% 
Loss D: 1.163 
Loss G: 0.6537 (0.6372) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,284 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3236 (0.3741) Acc D Real: 90.460% 
Loss D Fake: 0.7400 (0.7595) Acc D Fake: 5.000% 
Loss D: 1.064 
Loss G: 0.6559 (0.6379) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,291 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3800 (0.3743) Acc D Real: 90.190% 
Loss D Fake: 0.7376 (0.7588) Acc D Fake: 5.000% 
Loss D: 1.118 
Loss G: 0.6577 (0.6385) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,299 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3314 (0.3729) Acc D Real: 89.871% 
Loss D Fake: 0.7358 (0.7581) Acc D Fake: 5.000% 
Loss D: 1.067 
Loss G: 0.6591 (0.6391) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,306 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.4525 (0.3753) Acc D Real: 89.171% 
Loss D Fake: 0.7347 (0.7574) Acc D Fake: 5.000% 
Loss D: 1.187 
Loss G: 0.6598 (0.6398) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,313 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.4374 (0.3772) Acc D Real: 88.611% 
Loss D Fake: 0.7343 (0.7567) Acc D Fake: 5.000% 
Loss D: 1.172 
Loss G: 0.6598 (0.6404) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,320 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3447 (0.3762) Acc D Real: 88.265% 
Loss D Fake: 0.7345 (0.7561) Acc D Fake: 5.000% 
Loss D: 1.079 
Loss G: 0.6597 (0.6409) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,327 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.4359 (0.3779) Acc D Real: 87.593% 
Loss D Fake: 0.7347 (0.7555) Acc D Fake: 5.000% 
Loss D: 1.171 
Loss G: 0.6593 (0.6414) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,334 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.4277 (0.3792) Acc D Real: 87.085% 
Loss D Fake: 0.7354 (0.7549) Acc D Fake: 5.000% 
Loss D: 1.163 
Loss G: 0.6586 (0.6419) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,342 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.4407 (0.3809) Acc D Real: 86.676% 
Loss D Fake: 0.7363 (0.7544) Acc D Fake: 5.000% 
Loss D: 1.177 
Loss G: 0.6577 (0.6423) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,349 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3836 (0.3809) Acc D Real: 86.406% 
Loss D Fake: 0.7373 (0.7540) Acc D Fake: 5.000% 
Loss D: 1.121 
Loss G: 0.6569 (0.6427) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,356 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3491 (0.3801) Acc D Real: 86.120% 
Loss D Fake: 0.7378 (0.7536) Acc D Fake: 5.000% 
Loss D: 1.087 
Loss G: 0.6567 (0.6430) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,363 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3550 (0.3795) Acc D Real: 85.911% 
Loss D Fake: 0.7379 (0.7532) Acc D Fake: 5.000% 
Loss D: 1.093 
Loss G: 0.6566 (0.6434) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,371 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3405 (0.3786) Acc D Real: 85.725% 
Loss D Fake: 0.7378 (0.7528) Acc D Fake: 5.000% 
Loss D: 1.078 
Loss G: 0.6568 (0.6437) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,378 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3174 (0.3772) Acc D Real: 85.560% 
Loss D Fake: 0.7373 (0.7525) Acc D Fake: 5.000% 
Loss D: 1.055 
Loss G: 0.6577 (0.6440) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,385 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3938 (0.3775) Acc D Real: 85.232% 
Loss D Fake: 0.7365 (0.7521) Acc D Fake: 5.000% 
Loss D: 1.130 
Loss G: 0.6581 (0.6443) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,392 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3773 (0.3775) Acc D Real: 85.084% 
Loss D Fake: 0.7363 (0.7518) Acc D Fake: 5.000% 
Loss D: 1.114 
Loss G: 0.6584 (0.6446) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,399 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3765 (0.3775) Acc D Real: 84.825% 
Loss D Fake: 0.7360 (0.7514) Acc D Fake: 5.000% 
Loss D: 1.113 
Loss G: 0.6587 (0.6449) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,406 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4010 (0.3780) Acc D Real: 84.583% 
Loss D Fake: 0.7358 (0.7511) Acc D Fake: 5.000% 
Loss D: 1.137 
Loss G: 0.6589 (0.6452) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,414 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4023 (0.3785) Acc D Real: 84.243% 
Loss D Fake: 0.7358 (0.7508) Acc D Fake: 5.000% 
Loss D: 1.138 
Loss G: 0.6589 (0.6455) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,421 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3723 (0.3784) Acc D Real: 84.095% 
Loss D Fake: 0.7363 (0.7505) Acc D Fake: 5.000% 
Loss D: 1.109 
Loss G: 0.6576 (0.6458) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,428 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3056 (0.3769) Acc D Real: 84.035% 
Loss D Fake: 0.7382 (0.7502) Acc D Fake: 5.000% 
Loss D: 1.044 
Loss G: 0.6567 (0.6460) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,435 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.4196 (0.3778) Acc D Real: 83.798% 
Loss D Fake: 0.7394 (0.7500) Acc D Fake: 5.000% 
Loss D: 1.159 
Loss G: 0.6557 (0.6462) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,443 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4187 (0.3786) Acc D Real: 83.590% 
Loss D Fake: 0.7408 (0.7498) Acc D Fake: 5.000% 
Loss D: 1.160 
Loss G: 0.6545 (0.6463) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,450 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3471 (0.3780) Acc D Real: 83.477% 
Loss D Fake: 0.7423 (0.7497) Acc D Fake: 5.000% 
Loss D: 1.089 
Loss G: 0.6536 (0.6465) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,457 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3754 (0.3779) Acc D Real: 83.275% 
Loss D Fake: 0.7430 (0.7496) Acc D Fake: 5.000% 
Loss D: 1.118 
Loss G: 0.6537 (0.6466) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,464 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3438 (0.3773) Acc D Real: 83.030% 
Loss D Fake: 0.7420 (0.7494) Acc D Fake: 5.000% 
Loss D: 1.086 
Loss G: 0.6552 (0.6468) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,471 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3734 (0.3772) Acc D Real: 82.879% 
Loss D Fake: 0.7398 (0.7493) Acc D Fake: 5.000% 
Loss D: 1.113 
Loss G: 0.6571 (0.6470) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:12,478 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2927 (0.3758) Acc D Real: 82.913% 
Loss D Fake: 0.7372 (0.7490) Acc D Fake: 5.000% 
Loss D: 1.030 
Loss G: 0.6597 (0.6472) Acc G: 94.035% 
LR: 2.000e-04 

2023-03-02 01:46:12,486 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3085 (0.3746) Acc D Real: 82.840% 
Loss D Fake: 0.7338 (0.7488) Acc D Fake: 6.034% 
Loss D: 1.042 
Loss G: 0.6627 (0.6474) Acc G: 92.931% 
LR: 2.000e-04 

2023-03-02 01:46:12,493 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4132 (0.3752) Acc D Real: 82.631% 
Loss D Fake: 0.7305 (0.7485) Acc D Fake: 7.147% 
Loss D: 1.144 
Loss G: 0.6652 (0.6477) Acc G: 91.808% 
LR: 2.000e-04 

2023-03-02 01:46:12,500 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3641 (0.3751) Acc D Real: 82.503% 
Loss D Fake: 0.7281 (0.7481) Acc D Fake: 8.250% 
Loss D: 1.092 
Loss G: 0.6671 (0.6481) Acc G: 90.694% 
LR: 2.000e-04 

2023-03-02 01:46:12,507 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3651 (0.3749) Acc D Real: 82.453% 
Loss D Fake: 0.7261 (0.7478) Acc D Fake: 9.344% 
Loss D: 1.091 
Loss G: 0.6688 (0.6484) Acc G: 89.617% 
LR: 2.000e-04 

2023-03-02 01:46:12,514 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3988 (0.3753) Acc D Real: 82.198% 
Loss D Fake: 0.7243 (0.7474) Acc D Fake: 10.403% 
Loss D: 1.123 
Loss G: 0.6701 (0.6488) Acc G: 88.548% 
LR: 2.000e-04 

2023-03-02 01:46:12,521 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.4062 (0.3758) Acc D Real: 81.926% 
Loss D Fake: 0.7244 (0.7470) Acc D Fake: 11.429% 
Loss D: 1.131 
Loss G: 0.6678 (0.6491) Acc G: 87.540% 
LR: 2.000e-04 

2023-03-02 01:46:12,529 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3601 (0.3755) Acc D Real: 81.789% 
Loss D Fake: 0.7291 (0.7468) Acc D Fake: 12.396% 
Loss D: 1.089 
Loss G: 0.6637 (0.6493) Acc G: 86.615% 
LR: 2.000e-04 

2023-03-02 01:46:12,536 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3277 (0.3748) Acc D Real: 81.677% 
Loss D Fake: 0.7347 (0.7466) Acc D Fake: 13.308% 
Loss D: 1.062 
Loss G: 0.6595 (0.6494) Acc G: 85.744% 
LR: 2.000e-04 

2023-03-02 01:46:12,543 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3241 (0.3740) Acc D Real: 81.604% 
Loss D Fake: 0.7400 (0.7465) Acc D Fake: 14.141% 
Loss D: 1.064 
Loss G: 0.6570 (0.6496) Acc G: 84.924% 
LR: 2.000e-04 

2023-03-02 01:46:12,551 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4198 (0.3747) Acc D Real: 81.286% 
Loss D Fake: 0.7414 (0.7464) Acc D Fake: 14.950% 
Loss D: 1.161 
Loss G: 0.6584 (0.6497) Acc G: 84.104% 
LR: 2.000e-04 

2023-03-02 01:46:12,558 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.3399 (0.3742) Acc D Real: 81.140% 
Loss D Fake: 0.7369 (0.7463) Acc D Fake: 15.760% 
Loss D: 1.077 
Loss G: 0.6628 (0.6499) Acc G: 83.284% 
LR: 2.000e-04 

2023-03-02 01:46:12,565 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3496 (0.3738) Acc D Real: 81.039% 
Loss D Fake: 0.7315 (0.7460) Acc D Fake: 16.570% 
Loss D: 1.081 
Loss G: 0.6663 (0.6501) Acc G: 82.464% 
LR: 2.000e-04 

2023-03-02 01:46:12,573 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3780 (0.3739) Acc D Real: 80.874% 
Loss D Fake: 0.7277 (0.7458) Acc D Fake: 17.381% 
Loss D: 1.106 
Loss G: 0.6694 (0.6504) Acc G: 81.643% 
LR: 2.000e-04 

2023-03-02 01:46:12,581 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2940 (0.3728) Acc D Real: 80.848% 
Loss D Fake: 0.7240 (0.7455) Acc D Fake: 18.192% 
Loss D: 1.018 
Loss G: 0.6728 (0.6507) Acc G: 80.822% 
LR: 2.000e-04 

2023-03-02 01:46:12,588 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3720 (0.3728) Acc D Real: 80.654% 
Loss D Fake: 0.7201 (0.7451) Acc D Fake: 19.005% 
Loss D: 1.092 
Loss G: 0.6759 (0.6511) Acc G: 80.023% 
LR: 2.000e-04 

2023-03-02 01:46:12,596 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.4189 (0.3734) Acc D Real: 80.406% 
Loss D Fake: 0.7170 (0.7447) Acc D Fake: 19.817% 
Loss D: 1.136 
Loss G: 0.6782 (0.6514) Acc G: 79.224% 
LR: 2.000e-04 

2023-03-02 01:46:12,604 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.2592 (0.3718) Acc D Real: 80.398% 
Loss D Fake: 0.7146 (0.7443) Acc D Fake: 20.608% 
Loss D: 0.974 
Loss G: 0.6806 (0.6518) Acc G: 78.446% 
LR: 2.000e-04 

2023-03-02 01:46:12,611 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3156 (0.3711) Acc D Real: 80.291% 
Loss D Fake: 0.7118 (0.7439) Acc D Fake: 21.378% 
Loss D: 1.027 
Loss G: 0.6833 (0.6522) Acc G: 77.667% 
LR: 2.000e-04 

2023-03-02 01:46:12,618 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4259 (0.3718) Acc D Real: 79.941% 
Loss D Fake: 0.7091 (0.7434) Acc D Fake: 22.149% 
Loss D: 1.135 
Loss G: 0.6853 (0.6527) Acc G: 76.908% 
LR: 2.000e-04 

2023-03-02 01:46:12,626 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3299 (0.3713) Acc D Real: 79.823% 
Loss D Fake: 0.7072 (0.7430) Acc D Fake: 22.900% 
Loss D: 1.037 
Loss G: 0.6871 (0.6531) Acc G: 76.169% 
LR: 2.000e-04 

2023-03-02 01:46:12,633 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3420 (0.3709) Acc D Real: 79.787% 
Loss D Fake: 0.7058 (0.7425) Acc D Fake: 23.632% 
Loss D: 1.048 
Loss G: 0.6878 (0.6536) Acc G: 75.449% 
LR: 2.000e-04 

2023-03-02 01:46:12,641 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3875 (0.3711) Acc D Real: 79.558% 
Loss D Fake: 0.7058 (0.7420) Acc D Fake: 24.346% 
Loss D: 1.093 
Loss G: 0.6879 (0.6540) Acc G: 74.747% 
LR: 2.000e-04 

2023-03-02 01:46:12,648 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.2379 (0.3694) Acc D Real: 79.596% 
Loss D Fake: 0.7055 (0.7416) Acc D Fake: 25.042% 
Loss D: 0.943 
Loss G: 0.6891 (0.6544) Acc G: 74.062% 
LR: 2.000e-04 

2023-03-02 01:46:12,655 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3887 (0.3697) Acc D Real: 79.377% 
Loss D Fake: 0.7041 (0.7411) Acc D Fake: 25.720% 
Loss D: 1.093 
Loss G: 0.6905 (0.6549) Acc G: 73.395% 
LR: 2.000e-04 

2023-03-02 01:46:12,663 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.4200 (0.3703) Acc D Real: 79.165% 
Loss D Fake: 0.7032 (0.7406) Acc D Fake: 26.382% 
Loss D: 1.123 
Loss G: 0.6910 (0.6553) Acc G: 72.744% 
LR: 2.000e-04 

2023-03-02 01:46:12,670 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3512 (0.3701) Acc D Real: 79.010% 
Loss D Fake: 0.7031 (0.7402) Acc D Fake: 27.028% 
Loss D: 1.054 
Loss G: 0.6912 (0.6558) Acc G: 72.108% 
LR: 2.000e-04 

2023-03-02 01:46:12,678 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3249 (0.3695) Acc D Real: 78.925% 
Loss D Fake: 0.7029 (0.7397) Acc D Fake: 27.659% 
Loss D: 1.028 
Loss G: 0.6919 (0.6562) Acc G: 71.488% 
LR: 2.000e-04 

2023-03-02 01:46:12,685 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3810 (0.3697) Acc D Real: 78.777% 
Loss D Fake: 0.7021 (0.7393) Acc D Fake: 28.255% 
Loss D: 1.083 
Loss G: 0.6928 (0.6566) Acc G: 70.882% 
LR: 2.000e-04 

2023-03-02 01:46:12,692 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3705 (0.3697) Acc D Real: 78.631% 
Loss D Fake: 0.7012 (0.7389) Acc D Fake: 28.857% 
Loss D: 1.072 
Loss G: 0.6938 (0.6571) Acc G: 70.291% 
LR: 2.000e-04 

2023-03-02 01:46:12,700 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.2995 (0.3689) Acc D Real: 78.623% 
Loss D Fake: 0.7000 (0.7384) Acc D Fake: 29.444% 
Loss D: 1.000 
Loss G: 0.6953 (0.6575) Acc G: 69.713% 
LR: 2.000e-04 

2023-03-02 01:46:12,707 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3708 (0.3689) Acc D Real: 78.458% 
Loss D Fake: 0.6982 (0.7380) Acc D Fake: 30.019% 
Loss D: 1.069 
Loss G: 0.6971 (0.6579) Acc G: 69.148% 
LR: 2.000e-04 

2023-03-02 01:46:12,714 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3679 (0.3689) Acc D Real: 78.349% 
Loss D Fake: 0.6964 (0.7375) Acc D Fake: 30.581% 
Loss D: 1.064 
Loss G: 0.6987 (0.6584) Acc G: 68.596% 
LR: 2.000e-04 

2023-03-02 01:46:12,722 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3280 (0.3684) Acc D Real: 78.274% 
Loss D Fake: 0.6952 (0.7370) Acc D Fake: 31.130% 
Loss D: 1.023 
Loss G: 0.6993 (0.6589) Acc G: 68.056% 
LR: 2.000e-04 

2023-03-02 01:46:12,729 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.4674 (0.3695) Acc D Real: 78.022% 
Loss D Fake: 0.6956 (0.7366) Acc D Fake: 31.667% 
Loss D: 1.163 
Loss G: 0.6981 (0.6593) Acc G: 67.527% 
LR: 2.000e-04 

2023-03-02 01:46:12,736 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3723 (0.3695) Acc D Real: 77.868% 
Loss D Fake: 0.6975 (0.7361) Acc D Fake: 32.174% 
Loss D: 1.070 
Loss G: 0.6968 (0.6597) Acc G: 67.029% 
LR: 2.000e-04 

2023-03-02 01:46:12,744 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2783 (0.3686) Acc D Real: 77.905% 
Loss D Fake: 0.6984 (0.7357) Acc D Fake: 32.670% 
Loss D: 0.977 
Loss G: 0.6970 (0.6601) Acc G: 66.541% 
LR: 2.000e-04 

2023-03-02 01:46:12,751 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3290 (0.3681) Acc D Real: 77.805% 
Loss D Fake: 0.6976 (0.7353) Acc D Fake: 33.156% 
Loss D: 1.027 
Loss G: 0.6983 (0.6605) Acc G: 66.064% 
LR: 2.000e-04 

2023-03-02 01:46:12,758 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3619 (0.3681) Acc D Real: 77.672% 
Loss D Fake: 0.6959 (0.7349) Acc D Fake: 33.632% 
Loss D: 1.058 
Loss G: 0.7001 (0.6609) Acc G: 65.579% 
LR: 2.000e-04 

2023-03-02 01:46:12,766 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4406 (0.3688) Acc D Real: 77.444% 
Loss D Fake: 0.6941 (0.7345) Acc D Fake: 34.115% 
Loss D: 1.135 
Loss G: 0.7015 (0.6613) Acc G: 65.104% 
LR: 2.000e-04 

2023-03-02 01:46:12,773 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.3072 (0.3682) Acc D Real: 77.410% 
Loss D Fake: 0.6925 (0.7341) Acc D Fake: 34.588% 
Loss D: 1.000 
Loss G: 0.7032 (0.6618) Acc G: 64.639% 
LR: 2.000e-04 

2023-03-02 01:46:12,782 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3608 (0.3681) Acc D Real: 77.301% 
Loss D Fake: 0.6906 (0.7336) Acc D Fake: 35.051% 
Loss D: 1.051 
Loss G: 0.7049 (0.6622) Acc G: 64.184% 
LR: 2.000e-04 

2023-03-02 01:46:12,790 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3780 (0.3682) Acc D Real: 77.234% 
Loss D Fake: 0.6890 (0.7332) Acc D Fake: 35.522% 
Loss D: 1.067 
Loss G: 0.7063 (0.6627) Acc G: 63.721% 
LR: 2.000e-04 

2023-03-02 01:46:12,798 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3560 (0.3681) Acc D Real: 77.156% 
Loss D Fake: 0.6875 (0.7327) Acc D Fake: 35.983% 
Loss D: 1.044 
Loss G: 0.7076 (0.6631) Acc G: 63.267% 
LR: 2.000e-04 

2023-03-02 01:46:12,806 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.4881 (0.3693) Acc D Real: 76.892% 
Loss D Fake: 0.6867 (0.7322) Acc D Fake: 36.436% 
Loss D: 1.175 
Loss G: 0.7074 (0.6636) Acc G: 62.822% 
LR: 2.000e-04 

2023-03-02 01:46:12,815 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3396 (0.3690) Acc D Real: 76.814% 
Loss D Fake: 0.6871 (0.7318) Acc D Fake: 36.879% 
Loss D: 1.027 
Loss G: 0.7072 (0.6640) Acc G: 62.386% 
LR: 2.000e-04 

2023-03-02 01:46:12,823 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.5104 (0.3704) Acc D Real: 76.511% 
Loss D Fake: 0.6875 (0.7314) Acc D Fake: 37.314% 
Loss D: 1.198 
Loss G: 0.7060 (0.6644) Acc G: 61.958% 
LR: 2.000e-04 

2023-03-02 01:46:12,831 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.4360 (0.3710) Acc D Real: 76.317% 
Loss D Fake: 0.6891 (0.7310) Acc D Fake: 37.740% 
Loss D: 1.125 
Loss G: 0.7042 (0.6648) Acc G: 61.538% 
LR: 2.000e-04 

2023-03-02 01:46:12,840 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3790 (0.3711) Acc D Real: 76.189% 
Loss D Fake: 0.6908 (0.7306) Acc D Fake: 38.159% 
Loss D: 1.070 
Loss G: 0.7025 (0.6651) Acc G: 61.127% 
LR: 2.000e-04 

2023-03-02 01:46:12,848 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3394 (0.3708) Acc D Real: 76.140% 
Loss D Fake: 0.6922 (0.7302) Acc D Fake: 38.569% 
Loss D: 1.032 
Loss G: 0.7015 (0.6655) Acc G: 60.723% 
LR: 2.000e-04 

2023-03-02 01:46:12,855 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.4313 (0.3713) Acc D Real: 75.981% 
Loss D Fake: 0.6931 (0.7299) Acc D Fake: 38.972% 
Loss D: 1.124 
Loss G: 0.7003 (0.6658) Acc G: 60.327% 
LR: 2.000e-04 

2023-03-02 01:46:12,862 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2876 (0.3706) Acc D Real: 75.980% 
Loss D Fake: 0.6939 (0.7295) Acc D Fake: 39.367% 
Loss D: 0.982 
Loss G: 0.7000 (0.6661) Acc G: 59.938% 
LR: 2.000e-04 

2023-03-02 01:46:12,869 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3860 (0.3707) Acc D Real: 75.892% 
Loss D Fake: 0.6941 (0.7292) Acc D Fake: 39.755% 
Loss D: 1.080 
Loss G: 0.6994 (0.6664) Acc G: 59.557% 
LR: 2.000e-04 

2023-03-02 01:46:12,876 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3408 (0.3704) Acc D Real: 75.807% 
Loss D Fake: 0.6947 (0.7289) Acc D Fake: 40.136% 
Loss D: 1.036 
Loss G: 0.6992 (0.6667) Acc G: 59.182% 
LR: 2.000e-04 

2023-03-02 01:46:12,884 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3677 (0.3704) Acc D Real: 75.677% 
Loss D Fake: 0.6948 (0.7286) Acc D Fake: 40.511% 
Loss D: 1.063 
Loss G: 0.6991 (0.6670) Acc G: 58.814% 
LR: 2.000e-04 

2023-03-02 01:46:12,891 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3446 (0.3702) Acc D Real: 75.638% 
Loss D Fake: 0.6947 (0.7283) Acc D Fake: 40.878% 
Loss D: 1.039 
Loss G: 0.6995 (0.6673) Acc G: 58.452% 
LR: 2.000e-04 

2023-03-02 01:46:12,898 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3628 (0.3701) Acc D Real: 75.550% 
Loss D Fake: 0.6943 (0.7280) Acc D Fake: 41.239% 
Loss D: 1.057 
Loss G: 0.7000 (0.6676) Acc G: 58.097% 
LR: 2.000e-04 

2023-03-02 01:46:12,905 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.4184 (0.3705) Acc D Real: 75.427% 
Loss D Fake: 0.6937 (0.7277) Acc D Fake: 41.594% 
Loss D: 1.112 
Loss G: 0.7003 (0.6679) Acc G: 57.749% 
LR: 2.000e-04 

2023-03-02 01:46:12,912 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3687 (0.3705) Acc D Real: 75.322% 
Loss D Fake: 0.6935 (0.7274) Acc D Fake: 41.942% 
Loss D: 1.062 
Loss G: 0.7005 (0.6682) Acc G: 57.406% 
LR: 2.000e-04 

2023-03-02 01:46:12,919 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4659 (0.3713) Acc D Real: 75.129% 
Loss D Fake: 0.6935 (0.7271) Acc D Fake: 42.284% 
Loss D: 1.159 
Loss G: 0.7000 (0.6684) Acc G: 57.069% 
LR: 2.000e-04 

2023-03-02 01:46:12,927 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3772 (0.3714) Acc D Real: 75.033% 
Loss D Fake: 0.6941 (0.7268) Acc D Fake: 42.621% 
Loss D: 1.071 
Loss G: 0.6994 (0.6687) Acc G: 56.738% 
LR: 2.000e-04 

2023-03-02 01:46:12,934 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.4696 (0.3722) Acc D Real: 74.868% 
Loss D Fake: 0.6948 (0.7266) Acc D Fake: 42.952% 
Loss D: 1.164 
Loss G: 0.6981 (0.6689) Acc G: 56.412% 
LR: 2.000e-04 

2023-03-02 01:46:12,941 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4030 (0.3725) Acc D Real: 74.747% 
Loss D Fake: 0.6962 (0.7263) Acc D Fake: 43.277% 
Loss D: 1.099 
Loss G: 0.6966 (0.6692) Acc G: 56.092% 
LR: 2.000e-04 

2023-03-02 01:46:12,948 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2987 (0.3719) Acc D Real: 74.740% 
Loss D Fake: 0.6973 (0.7261) Acc D Fake: 43.597% 
Loss D: 0.996 
Loss G: 0.6959 (0.6694) Acc G: 55.778% 
LR: 2.000e-04 

2023-03-02 01:46:12,956 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3429 (0.3716) Acc D Real: 74.688% 
Loss D Fake: 0.6977 (0.7258) Acc D Fake: 43.912% 
Loss D: 1.041 
Loss G: 0.6958 (0.6696) Acc G: 55.468% 
LR: 2.000e-04 

2023-03-02 01:46:12,963 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3883 (0.3718) Acc D Real: 74.579% 
Loss D Fake: 0.6978 (0.7256) Acc D Fake: 44.221% 
Loss D: 1.086 
Loss G: 0.6954 (0.6698) Acc G: 55.164% 
LR: 2.000e-04 

2023-03-02 01:46:12,970 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3112 (0.3713) Acc D Real: 74.598% 
Loss D Fake: 0.6981 (0.7254) Acc D Fake: 44.526% 
Loss D: 1.009 
Loss G: 0.6955 (0.6700) Acc G: 54.864% 
LR: 2.000e-04 

2023-03-02 01:46:12,977 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3311 (0.3710) Acc D Real: 74.534% 
Loss D Fake: 0.6978 (0.7251) Acc D Fake: 44.825% 
Loss D: 1.029 
Loss G: 0.6961 (0.6703) Acc G: 54.570% 
LR: 2.000e-04 

2023-03-02 01:46:12,985 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3353 (0.3707) Acc D Real: 74.505% 
Loss D Fake: 0.6970 (0.7249) Acc D Fake: 45.120% 
Loss D: 1.032 
Loss G: 0.6971 (0.6705) Acc G: 54.280% 
LR: 2.000e-04 

2023-03-02 01:46:12,992 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.4534 (0.3713) Acc D Real: 74.315% 
Loss D Fake: 0.6962 (0.7247) Acc D Fake: 45.410% 
Loss D: 1.150 
Loss G: 0.6974 (0.6707) Acc G: 53.995% 
LR: 2.000e-04 

2023-03-02 01:46:12,999 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3643 (0.3713) Acc D Real: 74.252% 
Loss D Fake: 0.6961 (0.7245) Acc D Fake: 45.696% 
Loss D: 1.060 
Loss G: 0.6976 (0.6709) Acc G: 53.714% 
LR: 2.000e-04 

2023-03-02 01:46:13,007 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3221 (0.3709) Acc D Real: 74.269% 
Loss D Fake: 0.6958 (0.7242) Acc D Fake: 45.977% 
Loss D: 1.018 
Loss G: 0.6981 (0.6711) Acc G: 53.438% 
LR: 2.000e-04 

2023-03-02 01:46:13,014 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3718 (0.3709) Acc D Real: 74.209% 
Loss D Fake: 0.6953 (0.7240) Acc D Fake: 46.253% 
Loss D: 1.067 
Loss G: 0.6985 (0.6713) Acc G: 53.165% 
LR: 2.000e-04 

2023-03-02 01:46:13,021 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3311 (0.3706) Acc D Real: 74.175% 
Loss D Fake: 0.6948 (0.7238) Acc D Fake: 46.526% 
Loss D: 1.026 
Loss G: 0.6992 (0.6715) Acc G: 52.897% 
LR: 2.000e-04 

2023-03-02 01:46:13,028 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3923 (0.3707) Acc D Real: 74.050% 
Loss D Fake: 0.6942 (0.7236) Acc D Fake: 46.794% 
Loss D: 1.086 
Loss G: 0.6996 (0.6717) Acc G: 52.634% 
LR: 2.000e-04 

2023-03-02 01:46:13,036 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4194 (0.3711) Acc D Real: 73.917% 
Loss D Fake: 0.6940 (0.7233) Acc D Fake: 47.058% 
Loss D: 1.113 
Loss G: 0.6997 (0.6720) Acc G: 52.374% 
LR: 2.000e-04 

2023-03-02 01:46:13,043 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3862 (0.3712) Acc D Real: 73.837% 
Loss D Fake: 0.6942 (0.7231) Acc D Fake: 47.318% 
Loss D: 1.080 
Loss G: 0.6993 (0.6722) Acc G: 52.118% 
LR: 2.000e-04 

2023-03-02 01:46:13,050 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3779 (0.3713) Acc D Real: 73.766% 
Loss D Fake: 0.6946 (0.7229) Acc D Fake: 47.575% 
Loss D: 1.072 
Loss G: 0.6989 (0.6724) Acc G: 51.866% 
LR: 2.000e-04 

2023-03-02 01:46:13,057 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4510 (0.3719) Acc D Real: 73.635% 
Loss D Fake: 0.6953 (0.7227) Acc D Fake: 47.827% 
Loss D: 1.146 
Loss G: 0.6979 (0.6725) Acc G: 51.617% 
LR: 2.000e-04 

2023-03-02 01:46:13,065 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3623 (0.3718) Acc D Real: 73.555% 
Loss D Fake: 0.6964 (0.7225) Acc D Fake: 48.076% 
Loss D: 1.059 
Loss G: 0.6968 (0.6727) Acc G: 51.373% 
LR: 2.000e-04 

2023-03-02 01:46:13,072 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.2917 (0.3712) Acc D Real: 73.558% 
Loss D Fake: 0.6972 (0.7223) Acc D Fake: 48.321% 
Loss D: 0.989 
Loss G: 0.6966 (0.6729) Acc G: 51.131% 
LR: 2.000e-04 

2023-03-02 01:46:13,079 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3352 (0.3710) Acc D Real: 73.524% 
Loss D Fake: 0.6971 (0.7221) Acc D Fake: 48.563% 
Loss D: 1.032 
Loss G: 0.6970 (0.6731) Acc G: 50.894% 
LR: 2.000e-04 

2023-03-02 01:46:13,087 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3606 (0.3709) Acc D Real: 73.486% 
Loss D Fake: 0.6966 (0.7220) Acc D Fake: 48.801% 
Loss D: 1.057 
Loss G: 0.6975 (0.6733) Acc G: 50.659% 
LR: 2.000e-04 

2023-03-02 01:46:13,094 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3941 (0.3710) Acc D Real: 73.399% 
Loss D Fake: 0.6962 (0.7218) Acc D Fake: 49.036% 
Loss D: 1.090 
Loss G: 0.6977 (0.6734) Acc G: 50.429% 
LR: 2.000e-04 

2023-03-02 01:46:13,101 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3468 (0.3709) Acc D Real: 73.350% 
Loss D Fake: 0.6959 (0.7216) Acc D Fake: 49.267% 
Loss D: 1.043 
Loss G: 0.6982 (0.6736) Acc G: 50.201% 
LR: 2.000e-04 

2023-03-02 01:46:13,108 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3660 (0.3708) Acc D Real: 73.299% 
Loss D Fake: 0.6955 (0.7214) Acc D Fake: 49.495% 
Loss D: 1.061 
Loss G: 0.6983 (0.6738) Acc G: 49.977% 
LR: 2.000e-04 

2023-03-02 01:46:13,116 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4379 (0.3713) Acc D Real: 73.149% 
Loss D Fake: 0.6959 (0.7212) Acc D Fake: 49.720% 
Loss D: 1.134 
Loss G: 0.6975 (0.6739) Acc G: 49.755% 
LR: 2.000e-04 

2023-03-02 01:46:13,123 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3725 (0.3713) Acc D Real: 73.084% 
Loss D Fake: 0.6969 (0.7211) Acc D Fake: 49.942% 
Loss D: 1.069 
Loss G: 0.6967 (0.6741) Acc G: 49.537% 
LR: 2.000e-04 

2023-03-02 01:46:13,130 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.4021 (0.3715) Acc D Real: 72.995% 
Loss D Fake: 0.6978 (0.7209) Acc D Fake: 50.161% 
Loss D: 1.100 
Loss G: 0.6958 (0.6743) Acc G: 49.322% 
LR: 2.000e-04 

2023-03-02 01:46:13,137 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3826 (0.3716) Acc D Real: 72.936% 
Loss D Fake: 0.6988 (0.7208) Acc D Fake: 50.377% 
Loss D: 1.081 
Loss G: 0.6947 (0.6744) Acc G: 49.110% 
LR: 2.000e-04 

2023-03-02 01:46:13,144 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3127 (0.3712) Acc D Real: 72.926% 
Loss D Fake: 0.6998 (0.7206) Acc D Fake: 50.590% 
Loss D: 1.012 
Loss G: 0.6944 (0.6745) Acc G: 48.900% 
LR: 2.000e-04 

2023-03-02 01:46:13,152 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3041 (0.3707) Acc D Real: 72.915% 
Loss D Fake: 0.6997 (0.7205) Acc D Fake: 50.800% 
Loss D: 1.004 
Loss G: 0.6949 (0.6747) Acc G: 48.694% 
LR: 2.000e-04 

2023-03-02 01:46:13,159 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3748 (0.3708) Acc D Real: 72.852% 
Loss D Fake: 0.6991 (0.7203) Acc D Fake: 51.007% 
Loss D: 1.074 
Loss G: 0.6955 (0.6748) Acc G: 48.490% 
LR: 2.000e-04 

2023-03-02 01:46:13,166 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4155 (0.3711) Acc D Real: 72.746% 
Loss D Fake: 0.6988 (0.7202) Acc D Fake: 51.211% 
Loss D: 1.114 
Loss G: 0.6955 (0.6749) Acc G: 48.289% 
LR: 2.000e-04 

2023-03-02 01:46:13,174 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3745 (0.3711) Acc D Real: 72.708% 
Loss D Fake: 0.6988 (0.7200) Acc D Fake: 51.413% 
Loss D: 1.073 
Loss G: 0.6955 (0.6751) Acc G: 48.091% 
LR: 2.000e-04 

2023-03-02 01:46:13,181 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2806 (0.3705) Acc D Real: 72.707% 
Loss D Fake: 0.6987 (0.7199) Acc D Fake: 51.612% 
Loss D: 0.979 
Loss G: 0.6960 (0.6752) Acc G: 47.895% 
LR: 2.000e-04 

2023-03-02 01:46:13,188 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3312 (0.3702) Acc D Real: 72.699% 
Loss D Fake: 0.6980 (0.7198) Acc D Fake: 51.808% 
Loss D: 1.029 
Loss G: 0.6970 (0.6754) Acc G: 47.702% 
LR: 2.000e-04 

2023-03-02 01:46:13,195 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3248 (0.3700) Acc D Real: 72.660% 
Loss D Fake: 0.6969 (0.7196) Acc D Fake: 52.002% 
Loss D: 1.022 
Loss G: 0.6982 (0.6755) Acc G: 47.511% 
LR: 2.000e-04 

2023-03-02 01:46:13,203 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3244 (0.3697) Acc D Real: 72.654% 
Loss D Fake: 0.6956 (0.7195) Acc D Fake: 52.194% 
Loss D: 1.020 
Loss G: 0.6997 (0.6757) Acc G: 47.323% 
LR: 2.000e-04 

2023-03-02 01:46:13,210 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3124 (0.3693) Acc D Real: 72.664% 
Loss D Fake: 0.6940 (0.7193) Acc D Fake: 52.382% 
Loss D: 1.006 
Loss G: 0.7017 (0.6758) Acc G: 47.137% 
LR: 2.000e-04 

2023-03-02 01:46:13,217 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3139 (0.3689) Acc D Real: 72.666% 
Loss D Fake: 0.6918 (0.7191) Acc D Fake: 52.569% 
Loss D: 1.006 
Loss G: 0.7041 (0.6760) Acc G: 46.953% 
LR: 2.000e-04 

2023-03-02 01:46:13,224 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3486 (0.3688) Acc D Real: 72.627% 
Loss D Fake: 0.6895 (0.7189) Acc D Fake: 52.753% 
Loss D: 1.038 
Loss G: 0.7062 (0.6762) Acc G: 46.772% 
LR: 2.000e-04 

2023-03-02 01:46:13,231 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3704 (0.3688) Acc D Real: 72.582% 
Loss D Fake: 0.6876 (0.7187) Acc D Fake: 52.935% 
Loss D: 1.058 
Loss G: 0.7080 (0.6764) Acc G: 46.593% 
LR: 2.000e-04 

2023-03-02 01:46:13,239 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3783 (0.3689) Acc D Real: 72.535% 
Loss D Fake: 0.6861 (0.7185) Acc D Fake: 53.115% 
Loss D: 1.064 
Loss G: 0.7093 (0.6766) Acc G: 46.417% 
LR: 2.000e-04 

2023-03-02 01:46:13,246 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3367 (0.3687) Acc D Real: 72.513% 
Loss D Fake: 0.6849 (0.7183) Acc D Fake: 53.292% 
Loss D: 1.022 
Loss G: 0.7106 (0.6768) Acc G: 46.242% 
LR: 2.000e-04 

2023-03-02 01:46:13,253 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3400 (0.3685) Acc D Real: 72.494% 
Loss D Fake: 0.6837 (0.7181) Acc D Fake: 53.467% 
Loss D: 1.024 
Loss G: 0.7118 (0.6770) Acc G: 46.070% 
LR: 2.000e-04 

2023-03-02 01:46:13,261 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3721 (0.3685) Acc D Real: 72.439% 
Loss D Fake: 0.6826 (0.7179) Acc D Fake: 53.640% 
Loss D: 1.055 
Loss G: 0.7129 (0.6773) Acc G: 45.900% 
LR: 2.000e-04 

2023-03-02 01:46:13,269 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3470 (0.3684) Acc D Real: 72.418% 
Loss D Fake: 0.6815 (0.7177) Acc D Fake: 53.821% 
Loss D: 1.029 
Loss G: 0.7140 (0.6775) Acc G: 45.722% 
LR: 2.000e-04 

2023-03-02 01:46:13,277 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2791 (0.3679) Acc D Real: 72.458% 
Loss D Fake: 0.6802 (0.7174) Acc D Fake: 54.000% 
Loss D: 0.959 
Loss G: 0.7158 (0.6777) Acc G: 45.545% 
LR: 2.000e-04 

2023-03-02 01:46:13,284 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.2920 (0.3674) Acc D Real: 72.488% 
Loss D Fake: 0.6795 (0.7172) Acc D Fake: 54.167% 
Loss D: 0.971 
Loss G: 0.7144 (0.6779) Acc G: 45.382% 
LR: 2.000e-04 

2023-03-02 01:46:13,292 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3466 (0.3673) Acc D Real: 72.464% 
Loss D Fake: 0.6836 (0.7170) Acc D Fake: 54.321% 
Loss D: 1.030 
Loss G: 0.7105 (0.6781) Acc G: 45.240% 
LR: 2.000e-04 

2023-03-02 01:46:13,299 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3629 (0.3672) Acc D Real: 72.415% 
Loss D Fake: 0.6898 (0.7168) Acc D Fake: 54.464% 
Loss D: 1.053 
Loss G: 0.7053 (0.6783) Acc G: 45.109% 
LR: 2.000e-04 

2023-03-02 01:46:13,307 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.4480 (0.3677) Acc D Real: 72.332% 
Loss D Fake: 0.6965 (0.7167) Acc D Fake: 54.586% 
Loss D: 1.145 
Loss G: 0.7034 (0.6784) Acc G: 44.990% 
LR: 2.000e-04 

2023-03-02 01:46:13,314 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3885 (0.3678) Acc D Real: 72.308% 
Loss D Fake: 0.6937 (0.7166) Acc D Fake: 54.706% 
Loss D: 1.082 
Loss G: 0.7083 (0.6786) Acc G: 44.863% 
LR: 2.000e-04 

2023-03-02 01:46:13,321 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3906 (0.3680) Acc D Real: 72.286% 
Loss D Fake: 0.6865 (0.7164) Acc D Fake: 54.844% 
Loss D: 1.077 
Loss G: 0.7134 (0.6788) Acc G: 44.717% 
LR: 2.000e-04 

2023-03-02 01:46:13,329 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3389 (0.3678) Acc D Real: 72.276% 
Loss D Fake: 0.6813 (0.7162) Acc D Fake: 54.990% 
Loss D: 1.020 
Loss G: 0.7173 (0.6790) Acc G: 44.564% 
LR: 2.000e-04 

2023-03-02 01:46:13,336 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3346 (0.3676) Acc D Real: 72.258% 
Loss D Fake: 0.6773 (0.7160) Acc D Fake: 55.145% 
Loss D: 1.012 
Loss G: 0.7204 (0.6793) Acc G: 44.412% 
LR: 2.000e-04 

2023-03-02 01:46:13,344 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.2863 (0.3671) Acc D Real: 72.284% 
Loss D Fake: 0.6739 (0.7157) Acc D Fake: 55.307% 
Loss D: 0.960 
Loss G: 0.7236 (0.6795) Acc G: 44.253% 
LR: 2.000e-04 

2023-03-02 01:46:13,352 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3814 (0.3672) Acc D Real: 72.262% 
Loss D Fake: 0.6709 (0.7155) Acc D Fake: 55.467% 
Loss D: 1.052 
Loss G: 0.7259 (0.6798) Acc G: 44.095% 
LR: 2.000e-04 

2023-03-02 01:46:13,362 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3538 (0.3672) Acc D Real: 72.240% 
Loss D Fake: 0.6688 (0.7152) Acc D Fake: 55.625% 
Loss D: 1.023 
Loss G: 0.7278 (0.6801) Acc G: 43.939% 
LR: 2.000e-04 

2023-03-02 01:46:13,369 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.4784 (0.3678) Acc D Real: 72.147% 
Loss D Fake: 0.6674 (0.7150) Acc D Fake: 55.791% 
Loss D: 1.146 
Loss G: 0.7282 (0.6803) Acc G: 43.776% 
LR: 2.000e-04 

2023-03-02 01:46:13,377 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2667 (0.3672) Acc D Real: 72.185% 
Loss D Fake: 0.6670 (0.7147) Acc D Fake: 55.955% 
Loss D: 0.934 
Loss G: 0.7291 (0.6806) Acc G: 43.614% 
LR: 2.000e-04 

2023-03-02 01:46:13,384 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3872 (0.3673) Acc D Real: 72.135% 
Loss D Fake: 0.6660 (0.7144) Acc D Fake: 56.117% 
Loss D: 1.053 
Loss G: 0.7298 (0.6809) Acc G: 43.454% 
LR: 2.000e-04 

2023-03-02 01:46:13,391 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3425 (0.3672) Acc D Real: 72.110% 
Loss D Fake: 0.6654 (0.7141) Acc D Fake: 56.278% 
Loss D: 1.008 
Loss G: 0.7303 (0.6812) Acc G: 43.296% 
LR: 2.000e-04 

2023-03-02 01:46:13,399 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3894 (0.3673) Acc D Real: 72.048% 
Loss D Fake: 0.6650 (0.7139) Acc D Fake: 56.436% 
Loss D: 1.054 
Loss G: 0.7305 (0.6814) Acc G: 43.140% 
LR: 2.000e-04 

2023-03-02 01:46:13,406 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3947 (0.3675) Acc D Real: 71.981% 
Loss D Fake: 0.6650 (0.7136) Acc D Fake: 56.593% 
Loss D: 1.060 
Loss G: 0.7304 (0.6817) Acc G: 42.985% 
LR: 2.000e-04 

2023-03-02 01:46:13,413 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3231 (0.3672) Acc D Real: 71.993% 
Loss D Fake: 0.6651 (0.7133) Acc D Fake: 56.749% 
Loss D: 0.988 
Loss G: 0.7303 (0.6820) Acc G: 42.832% 
LR: 2.000e-04 

2023-03-02 01:46:13,420 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3637 (0.3672) Acc D Real: 71.959% 
Loss D Fake: 0.6652 (0.7131) Acc D Fake: 56.902% 
Loss D: 1.029 
Loss G: 0.7302 (0.6822) Acc G: 42.681% 
LR: 2.000e-04 

2023-03-02 01:46:13,428 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4397 (0.3676) Acc D Real: 71.881% 
Loss D Fake: 0.6656 (0.7128) Acc D Fake: 57.054% 
Loss D: 1.105 
Loss G: 0.7294 (0.6825) Acc G: 42.532% 
LR: 2.000e-04 

2023-03-02 01:46:13,436 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3825 (0.3677) Acc D Real: 71.846% 
Loss D Fake: 0.6665 (0.7126) Acc D Fake: 57.204% 
Loss D: 1.049 
Loss G: 0.7283 (0.6827) Acc G: 42.384% 
LR: 2.000e-04 

2023-03-02 01:46:13,443 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3990 (0.3678) Acc D Real: 71.784% 
Loss D Fake: 0.6675 (0.7123) Acc D Fake: 57.353% 
Loss D: 1.066 
Loss G: 0.7272 (0.6830) Acc G: 42.237% 
LR: 2.000e-04 

2023-03-02 01:46:13,450 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3691 (0.3678) Acc D Real: 71.763% 
Loss D Fake: 0.6686 (0.7121) Acc D Fake: 57.500% 
Loss D: 1.038 
Loss G: 0.7260 (0.6832) Acc G: 42.092% 
LR: 2.000e-04 

2023-03-02 01:46:13,457 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.4185 (0.3681) Acc D Real: 71.686% 
Loss D Fake: 0.6697 (0.7119) Acc D Fake: 57.646% 
Loss D: 1.088 
Loss G: 0.7246 (0.6834) Acc G: 41.949% 
LR: 2.000e-04 

2023-03-02 01:46:13,464 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3601 (0.3681) Acc D Real: 71.654% 
Loss D Fake: 0.6711 (0.7117) Acc D Fake: 57.789% 
Loss D: 1.031 
Loss G: 0.7233 (0.6836) Acc G: 41.807% 
LR: 2.000e-04 

2023-03-02 01:46:13,472 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3765 (0.3681) Acc D Real: 71.605% 
Loss D Fake: 0.6722 (0.7114) Acc D Fake: 57.932% 
Loss D: 1.049 
Loss G: 0.7222 (0.6838) Acc G: 41.667% 
LR: 2.000e-04 

2023-03-02 01:46:13,479 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.3551 (0.3680) Acc D Real: 71.573% 
Loss D Fake: 0.6731 (0.7112) Acc D Fake: 58.073% 
Loss D: 1.028 
Loss G: 0.7213 (0.6840) Acc G: 41.528% 
LR: 2.000e-04 

2023-03-02 01:46:13,486 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3017 (0.3677) Acc D Real: 71.570% 
Loss D Fake: 0.6736 (0.7111) Acc D Fake: 58.212% 
Loss D: 0.975 
Loss G: 0.7212 (0.6842) Acc G: 41.390% 
LR: 2.000e-04 

2023-03-02 01:46:13,494 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.4203 (0.3680) Acc D Real: 71.491% 
Loss D Fake: 0.6736 (0.7109) Acc D Fake: 58.351% 
Loss D: 1.094 
Loss G: 0.7208 (0.6844) Acc G: 41.254% 
LR: 2.000e-04 

2023-03-02 01:46:13,501 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.4195 (0.3682) Acc D Real: 71.408% 
Loss D Fake: 0.6743 (0.7107) Acc D Fake: 58.487% 
Loss D: 1.094 
Loss G: 0.7198 (0.6846) Acc G: 41.120% 
LR: 2.000e-04 

2023-03-02 01:46:13,508 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4137 (0.3685) Acc D Real: 71.354% 
Loss D Fake: 0.6754 (0.7105) Acc D Fake: 58.622% 
Loss D: 1.089 
Loss G: 0.7184 (0.6848) Acc G: 40.986% 
LR: 2.000e-04 

2023-03-02 01:46:13,516 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4111 (0.3687) Acc D Real: 71.300% 
Loss D Fake: 0.6770 (0.7103) Acc D Fake: 58.756% 
Loss D: 1.088 
Loss G: 0.7165 (0.6849) Acc G: 40.854% 
LR: 2.000e-04 

2023-03-02 01:46:13,523 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3537 (0.3686) Acc D Real: 71.268% 
Loss D Fake: 0.6787 (0.7102) Acc D Fake: 58.880% 
Loss D: 1.032 
Loss G: 0.7150 (0.6851) Acc G: 40.732% 
LR: 2.000e-04 

2023-03-02 01:46:13,530 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2288 (0.3679) Acc D Real: 71.331% 
Loss D Fake: 0.6796 (0.7100) Acc D Fake: 59.003% 
Loss D: 0.908 
Loss G: 0.7151 (0.6852) Acc G: 40.611% 
LR: 2.000e-04 

2023-03-02 01:46:13,537 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.4031 (0.3681) Acc D Real: 71.282% 
Loss D Fake: 0.6792 (0.7099) Acc D Fake: 59.125% 
Loss D: 1.082 
Loss G: 0.7151 (0.6854) Acc G: 40.492% 
LR: 2.000e-04 

2023-03-02 01:46:13,544 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3584 (0.3680) Acc D Real: 71.261% 
Loss D Fake: 0.6797 (0.7097) Acc D Fake: 59.245% 
Loss D: 1.038 
Loss G: 0.7141 (0.6855) Acc G: 40.373% 
LR: 2.000e-04 

2023-03-02 01:46:13,552 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3751 (0.3681) Acc D Real: 71.234% 
Loss D Fake: 0.6812 (0.7096) Acc D Fake: 59.365% 
Loss D: 1.056 
Loss G: 0.7129 (0.6857) Acc G: 40.256% 
LR: 2.000e-04 

2023-03-02 01:46:13,559 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2957 (0.3677) Acc D Real: 71.247% 
Loss D Fake: 0.6825 (0.7094) Acc D Fake: 59.475% 
Loss D: 0.978 
Loss G: 0.7123 (0.6858) Acc G: 40.148% 
LR: 2.000e-04 

2023-03-02 01:46:13,566 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3678 (0.3677) Acc D Real: 71.211% 
Loss D Fake: 0.6830 (0.7093) Acc D Fake: 59.575% 
Loss D: 1.051 
Loss G: 0.7121 (0.6859) Acc G: 40.049% 
LR: 2.000e-04 

2023-03-02 01:46:13,573 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3529 (0.3676) Acc D Real: 71.173% 
Loss D Fake: 0.6832 (0.7092) Acc D Fake: 59.667% 
Loss D: 1.036 
Loss G: 0.7122 (0.6860) Acc G: 39.959% 
LR: 2.000e-04 

2023-03-02 01:46:13,581 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3838 (0.3677) Acc D Real: 71.150% 
Loss D Fake: 0.6832 (0.7090) Acc D Fake: 59.757% 
Loss D: 1.067 
Loss G: 0.7123 (0.6862) Acc G: 39.871% 
LR: 2.000e-04 

2023-03-02 01:46:13,588 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3222 (0.3675) Acc D Real: 71.138% 
Loss D Fake: 0.6830 (0.7089) Acc D Fake: 59.847% 
Loss D: 1.005 
Loss G: 0.7129 (0.6863) Acc G: 39.783% 
LR: 2.000e-04 

2023-03-02 01:46:13,595 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3717 (0.3675) Acc D Real: 71.125% 
Loss D Fake: 0.6823 (0.7088) Acc D Fake: 59.936% 
Loss D: 1.054 
Loss G: 0.7134 (0.6864) Acc G: 39.696% 
LR: 2.000e-04 

2023-03-02 01:46:13,602 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.2404 (0.3669) Acc D Real: 71.166% 
Loss D Fake: 0.6823 (0.7087) Acc D Fake: 60.024% 
Loss D: 0.923 
Loss G: 0.7128 (0.6866) Acc G: 39.609% 
LR: 2.000e-04 

2023-03-02 01:46:13,609 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.3698 (0.3669) Acc D Real: 71.140% 
Loss D Fake: 0.6841 (0.7085) Acc D Fake: 60.111% 
Loss D: 1.054 
Loss G: 0.7116 (0.6867) Acc G: 39.532% 
LR: 2.000e-04 

2023-03-02 01:46:13,616 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3838 (0.3670) Acc D Real: 71.124% 
Loss D Fake: 0.6859 (0.7084) Acc D Fake: 60.190% 
Loss D: 1.070 
Loss G: 0.7104 (0.6868) Acc G: 39.455% 
LR: 2.000e-04 

2023-03-02 01:46:13,623 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.2780 (0.3666) Acc D Real: 71.155% 
Loss D Fake: 0.6874 (0.7083) Acc D Fake: 60.267% 
Loss D: 0.965 
Loss G: 0.7098 (0.6869) Acc G: 39.387% 
LR: 2.000e-04 

2023-03-02 01:46:13,631 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3122 (0.3663) Acc D Real: 71.168% 
Loss D Fake: 0.6876 (0.7082) Acc D Fake: 60.336% 
Loss D: 1.000 
Loss G: 0.7111 (0.6870) Acc G: 39.319% 
LR: 2.000e-04 

2023-03-02 01:46:13,638 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3238 (0.3661) Acc D Real: 71.179% 
Loss D Fake: 0.6853 (0.7081) Acc D Fake: 60.405% 
Loss D: 1.009 
Loss G: 0.7144 (0.6871) Acc G: 39.245% 
LR: 2.000e-04 

2023-03-02 01:46:13,645 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.4239 (0.3664) Acc D Real: 71.121% 
Loss D Fake: 0.6817 (0.7080) Acc D Fake: 60.481% 
Loss D: 1.106 
Loss G: 0.7171 (0.6873) Acc G: 39.171% 
LR: 2.000e-04 

2023-03-02 01:46:13,652 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2910 (0.3661) Acc D Real: 71.168% 
Loss D Fake: 0.6788 (0.7079) Acc D Fake: 60.556% 
Loss D: 0.970 
Loss G: 0.7203 (0.6874) Acc G: 39.090% 
LR: 2.000e-04 

2023-03-02 01:46:13,659 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2818 (0.3657) Acc D Real: 71.179% 
Loss D Fake: 0.6750 (0.7077) Acc D Fake: 60.637% 
Loss D: 0.957 
Loss G: 0.7242 (0.6876) Acc G: 39.009% 
LR: 2.000e-04 

2023-03-02 01:46:13,666 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2214 (0.3650) Acc D Real: 71.237% 
Loss D Fake: 0.6706 (0.7076) Acc D Fake: 60.719% 
Loss D: 0.892 
Loss G: 0.7292 (0.6878) Acc G: 38.922% 
LR: 2.000e-04 

2023-03-02 01:46:13,674 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2677 (0.3646) Acc D Real: 71.250% 
Loss D Fake: 0.6653 (0.7074) Acc D Fake: 60.807% 
Loss D: 0.933 
Loss G: 0.7347 (0.6880) Acc G: 38.836% 
LR: 2.000e-04 

2023-03-02 01:46:13,681 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3274 (0.3644) Acc D Real: 71.249% 
Loss D Fake: 0.6601 (0.7071) Acc D Fake: 60.894% 
Loss D: 0.988 
Loss G: 0.7398 (0.6882) Acc G: 38.750% 
LR: 2.000e-04 

2023-03-02 01:46:13,688 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3189 (0.3642) Acc D Real: 71.250% 
Loss D Fake: 0.6567 (0.7069) Acc D Fake: 60.980% 
Loss D: 0.976 
Loss G: 0.7411 (0.6885) Acc G: 38.665% 
LR: 2.000e-04 

2023-03-02 01:46:13,695 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3511 (0.3641) Acc D Real: 71.234% 
Loss D Fake: 0.6577 (0.7067) Acc D Fake: 61.066% 
Loss D: 1.009 
Loss G: 0.7406 (0.6887) Acc G: 38.589% 
LR: 2.000e-04 

2023-03-02 01:46:13,703 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3594 (0.3641) Acc D Real: 71.221% 
Loss D Fake: 0.6593 (0.7065) Acc D Fake: 61.143% 
Loss D: 1.019 
Loss G: 0.7398 (0.6889) Acc G: 38.513% 
LR: 2.000e-04 

2023-03-02 01:46:13,710 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3227 (0.3639) Acc D Real: 71.210% 
Loss D Fake: 0.6604 (0.7063) Acc D Fake: 61.213% 
Loss D: 0.983 
Loss G: 0.7400 (0.6892) Acc G: 38.445% 
LR: 2.000e-04 

2023-03-02 01:46:13,717 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.4505 (0.3643) Acc D Real: 71.167% 
Loss D Fake: 0.6606 (0.7061) Acc D Fake: 61.281% 
Loss D: 1.111 
Loss G: 0.7400 (0.6894) Acc G: 38.378% 
LR: 2.000e-04 

2023-03-02 01:46:13,724 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.1715 (0.3635) Acc D Real: 71.186% 
Loss D Fake: 0.6596 (0.7059) Acc D Fake: 61.299% 
Loss D: 0.831 
Loss G: 0.7436 (0.6896) Acc G: 38.361% 
LR: 2.000e-04 

2023-03-02 01:46:13,733 -                train: [    INFO] - 
Epoch: 14/20
2023-03-02 01:46:13,906 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2872 (0.2983) Acc D Real: 76.536% 
Loss D Fake: 0.6521 (0.6538) Acc D Fake: 77.500% 
Loss D: 0.939 
Loss G: 0.7516 (0.7493) Acc G: 21.667% 
LR: 2.000e-04 

2023-03-02 01:46:13,915 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3485 (0.3150) Acc D Real: 75.469% 
Loss D Fake: 0.6509 (0.6528) Acc D Fake: 77.778% 
Loss D: 0.999 
Loss G: 0.7451 (0.7479) Acc G: 22.222% 
LR: 2.000e-04 

2023-03-02 01:46:13,956 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2907 (0.3089) Acc D Real: 76.419% 
Loss D Fake: 0.6741 (0.6581) Acc D Fake: 75.833% 
Loss D: 0.965 
Loss G: 0.7467 (0.7476) Acc G: 22.500% 
LR: 2.000e-04 

2023-03-02 01:46:13,963 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3728 (0.3217) Acc D Real: 75.198% 
Loss D Fake: 0.6487 (0.6562) Acc D Fake: 76.333% 
Loss D: 1.022 
Loss G: 0.7598 (0.7500) Acc G: 22.000% 
LR: 2.000e-04 

2023-03-02 01:46:13,969 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3480 (0.3261) Acc D Real: 73.958% 
Loss D Fake: 0.6385 (0.6533) Acc D Fake: 77.222% 
Loss D: 0.986 
Loss G: 0.7675 (0.7530) Acc G: 21.389% 
LR: 2.000e-04 

2023-03-02 01:46:13,977 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3476 (0.3292) Acc D Real: 73.512% 
Loss D Fake: 0.6326 (0.6503) Acc D Fake: 77.857% 
Loss D: 0.980 
Loss G: 0.7717 (0.7556) Acc G: 20.714% 
LR: 2.000e-04 

2023-03-02 01:46:13,991 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.4416 (0.3432) Acc D Real: 71.712% 
Loss D Fake: 0.6296 (0.6477) Acc D Fake: 78.542% 
Loss D: 1.071 
Loss G: 0.7739 (0.7579) Acc G: 20.208% 
LR: 2.000e-04 

2023-03-02 01:46:14,001 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2885 (0.3371) Acc D Real: 72.714% 
Loss D Fake: 0.6277 (0.6455) Acc D Fake: 79.074% 
Loss D: 0.916 
Loss G: 0.7760 (0.7599) Acc G: 19.815% 
LR: 2.000e-04 

2023-03-02 01:46:14,008 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.3967 (0.3431) Acc D Real: 72.073% 
Loss D Fake: 0.6259 (0.6435) Acc D Fake: 79.500% 
Loss D: 1.023 
Loss G: 0.7773 (0.7617) Acc G: 19.500% 
LR: 2.000e-04 

2023-03-02 01:46:14,015 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.2820 (0.3375) Acc D Real: 72.836% 
Loss D Fake: 0.6248 (0.6418) Acc D Fake: 79.848% 
Loss D: 0.907 
Loss G: 0.7788 (0.7632) Acc G: 19.091% 
LR: 2.000e-04 

2023-03-02 01:46:14,024 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3957 (0.3424) Acc D Real: 71.940% 
Loss D Fake: 0.6234 (0.6403) Acc D Fake: 80.278% 
Loss D: 1.019 
Loss G: 0.7798 (0.7646) Acc G: 18.750% 
LR: 2.000e-04 

2023-03-02 01:46:14,031 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2956 (0.3388) Acc D Real: 72.436% 
Loss D Fake: 0.6226 (0.6389) Acc D Fake: 80.641% 
Loss D: 0.918 
Loss G: 0.7810 (0.7659) Acc G: 18.462% 
LR: 2.000e-04 

2023-03-02 01:46:14,038 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3080 (0.3366) Acc D Real: 72.493% 
Loss D Fake: 0.6214 (0.6377) Acc D Fake: 80.952% 
Loss D: 0.929 
Loss G: 0.7825 (0.7670) Acc G: 18.214% 
LR: 2.000e-04 

2023-03-02 01:46:14,045 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3545 (0.3378) Acc D Real: 72.149% 
Loss D Fake: 0.6200 (0.6365) Acc D Fake: 81.222% 
Loss D: 0.975 
Loss G: 0.7839 (0.7682) Acc G: 18.000% 
LR: 2.000e-04 

2023-03-02 01:46:14,052 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.4927 (0.3475) Acc D Real: 71.090% 
Loss D Fake: 0.6194 (0.6354) Acc D Fake: 81.458% 
Loss D: 1.112 
Loss G: 0.7834 (0.7691) Acc G: 17.812% 
LR: 2.000e-04 

2023-03-02 01:46:14,061 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3875 (0.3498) Acc D Real: 70.613% 
Loss D Fake: 0.6203 (0.6345) Acc D Fake: 81.667% 
Loss D: 1.008 
Loss G: 0.7822 (0.7699) Acc G: 17.647% 
LR: 2.000e-04 

2023-03-02 01:46:14,068 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2808 (0.3460) Acc D Real: 71.169% 
Loss D Fake: 0.6211 (0.6338) Acc D Fake: 81.852% 
Loss D: 0.902 
Loss G: 0.7818 (0.7706) Acc G: 17.500% 
LR: 2.000e-04 

2023-03-02 01:46:14,075 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3233 (0.3448) Acc D Real: 71.261% 
Loss D Fake: 0.6211 (0.6331) Acc D Fake: 82.018% 
Loss D: 0.944 
Loss G: 0.7820 (0.7712) Acc G: 17.368% 
LR: 2.000e-04 

2023-03-02 01:46:14,082 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3896 (0.3470) Acc D Real: 70.826% 
Loss D Fake: 0.6210 (0.6325) Acc D Fake: 82.167% 
Loss D: 1.011 
Loss G: 0.7817 (0.7717) Acc G: 17.250% 
LR: 2.000e-04 

2023-03-02 01:46:14,089 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.4693 (0.3529) Acc D Real: 70.064% 
Loss D Fake: 0.6217 (0.6320) Acc D Fake: 82.302% 
Loss D: 1.091 
Loss G: 0.7801 (0.7721) Acc G: 17.143% 
LR: 2.000e-04 

2023-03-02 01:46:14,096 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3294 (0.3518) Acc D Real: 70.066% 
Loss D Fake: 0.6231 (0.6316) Acc D Fake: 82.424% 
Loss D: 0.952 
Loss G: 0.7787 (0.7724) Acc G: 17.045% 
LR: 2.000e-04 

2023-03-02 01:46:14,103 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3569 (0.3520) Acc D Real: 70.068% 
Loss D Fake: 0.6241 (0.6313) Acc D Fake: 82.536% 
Loss D: 0.981 
Loss G: 0.7776 (0.7726) Acc G: 16.957% 
LR: 2.000e-04 

2023-03-02 01:46:14,110 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.4259 (0.3551) Acc D Real: 69.648% 
Loss D Fake: 0.6251 (0.6310) Acc D Fake: 82.639% 
Loss D: 1.051 
Loss G: 0.7759 (0.7727) Acc G: 16.875% 
LR: 2.000e-04 

2023-03-02 01:46:14,117 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.4545 (0.3591) Acc D Real: 69.152% 
Loss D Fake: 0.6269 (0.6309) Acc D Fake: 82.733% 
Loss D: 1.081 
Loss G: 0.7731 (0.7728) Acc G: 16.800% 
LR: 2.000e-04 

2023-03-02 01:46:14,124 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3249 (0.3578) Acc D Real: 69.375% 
Loss D Fake: 0.6293 (0.6308) Acc D Fake: 82.756% 
Loss D: 0.954 
Loss G: 0.7707 (0.7727) Acc G: 16.795% 
LR: 2.000e-04 

2023-03-02 01:46:14,131 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.2929 (0.3554) Acc D Real: 69.614% 
Loss D Fake: 0.6309 (0.6308) Acc D Fake: 82.778% 
Loss D: 0.924 
Loss G: 0.7694 (0.7726) Acc G: 16.790% 
LR: 2.000e-04 

2023-03-02 01:46:14,139 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2781 (0.3526) Acc D Real: 69.827% 
Loss D Fake: 0.6315 (0.6308) Acc D Fake: 82.798% 
Loss D: 0.910 
Loss G: 0.7693 (0.7724) Acc G: 16.786% 
LR: 2.000e-04 

2023-03-02 01:46:14,147 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3342 (0.3520) Acc D Real: 69.774% 
Loss D Fake: 0.6314 (0.6308) Acc D Fake: 82.816% 
Loss D: 0.966 
Loss G: 0.7695 (0.7723) Acc G: 16.782% 
LR: 2.000e-04 

2023-03-02 01:46:14,154 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.4053 (0.3537) Acc D Real: 69.542% 
Loss D Fake: 0.6314 (0.6309) Acc D Fake: 82.833% 
Loss D: 1.037 
Loss G: 0.7690 (0.7722) Acc G: 16.778% 
LR: 2.000e-04 

2023-03-02 01:46:14,161 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2934 (0.3518) Acc D Real: 69.829% 
Loss D Fake: 0.6326 (0.6309) Acc D Fake: 82.849% 
Loss D: 0.926 
Loss G: 0.7669 (0.7721) Acc G: 16.828% 
LR: 2.000e-04 

2023-03-02 01:46:14,169 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3274 (0.3510) Acc D Real: 69.945% 
Loss D Fake: 0.6356 (0.6311) Acc D Fake: 82.812% 
Loss D: 0.963 
Loss G: 0.7641 (0.7718) Acc G: 16.875% 
LR: 2.000e-04 

2023-03-02 01:46:14,176 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3481 (0.3509) Acc D Real: 69.998% 
Loss D Fake: 0.6395 (0.6313) Acc D Fake: 82.727% 
Loss D: 0.988 
Loss G: 0.7599 (0.7714) Acc G: 16.970% 
LR: 2.000e-04 

2023-03-02 01:46:14,183 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3942 (0.3522) Acc D Real: 69.874% 
Loss D Fake: 0.6454 (0.6317) Acc D Fake: 82.598% 
Loss D: 1.040 
Loss G: 0.7537 (0.7709) Acc G: 17.157% 
LR: 2.000e-04 

2023-03-02 01:46:14,190 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3235 (0.3514) Acc D Real: 70.030% 
Loss D Fake: 0.6531 (0.6323) Acc D Fake: 82.429% 
Loss D: 0.977 
Loss G: 0.7481 (0.7703) Acc G: 17.381% 
LR: 2.000e-04 

2023-03-02 01:46:14,197 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.2775 (0.3493) Acc D Real: 70.269% 
Loss D Fake: 0.6570 (0.6330) Acc D Fake: 82.222% 
Loss D: 0.935 
Loss G: 0.7499 (0.7697) Acc G: 17.593% 
LR: 2.000e-04 

2023-03-02 01:46:14,205 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2380 (0.3463) Acc D Real: 70.564% 
Loss D Fake: 0.6503 (0.6335) Acc D Fake: 82.072% 
Loss D: 0.888 
Loss G: 0.7579 (0.7694) Acc G: 17.703% 
LR: 2.000e-04 

2023-03-02 01:46:14,212 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2860 (0.3447) Acc D Real: 70.696% 
Loss D Fake: 0.6414 (0.6337) Acc D Fake: 81.974% 
Loss D: 0.927 
Loss G: 0.7661 (0.7693) Acc G: 17.763% 
LR: 2.000e-04 

2023-03-02 01:46:14,219 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3492 (0.3449) Acc D Real: 70.637% 
Loss D Fake: 0.6340 (0.6337) Acc D Fake: 81.923% 
Loss D: 0.983 
Loss G: 0.7725 (0.7694) Acc G: 17.821% 
LR: 2.000e-04 

2023-03-02 01:46:14,226 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2388 (0.3422) Acc D Real: 70.906% 
Loss D Fake: 0.6283 (0.6336) Acc D Fake: 81.917% 
Loss D: 0.867 
Loss G: 0.7786 (0.7696) Acc G: 17.833% 
LR: 2.000e-04 

2023-03-02 01:46:14,233 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4072 (0.3438) Acc D Real: 70.761% 
Loss D Fake: 0.6230 (0.6333) Acc D Fake: 81.911% 
Loss D: 1.030 
Loss G: 0.7832 (0.7700) Acc G: 17.846% 
LR: 2.000e-04 

2023-03-02 01:46:14,240 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.4680 (0.3468) Acc D Real: 70.444% 
Loss D Fake: 0.6200 (0.6330) Acc D Fake: 81.905% 
Loss D: 1.088 
Loss G: 0.7848 (0.7703) Acc G: 17.817% 
LR: 2.000e-04 

2023-03-02 01:46:14,248 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3048 (0.3458) Acc D Real: 70.566% 
Loss D Fake: 0.6191 (0.6327) Acc D Fake: 81.938% 
Loss D: 0.924 
Loss G: 0.7861 (0.7707) Acc G: 17.791% 
LR: 2.000e-04 

2023-03-02 01:46:14,255 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.2991 (0.3447) Acc D Real: 70.631% 
Loss D Fake: 0.6178 (0.6323) Acc D Fake: 81.970% 
Loss D: 0.917 
Loss G: 0.7878 (0.7711) Acc G: 17.765% 
LR: 2.000e-04 

2023-03-02 01:46:14,262 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3759 (0.3454) Acc D Real: 70.573% 
Loss D Fake: 0.6163 (0.6320) Acc D Fake: 82.000% 
Loss D: 0.992 
Loss G: 0.7891 (0.7715) Acc G: 17.741% 
LR: 2.000e-04 

2023-03-02 01:46:14,269 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3535 (0.3456) Acc D Real: 70.503% 
Loss D Fake: 0.6153 (0.6316) Acc D Fake: 82.029% 
Loss D: 0.969 
Loss G: 0.7900 (0.7719) Acc G: 17.717% 
LR: 2.000e-04 

2023-03-02 01:46:14,276 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4036 (0.3468) Acc D Real: 70.331% 
Loss D Fake: 0.6147 (0.6313) Acc D Fake: 82.057% 
Loss D: 1.018 
Loss G: 0.7902 (0.7723) Acc G: 17.695% 
LR: 2.000e-04 

2023-03-02 01:46:14,283 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.4788 (0.3496) Acc D Real: 69.990% 
Loss D Fake: 0.6152 (0.6309) Acc D Fake: 82.083% 
Loss D: 1.094 
Loss G: 0.7884 (0.7726) Acc G: 17.674% 
LR: 2.000e-04 

2023-03-02 01:46:14,291 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3888 (0.3504) Acc D Real: 69.889% 
Loss D Fake: 0.6171 (0.6306) Acc D Fake: 82.109% 
Loss D: 1.006 
Loss G: 0.7858 (0.7729) Acc G: 17.653% 
LR: 2.000e-04 

2023-03-02 01:46:14,298 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.4096 (0.3516) Acc D Real: 69.694% 
Loss D Fake: 0.6194 (0.6304) Acc D Fake: 82.133% 
Loss D: 1.029 
Loss G: 0.7826 (0.7731) Acc G: 17.633% 
LR: 2.000e-04 

2023-03-02 01:46:14,305 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3798 (0.3521) Acc D Real: 69.610% 
Loss D Fake: 0.6221 (0.6303) Acc D Fake: 82.157% 
Loss D: 1.002 
Loss G: 0.7794 (0.7732) Acc G: 17.614% 
LR: 2.000e-04 

2023-03-02 01:46:14,312 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2692 (0.3505) Acc D Real: 69.822% 
Loss D Fake: 0.6243 (0.6301) Acc D Fake: 82.179% 
Loss D: 0.894 
Loss G: 0.7777 (0.7733) Acc G: 17.596% 
LR: 2.000e-04 

2023-03-02 01:46:14,320 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4542 (0.3525) Acc D Real: 69.533% 
Loss D Fake: 0.6270 (0.6301) Acc D Fake: 82.201% 
Loss D: 1.081 
Loss G: 0.7711 (0.7732) Acc G: 17.610% 
LR: 2.000e-04 

2023-03-02 01:46:14,327 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3157 (0.3518) Acc D Real: 69.603% 
Loss D Fake: 0.6364 (0.6302) Acc D Fake: 82.160% 
Loss D: 0.952 
Loss G: 0.7606 (0.7730) Acc G: 17.685% 
LR: 2.000e-04 

2023-03-02 01:46:14,334 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2704 (0.3503) Acc D Real: 69.799% 
Loss D Fake: 0.6512 (0.6306) Acc D Fake: 82.061% 
Loss D: 0.922 
Loss G: 0.7411 (0.7724) Acc G: 17.848% 
LR: 2.000e-04 

2023-03-02 01:46:14,341 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2809 (0.3491) Acc D Real: 69.913% 
Loss D Fake: 1.2346 (0.6414) Acc D Fake: 80.625% 
Loss D: 1.515 
Loss G: 0.7579 (0.7722) Acc G: 17.917% 
LR: 2.000e-04 

2023-03-02 01:46:14,349 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3580 (0.3492) Acc D Real: 69.837% 
Loss D Fake: 0.6359 (0.6413) Acc D Fake: 80.614% 
Loss D: 0.994 
Loss G: 0.7720 (0.7721) Acc G: 17.924% 
LR: 2.000e-04 

2023-03-02 01:46:14,356 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2937 (0.3483) Acc D Real: 69.944% 
Loss D Fake: 0.6268 (0.6410) Acc D Fake: 80.661% 
Loss D: 0.921 
Loss G: 0.7784 (0.7723) Acc G: 17.902% 
LR: 2.000e-04 

2023-03-02 01:46:14,363 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4178 (0.3494) Acc D Real: 69.816% 
Loss D Fake: 0.6218 (0.6407) Acc D Fake: 80.706% 
Loss D: 1.040 
Loss G: 0.7818 (0.7724) Acc G: 17.853% 
LR: 2.000e-04 

2023-03-02 01:46:14,370 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.5375 (0.3526) Acc D Real: 69.424% 
Loss D Fake: 0.6200 (0.6404) Acc D Fake: 80.778% 
Loss D: 1.158 
Loss G: 0.7813 (0.7726) Acc G: 17.806% 
LR: 2.000e-04 

2023-03-02 01:46:14,378 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3536 (0.3526) Acc D Real: 69.385% 
Loss D Fake: 0.6208 (0.6400) Acc D Fake: 80.847% 
Loss D: 0.974 
Loss G: 0.7798 (0.7727) Acc G: 17.760% 
LR: 2.000e-04 

2023-03-02 01:46:14,385 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3719 (0.3529) Acc D Real: 69.396% 
Loss D Fake: 0.6218 (0.6397) Acc D Fake: 80.914% 
Loss D: 0.994 
Loss G: 0.7781 (0.7728) Acc G: 17.715% 
LR: 2.000e-04 

2023-03-02 01:46:14,392 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3937 (0.3536) Acc D Real: 69.243% 
Loss D Fake: 0.6232 (0.6395) Acc D Fake: 80.979% 
Loss D: 1.017 
Loss G: 0.7759 (0.7728) Acc G: 17.672% 
LR: 2.000e-04 

2023-03-02 01:46:14,399 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3208 (0.3530) Acc D Real: 69.252% 
Loss D Fake: 0.6248 (0.6392) Acc D Fake: 81.042% 
Loss D: 0.946 
Loss G: 0.7742 (0.7728) Acc G: 17.630% 
LR: 2.000e-04 

2023-03-02 01:46:14,406 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3930 (0.3537) Acc D Real: 69.150% 
Loss D Fake: 0.6261 (0.6390) Acc D Fake: 81.103% 
Loss D: 1.019 
Loss G: 0.7723 (0.7728) Acc G: 17.590% 
LR: 2.000e-04 

2023-03-02 01:46:14,414 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4845 (0.3556) Acc D Real: 68.845% 
Loss D Fake: 0.6281 (0.6389) Acc D Fake: 81.162% 
Loss D: 1.113 
Loss G: 0.7687 (0.7728) Acc G: 17.551% 
LR: 2.000e-04 

2023-03-02 01:46:14,421 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.4204 (0.3566) Acc D Real: 68.670% 
Loss D Fake: 0.6316 (0.6388) Acc D Fake: 81.219% 
Loss D: 1.052 
Loss G: 0.7642 (0.7726) Acc G: 17.512% 
LR: 2.000e-04 

2023-03-02 01:46:14,428 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4241 (0.3576) Acc D Real: 68.456% 
Loss D Fake: 0.6356 (0.6387) Acc D Fake: 81.275% 
Loss D: 1.060 
Loss G: 0.7592 (0.7724) Acc G: 17.475% 
LR: 2.000e-04 

2023-03-02 01:46:14,435 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.3851 (0.3580) Acc D Real: 68.370% 
Loss D Fake: 0.6399 (0.6387) Acc D Fake: 81.329% 
Loss D: 1.025 
Loss G: 0.7543 (0.7722) Acc G: 17.440% 
LR: 2.000e-04 

2023-03-02 01:46:14,442 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3821 (0.3583) Acc D Real: 68.244% 
Loss D Fake: 0.6440 (0.6388) Acc D Fake: 81.381% 
Loss D: 1.026 
Loss G: 0.7496 (0.7719) Acc G: 17.405% 
LR: 2.000e-04 

2023-03-02 01:46:14,450 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.4626 (0.3598) Acc D Real: 67.992% 
Loss D Fake: 0.6484 (0.6389) Acc D Fake: 81.432% 
Loss D: 1.111 
Loss G: 0.7440 (0.7715) Acc G: 17.371% 
LR: 2.000e-04 

2023-03-02 01:46:14,457 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3790 (0.3601) Acc D Real: 67.880% 
Loss D Fake: 0.6535 (0.6392) Acc D Fake: 81.481% 
Loss D: 1.032 
Loss G: 0.7386 (0.7710) Acc G: 17.338% 
LR: 2.000e-04 

2023-03-02 01:46:14,464 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3332 (0.3597) Acc D Real: 67.847% 
Loss D Fake: 0.6579 (0.6394) Acc D Fake: 81.530% 
Loss D: 0.991 
Loss G: 0.7343 (0.7705) Acc G: 17.306% 
LR: 2.000e-04 

2023-03-02 01:46:14,471 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.2719 (0.3585) Acc D Real: 67.960% 
Loss D Fake: 0.6609 (0.6397) Acc D Fake: 81.577% 
Loss D: 0.933 
Loss G: 0.7322 (0.7700) Acc G: 17.275% 
LR: 2.000e-04 

2023-03-02 01:46:14,478 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.2332 (0.3569) Acc D Real: 68.102% 
Loss D Fake: 0.6617 (0.6400) Acc D Fake: 81.622% 
Loss D: 0.895 
Loss G: 0.7327 (0.7695) Acc G: 17.244% 
LR: 2.000e-04 

2023-03-02 01:46:14,485 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3656 (0.3570) Acc D Real: 68.035% 
Loss D Fake: 0.6608 (0.6403) Acc D Fake: 81.667% 
Loss D: 1.026 
Loss G: 0.7335 (0.7690) Acc G: 17.215% 
LR: 2.000e-04 

2023-03-02 01:46:14,492 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3695 (0.3571) Acc D Real: 67.944% 
Loss D Fake: 0.6603 (0.6405) Acc D Fake: 81.710% 
Loss D: 1.030 
Loss G: 0.7338 (0.7686) Acc G: 17.186% 
LR: 2.000e-04 

2023-03-02 01:46:14,500 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3455 (0.3570) Acc D Real: 67.897% 
Loss D Fake: 0.6601 (0.6408) Acc D Fake: 81.752% 
Loss D: 1.006 
Loss G: 0.7341 (0.7681) Acc G: 17.158% 
LR: 2.000e-04 

2023-03-02 01:46:14,507 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3353 (0.3567) Acc D Real: 67.871% 
Loss D Fake: 0.6599 (0.6410) Acc D Fake: 81.772% 
Loss D: 0.995 
Loss G: 0.7345 (0.7677) Acc G: 17.152% 
LR: 2.000e-04 

2023-03-02 01:46:14,514 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4672 (0.3581) Acc D Real: 67.656% 
Loss D Fake: 0.6600 (0.6413) Acc D Fake: 81.792% 
Loss D: 1.127 
Loss G: 0.7333 (0.7673) Acc G: 17.146% 
LR: 2.000e-04 

2023-03-02 01:46:14,521 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3253 (0.3577) Acc D Real: 67.642% 
Loss D Fake: 0.6615 (0.6415) Acc D Fake: 81.811% 
Loss D: 0.987 
Loss G: 0.7320 (0.7668) Acc G: 17.140% 
LR: 2.000e-04 

2023-03-02 01:46:14,529 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3731 (0.3579) Acc D Real: 67.579% 
Loss D Fake: 0.6627 (0.6418) Acc D Fake: 81.829% 
Loss D: 1.036 
Loss G: 0.7308 (0.7664) Acc G: 17.134% 
LR: 2.000e-04 

2023-03-02 01:46:14,536 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.2401 (0.3564) Acc D Real: 67.718% 
Loss D Fake: 0.6633 (0.6420) Acc D Fake: 81.847% 
Loss D: 0.903 
Loss G: 0.7315 (0.7660) Acc G: 17.129% 
LR: 2.000e-04 

2023-03-02 01:46:14,543 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3650 (0.3566) Acc D Real: 67.641% 
Loss D Fake: 0.6623 (0.6423) Acc D Fake: 81.865% 
Loss D: 1.027 
Loss G: 0.7325 (0.7656) Acc G: 17.123% 
LR: 2.000e-04 

2023-03-02 01:46:14,550 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3748 (0.3568) Acc D Real: 67.591% 
Loss D Fake: 0.6618 (0.6425) Acc D Fake: 81.882% 
Loss D: 1.037 
Loss G: 0.7328 (0.7652) Acc G: 17.118% 
LR: 2.000e-04 

2023-03-02 01:46:14,557 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4119 (0.3574) Acc D Real: 67.483% 
Loss D Fake: 0.6621 (0.6427) Acc D Fake: 81.899% 
Loss D: 1.074 
Loss G: 0.7319 (0.7648) Acc G: 17.132% 
LR: 2.000e-04 

2023-03-02 01:46:14,564 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3776 (0.3576) Acc D Real: 67.402% 
Loss D Fake: 0.6635 (0.6430) Acc D Fake: 81.897% 
Loss D: 1.041 
Loss G: 0.7304 (0.7644) Acc G: 17.146% 
LR: 2.000e-04 

2023-03-02 01:46:14,572 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3249 (0.3573) Acc D Real: 67.412% 
Loss D Fake: 0.6649 (0.6432) Acc D Fake: 81.894% 
Loss D: 0.990 
Loss G: 0.7295 (0.7640) Acc G: 17.159% 
LR: 2.000e-04 

2023-03-02 01:46:14,579 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3158 (0.3568) Acc D Real: 67.425% 
Loss D Fake: 0.6656 (0.6435) Acc D Fake: 81.891% 
Loss D: 0.981 
Loss G: 0.7294 (0.7636) Acc G: 17.172% 
LR: 2.000e-04 

2023-03-02 01:46:14,586 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4490 (0.3578) Acc D Real: 67.256% 
Loss D Fake: 0.6662 (0.6437) Acc D Fake: 81.870% 
Loss D: 1.115 
Loss G: 0.7279 (0.7632) Acc G: 17.204% 
LR: 2.000e-04 

2023-03-02 01:46:14,593 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3284 (0.3575) Acc D Real: 67.269% 
Loss D Fake: 0.6680 (0.6440) Acc D Fake: 81.850% 
Loss D: 0.996 
Loss G: 0.7266 (0.7628) Acc G: 17.234% 
LR: 2.000e-04 

2023-03-02 01:46:14,601 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3694 (0.3576) Acc D Real: 67.219% 
Loss D Fake: 0.6691 (0.6443) Acc D Fake: 81.830% 
Loss D: 1.039 
Loss G: 0.7255 (0.7624) Acc G: 17.264% 
LR: 2.000e-04 

2023-03-02 01:46:14,608 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2869 (0.3569) Acc D Real: 67.293% 
Loss D Fake: 0.6698 (0.6445) Acc D Fake: 81.810% 
Loss D: 0.957 
Loss G: 0.7259 (0.7620) Acc G: 17.294% 
LR: 2.000e-04 

2023-03-02 01:46:14,615 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3136 (0.3564) Acc D Real: 67.314% 
Loss D Fake: 0.6690 (0.6448) Acc D Fake: 81.791% 
Loss D: 0.983 
Loss G: 0.7274 (0.7617) Acc G: 17.323% 
LR: 2.000e-04 

2023-03-02 01:46:14,622 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3079 (0.3559) Acc D Real: 67.390% 
Loss D Fake: 0.6673 (0.6450) Acc D Fake: 81.772% 
Loss D: 0.975 
Loss G: 0.7299 (0.7613) Acc G: 17.351% 
LR: 2.000e-04 

2023-03-02 01:46:14,629 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.3837 (0.3562) Acc D Real: 67.323% 
Loss D Fake: 0.6652 (0.6452) Acc D Fake: 81.753% 
Loss D: 1.049 
Loss G: 0.7317 (0.7610) Acc G: 17.378% 
LR: 2.000e-04 

2023-03-02 01:46:14,636 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.2937 (0.3555) Acc D Real: 67.390% 
Loss D Fake: 0.6637 (0.6454) Acc D Fake: 81.735% 
Loss D: 0.957 
Loss G: 0.7340 (0.7607) Acc G: 17.405% 
LR: 2.000e-04 

2023-03-02 01:46:14,644 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3549 (0.3555) Acc D Real: 67.371% 
Loss D Fake: 0.6616 (0.6456) Acc D Fake: 81.718% 
Loss D: 1.016 
Loss G: 0.7364 (0.7605) Acc G: 17.432% 
LR: 2.000e-04 

2023-03-02 01:46:14,651 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2755 (0.3547) Acc D Real: 67.458% 
Loss D Fake: 0.6593 (0.6457) Acc D Fake: 81.700% 
Loss D: 0.935 
Loss G: 0.7401 (0.7603) Acc G: 17.458% 
LR: 2.000e-04 

2023-03-02 01:46:14,658 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2859 (0.3540) Acc D Real: 67.576% 
Loss D Fake: 0.6555 (0.6458) Acc D Fake: 81.683% 
Loss D: 0.941 
Loss G: 0.7453 (0.7601) Acc G: 17.483% 
LR: 2.000e-04 

2023-03-02 01:46:14,665 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2135 (0.3526) Acc D Real: 67.743% 
Loss D Fake: 0.6503 (0.6459) Acc D Fake: 81.667% 
Loss D: 0.864 
Loss G: 0.7529 (0.7601) Acc G: 17.508% 
LR: 2.000e-04 

2023-03-02 01:46:14,672 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2471 (0.3516) Acc D Real: 67.873% 
Loss D Fake: 0.6432 (0.6458) Acc D Fake: 81.650% 
Loss D: 0.890 
Loss G: 0.7624 (0.7601) Acc G: 17.533% 
LR: 2.000e-04 

2023-03-02 01:46:14,680 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3399 (0.3515) Acc D Real: 67.849% 
Loss D Fake: 0.6355 (0.6457) Acc D Fake: 81.634% 
Loss D: 0.975 
Loss G: 0.7710 (0.7602) Acc G: 17.557% 
LR: 2.000e-04 

2023-03-02 01:46:14,687 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2366 (0.3504) Acc D Real: 68.002% 
Loss D Fake: 0.6285 (0.6456) Acc D Fake: 81.619% 
Loss D: 0.865 
Loss G: 0.7807 (0.7604) Acc G: 17.580% 
LR: 2.000e-04 

2023-03-02 01:46:14,694 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3051 (0.3500) Acc D Real: 68.053% 
Loss D Fake: 0.6207 (0.6453) Acc D Fake: 81.603% 
Loss D: 0.926 
Loss G: 0.7903 (0.7607) Acc G: 17.603% 
LR: 2.000e-04 

2023-03-02 01:46:14,701 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.4901 (0.3513) Acc D Real: 67.883% 
Loss D Fake: 0.6146 (0.6451) Acc D Fake: 81.588% 
Loss D: 1.105 
Loss G: 0.7949 (0.7610) Acc G: 17.626% 
LR: 2.000e-04 

2023-03-02 01:46:14,708 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.3684 (0.3514) Acc D Real: 67.868% 
Loss D Fake: 0.6125 (0.6448) Acc D Fake: 81.573% 
Loss D: 0.981 
Loss G: 0.7970 (0.7613) Acc G: 17.648% 
LR: 2.000e-04 

2023-03-02 01:46:14,716 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3952 (0.3519) Acc D Real: 67.821% 
Loss D Fake: 0.6117 (0.6444) Acc D Fake: 81.559% 
Loss D: 1.007 
Loss G: 0.7970 (0.7617) Acc G: 17.670% 
LR: 2.000e-04 

2023-03-02 01:46:14,723 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.2786 (0.3512) Acc D Real: 67.918% 
Loss D Fake: 0.6116 (0.6441) Acc D Fake: 81.544% 
Loss D: 0.890 
Loss G: 0.7981 (0.7620) Acc G: 17.691% 
LR: 2.000e-04 

2023-03-02 01:46:14,730 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.2737 (0.3505) Acc D Real: 67.995% 
Loss D Fake: 0.6100 (0.6438) Acc D Fake: 81.530% 
Loss D: 0.884 
Loss G: 0.8010 (0.7623) Acc G: 17.712% 
LR: 2.000e-04 

2023-03-02 01:46:14,737 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3441 (0.3504) Acc D Real: 68.018% 
Loss D Fake: 0.6077 (0.6435) Acc D Fake: 81.517% 
Loss D: 0.952 
Loss G: 0.8035 (0.7627) Acc G: 17.733% 
LR: 2.000e-04 

2023-03-02 01:46:14,744 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.2532 (0.3496) Acc D Real: 68.115% 
Loss D Fake: 0.6054 (0.6432) Acc D Fake: 81.503% 
Loss D: 0.859 
Loss G: 0.8075 (0.7631) Acc G: 17.753% 
LR: 2.000e-04 

2023-03-02 01:46:14,752 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3957 (0.3500) Acc D Real: 68.094% 
Loss D Fake: 0.6025 (0.6428) Acc D Fake: 81.490% 
Loss D: 0.998 
Loss G: 0.8099 (0.7635) Acc G: 17.758% 
LR: 2.000e-04 

2023-03-02 01:46:14,759 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3799 (0.3502) Acc D Real: 68.072% 
Loss D Fake: 0.6016 (0.6424) Acc D Fake: 81.491% 
Loss D: 0.982 
Loss G: 0.8097 (0.7639) Acc G: 17.763% 
LR: 2.000e-04 

2023-03-02 01:46:14,766 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4221 (0.3508) Acc D Real: 68.047% 
Loss D Fake: 0.6030 (0.6421) Acc D Fake: 81.493% 
Loss D: 1.025 
Loss G: 0.8059 (0.7643) Acc G: 17.768% 
LR: 2.000e-04 

2023-03-02 01:46:14,773 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3352 (0.3507) Acc D Real: 68.070% 
Loss D Fake: 0.6067 (0.6418) Acc D Fake: 81.480% 
Loss D: 0.942 
Loss G: 0.8012 (0.7646) Acc G: 17.787% 
LR: 2.000e-04 

2023-03-02 01:46:14,780 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3755 (0.3509) Acc D Real: 68.046% 
Loss D Fake: 0.6108 (0.6415) Acc D Fake: 81.467% 
Loss D: 0.986 
Loss G: 0.7952 (0.7649) Acc G: 17.806% 
LR: 2.000e-04 

2023-03-02 01:46:14,788 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3230 (0.3507) Acc D Real: 68.088% 
Loss D Fake: 0.6155 (0.6413) Acc D Fake: 81.455% 
Loss D: 0.939 
Loss G: 0.7905 (0.7651) Acc G: 17.825% 
LR: 2.000e-04 

2023-03-02 01:46:14,795 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3009 (0.3503) Acc D Real: 68.129% 
Loss D Fake: 0.6182 (0.6411) Acc D Fake: 81.443% 
Loss D: 0.919 
Loss G: 0.7889 (0.7653) Acc G: 17.843% 
LR: 2.000e-04 

2023-03-02 01:46:14,802 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3850 (0.3506) Acc D Real: 68.127% 
Loss D Fake: 0.6193 (0.6409) Acc D Fake: 81.431% 
Loss D: 1.004 
Loss G: 0.7869 (0.7655) Acc G: 17.861% 
LR: 2.000e-04 

2023-03-02 01:46:14,809 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3113 (0.3502) Acc D Real: 68.151% 
Loss D Fake: 0.6210 (0.6408) Acc D Fake: 81.419% 
Loss D: 0.932 
Loss G: 0.7861 (0.7656) Acc G: 17.879% 
LR: 2.000e-04 

2023-03-02 01:46:14,816 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4501 (0.3511) Acc D Real: 68.037% 
Loss D Fake: 0.6235 (0.6406) Acc D Fake: 81.407% 
Loss D: 1.074 
Loss G: 0.7791 (0.7658) Acc G: 17.896% 
LR: 2.000e-04 

2023-03-02 01:46:14,823 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2908 (0.3506) Acc D Real: 68.084% 
Loss D Fake: 0.6307 (0.6405) Acc D Fake: 81.396% 
Loss D: 0.921 
Loss G: 0.7741 (0.7658) Acc G: 17.927% 
LR: 2.000e-04 

2023-03-02 01:46:14,831 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3369 (0.3505) Acc D Real: 68.089% 
Loss D Fake: 0.6339 (0.6405) Acc D Fake: 81.371% 
Loss D: 0.971 
Loss G: 0.7725 (0.7659) Acc G: 17.957% 
LR: 2.000e-04 

2023-03-02 01:46:14,838 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.2740 (0.3498) Acc D Real: 68.168% 
Loss D Fake: 0.6332 (0.6404) Acc D Fake: 81.347% 
Loss D: 0.907 
Loss G: 0.7796 (0.7660) Acc G: 17.987% 
LR: 2.000e-04 

2023-03-02 01:46:14,845 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3472 (0.3498) Acc D Real: 68.187% 
Loss D Fake: 0.6259 (0.6403) Acc D Fake: 81.323% 
Loss D: 0.973 
Loss G: 0.7889 (0.7662) Acc G: 18.016% 
LR: 2.000e-04 

2023-03-02 01:46:14,852 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.2167 (0.3488) Acc D Real: 68.293% 
Loss D Fake: 0.6175 (0.6401) Acc D Fake: 81.299% 
Loss D: 0.834 
Loss G: 0.8031 (0.7665) Acc G: 18.045% 
LR: 2.000e-04 

2023-03-02 01:46:14,859 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.2704 (0.3482) Acc D Real: 68.383% 
Loss D Fake: 0.6054 (0.6399) Acc D Fake: 81.276% 
Loss D: 0.876 
Loss G: 0.8185 (0.7669) Acc G: 18.073% 
LR: 2.000e-04 

2023-03-02 01:46:14,867 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3161 (0.3479) Acc D Real: 68.421% 
Loss D Fake: 0.5953 (0.6395) Acc D Fake: 81.253% 
Loss D: 0.911 
Loss G: 0.8280 (0.7673) Acc G: 18.101% 
LR: 2.000e-04 

2023-03-02 01:46:14,874 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3397 (0.3478) Acc D Real: 68.444% 
Loss D Fake: 0.5909 (0.6392) Acc D Fake: 81.231% 
Loss D: 0.931 
Loss G: 0.8303 (0.7678) Acc G: 18.115% 
LR: 2.000e-04 

2023-03-02 01:46:14,881 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3030 (0.3475) Acc D Real: 68.523% 
Loss D Fake: 0.5913 (0.6388) Acc D Fake: 81.221% 
Loss D: 0.894 
Loss G: 0.8293 (0.7683) Acc G: 18.130% 
LR: 2.000e-04 

2023-03-02 01:46:14,888 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.4493 (0.3483) Acc D Real: 68.466% 
Loss D Fake: 0.5962 (0.6385) Acc D Fake: 81.212% 
Loss D: 1.045 
Loss G: 0.8145 (0.7686) Acc G: 18.144% 
LR: 2.000e-04 

2023-03-02 01:46:14,895 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.2666 (0.3477) Acc D Real: 68.562% 
Loss D Fake: 0.6134 (0.6383) Acc D Fake: 81.190% 
Loss D: 0.880 
Loss G: 0.8026 (0.7689) Acc G: 18.170% 
LR: 2.000e-04 

2023-03-02 01:46:14,902 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2386 (0.3468) Acc D Real: 68.678% 
Loss D Fake: 0.6119 (0.6381) Acc D Fake: 81.169% 
Loss D: 0.851 
Loss G: 0.8229 (0.7693) Acc G: 18.184% 
LR: 2.000e-04 

2023-03-02 01:46:14,910 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.2614 (0.3462) Acc D Real: 68.800% 
Loss D Fake: 0.5851 (0.6377) Acc D Fake: 81.160% 
Loss D: 0.847 
Loss G: 0.8554 (0.7699) Acc G: 18.198% 
LR: 2.000e-04 

2023-03-02 01:46:14,917 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3081 (0.3459) Acc D Real: 68.870% 
Loss D Fake: 0.5654 (0.6372) Acc D Fake: 81.152% 
Loss D: 0.873 
Loss G: 0.8722 (0.7707) Acc G: 18.199% 
LR: 2.000e-04 

2023-03-02 01:46:14,924 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3935 (0.3463) Acc D Real: 68.859% 
Loss D Fake: 0.5602 (0.6366) Acc D Fake: 81.144% 
Loss D: 0.954 
Loss G: 0.8691 (0.7714) Acc G: 18.212% 
LR: 2.000e-04 

2023-03-02 01:46:14,931 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.2670 (0.3457) Acc D Real: 68.952% 
Loss D Fake: 0.5708 (0.6361) Acc D Fake: 81.135% 
Loss D: 0.838 
Loss G: 0.8472 (0.7720) Acc G: 18.225% 
LR: 2.000e-04 

2023-03-02 01:46:14,939 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3367 (0.3456) Acc D Real: 69.045% 
Loss D Fake: 0.6058 (0.6359) Acc D Fake: 81.127% 
Loss D: 0.943 
Loss G: 0.8349 (0.7724) Acc G: 18.237% 
LR: 2.000e-04 

2023-03-02 01:46:14,946 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3415 (0.3456) Acc D Real: 69.182% 
Loss D Fake: 0.5812 (0.6355) Acc D Fake: 81.119% 
Loss D: 0.923 
Loss G: 0.8746 (0.7731) Acc G: 18.238% 
LR: 2.000e-04 

2023-03-02 01:46:14,953 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3440 (0.3456) Acc D Real: 69.268% 
Loss D Fake: 0.5539 (0.6349) Acc D Fake: 81.123% 
Loss D: 0.898 
Loss G: 0.8892 (0.7740) Acc G: 18.239% 
LR: 2.000e-04 

2023-03-02 01:46:14,961 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3028 (0.3453) Acc D Real: 69.339% 
Loss D Fake: 0.5607 (0.6344) Acc D Fake: 81.127% 
Loss D: 0.863 
Loss G: 0.8406 (0.7744) Acc G: 18.251% 
LR: 2.000e-04 

2023-03-02 01:46:14,969 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2992 (0.3450) Acc D Real: 69.499% 
Loss D Fake: 0.6943 (0.6348) Acc D Fake: 81.119% 
Loss D: 0.994 
Loss G: 0.8820 (0.7752) Acc G: 18.252% 
LR: 2.000e-04 

2023-03-02 01:46:14,976 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3050 (0.3447) Acc D Real: 69.616% 
Loss D Fake: 0.5337 (0.6341) Acc D Fake: 81.123% 
Loss D: 0.839 
Loss G: 0.9301 (0.7763) Acc G: 18.252% 
LR: 2.000e-04 

2023-03-02 01:46:14,984 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3640 (0.3448) Acc D Real: 69.679% 
Loss D Fake: 0.5167 (0.6333) Acc D Fake: 81.138% 
Loss D: 0.881 
Loss G: 0.9445 (0.7774) Acc G: 18.241% 
LR: 2.000e-04 

2023-03-02 01:46:14,991 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3853 (0.3451) Acc D Real: 69.697% 
Loss D Fake: 0.5118 (0.6325) Acc D Fake: 81.153% 
Loss D: 0.897 
Loss G: 0.9461 (0.7786) Acc G: 18.231% 
LR: 2.000e-04 

2023-03-02 01:46:14,999 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3516 (0.3452) Acc D Real: 69.732% 
Loss D Fake: 0.5181 (0.6317) Acc D Fake: 81.168% 
Loss D: 0.870 
Loss G: 0.9139 (0.7795) Acc G: 18.231% 
LR: 2.000e-04 

2023-03-02 01:46:15,006 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.2525 (0.3445) Acc D Real: 69.876% 
Loss D Fake: 0.5707 (0.6313) Acc D Fake: 81.171% 
Loss D: 0.823 
Loss G: 0.8131 (0.7797) Acc G: 18.243% 
LR: 2.000e-04 

2023-03-02 01:46:15,014 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3433 (0.3445) Acc D Real: 70.002% 
Loss D Fake: 0.7229 (0.6319) Acc D Fake: 80.861% 
Loss D: 1.066 
Loss G: 0.8733 (0.7804) Acc G: 18.244% 
LR: 2.000e-04 

2023-03-02 01:46:15,021 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3860 (0.3448) Acc D Real: 70.068% 
Loss D Fake: 0.5393 (0.6313) Acc D Fake: 80.867% 
Loss D: 0.925 
Loss G: 0.9242 (0.7813) Acc G: 18.244% 
LR: 2.000e-04 

2023-03-02 01:46:15,029 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3359 (0.3447) Acc D Real: 70.155% 
Loss D Fake: 0.5212 (0.6306) Acc D Fake: 80.872% 
Loss D: 0.857 
Loss G: 0.9375 (0.7823) Acc G: 18.234% 
LR: 2.000e-04 

2023-03-02 01:46:15,036 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3160 (0.3445) Acc D Real: 70.237% 
Loss D Fake: 0.5169 (0.6298) Acc D Fake: 80.888% 
Loss D: 0.833 
Loss G: 0.9394 (0.7834) Acc G: 18.224% 
LR: 2.000e-04 

2023-03-02 01:46:15,044 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3359 (0.3445) Acc D Real: 70.298% 
Loss D Fake: 0.5183 (0.6291) Acc D Fake: 80.904% 
Loss D: 0.854 
Loss G: 0.9338 (0.7844) Acc G: 18.224% 
LR: 2.000e-04 

2023-03-02 01:46:15,051 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.1818 (0.3434) Acc D Real: 70.476% 
Loss D Fake: 0.5233 (0.6284) Acc D Fake: 80.909% 
Loss D: 0.705 
Loss G: 0.9276 (0.7853) Acc G: 18.225% 
LR: 2.000e-04 

2023-03-02 01:46:15,059 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3145 (0.3432) Acc D Real: 70.575% 
Loss D Fake: 0.5301 (0.6278) Acc D Fake: 80.914% 
Loss D: 0.845 
Loss G: 0.9108 (0.7861) Acc G: 18.226% 
LR: 2.000e-04 

2023-03-02 01:46:15,066 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.2809 (0.3429) Acc D Real: 70.697% 
Loss D Fake: 0.5475 (0.6272) Acc D Fake: 80.919% 
Loss D: 0.828 
Loss G: 0.8925 (0.7868) Acc G: 18.226% 
LR: 2.000e-04 

2023-03-02 01:46:15,074 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.2951 (0.3425) Acc D Real: 70.845% 
Loss D Fake: 0.5574 (0.6268) Acc D Fake: 80.913% 
Loss D: 0.853 
Loss G: 0.9009 (0.7875) Acc G: 18.227% 
LR: 2.000e-04 

2023-03-02 01:46:15,081 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2993 (0.3423) Acc D Real: 70.926% 
Loss D Fake: 0.5384 (0.6262) Acc D Fake: 80.918% 
Loss D: 0.838 
Loss G: 0.9238 (0.7884) Acc G: 18.228% 
LR: 2.000e-04 

2023-03-02 01:46:15,089 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2409 (0.3416) Acc D Real: 71.048% 
Loss D Fake: 0.5229 (0.6256) Acc D Fake: 80.922% 
Loss D: 0.764 
Loss G: 0.9427 (0.7893) Acc G: 18.229% 
LR: 2.000e-04 

2023-03-02 01:46:15,096 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.2492 (0.3411) Acc D Real: 71.174% 
Loss D Fake: 0.5115 (0.6249) Acc D Fake: 80.927% 
Loss D: 0.761 
Loss G: 0.9581 (0.7904) Acc G: 18.219% 
LR: 2.000e-04 

2023-03-02 01:46:15,104 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2748 (0.3406) Acc D Real: 71.285% 
Loss D Fake: 0.5123 (0.6242) Acc D Fake: 80.932% 
Loss D: 0.787 
Loss G: 0.9109 (0.7911) Acc G: 18.219% 
LR: 2.000e-04 

2023-03-02 01:46:15,111 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.2186 (0.3399) Acc D Real: 71.410% 
Loss D Fake: 0.8049 (0.6253) Acc D Fake: 80.432% 
Loss D: 1.023 
Loss G: 0.5801 (0.7898) Acc G: 18.724% 
LR: 2.000e-04 

2023-03-02 01:46:15,119 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.2831 (0.3395) Acc D Real: 71.569% 
Loss D Fake: 0.8428 (0.6266) Acc D Fake: 79.939% 
Loss D: 1.126 
Loss G: 0.5590 (0.7884) Acc G: 19.223% 
LR: 2.000e-04 

2023-03-02 01:46:15,127 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2464 (0.3390) Acc D Real: 71.734% 
Loss D Fake: 0.8584 (0.6280) Acc D Fake: 79.451% 
Loss D: 1.105 
Loss G: 0.5534 (0.7870) Acc G: 19.715% 
LR: 2.000e-04 

2023-03-02 01:46:15,134 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2170 (0.3382) Acc D Real: 71.897% 
Loss D Fake: 0.8594 (0.6294) Acc D Fake: 78.970% 
Loss D: 1.076 
Loss G: 0.5561 (0.7856) Acc G: 20.202% 
LR: 2.000e-04 

2023-03-02 01:46:15,141 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.2141 (0.3375) Acc D Real: 72.059% 
Loss D Fake: 0.8514 (0.6308) Acc D Fake: 78.494% 
Loss D: 1.065 
Loss G: 0.5643 (0.7843) Acc G: 20.683% 
LR: 2.000e-04 

2023-03-02 01:46:15,149 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2845 (0.3372) Acc D Real: 72.217% 
Loss D Fake: 0.8380 (0.6320) Acc D Fake: 78.024% 
Loss D: 1.123 
Loss G: 0.5751 (0.7830) Acc G: 21.158% 
LR: 2.000e-04 

2023-03-02 01:46:15,156 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.1655 (0.3361) Acc D Real: 72.376% 
Loss D Fake: 0.8219 (0.6332) Acc D Fake: 77.560% 
Loss D: 0.987 
Loss G: 0.5895 (0.7819) Acc G: 21.627% 
LR: 2.000e-04 

2023-03-02 01:46:15,164 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3311 (0.3361) Acc D Real: 72.529% 
Loss D Fake: 0.8029 (0.6342) Acc D Fake: 77.101% 
Loss D: 1.134 
Loss G: 0.6037 (0.7808) Acc G: 22.091% 
LR: 2.000e-04 

2023-03-02 01:46:15,171 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.2127 (0.3354) Acc D Real: 72.682% 
Loss D Fake: 0.7855 (0.6350) Acc D Fake: 76.647% 
Loss D: 0.998 
Loss G: 0.6188 (0.7799) Acc G: 22.549% 
LR: 2.000e-04 

2023-03-02 01:46:15,179 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.2980 (0.3352) Acc D Real: 72.832% 
Loss D Fake: 0.7679 (0.6358) Acc D Fake: 76.199% 
Loss D: 1.066 
Loss G: 0.6328 (0.7790) Acc G: 23.002% 
LR: 2.000e-04 

2023-03-02 01:46:15,187 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.2855 (0.3349) Acc D Real: 72.981% 
Loss D Fake: 0.7528 (0.6365) Acc D Fake: 75.756% 
Loss D: 1.038 
Loss G: 0.6457 (0.7782) Acc G: 23.450% 
LR: 2.000e-04 

2023-03-02 01:46:15,194 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.2995 (0.3347) Acc D Real: 73.126% 
Loss D Fake: 0.7392 (0.6371) Acc D Fake: 75.318% 
Loss D: 1.039 
Loss G: 0.6575 (0.7775) Acc G: 23.892% 
LR: 2.000e-04 

2023-03-02 01:46:15,202 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3331 (0.3347) Acc D Real: 73.264% 
Loss D Fake: 0.7274 (0.6376) Acc D Fake: 74.885% 
Loss D: 1.061 
Loss G: 0.6676 (0.7769) Acc G: 24.330% 
LR: 2.000e-04 

2023-03-02 01:46:15,209 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3100 (0.3345) Acc D Real: 73.391% 
Loss D Fake: 0.7175 (0.6381) Acc D Fake: 74.476% 
Loss D: 1.028 
Loss G: 0.6767 (0.7763) Acc G: 24.714% 
LR: 2.000e-04 

2023-03-02 01:46:15,217 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3507 (0.3346) Acc D Real: 73.469% 
Loss D Fake: 0.7089 (0.6385) Acc D Fake: 74.176% 
Loss D: 1.060 
Loss G: 0.6843 (0.7758) Acc G: 24.678% 
LR: 2.000e-04 

2023-03-02 01:46:15,224 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2993 (0.3344) Acc D Real: 73.538% 
Loss D Fake: 0.7019 (0.6388) Acc D Fake: 74.218% 
Loss D: 1.001 
Loss G: 0.6911 (0.7753) Acc G: 24.642% 
LR: 2.000e-04 

2023-03-02 01:46:15,232 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2191 (0.3338) Acc D Real: 73.665% 
Loss D Fake: 0.6949 (0.6391) Acc D Fake: 74.260% 
Loss D: 0.914 
Loss G: 0.6989 (0.7749) Acc G: 24.607% 
LR: 2.000e-04 

2023-03-02 01:46:15,239 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3763 (0.3340) Acc D Real: 73.644% 
Loss D Fake: 0.6875 (0.6394) Acc D Fake: 74.302% 
Loss D: 1.064 
Loss G: 0.7054 (0.7745) Acc G: 24.572% 
LR: 2.000e-04 

2023-03-02 01:46:15,247 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.2680 (0.3336) Acc D Real: 73.703% 
Loss D Fake: 0.6817 (0.6397) Acc D Fake: 74.343% 
Loss D: 0.950 
Loss G: 0.7117 (0.7741) Acc G: 24.537% 
LR: 2.000e-04 

2023-03-02 01:46:15,254 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.2579 (0.3332) Acc D Real: 73.721% 
Loss D Fake: 0.6755 (0.6399) Acc D Fake: 74.383% 
Loss D: 0.933 
Loss G: 0.7187 (0.7738) Acc G: 24.494% 
LR: 2.000e-04 

2023-03-02 01:46:15,262 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3942 (0.3336) Acc D Real: 73.686% 
Loss D Fake: 0.6694 (0.6400) Acc D Fake: 74.432% 
Loss D: 1.064 
Loss G: 0.7238 (0.7736) Acc G: 24.451% 
LR: 2.000e-04 

2023-03-02 01:46:15,269 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2982 (0.3334) Acc D Real: 73.690% 
Loss D Fake: 0.6654 (0.6402) Acc D Fake: 74.481% 
Loss D: 0.964 
Loss G: 0.7281 (0.7733) Acc G: 24.408% 
LR: 2.000e-04 

2023-03-02 01:46:15,277 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3028 (0.3332) Acc D Real: 73.690% 
Loss D Fake: 0.6614 (0.6403) Acc D Fake: 74.529% 
Loss D: 0.964 
Loss G: 0.7326 (0.7731) Acc G: 24.366% 
LR: 2.000e-04 

2023-03-02 01:46:15,284 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2523 (0.3328) Acc D Real: 73.722% 
Loss D Fake: 0.6571 (0.6404) Acc D Fake: 74.577% 
Loss D: 0.909 
Loss G: 0.7380 (0.7729) Acc G: 24.324% 
LR: 2.000e-04 

2023-03-02 01:46:15,292 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3133 (0.3327) Acc D Real: 73.705% 
Loss D Fake: 0.6520 (0.6404) Acc D Fake: 74.624% 
Loss D: 0.965 
Loss G: 0.7434 (0.7727) Acc G: 24.283% 
LR: 2.000e-04 

2023-03-02 01:46:15,300 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.2324 (0.3321) Acc D Real: 73.737% 
Loss D Fake: 0.6471 (0.6405) Acc D Fake: 74.670% 
Loss D: 0.880 
Loss G: 0.7497 (0.7726) Acc G: 24.242% 
LR: 2.000e-04 

2023-03-02 01:46:15,307 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3621 (0.3323) Acc D Real: 73.683% 
Loss D Fake: 0.6419 (0.6405) Acc D Fake: 74.716% 
Loss D: 1.004 
Loss G: 0.7543 (0.7725) Acc G: 24.202% 
LR: 2.000e-04 

2023-03-02 01:46:15,315 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3259 (0.3323) Acc D Real: 73.669% 
Loss D Fake: 0.6386 (0.6405) Acc D Fake: 74.762% 
Loss D: 0.965 
Loss G: 0.7576 (0.7725) Acc G: 24.162% 
LR: 2.000e-04 

2023-03-02 01:46:15,322 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.2530 (0.3318) Acc D Real: 73.700% 
Loss D Fake: 0.6359 (0.6404) Acc D Fake: 74.807% 
Loss D: 0.889 
Loss G: 0.7613 (0.7724) Acc G: 24.123% 
LR: 2.000e-04 

2023-03-02 01:46:15,330 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3772 (0.3321) Acc D Real: 73.634% 
Loss D Fake: 0.6334 (0.6404) Acc D Fake: 74.852% 
Loss D: 1.011 
Loss G: 0.7624 (0.7723) Acc G: 24.084% 
LR: 2.000e-04 

2023-03-02 01:46:15,337 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.2647 (0.3317) Acc D Real: 73.661% 
Loss D Fake: 0.6328 (0.6404) Acc D Fake: 74.896% 
Loss D: 0.898 
Loss G: 0.7640 (0.7723) Acc G: 24.045% 
LR: 2.000e-04 

2023-03-02 01:46:15,345 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3634 (0.3319) Acc D Real: 73.621% 
Loss D Fake: 0.6318 (0.6403) Acc D Fake: 74.940% 
Loss D: 0.995 
Loss G: 0.7637 (0.7723) Acc G: 24.007% 
LR: 2.000e-04 

2023-03-02 01:46:15,352 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2950 (0.3317) Acc D Real: 73.615% 
Loss D Fake: 0.6330 (0.6403) Acc D Fake: 74.983% 
Loss D: 0.928 
Loss G: 0.7620 (0.7722) Acc G: 23.969% 
LR: 2.000e-04 

2023-03-02 01:46:15,360 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.2553 (0.3313) Acc D Real: 73.648% 
Loss D Fake: 0.6342 (0.6402) Acc D Fake: 75.026% 
Loss D: 0.890 
Loss G: 0.7620 (0.7721) Acc G: 23.932% 
LR: 2.000e-04 

2023-03-02 01:46:15,367 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4471 (0.3319) Acc D Real: 73.585% 
Loss D Fake: 0.6356 (0.6402) Acc D Fake: 75.068% 
Loss D: 1.083 
Loss G: 0.7563 (0.7721) Acc G: 23.895% 
LR: 2.000e-04 

2023-03-02 01:46:15,375 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3109 (0.3318) Acc D Real: 73.582% 
Loss D Fake: 0.6425 (0.6402) Acc D Fake: 75.110% 
Loss D: 0.953 
Loss G: 0.7499 (0.7720) Acc G: 23.858% 
LR: 2.000e-04 

2023-03-02 01:46:15,382 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2750 (0.3315) Acc D Real: 73.608% 
Loss D Fake: 0.6472 (0.6403) Acc D Fake: 75.152% 
Loss D: 0.922 
Loss G: 0.7465 (0.7718) Acc G: 23.822% 
LR: 2.000e-04 

2023-03-02 01:46:15,390 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2310 (0.3310) Acc D Real: 73.685% 
Loss D Fake: 0.6471 (0.6403) Acc D Fake: 75.193% 
Loss D: 0.878 
Loss G: 0.7528 (0.7717) Acc G: 23.786% 
LR: 2.000e-04 

2023-03-02 01:46:15,397 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2967 (0.3308) Acc D Real: 73.733% 
Loss D Fake: 0.6386 (0.6403) Acc D Fake: 75.233% 
Loss D: 0.935 
Loss G: 0.7635 (0.7717) Acc G: 23.750% 
LR: 2.000e-04 

2023-03-02 01:46:15,405 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2243 (0.3303) Acc D Real: 73.797% 
Loss D Fake: 0.6275 (0.6402) Acc D Fake: 75.274% 
Loss D: 0.852 
Loss G: 0.7797 (0.7717) Acc G: 23.715% 
LR: 2.000e-04 

2023-03-02 01:46:15,413 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3503 (0.3304) Acc D Real: 73.801% 
Loss D Fake: 0.6142 (0.6401) Acc D Fake: 75.314% 
Loss D: 0.964 
Loss G: 0.7924 (0.7718) Acc G: 23.680% 
LR: 2.000e-04 

2023-03-02 01:46:15,420 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2105 (0.3298) Acc D Real: 73.872% 
Loss D Fake: 0.6041 (0.6399) Acc D Fake: 75.361% 
Loss D: 0.815 
Loss G: 0.8074 (0.7720) Acc G: 23.637% 
LR: 2.000e-04 

2023-03-02 01:46:15,428 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.1803 (0.3291) Acc D Real: 73.937% 
Loss D Fake: 0.5911 (0.6397) Acc D Fake: 75.417% 
Loss D: 0.771 
Loss G: 0.8246 (0.7723) Acc G: 23.587% 
LR: 2.000e-04 

2023-03-02 01:46:15,435 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2960 (0.3289) Acc D Real: 73.950% 
Loss D Fake: 0.5795 (0.6394) Acc D Fake: 75.472% 
Loss D: 0.875 
Loss G: 0.8361 (0.7726) Acc G: 23.537% 
LR: 2.000e-04 

2023-03-02 01:46:15,443 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3803 (0.3292) Acc D Real: 73.940% 
Loss D Fake: 0.5746 (0.6391) Acc D Fake: 75.526% 
Loss D: 0.955 
Loss G: 0.8368 (0.7729) Acc G: 23.487% 
LR: 2.000e-04 

2023-03-02 01:46:15,450 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3675 (0.3293) Acc D Real: 73.913% 
Loss D Fake: 0.5790 (0.6388) Acc D Fake: 75.580% 
Loss D: 0.946 
Loss G: 0.8254 (0.7731) Acc G: 23.438% 
LR: 2.000e-04 

2023-03-02 01:46:15,457 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3485 (0.3294) Acc D Real: 73.918% 
Loss D Fake: 0.5946 (0.6386) Acc D Fake: 75.633% 
Loss D: 0.943 
Loss G: 0.8035 (0.7733) Acc G: 23.389% 
LR: 2.000e-04 

2023-03-02 01:46:15,465 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.2701 (0.3292) Acc D Real: 73.992% 
Loss D Fake: 0.6147 (0.6385) Acc D Fake: 75.686% 
Loss D: 0.885 
Loss G: 0.8049 (0.7734) Acc G: 23.341% 
LR: 2.000e-04 

2023-03-02 01:46:15,472 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2554 (0.3288) Acc D Real: 74.045% 
Loss D Fake: 0.5899 (0.6382) Acc D Fake: 75.738% 
Loss D: 0.845 
Loss G: 0.8418 (0.7738) Acc G: 23.294% 
LR: 2.000e-04 

2023-03-02 01:46:15,480 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2665 (0.3285) Acc D Real: 74.101% 
Loss D Fake: 0.5613 (0.6379) Acc D Fake: 75.790% 
Loss D: 0.828 
Loss G: 0.8697 (0.7742) Acc G: 23.246% 
LR: 2.000e-04 

2023-03-02 01:46:15,487 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.2630 (0.3282) Acc D Real: 74.137% 
Loss D Fake: 0.5465 (0.6374) Acc D Fake: 75.841% 
Loss D: 0.810 
Loss G: 0.8844 (0.7747) Acc G: 23.200% 
LR: 2.000e-04 

2023-03-02 01:46:15,495 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2839 (0.3280) Acc D Real: 74.202% 
Loss D Fake: 0.5399 (0.6370) Acc D Fake: 75.892% 
Loss D: 0.824 
Loss G: 0.8903 (0.7753) Acc G: 23.153% 
LR: 2.000e-04 

2023-03-02 01:46:15,503 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.4080 (0.3284) Acc D Real: 74.177% 
Loss D Fake: 0.5408 (0.6365) Acc D Fake: 75.942% 
Loss D: 0.949 
Loss G: 0.8790 (0.7758) Acc G: 23.107% 
LR: 2.000e-04 

2023-03-02 01:46:15,510 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.2550 (0.3280) Acc D Real: 74.214% 
Loss D Fake: 0.5553 (0.6361) Acc D Fake: 75.992% 
Loss D: 0.810 
Loss G: 0.8593 (0.7762) Acc G: 23.062% 
LR: 2.000e-04 

2023-03-02 01:46:15,518 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.1717 (0.3273) Acc D Real: 74.296% 
Loss D Fake: 0.5713 (0.6358) Acc D Fake: 76.034% 
Loss D: 0.743 
Loss G: 0.8692 (0.7766) Acc G: 23.017% 
LR: 2.000e-04 

2023-03-02 01:46:15,525 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3115 (0.3272) Acc D Real: 74.351% 
Loss D Fake: 0.5465 (0.6354) Acc D Fake: 76.083% 
Loss D: 0.858 
Loss G: 0.8930 (0.7771) Acc G: 22.972% 
LR: 2.000e-04 

2023-03-02 01:46:15,533 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3481 (0.3273) Acc D Real: 74.378% 
Loss D Fake: 0.5352 (0.6350) Acc D Fake: 76.131% 
Loss D: 0.883 
Loss G: 0.8988 (0.7777) Acc G: 22.928% 
LR: 2.000e-04 

2023-03-02 01:46:15,540 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2765 (0.3271) Acc D Real: 74.432% 
Loss D Fake: 0.5393 (0.6345) Acc D Fake: 76.180% 
Loss D: 0.816 
Loss G: 0.8847 (0.7782) Acc G: 22.884% 
LR: 2.000e-04 

2023-03-02 01:46:15,548 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2452 (0.3267) Acc D Real: 74.506% 
Loss D Fake: 0.5642 (0.6342) Acc D Fake: 76.227% 
Loss D: 0.809 
Loss G: 0.9245 (0.7788) Acc G: 22.841% 
LR: 2.000e-04 

2023-03-02 01:46:15,555 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2355 (0.3263) Acc D Real: 74.570% 
Loss D Fake: 0.4993 (0.6336) Acc D Fake: 76.275% 
Loss D: 0.735 
Loss G: 0.9793 (0.7797) Acc G: 22.790% 
LR: 2.000e-04 

2023-03-02 01:46:15,563 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2712 (0.3261) Acc D Real: 74.623% 
Loss D Fake: 0.4737 (0.6329) Acc D Fake: 76.329% 
Loss D: 0.745 
Loss G: 1.0531 (0.7810) Acc G: 22.740% 
LR: 2.000e-04 

2023-03-02 01:46:15,570 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2622 (0.3258) Acc D Real: 74.680% 
Loss D Fake: 0.4368 (0.6320) Acc D Fake: 76.383% 
Loss D: 0.699 
Loss G: 1.4010 (0.7838) Acc G: 22.683% 
LR: 2.000e-04 

2023-03-02 01:46:15,578 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2529 (0.3254) Acc D Real: 74.737% 
Loss D Fake: 0.3090 (0.6306) Acc D Fake: 76.443% 
Loss D: 0.562 
Loss G: 1.4573 (0.7868) Acc G: 22.619% 
LR: 2.000e-04 

2023-03-02 01:46:15,585 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.2844 (0.3253) Acc D Real: 74.792% 
Loss D Fake: 0.2979 (0.6291) Acc D Fake: 76.511% 
Loss D: 0.582 
Loss G: 1.4775 (0.7898) Acc G: 22.556% 
LR: 2.000e-04 

2023-03-02 01:46:15,592 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.4227 (0.3257) Acc D Real: 74.795% 
Loss D Fake: 0.2947 (0.6276) Acc D Fake: 76.528% 
Loss D: 0.717 
Loss G: 1.4858 (0.7929) Acc G: 22.540% 
LR: 2.000e-04 

2023-03-02 01:46:15,603 -                train: [    INFO] - 
Epoch: 15/20
2023-03-02 01:46:15,786 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.4599 (0.5843) Acc D Real: 68.047% 
Loss D Fake: 0.3013 (0.2982) Acc D Fake: 90.833% 
Loss D: 0.761 
Loss G: 1.4589 (1.4676) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:46:15,793 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.5070 (0.5585) Acc D Real: 68.993% 
Loss D Fake: 0.3093 (0.3019) Acc D Fake: 90.000% 
Loss D: 0.816 
Loss G: 1.4351 (1.4568) Acc G: 11.111% 
LR: 2.000e-04 

2023-03-02 01:46:15,801 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.4257 (0.5253) Acc D Real: 71.589% 
Loss D Fake: 0.3201 (0.3065) Acc D Fake: 89.167% 
Loss D: 0.746 
Loss G: 1.4059 (1.4441) Acc G: 12.083% 
LR: 2.000e-04 

2023-03-02 01:46:15,819 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.4114 (0.5025) Acc D Real: 72.833% 
Loss D Fake: 0.3501 (0.3152) Acc D Fake: 88.000% 
Loss D: 0.761 
Loss G: 0.2681 (1.2089) Acc G: 29.667% 
LR: 2.000e-04 

2023-03-02 01:46:15,826 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.3202 (0.4721) Acc D Real: 75.321% 
Loss D Fake: 1.6642 (0.5400) Acc D Fake: 73.333% 
Loss D: 1.984 
Loss G: 0.2417 (1.0477) Acc G: 41.389% 
LR: 2.000e-04 

2023-03-02 01:46:15,833 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2436 (0.4395) Acc D Real: 77.173% 
Loss D Fake: 1.7092 (0.7070) Acc D Fake: 62.857% 
Loss D: 1.953 
Loss G: 0.2304 (0.9309) Acc G: 49.762% 
LR: 2.000e-04 

2023-03-02 01:46:15,840 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.1493 (0.4032) Acc D Real: 79.824% 
Loss D Fake: 1.7357 (0.8356) Acc D Fake: 55.000% 
Loss D: 1.885 
Loss G: 0.2254 (0.8427) Acc G: 56.042% 
LR: 2.000e-04 

2023-03-02 01:46:15,847 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.1755 (0.3779) Acc D Real: 81.852% 
Loss D Fake: 1.7410 (0.9362) Acc D Fake: 48.889% 
Loss D: 1.916 
Loss G: 0.2244 (0.7740) Acc G: 60.926% 
LR: 2.000e-04 

2023-03-02 01:46:15,855 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1780 (0.3579) Acc D Real: 83.536% 
Loss D Fake: 1.7340 (1.0160) Acc D Fake: 44.000% 
Loss D: 1.912 
Loss G: 0.2253 (0.7192) Acc G: 64.833% 
LR: 2.000e-04 

2023-03-02 01:46:15,862 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1419 (0.3383) Acc D Real: 84.910% 
Loss D Fake: 1.7183 (1.0798) Acc D Fake: 40.000% 
Loss D: 1.860 
Loss G: 0.2284 (0.6745) Acc G: 68.030% 
LR: 2.000e-04 

2023-03-02 01:46:15,869 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1597 (0.3234) Acc D Real: 86.020% 
Loss D Fake: 1.6944 (1.1311) Acc D Fake: 36.667% 
Loss D: 1.854 
Loss G: 0.2332 (0.6378) Acc G: 70.694% 
LR: 2.000e-04 

2023-03-02 01:46:15,875 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.1111 (0.3071) Acc D Real: 87.003% 
Loss D Fake: 1.6648 (1.1721) Acc D Fake: 33.846% 
Loss D: 1.776 
Loss G: 0.2395 (0.6071) Acc G: 72.949% 
LR: 2.000e-04 

2023-03-02 01:46:15,882 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.1082 (0.2929) Acc D Real: 87.835% 
Loss D Fake: 1.6303 (1.2048) Acc D Fake: 31.429% 
Loss D: 1.738 
Loss G: 0.2472 (0.5814) Acc G: 74.881% 
LR: 2.000e-04 

2023-03-02 01:46:15,889 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.1273 (0.2818) Acc D Real: 88.562% 
Loss D Fake: 1.5924 (1.2307) Acc D Fake: 29.333% 
Loss D: 1.720 
Loss G: 0.2559 (0.5597) Acc G: 76.556% 
LR: 2.000e-04 

2023-03-02 01:46:15,896 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.1701 (0.2748) Acc D Real: 89.176% 
Loss D Fake: 1.5530 (1.2508) Acc D Fake: 27.500% 
Loss D: 1.723 
Loss G: 0.2653 (0.5413) Acc G: 78.021% 
LR: 2.000e-04 

2023-03-02 01:46:15,904 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.1490 (0.2674) Acc D Real: 89.724% 
Loss D Fake: 1.5132 (1.2662) Acc D Fake: 25.882% 
Loss D: 1.662 
Loss G: 0.2754 (0.5257) Acc G: 79.314% 
LR: 2.000e-04 

2023-03-02 01:46:15,912 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1263 (0.2596) Acc D Real: 90.252% 
Loss D Fake: 1.4730 (1.2777) Acc D Fake: 24.444% 
Loss D: 1.599 
Loss G: 0.2863 (0.5124) Acc G: 80.463% 
LR: 2.000e-04 

2023-03-02 01:46:15,919 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.1321 (0.2529) Acc D Real: 90.707% 
Loss D Fake: 1.4325 (1.2859) Acc D Fake: 23.158% 
Loss D: 1.565 
Loss G: 0.2978 (0.5011) Acc G: 81.491% 
LR: 2.000e-04 

2023-03-02 01:46:15,926 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1169 (0.2461) Acc D Real: 91.112% 
Loss D Fake: 1.3919 (1.2912) Acc D Fake: 22.000% 
Loss D: 1.509 
Loss G: 0.3101 (0.4915) Acc G: 82.417% 
LR: 2.000e-04 

2023-03-02 01:46:15,934 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.1460 (0.2413) Acc D Real: 91.468% 
Loss D Fake: 1.3517 (1.2941) Acc D Fake: 20.952% 
Loss D: 1.498 
Loss G: 0.3228 (0.4835) Acc G: 83.254% 
LR: 2.000e-04 

2023-03-02 01:46:15,941 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.2032 (0.2396) Acc D Real: 91.785% 
Loss D Fake: 1.3129 (1.2949) Acc D Fake: 20.000% 
Loss D: 1.516 
Loss G: 0.3355 (0.4768) Acc G: 84.015% 
LR: 2.000e-04 

2023-03-02 01:46:15,948 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1597 (0.2361) Acc D Real: 92.072% 
Loss D Fake: 1.2760 (1.2941) Acc D Fake: 19.130% 
Loss D: 1.436 
Loss G: 0.3485 (0.4712) Acc G: 84.710% 
LR: 2.000e-04 

2023-03-02 01:46:15,956 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.1755 (0.2336) Acc D Real: 92.337% 
Loss D Fake: 1.2404 (1.2919) Acc D Fake: 18.333% 
Loss D: 1.416 
Loss G: 0.3617 (0.4666) Acc G: 85.347% 
LR: 2.000e-04 

2023-03-02 01:46:15,963 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1866 (0.2317) Acc D Real: 92.575% 
Loss D Fake: 1.2063 (1.2884) Acc D Fake: 17.600% 
Loss D: 1.393 
Loss G: 0.3749 (0.4630) Acc G: 85.933% 
LR: 2.000e-04 

2023-03-02 01:46:15,971 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.1628 (0.2291) Acc D Real: 92.823% 
Loss D Fake: 1.1738 (1.2840) Acc D Fake: 16.923% 
Loss D: 1.337 
Loss G: 0.3883 (0.4601) Acc G: 86.474% 
LR: 2.000e-04 

2023-03-02 01:46:15,979 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1791 (0.2272) Acc D Real: 93.042% 
Loss D Fake: 1.1423 (1.2788) Acc D Fake: 16.296% 
Loss D: 1.321 
Loss G: 0.4019 (0.4579) Acc G: 86.975% 
LR: 2.000e-04 

2023-03-02 01:46:15,987 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1839 (0.2257) Acc D Real: 93.237% 
Loss D Fake: 1.1121 (1.2728) Acc D Fake: 15.714% 
Loss D: 1.296 
Loss G: 0.4155 (0.4564) Acc G: 87.381% 
LR: 2.000e-04 

2023-03-02 01:46:15,995 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.2132 (0.2252) Acc D Real: 93.409% 
Loss D Fake: 1.0834 (1.2663) Acc D Fake: 15.287% 
Loss D: 1.297 
Loss G: 0.4289 (0.4555) Acc G: 87.701% 
LR: 2.000e-04 

2023-03-02 01:46:16,003 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.2195 (0.2250) Acc D Real: 93.573% 
Loss D Fake: 1.0565 (1.2593) Acc D Fake: 14.889% 
Loss D: 1.276 
Loss G: 0.4420 (0.4550) Acc G: 88.000% 
LR: 2.000e-04 

2023-03-02 01:46:16,010 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2502 (0.2259) Acc D Real: 93.701% 
Loss D Fake: 1.0313 (1.2519) Acc D Fake: 14.570% 
Loss D: 1.281 
Loss G: 0.4547 (0.4550) Acc G: 88.226% 
LR: 2.000e-04 

2023-03-02 01:46:16,018 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1535 (0.2236) Acc D Real: 93.864% 
Loss D Fake: 1.0077 (1.2443) Acc D Fake: 14.271% 
Loss D: 1.161 
Loss G: 0.4677 (0.4554) Acc G: 88.438% 
LR: 2.000e-04 

2023-03-02 01:46:16,025 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2261 (0.2237) Acc D Real: 93.996% 
Loss D Fake: 0.9844 (1.2364) Acc D Fake: 13.990% 
Loss D: 1.210 
Loss G: 0.4805 (0.4562) Acc G: 88.636% 
LR: 2.000e-04 

2023-03-02 01:46:16,033 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.2168 (0.2235) Acc D Real: 94.115% 
Loss D Fake: 0.9625 (1.2284) Acc D Fake: 13.725% 
Loss D: 1.179 
Loss G: 0.4931 (0.4573) Acc G: 88.824% 
LR: 2.000e-04 

2023-03-02 01:46:16,040 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.1516 (0.2214) Acc D Real: 94.250% 
Loss D Fake: 0.9414 (1.2202) Acc D Fake: 13.476% 
Loss D: 1.093 
Loss G: 0.5061 (0.4587) Acc G: 89.000% 
LR: 2.000e-04 

2023-03-02 01:46:16,047 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.2425 (0.2220) Acc D Real: 94.349% 
Loss D Fake: 0.9206 (1.2119) Acc D Fake: 13.241% 
Loss D: 1.163 
Loss G: 0.5189 (0.4603) Acc G: 89.167% 
LR: 2.000e-04 

2023-03-02 01:46:16,055 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2193 (0.2219) Acc D Real: 94.441% 
Loss D Fake: 0.9012 (1.2035) Acc D Fake: 13.018% 
Loss D: 1.121 
Loss G: 0.5313 (0.4622) Acc G: 89.324% 
LR: 2.000e-04 

2023-03-02 01:46:16,062 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2668 (0.2231) Acc D Real: 94.526% 
Loss D Fake: 0.8830 (1.1950) Acc D Fake: 12.807% 
Loss D: 1.150 
Loss G: 0.5432 (0.4644) Acc G: 89.474% 
LR: 2.000e-04 

2023-03-02 01:46:16,070 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.2675 (0.2242) Acc D Real: 94.611% 
Loss D Fake: 0.8665 (1.1866) Acc D Fake: 12.607% 
Loss D: 1.134 
Loss G: 0.5542 (0.4667) Acc G: 89.615% 
LR: 2.000e-04 

2023-03-02 01:46:16,078 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2812 (0.2257) Acc D Real: 94.695% 
Loss D Fake: 0.8515 (1.1782) Acc D Fake: 12.417% 
Loss D: 1.133 
Loss G: 0.5645 (0.4691) Acc G: 89.750% 
LR: 2.000e-04 

2023-03-02 01:46:16,086 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.2699 (0.2268) Acc D Real: 94.771% 
Loss D Fake: 0.8378 (1.1699) Acc D Fake: 12.236% 
Loss D: 1.108 
Loss G: 0.5744 (0.4717) Acc G: 89.878% 
LR: 2.000e-04 

2023-03-02 01:46:16,094 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3404 (0.2295) Acc D Real: 94.823% 
Loss D Fake: 0.8252 (1.1617) Acc D Fake: 12.063% 
Loss D: 1.166 
Loss G: 0.5832 (0.4743) Acc G: 90.000% 
LR: 2.000e-04 

2023-03-02 01:46:16,101 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3254 (0.2317) Acc D Real: 94.875% 
Loss D Fake: 0.8143 (1.1536) Acc D Fake: 11.899% 
Loss D: 1.140 
Loss G: 0.5912 (0.4771) Acc G: 90.078% 
LR: 2.000e-04 

2023-03-02 01:46:16,109 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3460 (0.2343) Acc D Real: 94.925% 
Loss D Fake: 0.8047 (1.1457) Acc D Fake: 11.780% 
Loss D: 1.151 
Loss G: 0.5982 (0.4798) Acc G: 90.152% 
LR: 2.000e-04 

2023-03-02 01:46:16,116 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2659 (0.2350) Acc D Real: 94.985% 
Loss D Fake: 0.7962 (1.1379) Acc D Fake: 11.667% 
Loss D: 1.062 
Loss G: 0.6051 (0.4826) Acc G: 90.222% 
LR: 2.000e-04 

2023-03-02 01:46:16,123 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3346 (0.2372) Acc D Real: 95.026% 
Loss D Fake: 0.7878 (1.1303) Acc D Fake: 11.558% 
Loss D: 1.122 
Loss G: 0.6115 (0.4854) Acc G: 90.290% 
LR: 2.000e-04 

2023-03-02 01:46:16,131 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1986 (0.2363) Acc D Real: 95.079% 
Loss D Fake: 0.7800 (1.1229) Acc D Fake: 11.489% 
Loss D: 0.979 
Loss G: 0.6184 (0.4882) Acc G: 90.319% 
LR: 2.000e-04 

2023-03-02 01:46:16,139 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3099 (0.2379) Acc D Real: 95.081% 
Loss D Fake: 0.7716 (1.1156) Acc D Fake: 11.424% 
Loss D: 1.082 
Loss G: 0.6252 (0.4911) Acc G: 90.347% 
LR: 2.000e-04 

2023-03-02 01:46:16,147 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2932 (0.2390) Acc D Real: 95.102% 
Loss D Fake: 0.7639 (1.1084) Acc D Fake: 11.361% 
Loss D: 1.057 
Loss G: 0.6317 (0.4940) Acc G: 90.374% 
LR: 2.000e-04 

2023-03-02 01:46:16,154 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2622 (0.2395) Acc D Real: 95.126% 
Loss D Fake: 0.7563 (1.1013) Acc D Fake: 11.300% 
Loss D: 1.018 
Loss G: 0.6383 (0.4968) Acc G: 90.400% 
LR: 2.000e-04 

2023-03-02 01:46:16,162 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3565 (0.2418) Acc D Real: 95.111% 
Loss D Fake: 0.7490 (1.0944) Acc D Fake: 11.242% 
Loss D: 1.106 
Loss G: 0.6443 (0.4997) Acc G: 90.392% 
LR: 2.000e-04 

2023-03-02 01:46:16,169 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3955 (0.2447) Acc D Real: 95.063% 
Loss D Fake: 0.7430 (1.0877) Acc D Fake: 11.250% 
Loss D: 1.139 
Loss G: 0.6489 (0.5026) Acc G: 90.353% 
LR: 2.000e-04 

2023-03-02 01:46:16,176 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2453 (0.2447) Acc D Real: 95.084% 
Loss D Fake: 0.7380 (1.0811) Acc D Fake: 11.258% 
Loss D: 0.983 
Loss G: 0.6539 (0.5055) Acc G: 90.283% 
LR: 2.000e-04 

2023-03-02 01:46:16,184 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2669 (0.2451) Acc D Real: 95.083% 
Loss D Fake: 0.7322 (1.0746) Acc D Fake: 11.296% 
Loss D: 0.999 
Loss G: 0.6593 (0.5083) Acc G: 90.216% 
LR: 2.000e-04 

2023-03-02 01:46:16,191 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2455 (0.2451) Acc D Real: 95.108% 
Loss D Fake: 0.7262 (1.0683) Acc D Fake: 11.364% 
Loss D: 0.972 
Loss G: 0.6651 (0.5112) Acc G: 90.121% 
LR: 2.000e-04 

2023-03-02 01:46:16,199 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3872 (0.2477) Acc D Real: 95.052% 
Loss D Fake: 0.7202 (1.0621) Acc D Fake: 11.458% 
Loss D: 1.107 
Loss G: 0.6701 (0.5140) Acc G: 89.970% 
LR: 2.000e-04 

2023-03-02 01:46:16,206 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3111 (0.2488) Acc D Real: 95.007% 
Loss D Fake: 0.7153 (1.0560) Acc D Fake: 11.579% 
Loss D: 1.026 
Loss G: 0.6746 (0.5168) Acc G: 89.795% 
LR: 2.000e-04 

2023-03-02 01:46:16,214 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2159 (0.2482) Acc D Real: 95.012% 
Loss D Fake: 0.7103 (1.0500) Acc D Fake: 11.724% 
Loss D: 0.926 
Loss G: 0.6798 (0.5196) Acc G: 89.598% 
LR: 2.000e-04 

2023-03-02 01:46:16,221 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3612 (0.2501) Acc D Real: 94.916% 
Loss D Fake: 0.7049 (1.0442) Acc D Fake: 11.893% 
Loss D: 1.066 
Loss G: 0.6846 (0.5224) Acc G: 89.379% 
LR: 2.000e-04 

2023-03-02 01:46:16,229 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.2804 (0.2506) Acc D Real: 94.832% 
Loss D Fake: 0.7002 (1.0384) Acc D Fake: 12.111% 
Loss D: 0.981 
Loss G: 0.6893 (0.5252) Acc G: 89.111% 
LR: 2.000e-04 

2023-03-02 01:46:16,236 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3812 (0.2528) Acc D Real: 94.624% 
Loss D Fake: 0.6957 (1.0328) Acc D Fake: 12.377% 
Loss D: 1.077 
Loss G: 0.6932 (0.5280) Acc G: 88.798% 
LR: 2.000e-04 

2023-03-02 01:46:16,244 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3877 (0.2550) Acc D Real: 94.267% 
Loss D Fake: 0.6924 (1.0273) Acc D Fake: 12.688% 
Loss D: 1.080 
Loss G: 0.6960 (0.5307) Acc G: 88.414% 
LR: 2.000e-04 

2023-03-02 01:46:16,252 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2600 (0.2550) Acc D Real: 94.136% 
Loss D Fake: 0.6896 (1.0220) Acc D Fake: 13.095% 
Loss D: 0.950 
Loss G: 0.6992 (0.5333) Acc G: 87.857% 
LR: 2.000e-04 

2023-03-02 01:46:16,260 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.2598 (0.2551) Acc D Real: 94.032% 
Loss D Fake: 0.6861 (1.0167) Acc D Fake: 13.802% 
Loss D: 0.946 
Loss G: 0.7030 (0.5360) Acc G: 86.536% 
LR: 2.000e-04 

2023-03-02 01:46:16,268 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3314 (0.2563) Acc D Real: 93.753% 
Loss D Fake: 0.6823 (1.0116) Acc D Fake: 15.077% 
Loss D: 1.014 
Loss G: 0.7067 (0.5386) Acc G: 85.256% 
LR: 2.000e-04 

2023-03-02 01:46:16,275 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4157 (0.2587) Acc D Real: 93.300% 
Loss D Fake: 0.6792 (1.0065) Acc D Fake: 16.313% 
Loss D: 1.095 
Loss G: 0.7092 (0.5412) Acc G: 84.015% 
LR: 2.000e-04 

2023-03-02 01:46:16,283 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3268 (0.2597) Acc D Real: 93.048% 
Loss D Fake: 0.6770 (1.0016) Acc D Fake: 17.512% 
Loss D: 1.004 
Loss G: 0.7114 (0.5437) Acc G: 82.836% 
LR: 2.000e-04 

2023-03-02 01:46:16,290 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.2756 (0.2600) Acc D Real: 92.738% 
Loss D Fake: 0.6747 (0.9968) Acc D Fake: 18.652% 
Loss D: 0.950 
Loss G: 0.7142 (0.5462) Acc G: 81.691% 
LR: 2.000e-04 

2023-03-02 01:46:16,298 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.2066 (0.2592) Acc D Real: 92.610% 
Loss D Fake: 0.6716 (0.9921) Acc D Fake: 19.758% 
Loss D: 0.878 
Loss G: 0.7181 (0.5487) Acc G: 80.580% 
LR: 2.000e-04 

2023-03-02 01:46:16,305 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.3074 (0.2599) Acc D Real: 92.317% 
Loss D Fake: 0.6675 (0.9875) Acc D Fake: 20.833% 
Loss D: 0.975 
Loss G: 0.7223 (0.5512) Acc G: 79.500% 
LR: 2.000e-04 

2023-03-02 01:46:16,313 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2702 (0.2600) Acc D Real: 92.017% 
Loss D Fake: 0.6635 (0.9829) Acc D Fake: 21.878% 
Loss D: 0.934 
Loss G: 0.7268 (0.5537) Acc G: 78.451% 
LR: 2.000e-04 

2023-03-02 01:46:16,320 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3399 (0.2611) Acc D Real: 91.579% 
Loss D Fake: 0.6595 (0.9784) Acc D Fake: 22.894% 
Loss D: 0.999 
Loss G: 0.7308 (0.5561) Acc G: 77.431% 
LR: 2.000e-04 

2023-03-02 01:46:16,328 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.2724 (0.2613) Acc D Real: 91.255% 
Loss D Fake: 0.6558 (0.9740) Acc D Fake: 23.881% 
Loss D: 0.928 
Loss G: 0.7349 (0.5586) Acc G: 76.438% 
LR: 2.000e-04 

2023-03-02 01:46:16,336 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.4001 (0.2632) Acc D Real: 90.726% 
Loss D Fake: 0.6527 (0.9696) Acc D Fake: 24.842% 
Loss D: 1.053 
Loss G: 0.7367 (0.5610) Acc G: 75.473% 
LR: 2.000e-04 

2023-03-02 01:46:16,344 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3290 (0.2640) Acc D Real: 90.316% 
Loss D Fake: 0.6518 (0.9654) Acc D Fake: 25.778% 
Loss D: 0.981 
Loss G: 0.7378 (0.5634) Acc G: 74.533% 
LR: 2.000e-04 

2023-03-02 01:46:16,351 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4039 (0.2659) Acc D Real: 89.790% 
Loss D Fake: 0.6510 (0.9613) Acc D Fake: 26.689% 
Loss D: 1.055 
Loss G: 0.7382 (0.5657) Acc G: 73.618% 
LR: 2.000e-04 

2023-03-02 01:46:16,359 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2309 (0.2654) Acc D Real: 89.573% 
Loss D Fake: 0.6505 (0.9572) Acc D Fake: 27.576% 
Loss D: 0.881 
Loss G: 0.7395 (0.5679) Acc G: 72.727% 
LR: 2.000e-04 

2023-03-02 01:46:16,367 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3245 (0.2662) Acc D Real: 89.200% 
Loss D Fake: 0.6490 (0.9533) Acc D Fake: 28.440% 
Loss D: 0.973 
Loss G: 0.7412 (0.5701) Acc G: 71.859% 
LR: 2.000e-04 

2023-03-02 01:46:16,374 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3051 (0.2667) Acc D Real: 88.879% 
Loss D Fake: 0.6474 (0.9494) Acc D Fake: 29.283% 
Loss D: 0.952 
Loss G: 0.7431 (0.5723) Acc G: 71.013% 
LR: 2.000e-04 

2023-03-02 01:46:16,381 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3479 (0.2677) Acc D Real: 88.503% 
Loss D Fake: 0.6457 (0.9456) Acc D Fake: 30.104% 
Loss D: 0.994 
Loss G: 0.7448 (0.5745) Acc G: 70.188% 
LR: 2.000e-04 

2023-03-02 01:46:16,389 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.2812 (0.2678) Acc D Real: 88.241% 
Loss D Fake: 0.6442 (0.9419) Acc D Fake: 30.905% 
Loss D: 0.925 
Loss G: 0.7469 (0.5766) Acc G: 69.383% 
LR: 2.000e-04 

2023-03-02 01:46:16,396 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3853 (0.2693) Acc D Real: 87.812% 
Loss D Fake: 0.6424 (0.9382) Acc D Fake: 31.646% 
Loss D: 1.028 
Loss G: 0.7484 (0.5787) Acc G: 68.638% 
LR: 2.000e-04 

2023-03-02 01:46:16,403 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3271 (0.2700) Acc D Real: 87.478% 
Loss D Fake: 0.6413 (0.9347) Acc D Fake: 32.369% 
Loss D: 0.968 
Loss G: 0.7497 (0.5808) Acc G: 67.912% 
LR: 2.000e-04 

2023-03-02 01:46:16,412 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3520 (0.2710) Acc D Real: 87.129% 
Loss D Fake: 0.6402 (0.9312) Acc D Fake: 33.075% 
Loss D: 0.992 
Loss G: 0.7508 (0.5828) Acc G: 67.202% 
LR: 2.000e-04 

2023-03-02 01:46:16,419 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3910 (0.2724) Acc D Real: 86.721% 
Loss D Fake: 0.6395 (0.9277) Acc D Fake: 33.765% 
Loss D: 1.030 
Loss G: 0.7512 (0.5848) Acc G: 66.510% 
LR: 2.000e-04 

2023-03-02 01:46:16,427 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.3263 (0.2730) Acc D Real: 86.424% 
Loss D Fake: 0.6392 (0.9244) Acc D Fake: 34.438% 
Loss D: 0.966 
Loss G: 0.7517 (0.5867) Acc G: 65.833% 
LR: 2.000e-04 

2023-03-02 01:46:16,435 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3499 (0.2739) Acc D Real: 86.097% 
Loss D Fake: 0.6388 (0.9211) Acc D Fake: 35.096% 
Loss D: 0.989 
Loss G: 0.7520 (0.5886) Acc G: 65.172% 
LR: 2.000e-04 

2023-03-02 01:46:16,442 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.4604 (0.2760) Acc D Real: 85.620% 
Loss D Fake: 0.6391 (0.9179) Acc D Fake: 35.739% 
Loss D: 1.100 
Loss G: 0.7506 (0.5905) Acc G: 64.527% 
LR: 2.000e-04 

2023-03-02 01:46:16,449 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.2828 (0.2761) Acc D Real: 85.408% 
Loss D Fake: 0.6406 (0.9148) Acc D Fake: 36.367% 
Loss D: 0.923 
Loss G: 0.7495 (0.5922) Acc G: 63.895% 
LR: 2.000e-04 

2023-03-02 01:46:16,457 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4206 (0.2777) Acc D Real: 85.001% 
Loss D Fake: 0.6415 (0.9117) Acc D Fake: 36.963% 
Loss D: 1.062 
Loss G: 0.7481 (0.5940) Acc G: 63.296% 
LR: 2.000e-04 

2023-03-02 01:46:16,464 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.2789 (0.2777) Acc D Real: 84.803% 
Loss D Fake: 0.6427 (0.9088) Acc D Fake: 37.546% 
Loss D: 0.922 
Loss G: 0.7474 (0.5957) Acc G: 62.711% 
LR: 2.000e-04 

2023-03-02 01:46:16,471 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.2334 (0.2772) Acc D Real: 84.674% 
Loss D Fake: 0.6427 (0.9059) Acc D Fake: 38.098% 
Loss D: 0.876 
Loss G: 0.7482 (0.5973) Acc G: 62.156% 
LR: 2.000e-04 

2023-03-02 01:46:16,478 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2835 (0.2773) Acc D Real: 84.477% 
Loss D Fake: 0.6416 (0.9030) Acc D Fake: 38.638% 
Loss D: 0.925 
Loss G: 0.7498 (0.5990) Acc G: 61.613% 
LR: 2.000e-04 

2023-03-02 01:46:16,486 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3728 (0.2783) Acc D Real: 84.162% 
Loss D Fake: 0.6402 (0.9003) Acc D Fake: 39.167% 
Loss D: 1.013 
Loss G: 0.7510 (0.6006) Acc G: 61.082% 
LR: 2.000e-04 

2023-03-02 01:46:16,495 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.2622 (0.2781) Acc D Real: 84.008% 
Loss D Fake: 0.6391 (0.8975) Acc D Fake: 39.684% 
Loss D: 0.901 
Loss G: 0.7528 (0.6022) Acc G: 60.561% 
LR: 2.000e-04 

2023-03-02 01:46:16,503 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4445 (0.2799) Acc D Real: 83.610% 
Loss D Fake: 0.6377 (0.8948) Acc D Fake: 40.191% 
Loss D: 1.082 
Loss G: 0.7535 (0.6038) Acc G: 60.052% 
LR: 2.000e-04 

2023-03-02 01:46:16,511 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.2840 (0.2799) Acc D Real: 83.439% 
Loss D Fake: 0.6373 (0.8921) Acc D Fake: 40.687% 
Loss D: 0.921 
Loss G: 0.7544 (0.6053) Acc G: 59.553% 
LR: 2.000e-04 

2023-03-02 01:46:16,518 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3265 (0.2804) Acc D Real: 83.223% 
Loss D Fake: 0.6364 (0.8895) Acc D Fake: 41.173% 
Loss D: 0.963 
Loss G: 0.7555 (0.6068) Acc G: 59.065% 
LR: 2.000e-04 

2023-03-02 01:46:16,525 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2359 (0.2799) Acc D Real: 83.130% 
Loss D Fake: 0.6351 (0.8870) Acc D Fake: 41.650% 
Loss D: 0.871 
Loss G: 0.7575 (0.6084) Acc G: 58.586% 
LR: 2.000e-04 

2023-03-02 01:46:16,532 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2429 (0.2796) Acc D Real: 83.032% 
Loss D Fake: 0.6329 (0.8844) Acc D Fake: 42.117% 
Loss D: 0.876 
Loss G: 0.7606 (0.6099) Acc G: 58.117% 
LR: 2.000e-04 

2023-03-02 01:46:16,540 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2626 (0.2794) Acc D Real: 82.902% 
Loss D Fake: 0.6300 (0.8819) Acc D Fake: 42.574% 
Loss D: 0.893 
Loss G: 0.7642 (0.6114) Acc G: 57.657% 
LR: 2.000e-04 

2023-03-02 01:46:16,547 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3074 (0.2797) Acc D Real: 82.724% 
Loss D Fake: 0.6268 (0.8794) Acc D Fake: 43.023% 
Loss D: 0.934 
Loss G: 0.7678 (0.6129) Acc G: 57.206% 
LR: 2.000e-04 

2023-03-02 01:46:16,554 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.2666 (0.2795) Acc D Real: 82.604% 
Loss D Fake: 0.6237 (0.8769) Acc D Fake: 43.463% 
Loss D: 0.890 
Loss G: 0.7716 (0.6145) Acc G: 56.764% 
LR: 2.000e-04 

2023-03-02 01:46:16,561 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3266 (0.2800) Acc D Real: 82.416% 
Loss D Fake: 0.6206 (0.8745) Acc D Fake: 43.894% 
Loss D: 0.947 
Loss G: 0.7751 (0.6160) Acc G: 56.330% 
LR: 2.000e-04 

2023-03-02 01:46:16,569 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2929 (0.2801) Acc D Real: 82.268% 
Loss D Fake: 0.6178 (0.8720) Acc D Fake: 44.317% 
Loss D: 0.911 
Loss G: 0.7785 (0.6176) Acc G: 55.905% 
LR: 2.000e-04 

2023-03-02 01:46:16,576 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3868 (0.2811) Acc D Real: 82.014% 
Loss D Fake: 0.6153 (0.8696) Acc D Fake: 44.733% 
Loss D: 1.002 
Loss G: 0.7809 (0.6191) Acc G: 55.487% 
LR: 2.000e-04 

2023-03-02 01:46:16,583 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2196 (0.2805) Acc D Real: 81.963% 
Loss D Fake: 0.6132 (0.8672) Acc D Fake: 45.140% 
Loss D: 0.833 
Loss G: 0.7840 (0.6207) Acc G: 55.078% 
LR: 2.000e-04 

2023-03-02 01:46:16,590 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2977 (0.2807) Acc D Real: 81.822% 
Loss D Fake: 0.6105 (0.8648) Acc D Fake: 45.540% 
Loss D: 0.908 
Loss G: 0.7873 (0.6222) Acc G: 54.676% 
LR: 2.000e-04 

2023-03-02 01:46:16,597 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3739 (0.2816) Acc D Real: 81.595% 
Loss D Fake: 0.6080 (0.8625) Acc D Fake: 45.933% 
Loss D: 0.982 
Loss G: 0.7897 (0.6237) Acc G: 54.281% 
LR: 2.000e-04 

2023-03-02 01:46:16,605 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3167 (0.2819) Acc D Real: 81.440% 
Loss D Fake: 0.6063 (0.8601) Acc D Fake: 46.318% 
Loss D: 0.923 
Loss G: 0.7918 (0.6253) Acc G: 53.894% 
LR: 2.000e-04 

2023-03-02 01:46:16,612 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.2760 (0.2818) Acc D Real: 81.336% 
Loss D Fake: 0.6046 (0.8578) Acc D Fake: 46.697% 
Loss D: 0.881 
Loss G: 0.7942 (0.6268) Acc G: 53.514% 
LR: 2.000e-04 

2023-03-02 01:46:16,619 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3440 (0.2824) Acc D Real: 81.148% 
Loss D Fake: 0.6027 (0.8555) Acc D Fake: 47.068% 
Loss D: 0.947 
Loss G: 0.7962 (0.6283) Acc G: 53.140% 
LR: 2.000e-04 

2023-03-02 01:46:16,626 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.2726 (0.2823) Acc D Real: 81.047% 
Loss D Fake: 0.6011 (0.8533) Acc D Fake: 47.434% 
Loss D: 0.874 
Loss G: 0.7984 (0.6298) Acc G: 52.773% 
LR: 2.000e-04 

2023-03-02 01:46:16,634 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3488 (0.2829) Acc D Real: 80.864% 
Loss D Fake: 0.5994 (0.8511) Acc D Fake: 47.792% 
Loss D: 0.948 
Loss G: 0.8004 (0.6313) Acc G: 52.412% 
LR: 2.000e-04 

2023-03-02 01:46:16,641 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3459 (0.2834) Acc D Real: 80.697% 
Loss D Fake: 0.5981 (0.8489) Acc D Fake: 48.145% 
Loss D: 0.944 
Loss G: 0.8016 (0.6328) Acc G: 52.058% 
LR: 2.000e-04 

2023-03-02 01:46:16,648 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.2512 (0.2831) Acc D Real: 80.628% 
Loss D Fake: 0.5972 (0.8467) Acc D Fake: 48.491% 
Loss D: 0.848 
Loss G: 0.8033 (0.6343) Acc G: 51.710% 
LR: 2.000e-04 

2023-03-02 01:46:16,656 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2751 (0.2831) Acc D Real: 80.536% 
Loss D Fake: 0.5955 (0.8446) Acc D Fake: 48.832% 
Loss D: 0.871 
Loss G: 0.8056 (0.6357) Acc G: 51.368% 
LR: 2.000e-04 

2023-03-02 01:46:16,663 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.2473 (0.2828) Acc D Real: 80.480% 
Loss D Fake: 0.5935 (0.8424) Acc D Fake: 49.167% 
Loss D: 0.841 
Loss G: 0.8085 (0.6372) Acc G: 51.031% 
LR: 2.000e-04 

2023-03-02 01:46:16,671 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3745 (0.2835) Acc D Real: 80.288% 
Loss D Fake: 0.5914 (0.8403) Acc D Fake: 49.496% 
Loss D: 0.966 
Loss G: 0.8108 (0.6386) Acc G: 50.700% 
LR: 2.000e-04 

2023-03-02 01:46:16,678 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2036 (0.2829) Acc D Real: 80.280% 
Loss D Fake: 0.5896 (0.8382) Acc D Fake: 49.819% 
Loss D: 0.793 
Loss G: 0.8136 (0.6401) Acc G: 50.375% 
LR: 2.000e-04 

2023-03-02 01:46:16,685 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2747 (0.2828) Acc D Real: 80.201% 
Loss D Fake: 0.5872 (0.8362) Acc D Fake: 50.138% 
Loss D: 0.862 
Loss G: 0.8168 (0.6416) Acc G: 50.055% 
LR: 2.000e-04 

2023-03-02 01:46:16,692 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3085 (0.2830) Acc D Real: 80.082% 
Loss D Fake: 0.5848 (0.8341) Acc D Fake: 50.451% 
Loss D: 0.893 
Loss G: 0.8199 (0.6430) Acc G: 49.740% 
LR: 2.000e-04 

2023-03-02 01:46:16,700 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3311 (0.2834) Acc D Real: 79.946% 
Loss D Fake: 0.5827 (0.8321) Acc D Fake: 50.759% 
Loss D: 0.914 
Loss G: 0.8224 (0.6445) Acc G: 49.431% 
LR: 2.000e-04 

2023-03-02 01:46:16,708 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3136 (0.2837) Acc D Real: 79.835% 
Loss D Fake: 0.5810 (0.8300) Acc D Fake: 51.062% 
Loss D: 0.895 
Loss G: 0.8246 (0.6459) Acc G: 49.126% 
LR: 2.000e-04 

2023-03-02 01:46:16,715 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.2098 (0.2831) Acc D Real: 79.832% 
Loss D Fake: 0.5791 (0.8280) Acc D Fake: 51.360% 
Loss D: 0.789 
Loss G: 0.8278 (0.6474) Acc G: 48.827% 
LR: 2.000e-04 

2023-03-02 01:46:16,723 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3423 (0.2835) Acc D Real: 79.698% 
Loss D Fake: 0.5767 (0.8260) Acc D Fake: 51.653% 
Loss D: 0.919 
Loss G: 0.8306 (0.6488) Acc G: 48.532% 
LR: 2.000e-04 

2023-03-02 01:46:16,732 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3639 (0.2842) Acc D Real: 79.541% 
Loss D Fake: 0.5749 (0.8240) Acc D Fake: 51.942% 
Loss D: 0.939 
Loss G: 0.8325 (0.6503) Acc G: 48.241% 
LR: 2.000e-04 

2023-03-02 01:46:16,740 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.2379 (0.2838) Acc D Real: 79.506% 
Loss D Fake: 0.5735 (0.8221) Acc D Fake: 52.227% 
Loss D: 0.811 
Loss G: 0.8349 (0.6517) Acc G: 47.956% 
LR: 2.000e-04 

2023-03-02 01:46:16,748 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.2601 (0.2836) Acc D Real: 79.459% 
Loss D Fake: 0.5716 (0.8201) Acc D Fake: 52.506% 
Loss D: 0.832 
Loss G: 0.8378 (0.6532) Acc G: 47.674% 
LR: 2.000e-04 

2023-03-02 01:46:16,755 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3192 (0.2839) Acc D Real: 79.357% 
Loss D Fake: 0.5694 (0.8182) Acc D Fake: 52.782% 
Loss D: 0.889 
Loss G: 0.8405 (0.6546) Acc G: 47.397% 
LR: 2.000e-04 

2023-03-02 01:46:16,763 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.2378 (0.2835) Acc D Real: 79.327% 
Loss D Fake: 0.5674 (0.8163) Acc D Fake: 53.053% 
Loss D: 0.805 
Loss G: 0.8437 (0.6561) Acc G: 47.125% 
LR: 2.000e-04 

2023-03-02 01:46:16,770 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3156 (0.2838) Acc D Real: 79.233% 
Loss D Fake: 0.5651 (0.8144) Acc D Fake: 53.321% 
Loss D: 0.881 
Loss G: 0.8466 (0.6575) Acc G: 46.856% 
LR: 2.000e-04 

2023-03-02 01:46:16,778 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.2901 (0.2838) Acc D Real: 79.158% 
Loss D Fake: 0.5631 (0.8125) Acc D Fake: 53.584% 
Loss D: 0.853 
Loss G: 0.8494 (0.6589) Acc G: 46.591% 
LR: 2.000e-04 

2023-03-02 01:46:16,786 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2643 (0.2837) Acc D Real: 79.113% 
Loss D Fake: 0.5610 (0.8106) Acc D Fake: 53.843% 
Loss D: 0.825 
Loss G: 0.8524 (0.6604) Acc G: 46.331% 
LR: 2.000e-04 

2023-03-02 01:46:16,793 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3505 (0.2842) Acc D Real: 78.992% 
Loss D Fake: 0.5591 (0.8088) Acc D Fake: 54.099% 
Loss D: 0.910 
Loss G: 0.8547 (0.6618) Acc G: 46.074% 
LR: 2.000e-04 

2023-03-02 01:46:16,801 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3455 (0.2846) Acc D Real: 78.873% 
Loss D Fake: 0.5577 (0.8069) Acc D Fake: 54.350% 
Loss D: 0.903 
Loss G: 0.8563 (0.6633) Acc G: 45.821% 
LR: 2.000e-04 

2023-03-02 01:46:16,808 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3682 (0.2852) Acc D Real: 78.737% 
Loss D Fake: 0.5569 (0.8051) Acc D Fake: 54.599% 
Loss D: 0.925 
Loss G: 0.8570 (0.6647) Acc G: 45.572% 
LR: 2.000e-04 

2023-03-02 01:46:16,816 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.4246 (0.2863) Acc D Real: 78.562% 
Loss D Fake: 0.5569 (0.8033) Acc D Fake: 54.843% 
Loss D: 0.982 
Loss G: 0.8563 (0.6661) Acc G: 45.326% 
LR: 2.000e-04 

2023-03-02 01:46:16,824 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.3209 (0.2865) Acc D Real: 78.476% 
Loss D Fake: 0.5576 (0.8015) Acc D Fake: 55.084% 
Loss D: 0.879 
Loss G: 0.8555 (0.6674) Acc G: 45.084% 
LR: 2.000e-04 

2023-03-02 01:46:16,831 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2961 (0.2866) Acc D Real: 78.414% 
Loss D Fake: 0.5581 (0.7998) Acc D Fake: 55.321% 
Loss D: 0.854 
Loss G: 0.8550 (0.6688) Acc G: 44.845% 
LR: 2.000e-04 

2023-03-02 01:46:16,839 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.2988 (0.2867) Acc D Real: 78.350% 
Loss D Fake: 0.5583 (0.7981) Acc D Fake: 55.556% 
Loss D: 0.857 
Loss G: 0.8551 (0.6701) Acc G: 44.610% 
LR: 2.000e-04 

2023-03-02 01:46:16,846 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1406 (0.2856) Acc D Real: 78.413% 
Loss D Fake: 0.5578 (0.7964) Acc D Fake: 55.786% 
Loss D: 0.698 
Loss G: 0.8571 (0.6714) Acc G: 44.378% 
LR: 2.000e-04 

2023-03-02 01:46:16,853 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.4727 (0.2869) Acc D Real: 78.203% 
Loss D Fake: 0.5564 (0.7947) Acc D Fake: 56.014% 
Loss D: 1.029 
Loss G: 0.8577 (0.6727) Acc G: 44.149% 
LR: 2.000e-04 

2023-03-02 01:46:16,861 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3460 (0.2873) Acc D Real: 78.101% 
Loss D Fake: 0.5566 (0.7931) Acc D Fake: 56.238% 
Loss D: 0.903 
Loss G: 0.8574 (0.6740) Acc G: 43.924% 
LR: 2.000e-04 

2023-03-02 01:46:16,869 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2912 (0.2874) Acc D Real: 78.047% 
Loss D Fake: 0.5569 (0.7914) Acc D Fake: 56.460% 
Loss D: 0.848 
Loss G: 0.8573 (0.6752) Acc G: 43.701% 
LR: 2.000e-04 

2023-03-02 01:46:16,876 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3564 (0.2878) Acc D Real: 77.942% 
Loss D Fake: 0.5570 (0.7898) Acc D Fake: 56.678% 
Loss D: 0.913 
Loss G: 0.8570 (0.6765) Acc G: 43.482% 
LR: 2.000e-04 

2023-03-02 01:46:16,884 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.2931 (0.2879) Acc D Real: 77.879% 
Loss D Fake: 0.5572 (0.7882) Acc D Fake: 56.893% 
Loss D: 0.850 
Loss G: 0.8569 (0.6777) Acc G: 43.265% 
LR: 2.000e-04 

2023-03-02 01:46:16,892 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3502 (0.2883) Acc D Real: 77.782% 
Loss D Fake: 0.5573 (0.7867) Acc D Fake: 57.106% 
Loss D: 0.908 
Loss G: 0.8566 (0.6789) Acc G: 43.052% 
LR: 2.000e-04 

2023-03-02 01:46:16,899 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3279 (0.2886) Acc D Real: 77.704% 
Loss D Fake: 0.5576 (0.7851) Acc D Fake: 57.315% 
Loss D: 0.885 
Loss G: 0.8563 (0.6801) Acc G: 42.841% 
LR: 2.000e-04 

2023-03-02 01:46:16,907 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3442 (0.2889) Acc D Real: 77.608% 
Loss D Fake: 0.5580 (0.7836) Acc D Fake: 57.522% 
Loss D: 0.902 
Loss G: 0.8557 (0.6813) Acc G: 42.633% 
LR: 2.000e-04 

2023-03-02 01:46:16,914 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3168 (0.2891) Acc D Real: 77.539% 
Loss D Fake: 0.5584 (0.7821) Acc D Fake: 57.726% 
Loss D: 0.875 
Loss G: 0.8552 (0.6824) Acc G: 42.428% 
LR: 2.000e-04 

2023-03-02 01:46:16,922 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.3203 (0.2893) Acc D Real: 77.466% 
Loss D Fake: 0.5588 (0.7807) Acc D Fake: 57.917% 
Loss D: 0.879 
Loss G: 0.8548 (0.6836) Acc G: 42.237% 
LR: 2.000e-04 

2023-03-02 01:46:16,929 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3219 (0.2895) Acc D Real: 77.394% 
Loss D Fake: 0.5591 (0.7792) Acc D Fake: 58.105% 
Loss D: 0.881 
Loss G: 0.8545 (0.6847) Acc G: 42.048% 
LR: 2.000e-04 

2023-03-02 01:46:16,937 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4010 (0.2903) Acc D Real: 77.266% 
Loss D Fake: 0.5648 (0.7778) Acc D Fake: 58.290% 
Loss D: 0.966 
Loss G: 0.8360 (0.6857) Acc G: 41.872% 
LR: 2.000e-04 

2023-03-02 01:46:16,944 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.1888 (0.2896) Acc D Real: 77.299% 
Loss D Fake: 0.5870 (0.7766) Acc D Fake: 58.462% 
Loss D: 0.776 
Loss G: 0.8083 (0.6865) Acc G: 41.710% 
LR: 2.000e-04 

2023-03-02 01:46:16,952 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3539 (0.2900) Acc D Real: 77.202% 
Loss D Fake: 0.6279 (0.7756) Acc D Fake: 58.600% 
Loss D: 0.982 
Loss G: 0.7358 (0.6868) Acc G: 41.624% 
LR: 2.000e-04 

2023-03-02 01:46:16,959 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3623 (0.2905) Acc D Real: 77.097% 
Loss D Fake: 5.0388 (0.8028) Acc D Fake: 58.227% 
Loss D: 5.401 
Loss G: 0.0685 (0.6828) Acc G: 41.996% 
LR: 2.000e-04 

2023-03-02 01:46:16,967 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2267 (0.2901) Acc D Real: 77.102% 
Loss D Fake: 5.2434 (0.8309) Acc D Fake: 57.859% 
Loss D: 5.470 
Loss G: 0.0624 (0.6789) Acc G: 42.363% 
LR: 2.000e-04 

2023-03-02 01:46:16,974 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3774 (0.2906) Acc D Real: 76.986% 
Loss D Fake: 5.2685 (0.8588) Acc D Fake: 57.495% 
Loss D: 5.646 
Loss G: 0.0589 (0.6750) Acc G: 42.725% 
LR: 2.000e-04 

2023-03-02 01:46:16,981 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.2126 (0.2901) Acc D Real: 76.995% 
Loss D Fake: 5.2524 (0.8863) Acc D Fake: 57.135% 
Loss D: 5.465 
Loss G: 0.0566 (0.6712) Acc G: 43.083% 
LR: 2.000e-04 

2023-03-02 01:46:16,989 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2932 (0.2902) Acc D Real: 76.938% 
Loss D Fake: 5.2154 (0.9132) Acc D Fake: 56.781% 
Loss D: 5.509 
Loss G: 0.0551 (0.6673) Acc G: 43.437% 
LR: 2.000e-04 

2023-03-02 01:46:16,996 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3751 (0.2907) Acc D Real: 76.817% 
Loss D Fake: 5.1642 (0.9394) Acc D Fake: 56.430% 
Loss D: 5.539 
Loss G: 0.0541 (0.6635) Acc G: 43.786% 
LR: 2.000e-04 

2023-03-02 01:46:17,004 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.2122 (0.2902) Acc D Real: 76.825% 
Loss D Fake: 5.1022 (0.9649) Acc D Fake: 56.084% 
Loss D: 5.314 
Loss G: 0.0535 (0.6598) Acc G: 44.131% 
LR: 2.000e-04 

2023-03-02 01:46:17,011 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.1049 (0.2891) Acc D Real: 76.917% 
Loss D Fake: 5.0320 (0.9897) Acc D Fake: 55.742% 
Loss D: 5.137 
Loss G: 0.0531 (0.6561) Acc G: 44.472% 
LR: 2.000e-04 

2023-03-02 01:46:17,019 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1289 (0.2881) Acc D Real: 76.991% 
Loss D Fake: 4.9558 (1.0138) Acc D Fake: 55.404% 
Loss D: 5.085 
Loss G: 0.0530 (0.6524) Acc G: 44.808% 
LR: 2.000e-04 

2023-03-02 01:46:17,027 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.1821 (0.2875) Acc D Real: 77.059% 
Loss D Fake: 4.8763 (1.0370) Acc D Fake: 55.070% 
Loss D: 5.058 
Loss G: 0.0531 (0.6488) Acc G: 45.141% 
LR: 2.000e-04 

2023-03-02 01:46:17,037 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.1184 (0.2865) Acc D Real: 77.164% 
Loss D Fake: 4.7964 (1.0596) Acc D Fake: 54.741% 
Loss D: 4.915 
Loss G: 0.0533 (0.6453) Acc G: 45.469% 
LR: 2.000e-04 

2023-03-02 01:46:17,046 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.1547 (0.2857) Acc D Real: 77.267% 
Loss D Fake: 4.7179 (1.0813) Acc D Fake: 54.415% 
Loss D: 4.873 
Loss G: 0.0536 (0.6417) Acc G: 45.794% 
LR: 2.000e-04 

2023-03-02 01:46:17,056 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.2116 (0.2852) Acc D Real: 77.360% 
Loss D Fake: 4.6415 (1.1024) Acc D Fake: 54.093% 
Loss D: 4.853 
Loss G: 0.0539 (0.6383) Acc G: 46.114% 
LR: 2.000e-04 

2023-03-02 01:46:17,064 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.1431 (0.2844) Acc D Real: 77.469% 
Loss D Fake: 4.5674 (1.1228) Acc D Fake: 53.775% 
Loss D: 4.710 
Loss G: 0.0543 (0.6348) Acc G: 46.431% 
LR: 2.000e-04 

2023-03-02 01:46:17,071 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.0663 (0.2831) Acc D Real: 77.596% 
Loss D Fake: 4.4952 (1.1425) Acc D Fake: 53.460% 
Loss D: 4.561 
Loss G: 0.0548 (0.6314) Acc G: 46.745% 
LR: 2.000e-04 

2023-03-02 01:46:17,079 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.1041 (0.2821) Acc D Real: 77.716% 
Loss D Fake: 4.4250 (1.1616) Acc D Fake: 53.149% 
Loss D: 4.529 
Loss G: 0.0554 (0.6281) Acc G: 47.054% 
LR: 2.000e-04 

2023-03-02 01:46:17,086 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.1357 (0.2812) Acc D Real: 77.837% 
Loss D Fake: 4.3568 (1.1801) Acc D Fake: 52.842% 
Loss D: 4.493 
Loss G: 0.0560 (0.6248) Acc G: 47.360% 
LR: 2.000e-04 

2023-03-02 01:46:17,093 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1229 (0.2803) Acc D Real: 77.954% 
Loss D Fake: 4.2909 (1.1979) Acc D Fake: 52.538% 
Loss D: 4.414 
Loss G: 0.0566 (0.6215) Acc G: 47.663% 
LR: 2.000e-04 

2023-03-02 01:46:17,101 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.0814 (0.2792) Acc D Real: 78.074% 
Loss D Fake: 4.2265 (1.2152) Acc D Fake: 52.238% 
Loss D: 4.308 
Loss G: 0.0573 (0.6183) Acc G: 47.962% 
LR: 2.000e-04 

2023-03-02 01:46:17,108 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.0643 (0.2780) Acc D Real: 78.193% 
Loss D Fake: 4.1638 (1.2320) Acc D Fake: 51.941% 
Loss D: 4.228 
Loss G: 0.0581 (0.6151) Acc G: 48.258% 
LR: 2.000e-04 

2023-03-02 01:46:17,116 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.0818 (0.2769) Acc D Real: 78.309% 
Loss D Fake: 4.1024 (1.2482) Acc D Fake: 51.648% 
Loss D: 4.184 
Loss G: 0.0589 (0.6120) Acc G: 48.550% 
LR: 2.000e-04 

2023-03-02 01:46:17,123 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.0691 (0.2757) Acc D Real: 78.423% 
Loss D Fake: 4.0425 (1.2639) Acc D Fake: 51.358% 
Loss D: 4.112 
Loss G: 0.0597 (0.6089) Acc G: 48.839% 
LR: 2.000e-04 

2023-03-02 01:46:17,130 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.0576 (0.2745) Acc D Real: 78.540% 
Loss D Fake: 3.9837 (1.2791) Acc D Fake: 51.071% 
Loss D: 4.041 
Loss G: 0.0606 (0.6058) Acc G: 49.125% 
LR: 2.000e-04 

2023-03-02 01:46:17,138 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.0538 (0.2732) Acc D Real: 78.655% 
Loss D Fake: 3.9261 (1.2938) Acc D Fake: 50.787% 
Loss D: 3.980 
Loss G: 0.0616 (0.6028) Acc G: 49.407% 
LR: 2.000e-04 

2023-03-02 01:46:17,145 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.0687 (0.2721) Acc D Real: 78.766% 
Loss D Fake: 3.8695 (1.3080) Acc D Fake: 50.506% 
Loss D: 3.938 
Loss G: 0.0626 (0.5998) Acc G: 49.687% 
LR: 2.000e-04 

2023-03-02 01:46:17,153 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.0641 (0.2710) Acc D Real: 78.878% 
Loss D Fake: 3.8139 (1.3218) Acc D Fake: 50.229% 
Loss D: 3.878 
Loss G: 0.0637 (0.5969) Acc G: 49.963% 
LR: 2.000e-04 

2023-03-02 01:46:17,161 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.0602 (0.2698) Acc D Real: 78.987% 
Loss D Fake: 3.7592 (1.3351) Acc D Fake: 49.954% 
Loss D: 3.819 
Loss G: 0.0648 (0.5939) Acc G: 50.237% 
LR: 2.000e-04 

2023-03-02 01:46:17,168 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.0679 (0.2687) Acc D Real: 79.095% 
Loss D Fake: 3.7053 (1.3480) Acc D Fake: 49.683% 
Loss D: 3.773 
Loss G: 0.0659 (0.5911) Acc G: 50.507% 
LR: 2.000e-04 

2023-03-02 01:46:17,176 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.0657 (0.2676) Acc D Real: 79.202% 
Loss D Fake: 3.6521 (1.3605) Acc D Fake: 49.414% 
Loss D: 3.718 
Loss G: 0.0671 (0.5882) Acc G: 50.766% 
LR: 2.000e-04 

2023-03-02 01:46:17,183 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.0643 (0.2665) Acc D Real: 79.308% 
Loss D Fake: 3.5997 (1.3725) Acc D Fake: 49.158% 
Loss D: 3.664 
Loss G: 0.0684 (0.5854) Acc G: 51.022% 
LR: 2.000e-04 

2023-03-02 01:46:17,191 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.0675 (0.2655) Acc D Real: 79.411% 
Loss D Fake: 3.5479 (1.3841) Acc D Fake: 48.904% 
Loss D: 3.615 
Loss G: 0.0697 (0.5827) Acc G: 51.275% 
LR: 2.000e-04 

2023-03-02 01:46:17,198 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.0631 (0.2644) Acc D Real: 79.516% 
Loss D Fake: 3.4967 (1.3954) Acc D Fake: 48.652% 
Loss D: 3.560 
Loss G: 0.0710 (0.5800) Acc G: 51.525% 
LR: 2.000e-04 

2023-03-02 01:46:17,206 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.0662 (0.2633) Acc D Real: 79.618% 
Loss D Fake: 3.4461 (1.4062) Acc D Fake: 48.404% 
Loss D: 3.512 
Loss G: 0.0724 (0.5773) Acc G: 51.772% 
LR: 2.000e-04 

2023-03-02 01:46:17,213 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.0706 (0.2623) Acc D Real: 79.720% 
Loss D Fake: 3.3960 (1.4167) Acc D Fake: 48.158% 
Loss D: 3.467 
Loss G: 0.0739 (0.5746) Acc G: 52.018% 
LR: 2.000e-04 

2023-03-02 01:46:17,220 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.0719 (0.2613) Acc D Real: 79.818% 
Loss D Fake: 3.3464 (1.4268) Acc D Fake: 47.914% 
Loss D: 3.418 
Loss G: 0.0755 (0.5720) Acc G: 52.260% 
LR: 2.000e-04 

2023-03-02 01:46:17,228 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.0764 (0.2604) Acc D Real: 79.915% 
Loss D Fake: 3.2972 (1.4365) Acc D Fake: 47.674% 
Loss D: 3.374 
Loss G: 0.0771 (0.5694) Acc G: 52.500% 
LR: 2.000e-04 

2023-03-02 01:46:17,235 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.0671 (0.2594) Acc D Real: 80.016% 
Loss D Fake: 3.2484 (1.4459) Acc D Fake: 47.435% 
Loss D: 3.316 
Loss G: 0.0787 (0.5669) Acc G: 52.737% 
LR: 2.000e-04 

2023-03-02 01:46:17,244 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.0729 (0.2584) Acc D Real: 80.113% 
Loss D Fake: 3.2001 (1.4550) Acc D Fake: 47.199% 
Loss D: 3.273 
Loss G: 0.0805 (0.5644) Acc G: 52.973% 
LR: 2.000e-04 

2023-03-02 01:46:17,251 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.0760 (0.2575) Acc D Real: 80.209% 
Loss D Fake: 3.1521 (1.4637) Acc D Fake: 46.966% 
Loss D: 3.228 
Loss G: 0.0823 (0.5619) Acc G: 53.205% 
LR: 2.000e-04 

2023-03-02 01:46:17,259 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.0820 (0.2566) Acc D Real: 80.304% 
Loss D Fake: 3.1045 (1.4720) Acc D Fake: 46.735% 
Loss D: 3.187 
Loss G: 0.0842 (0.5595) Acc G: 53.435% 
LR: 2.000e-04 

2023-03-02 01:46:17,266 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.0815 (0.2557) Acc D Real: 80.397% 
Loss D Fake: 3.0572 (1.4801) Acc D Fake: 46.506% 
Loss D: 3.139 
Loss G: 0.0862 (0.5571) Acc G: 53.663% 
LR: 2.000e-04 

2023-03-02 01:46:17,273 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.0791 (0.2548) Acc D Real: 80.490% 
Loss D Fake: 3.0103 (1.4878) Acc D Fake: 46.279% 
Loss D: 3.089 
Loss G: 0.0883 (0.5547) Acc G: 53.889% 
LR: 2.000e-04 

2023-03-02 01:46:17,281 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.0880 (0.2540) Acc D Real: 80.581% 
Loss D Fake: 2.9637 (1.4952) Acc D Fake: 46.055% 
Loss D: 3.052 
Loss G: 0.0904 (0.5524) Acc G: 54.112% 
LR: 2.000e-04 

2023-03-02 01:46:17,288 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.0859 (0.2531) Acc D Real: 80.670% 
Loss D Fake: 2.9174 (1.5023) Acc D Fake: 45.833% 
Loss D: 3.003 
Loss G: 0.0927 (0.5501) Acc G: 54.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,295 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.0811 (0.2523) Acc D Real: 80.762% 
Loss D Fake: 2.8715 (1.5092) Acc D Fake: 45.614% 
Loss D: 2.953 
Loss G: 0.0951 (0.5478) Acc G: 54.552% 
LR: 2.000e-04 

2023-03-02 01:46:17,304 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.0947 (0.2515) Acc D Real: 80.847% 
Loss D Fake: 2.8259 (1.5157) Acc D Fake: 45.396% 
Loss D: 2.921 
Loss G: 0.0975 (0.5456) Acc G: 54.769% 
LR: 2.000e-04 

2023-03-02 01:46:17,312 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.0925 (0.2507) Acc D Real: 80.933% 
Loss D Fake: 2.7806 (1.5219) Acc D Fake: 45.181% 
Loss D: 2.873 
Loss G: 0.1001 (0.5434) Acc G: 54.984% 
LR: 2.000e-04 

2023-03-02 01:46:17,319 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.0980 (0.2499) Acc D Real: 81.018% 
Loss D Fake: 2.7357 (1.5279) Acc D Fake: 44.967% 
Loss D: 2.834 
Loss G: 0.1027 (0.5412) Acc G: 55.196% 
LR: 2.000e-04 

2023-03-02 01:46:17,327 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.0954 (0.2492) Acc D Real: 81.104% 
Loss D Fake: 2.6911 (1.5335) Acc D Fake: 44.756% 
Loss D: 2.786 
Loss G: 0.1055 (0.5391) Acc G: 55.407% 
LR: 2.000e-04 

2023-03-02 01:46:17,334 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.1054 (0.2485) Acc D Real: 81.187% 
Loss D Fake: 2.6469 (1.5389) Acc D Fake: 44.547% 
Loss D: 2.752 
Loss G: 0.1084 (0.5370) Acc G: 55.615% 
LR: 2.000e-04 

2023-03-02 01:46:17,341 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.1023 (0.2478) Acc D Real: 81.271% 
Loss D Fake: 2.6031 (1.5441) Acc D Fake: 44.340% 
Loss D: 2.705 
Loss G: 0.1114 (0.5350) Acc G: 55.821% 
LR: 2.000e-04 

2023-03-02 01:46:17,349 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.1090 (0.2471) Acc D Real: 81.352% 
Loss D Fake: 2.5597 (1.5490) Acc D Fake: 44.135% 
Loss D: 2.669 
Loss G: 0.1145 (0.5329) Acc G: 56.026% 
LR: 2.000e-04 

2023-03-02 01:46:17,356 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1096 (0.2465) Acc D Real: 81.433% 
Loss D Fake: 2.5166 (1.5536) Acc D Fake: 43.931% 
Loss D: 2.626 
Loss G: 0.1178 (0.5310) Acc G: 56.228% 
LR: 2.000e-04 

2023-03-02 01:46:17,363 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.1104 (0.2458) Acc D Real: 81.515% 
Loss D Fake: 2.4740 (1.5580) Acc D Fake: 43.730% 
Loss D: 2.584 
Loss G: 0.1212 (0.5290) Acc G: 56.429% 
LR: 2.000e-04 

2023-03-02 01:46:17,370 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.1150 (0.2452) Acc D Real: 81.594% 
Loss D Fake: 2.4319 (1.5621) Acc D Fake: 43.531% 
Loss D: 2.547 
Loss G: 0.1247 (0.5271) Acc G: 56.627% 
LR: 2.000e-04 

2023-03-02 01:46:17,377 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.1220 (0.2446) Acc D Real: 81.672% 
Loss D Fake: 2.3902 (1.5660) Acc D Fake: 43.333% 
Loss D: 2.512 
Loss G: 0.1283 (0.5252) Acc G: 56.824% 
LR: 2.000e-04 

2023-03-02 01:46:17,385 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.1261 (0.2441) Acc D Real: 81.748% 
Loss D Fake: 2.3490 (1.5697) Acc D Fake: 43.138% 
Loss D: 2.475 
Loss G: 0.1321 (0.5234) Acc G: 57.019% 
LR: 2.000e-04 

2023-03-02 01:46:17,392 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.1321 (0.2435) Acc D Real: 81.823% 
Loss D Fake: 2.3083 (1.5731) Acc D Fake: 42.944% 
Loss D: 2.440 
Loss G: 0.1360 (0.5216) Acc G: 57.212% 
LR: 2.000e-04 

2023-03-02 01:46:17,399 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.1325 (0.2430) Acc D Real: 81.898% 
Loss D Fake: 2.2680 (1.5764) Acc D Fake: 42.752% 
Loss D: 2.400 
Loss G: 0.1401 (0.5198) Acc G: 57.403% 
LR: 2.000e-04 

2023-03-02 01:46:17,407 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.1381 (0.2425) Acc D Real: 81.972% 
Loss D Fake: 2.2282 (1.5794) Acc D Fake: 42.562% 
Loss D: 2.366 
Loss G: 0.1443 (0.5180) Acc G: 57.593% 
LR: 2.000e-04 

2023-03-02 01:46:17,414 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.1365 (0.2420) Acc D Real: 82.048% 
Loss D Fake: 2.1889 (1.5822) Acc D Fake: 42.373% 
Loss D: 2.325 
Loss G: 0.1486 (0.5163) Acc G: 57.780% 
LR: 2.000e-04 

2023-03-02 01:46:17,421 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.1458 (0.2416) Acc D Real: 82.119% 
Loss D Fake: 2.1502 (1.5848) Acc D Fake: 42.187% 
Loss D: 2.296 
Loss G: 0.1531 (0.5147) Acc G: 57.966% 
LR: 2.000e-04 

2023-03-02 01:46:17,428 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.1472 (0.2412) Acc D Real: 82.191% 
Loss D Fake: 2.1121 (1.5872) Acc D Fake: 42.002% 
Loss D: 2.259 
Loss G: 0.1577 (0.5130) Acc G: 58.151% 
LR: 2.000e-04 

2023-03-02 01:46:17,436 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.1480 (0.2408) Acc D Real: 82.263% 
Loss D Fake: 2.0746 (1.5894) Acc D Fake: 41.818% 
Loss D: 2.223 
Loss G: 0.1624 (0.5114) Acc G: 58.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,444 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.1525 (0.2404) Acc D Real: 82.334% 
Loss D Fake: 2.0378 (1.5915) Acc D Fake: 41.637% 
Loss D: 2.190 
Loss G: 0.1673 (0.5099) Acc G: 58.514% 
LR: 2.000e-04 

2023-03-02 01:46:17,451 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.1630 (0.2400) Acc D Real: 82.404% 
Loss D Fake: 2.0016 (1.5933) Acc D Fake: 41.456% 
Loss D: 2.165 
Loss G: 0.1722 (0.5084) Acc G: 58.694% 
LR: 2.000e-04 

2023-03-02 01:46:17,458 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.1635 (0.2397) Acc D Real: 82.472% 
Loss D Fake: 1.9661 (1.5950) Acc D Fake: 41.278% 
Loss D: 2.130 
Loss G: 0.1774 (0.5069) Acc G: 58.871% 
LR: 2.000e-04 

2023-03-02 01:46:17,465 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.1689 (0.2393) Acc D Real: 82.542% 
Loss D Fake: 1.9314 (1.5965) Acc D Fake: 41.101% 
Loss D: 2.100 
Loss G: 0.1826 (0.5054) Acc G: 59.048% 
LR: 2.000e-04 

2023-03-02 01:46:17,473 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.1746 (0.2391) Acc D Real: 82.609% 
Loss D Fake: 1.8973 (1.5978) Acc D Fake: 40.926% 
Loss D: 2.072 
Loss G: 0.1879 (0.5040) Acc G: 59.222% 
LR: 2.000e-04 

2023-03-02 01:46:17,481 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.1778 (0.2388) Acc D Real: 82.626% 
Loss D Fake: 1.8639 (1.5990) Acc D Fake: 40.882% 
Loss D: 2.042 
Loss G: 0.1934 (0.5026) Acc G: 59.266% 
LR: 2.000e-04 

2023-03-02 01:46:17,492 -                train: [    INFO] - 
Epoch: 16/20
2023-03-02 01:46:17,647 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.1865 (0.1853) Acc D Real: 97.578% 
Loss D Fake: 1.7993 (1.8153) Acc D Fake: 1.667% 
Loss D: 1.986 
Loss G: 0.2046 (0.2018) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,655 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.1950 (0.1885) Acc D Real: 97.604% 
Loss D Fake: 1.7682 (1.7996) Acc D Fake: 1.667% 
Loss D: 1.963 
Loss G: 0.2103 (0.2046) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,662 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.1986 (0.1910) Acc D Real: 97.656% 
Loss D Fake: 1.7377 (1.7841) Acc D Fake: 1.667% 
Loss D: 1.936 
Loss G: 0.2162 (0.2075) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,680 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2032 (0.1935) Acc D Real: 97.719% 
Loss D Fake: 1.7079 (1.7689) Acc D Fake: 1.667% 
Loss D: 1.911 
Loss G: 0.2221 (0.2104) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,687 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2104 (0.1963) Acc D Real: 97.717% 
Loss D Fake: 1.6790 (1.7539) Acc D Fake: 1.667% 
Loss D: 1.889 
Loss G: 0.2281 (0.2134) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,694 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.2177 (0.1993) Acc D Real: 97.686% 
Loss D Fake: 1.6508 (1.7392) Acc D Fake: 1.667% 
Loss D: 1.868 
Loss G: 0.2341 (0.2163) Acc G: 98.333% 
LR: 2.000e-04 

2023-03-02 01:46:17,701 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2178 (0.2017) Acc D Real: 97.734% 
Loss D Fake: 1.6234 (1.7247) Acc D Fake: 1.875% 
Loss D: 1.841 
Loss G: 0.2402 (0.2193) Acc G: 98.125% 
LR: 2.000e-04 

2023-03-02 01:46:17,708 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2243 (0.2042) Acc D Real: 97.737% 
Loss D Fake: 1.5967 (1.7105) Acc D Fake: 2.037% 
Loss D: 1.821 
Loss G: 0.2463 (0.2223) Acc G: 97.963% 
LR: 2.000e-04 

2023-03-02 01:46:17,715 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2322 (0.2070) Acc D Real: 97.719% 
Loss D Fake: 1.5709 (1.6965) Acc D Fake: 2.167% 
Loss D: 1.803 
Loss G: 0.2525 (0.2253) Acc G: 97.833% 
LR: 2.000e-04 

2023-03-02 01:46:17,722 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.2358 (0.2096) Acc D Real: 97.727% 
Loss D Fake: 1.5457 (1.6828) Acc D Fake: 2.273% 
Loss D: 1.782 
Loss G: 0.2587 (0.2284) Acc G: 97.727% 
LR: 2.000e-04 

2023-03-02 01:46:17,729 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2425 (0.2123) Acc D Real: 97.730% 
Loss D Fake: 1.5214 (1.6694) Acc D Fake: 2.361% 
Loss D: 1.764 
Loss G: 0.2649 (0.2314) Acc G: 97.639% 
LR: 2.000e-04 

2023-03-02 01:46:17,736 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2461 (0.2149) Acc D Real: 97.732% 
Loss D Fake: 1.4979 (1.6562) Acc D Fake: 2.436% 
Loss D: 1.744 
Loss G: 0.2711 (0.2345) Acc G: 97.564% 
LR: 2.000e-04 

2023-03-02 01:46:17,743 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2540 (0.2177) Acc D Real: 97.731% 
Loss D Fake: 1.4751 (1.6432) Acc D Fake: 2.500% 
Loss D: 1.729 
Loss G: 0.2772 (0.2375) Acc G: 97.500% 
LR: 2.000e-04 

2023-03-02 01:46:17,750 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2590 (0.2205) Acc D Real: 97.722% 
Loss D Fake: 1.4532 (1.6306) Acc D Fake: 2.556% 
Loss D: 1.712 
Loss G: 0.2834 (0.2406) Acc G: 97.444% 
LR: 2.000e-04 

2023-03-02 01:46:17,758 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.2705 (0.2236) Acc D Real: 97.721% 
Loss D Fake: 1.4321 (1.6182) Acc D Fake: 2.604% 
Loss D: 1.703 
Loss G: 0.2894 (0.2436) Acc G: 97.396% 
LR: 2.000e-04 

2023-03-02 01:46:17,765 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.2713 (0.2264) Acc D Real: 97.733% 
Loss D Fake: 1.4120 (1.6060) Acc D Fake: 2.647% 
Loss D: 1.683 
Loss G: 0.2953 (0.2467) Acc G: 97.353% 
LR: 2.000e-04 

2023-03-02 01:46:17,772 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2708 (0.2289) Acc D Real: 97.740% 
Loss D Fake: 1.3926 (1.5942) Acc D Fake: 2.685% 
Loss D: 1.663 
Loss G: 0.3012 (0.2497) Acc G: 97.315% 
LR: 2.000e-04 

2023-03-02 01:46:17,779 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.2774 (0.2314) Acc D Real: 97.736% 
Loss D Fake: 1.3738 (1.5826) Acc D Fake: 2.719% 
Loss D: 1.651 
Loss G: 0.3071 (0.2527) Acc G: 97.281% 
LR: 2.000e-04 

2023-03-02 01:46:17,786 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.2914 (0.2344) Acc D Real: 97.719% 
Loss D Fake: 1.3556 (1.5712) Acc D Fake: 2.750% 
Loss D: 1.647 
Loss G: 0.3129 (0.2557) Acc G: 97.250% 
LR: 2.000e-04 

2023-03-02 01:46:17,793 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3004 (0.2376) Acc D Real: 97.698% 
Loss D Fake: 1.3383 (1.5601) Acc D Fake: 2.778% 
Loss D: 1.639 
Loss G: 0.3186 (0.2587) Acc G: 97.222% 
LR: 2.000e-04 

2023-03-02 01:46:17,800 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.2994 (0.2404) Acc D Real: 97.685% 
Loss D Fake: 1.3218 (1.5493) Acc D Fake: 2.803% 
Loss D: 1.621 
Loss G: 0.3241 (0.2617) Acc G: 97.197% 
LR: 2.000e-04 

2023-03-02 01:46:17,807 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3081 (0.2433) Acc D Real: 97.683% 
Loss D Fake: 1.3060 (1.5387) Acc D Fake: 2.826% 
Loss D: 1.614 
Loss G: 0.3295 (0.2646) Acc G: 97.174% 
LR: 2.000e-04 

2023-03-02 01:46:17,814 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3059 (0.2459) Acc D Real: 97.689% 
Loss D Fake: 1.2911 (1.5284) Acc D Fake: 2.847% 
Loss D: 1.597 
Loss G: 0.3348 (0.2676) Acc G: 97.153% 
LR: 2.000e-04 

2023-03-02 01:46:17,821 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.3159 (0.2487) Acc D Real: 97.681% 
Loss D Fake: 1.2768 (1.5183) Acc D Fake: 2.867% 
Loss D: 1.593 
Loss G: 0.3399 (0.2705) Acc G: 97.133% 
LR: 2.000e-04 

2023-03-02 01:46:17,828 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3213 (0.2515) Acc D Real: 97.676% 
Loss D Fake: 1.2631 (1.5085) Acc D Fake: 2.885% 
Loss D: 1.584 
Loss G: 0.3448 (0.2733) Acc G: 97.115% 
LR: 2.000e-04 

2023-03-02 01:46:17,835 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3205 (0.2541) Acc D Real: 97.679% 
Loss D Fake: 1.2502 (1.4990) Acc D Fake: 2.901% 
Loss D: 1.571 
Loss G: 0.3496 (0.2761) Acc G: 97.099% 
LR: 2.000e-04 

2023-03-02 01:46:17,842 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3301 (0.2568) Acc D Real: 97.669% 
Loss D Fake: 1.2379 (1.4896) Acc D Fake: 2.917% 
Loss D: 1.568 
Loss G: 0.3543 (0.2789) Acc G: 97.083% 
LR: 2.000e-04 

2023-03-02 01:46:17,849 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3308 (0.2593) Acc D Real: 97.660% 
Loss D Fake: 1.2261 (1.4805) Acc D Fake: 2.931% 
Loss D: 1.557 
Loss G: 0.3589 (0.2817) Acc G: 97.069% 
LR: 2.000e-04 

2023-03-02 01:46:17,857 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3238 (0.2615) Acc D Real: 97.656% 
Loss D Fake: 1.2148 (1.4717) Acc D Fake: 2.944% 
Loss D: 1.539 
Loss G: 0.3634 (0.2844) Acc G: 97.056% 
LR: 2.000e-04 

2023-03-02 01:46:17,864 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3391 (0.2640) Acc D Real: 97.650% 
Loss D Fake: 1.2038 (1.4630) Acc D Fake: 2.957% 
Loss D: 1.543 
Loss G: 0.3678 (0.2871) Acc G: 97.043% 
LR: 2.000e-04 

2023-03-02 01:46:17,872 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3451 (0.2665) Acc D Real: 97.650% 
Loss D Fake: 1.1933 (1.4546) Acc D Fake: 2.969% 
Loss D: 1.538 
Loss G: 0.3720 (0.2898) Acc G: 97.031% 
LR: 2.000e-04 

2023-03-02 01:46:17,880 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3416 (0.2688) Acc D Real: 97.648% 
Loss D Fake: 1.1835 (1.4464) Acc D Fake: 2.980% 
Loss D: 1.525 
Loss G: 0.3760 (0.2924) Acc G: 97.020% 
LR: 2.000e-04 

2023-03-02 01:46:17,887 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3446 (0.2710) Acc D Real: 97.644% 
Loss D Fake: 1.1741 (1.4384) Acc D Fake: 2.990% 
Loss D: 1.519 
Loss G: 0.3800 (0.2950) Acc G: 97.010% 
LR: 2.000e-04 

2023-03-02 01:46:17,894 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3502 (0.2733) Acc D Real: 97.634% 
Loss D Fake: 1.1650 (1.4306) Acc D Fake: 3.000% 
Loss D: 1.515 
Loss G: 0.3839 (0.2975) Acc G: 97.000% 
LR: 2.000e-04 

2023-03-02 01:46:17,902 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3460 (0.2753) Acc D Real: 97.632% 
Loss D Fake: 1.1562 (1.4230) Acc D Fake: 3.009% 
Loss D: 1.502 
Loss G: 0.3877 (0.3000) Acc G: 96.991% 
LR: 2.000e-04 

2023-03-02 01:46:17,909 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3588 (0.2776) Acc D Real: 97.631% 
Loss D Fake: 1.1477 (1.4155) Acc D Fake: 3.018% 
Loss D: 1.507 
Loss G: 0.3914 (0.3025) Acc G: 96.982% 
LR: 2.000e-04 

2023-03-02 01:46:17,917 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3608 (0.2798) Acc D Real: 97.626% 
Loss D Fake: 1.1398 (1.4083) Acc D Fake: 3.026% 
Loss D: 1.501 
Loss G: 0.3948 (0.3049) Acc G: 96.974% 
LR: 2.000e-04 

2023-03-02 01:46:17,924 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3781 (0.2823) Acc D Real: 97.619% 
Loss D Fake: 1.1325 (1.4012) Acc D Fake: 3.034% 
Loss D: 1.511 
Loss G: 0.3980 (0.3073) Acc G: 96.966% 
LR: 2.000e-04 

2023-03-02 01:46:17,932 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3673 (0.2844) Acc D Real: 97.612% 
Loss D Fake: 1.1258 (1.3943) Acc D Fake: 3.042% 
Loss D: 1.493 
Loss G: 0.4010 (0.3096) Acc G: 96.958% 
LR: 2.000e-04 

2023-03-02 01:46:17,940 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3703 (0.2865) Acc D Real: 97.611% 
Loss D Fake: 1.1195 (1.3876) Acc D Fake: 3.049% 
Loss D: 1.490 
Loss G: 0.4039 (0.3119) Acc G: 96.951% 
LR: 2.000e-04 

2023-03-02 01:46:17,948 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3817 (0.2888) Acc D Real: 97.602% 
Loss D Fake: 1.1135 (1.3811) Acc D Fake: 3.056% 
Loss D: 1.495 
Loss G: 0.4066 (0.3142) Acc G: 96.944% 
LR: 2.000e-04 

2023-03-02 01:46:17,955 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3731 (0.2907) Acc D Real: 97.604% 
Loss D Fake: 1.1080 (1.3747) Acc D Fake: 3.062% 
Loss D: 1.481 
Loss G: 0.4092 (0.3164) Acc G: 96.938% 
LR: 2.000e-04 

2023-03-02 01:46:17,964 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3801 (0.2928) Acc D Real: 97.601% 
Loss D Fake: 1.1027 (1.3685) Acc D Fake: 3.068% 
Loss D: 1.483 
Loss G: 0.4116 (0.3186) Acc G: 96.932% 
LR: 2.000e-04 

2023-03-02 01:46:17,972 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3775 (0.2946) Acc D Real: 97.596% 
Loss D Fake: 1.0978 (1.3625) Acc D Fake: 3.074% 
Loss D: 1.475 
Loss G: 0.4140 (0.3207) Acc G: 96.926% 
LR: 2.000e-04 

2023-03-02 01:46:17,980 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3911 (0.2967) Acc D Real: 97.588% 
Loss D Fake: 1.0930 (1.3567) Acc D Fake: 3.080% 
Loss D: 1.484 
Loss G: 0.4162 (0.3228) Acc G: 96.920% 
LR: 2.000e-04 

2023-03-02 01:46:17,987 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3688 (0.2983) Acc D Real: 97.594% 
Loss D Fake: 1.0886 (1.3510) Acc D Fake: 3.085% 
Loss D: 1.457 
Loss G: 0.4183 (0.3248) Acc G: 96.915% 
LR: 2.000e-04 

2023-03-02 01:46:17,995 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3788 (0.3000) Acc D Real: 97.595% 
Loss D Fake: 1.0842 (1.3454) Acc D Fake: 3.090% 
Loss D: 1.463 
Loss G: 0.4205 (0.3268) Acc G: 96.910% 
LR: 2.000e-04 

2023-03-02 01:46:18,003 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3906 (0.3018) Acc D Real: 97.587% 
Loss D Fake: 1.0800 (1.3400) Acc D Fake: 3.095% 
Loss D: 1.471 
Loss G: 0.4225 (0.3287) Acc G: 96.905% 
LR: 2.000e-04 

2023-03-02 01:46:18,010 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3837 (0.3034) Acc D Real: 97.588% 
Loss D Fake: 1.0761 (1.3347) Acc D Fake: 3.100% 
Loss D: 1.460 
Loss G: 0.4244 (0.3306) Acc G: 96.900% 
LR: 2.000e-04 

2023-03-02 01:46:18,017 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.3814 (0.3050) Acc D Real: 97.588% 
Loss D Fake: 1.0724 (1.3296) Acc D Fake: 3.105% 
Loss D: 1.454 
Loss G: 0.4263 (0.3325) Acc G: 96.895% 
LR: 2.000e-04 

2023-03-02 01:46:18,025 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3899 (0.3066) Acc D Real: 97.589% 
Loss D Fake: 1.0687 (1.3246) Acc D Fake: 3.109% 
Loss D: 1.459 
Loss G: 0.4281 (0.3344) Acc G: 96.891% 
LR: 2.000e-04 

2023-03-02 01:46:18,033 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.4054 (0.3085) Acc D Real: 97.583% 
Loss D Fake: 1.0654 (1.3197) Acc D Fake: 3.113% 
Loss D: 1.471 
Loss G: 0.4296 (0.3362) Acc G: 96.887% 
LR: 2.000e-04 

2023-03-02 01:46:18,040 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3992 (0.3101) Acc D Real: 97.578% 
Loss D Fake: 1.0625 (1.3149) Acc D Fake: 3.117% 
Loss D: 1.462 
Loss G: 0.4310 (0.3379) Acc G: 96.883% 
LR: 2.000e-04 

