2023-03-02 01:46:18,048 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3956 (0.3117) Acc D Real: 97.577% 
Loss D Fake: 1.0599 (1.3103) Acc D Fake: 3.121% 
Loss D: 1.456 
Loss G: 0.4323 (0.3396) Acc G: 96.879% 
LR: 2.000e-04 

2023-03-02 01:46:18,057 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.3835 (0.3130) Acc D Real: 97.572% 
Loss D Fake: 1.0574 (1.3057) Acc D Fake: 3.125% 
Loss D: 1.441 
Loss G: 0.4337 (0.3413) Acc G: 96.875% 
LR: 2.000e-04 

2023-03-02 01:46:18,064 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3978 (0.3145) Acc D Real: 97.569% 
Loss D Fake: 1.0547 (1.3013) Acc D Fake: 3.129% 
Loss D: 1.453 
Loss G: 0.4350 (0.3430) Acc G: 96.871% 
LR: 2.000e-04 

2023-03-02 01:46:18,072 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3938 (0.3158) Acc D Real: 97.566% 
Loss D Fake: 1.0522 (1.2970) Acc D Fake: 3.132% 
Loss D: 1.446 
Loss G: 0.4363 (0.3446) Acc G: 96.868% 
LR: 2.000e-04 

2023-03-02 01:46:18,079 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.3999 (0.3173) Acc D Real: 97.564% 
Loss D Fake: 1.0497 (1.2929) Acc D Fake: 3.136% 
Loss D: 1.450 
Loss G: 0.4376 (0.3461) Acc G: 96.864% 
LR: 2.000e-04 

2023-03-02 01:46:18,087 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3945 (0.3186) Acc D Real: 97.566% 
Loss D Fake: 1.0474 (1.2888) Acc D Fake: 3.139% 
Loss D: 1.442 
Loss G: 0.4388 (0.3477) Acc G: 96.861% 
LR: 2.000e-04 

2023-03-02 01:46:18,096 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3841 (0.3196) Acc D Real: 97.564% 
Loss D Fake: 1.0451 (1.2848) Acc D Fake: 3.142% 
Loss D: 1.429 
Loss G: 0.4400 (0.3492) Acc G: 96.858% 
LR: 2.000e-04 

2023-03-02 01:46:18,104 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.3804 (0.3206) Acc D Real: 97.562% 
Loss D Fake: 1.0426 (1.2809) Acc D Fake: 3.145% 
Loss D: 1.423 
Loss G: 0.4414 (0.3507) Acc G: 96.855% 
LR: 2.000e-04 

2023-03-02 01:46:18,113 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3693 (0.3214) Acc D Real: 97.560% 
Loss D Fake: 1.0398 (1.2770) Acc D Fake: 3.148% 
Loss D: 1.409 
Loss G: 0.4431 (0.3522) Acc G: 96.852% 
LR: 2.000e-04 

2023-03-02 01:46:18,121 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3783 (0.3223) Acc D Real: 97.558% 
Loss D Fake: 1.0366 (1.2733) Acc D Fake: 3.151% 
Loss D: 1.415 
Loss G: 0.4448 (0.3536) Acc G: 96.849% 
LR: 2.000e-04 

2023-03-02 01:46:18,130 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3863 (0.3233) Acc D Real: 97.551% 
Loss D Fake: 1.0333 (1.2696) Acc D Fake: 3.154% 
Loss D: 1.420 
Loss G: 0.4466 (0.3550) Acc G: 96.846% 
LR: 2.000e-04 

2023-03-02 01:46:18,138 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.4008 (0.3244) Acc D Real: 97.546% 
Loss D Fake: 1.0300 (1.2660) Acc D Fake: 3.157% 
Loss D: 1.431 
Loss G: 0.4483 (0.3564) Acc G: 96.843% 
LR: 2.000e-04 

2023-03-02 01:46:18,146 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.3993 (0.3255) Acc D Real: 97.547% 
Loss D Fake: 1.0271 (1.2624) Acc D Fake: 3.159% 
Loss D: 1.426 
Loss G: 0.4498 (0.3578) Acc G: 96.841% 
LR: 2.000e-04 

2023-03-02 01:46:18,155 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.4255 (0.3270) Acc D Real: 97.543% 
Loss D Fake: 1.0246 (1.2589) Acc D Fake: 3.162% 
Loss D: 1.450 
Loss G: 0.4510 (0.3592) Acc G: 96.838% 
LR: 2.000e-04 

2023-03-02 01:46:18,163 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.4189 (0.3283) Acc D Real: 97.541% 
Loss D Fake: 1.0227 (1.2555) Acc D Fake: 3.164% 
Loss D: 1.442 
Loss G: 0.4519 (0.3605) Acc G: 96.836% 
LR: 2.000e-04 

2023-03-02 01:46:18,172 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.4021 (0.3294) Acc D Real: 97.537% 
Loss D Fake: 1.0212 (1.2521) Acc D Fake: 3.167% 
Loss D: 1.423 
Loss G: 0.4527 (0.3619) Acc G: 96.833% 
LR: 2.000e-04 

2023-03-02 01:46:18,180 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.3798 (0.3301) Acc D Real: 97.534% 
Loss D Fake: 1.0196 (1.2489) Acc D Fake: 3.169% 
Loss D: 1.399 
Loss G: 0.4537 (0.3632) Acc G: 96.831% 
LR: 2.000e-04 

2023-03-02 01:46:18,189 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3706 (0.3307) Acc D Real: 97.532% 
Loss D Fake: 1.0175 (1.2456) Acc D Fake: 3.171% 
Loss D: 1.388 
Loss G: 0.4550 (0.3644) Acc G: 96.829% 
LR: 2.000e-04 

2023-03-02 01:46:18,197 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3798 (0.3313) Acc D Real: 97.529% 
Loss D Fake: 1.0150 (1.2425) Acc D Fake: 3.174% 
Loss D: 1.395 
Loss G: 0.4565 (0.3657) Acc G: 96.826% 
LR: 2.000e-04 

2023-03-02 01:46:18,206 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3881 (0.3321) Acc D Real: 97.525% 
Loss D Fake: 1.0123 (1.2394) Acc D Fake: 3.176% 
Loss D: 1.400 
Loss G: 0.4580 (0.3669) Acc G: 96.824% 
LR: 2.000e-04 

2023-03-02 01:46:18,214 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3732 (0.3327) Acc D Real: 97.522% 
Loss D Fake: 1.0094 (1.2363) Acc D Fake: 3.178% 
Loss D: 1.383 
Loss G: 0.4597 (0.3682) Acc G: 96.822% 
LR: 2.000e-04 

2023-03-02 01:46:18,221 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3940 (0.3335) Acc D Real: 97.519% 
Loss D Fake: 1.0064 (1.2333) Acc D Fake: 3.180% 
Loss D: 1.400 
Loss G: 0.4614 (0.3694) Acc G: 96.820% 
LR: 2.000e-04 

2023-03-02 01:46:18,229 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3777 (0.3340) Acc D Real: 97.519% 
Loss D Fake: 1.0035 (1.2303) Acc D Fake: 3.182% 
Loss D: 1.381 
Loss G: 0.4631 (0.3706) Acc G: 96.818% 
LR: 2.000e-04 

2023-03-02 01:46:18,236 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4040 (0.3349) Acc D Real: 97.519% 
Loss D Fake: 1.0007 (1.2274) Acc D Fake: 3.184% 
Loss D: 1.405 
Loss G: 0.4646 (0.3718) Acc G: 96.816% 
LR: 2.000e-04 

2023-03-02 01:46:18,244 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3912 (0.3357) Acc D Real: 97.514% 
Loss D Fake: 0.9982 (1.2244) Acc D Fake: 3.186% 
Loss D: 1.389 
Loss G: 0.4660 (0.3730) Acc G: 96.814% 
LR: 2.000e-04 

2023-03-02 01:46:18,251 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3746 (0.3361) Acc D Real: 97.513% 
Loss D Fake: 0.9957 (1.2216) Acc D Fake: 3.188% 
Loss D: 1.370 
Loss G: 0.4675 (0.3742) Acc G: 96.812% 
LR: 2.000e-04 

2023-03-02 01:46:18,259 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.4139 (0.3371) Acc D Real: 97.512% 
Loss D Fake: 0.9931 (1.2188) Acc D Fake: 3.189% 
Loss D: 1.407 
Loss G: 0.4688 (0.3754) Acc G: 96.811% 
LR: 2.000e-04 

2023-03-02 01:46:18,266 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3505 (0.3373) Acc D Real: 97.509% 
Loss D Fake: 0.9909 (1.2160) Acc D Fake: 3.191% 
Loss D: 1.341 
Loss G: 0.4703 (0.3765) Acc G: 96.809% 
LR: 2.000e-04 

2023-03-02 01:46:18,273 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3952 (0.3380) Acc D Real: 97.504% 
Loss D Fake: 0.9882 (1.2132) Acc D Fake: 3.193% 
Loss D: 1.383 
Loss G: 0.4718 (0.3777) Acc G: 96.807% 
LR: 2.000e-04 

2023-03-02 01:46:18,281 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3836 (0.3385) Acc D Real: 97.504% 
Loss D Fake: 0.9857 (1.2105) Acc D Fake: 3.194% 
Loss D: 1.369 
Loss G: 0.4733 (0.3788) Acc G: 96.806% 
LR: 2.000e-04 

2023-03-02 01:46:18,288 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3806 (0.3390) Acc D Real: 97.502% 
Loss D Fake: 0.9831 (1.2079) Acc D Fake: 3.196% 
Loss D: 1.364 
Loss G: 0.4748 (0.3799) Acc G: 96.804% 
LR: 2.000e-04 

2023-03-02 01:46:18,296 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4066 (0.3398) Acc D Real: 97.501% 
Loss D Fake: 0.9807 (1.2052) Acc D Fake: 3.198% 
Loss D: 1.387 
Loss G: 0.4761 (0.3811) Acc G: 96.802% 
LR: 2.000e-04 

2023-03-02 01:46:18,303 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3603 (0.3400) Acc D Real: 97.501% 
Loss D Fake: 0.9786 (1.2026) Acc D Fake: 3.199% 
Loss D: 1.339 
Loss G: 0.4775 (0.3822) Acc G: 96.801% 
LR: 2.000e-04 

2023-03-02 01:46:18,312 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3888 (0.3406) Acc D Real: 97.499% 
Loss D Fake: 0.9762 (1.2000) Acc D Fake: 3.201% 
Loss D: 1.365 
Loss G: 0.4789 (0.3833) Acc G: 96.799% 
LR: 2.000e-04 

2023-03-02 01:46:18,320 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3948 (0.3412) Acc D Real: 97.494% 
Loss D Fake: 0.9739 (1.1975) Acc D Fake: 3.202% 
Loss D: 1.369 
Loss G: 0.4802 (0.3844) Acc G: 96.798% 
LR: 2.000e-04 

2023-03-02 01:46:18,327 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3855 (0.3417) Acc D Real: 97.493% 
Loss D Fake: 0.9717 (1.1950) Acc D Fake: 3.204% 
Loss D: 1.357 
Loss G: 0.4815 (0.3854) Acc G: 96.796% 
LR: 2.000e-04 

2023-03-02 01:46:18,335 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3731 (0.3420) Acc D Real: 97.491% 
Loss D Fake: 0.9695 (1.1925) Acc D Fake: 3.205% 
Loss D: 1.343 
Loss G: 0.4829 (0.3865) Acc G: 96.795% 
LR: 2.000e-04 

2023-03-02 01:46:18,343 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3833 (0.3425) Acc D Real: 97.490% 
Loss D Fake: 0.9671 (1.1901) Acc D Fake: 3.207% 
Loss D: 1.350 
Loss G: 0.4843 (0.3876) Acc G: 96.793% 
LR: 2.000e-04 

2023-03-02 01:46:18,351 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3396 (0.3424) Acc D Real: 97.489% 
Loss D Fake: 0.9647 (1.1876) Acc D Fake: 3.208% 
Loss D: 1.304 
Loss G: 0.4860 (0.3886) Acc G: 96.792% 
LR: 2.000e-04 

2023-03-02 01:46:18,360 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.4031 (0.3431) Acc D Real: 97.488% 
Loss D Fake: 0.9619 (1.1852) Acc D Fake: 3.209% 
Loss D: 1.365 
Loss G: 0.4876 (0.3897) Acc G: 96.791% 
LR: 2.000e-04 

2023-03-02 01:46:18,369 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3555 (0.3432) Acc D Real: 97.485% 
Loss D Fake: 0.9593 (1.1829) Acc D Fake: 3.211% 
Loss D: 1.315 
Loss G: 0.4892 (0.3907) Acc G: 96.789% 
LR: 2.000e-04 

2023-03-02 01:46:18,377 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.4362 (0.3442) Acc D Real: 97.483% 
Loss D Fake: 0.9568 (1.1805) Acc D Fake: 3.212% 
Loss D: 1.393 
Loss G: 0.4905 (0.3918) Acc G: 96.788% 
LR: 2.000e-04 

2023-03-02 01:46:18,386 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4186 (0.3450) Acc D Real: 97.483% 
Loss D Fake: 0.9551 (1.1782) Acc D Fake: 3.213% 
Loss D: 1.374 
Loss G: 0.4914 (0.3928) Acc G: 96.787% 
LR: 2.000e-04 

2023-03-02 01:46:18,395 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3990 (0.3455) Acc D Real: 97.482% 
Loss D Fake: 0.9537 (1.1759) Acc D Fake: 3.214% 
Loss D: 1.353 
Loss G: 0.4923 (0.3938) Acc G: 96.786% 
LR: 2.000e-04 

2023-03-02 01:46:18,403 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.3568 (0.3456) Acc D Real: 97.483% 
Loss D Fake: 0.9523 (1.1736) Acc D Fake: 3.215% 
Loss D: 1.309 
Loss G: 0.4933 (0.3948) Acc G: 96.785% 
LR: 2.000e-04 

2023-03-02 01:46:18,411 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3587 (0.3458) Acc D Real: 97.483% 
Loss D Fake: 0.9503 (1.1714) Acc D Fake: 3.217% 
Loss D: 1.309 
Loss G: 0.4947 (0.3958) Acc G: 96.783% 
LR: 2.000e-04 

2023-03-02 01:46:18,418 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3750 (0.3460) Acc D Real: 97.481% 
Loss D Fake: 0.9480 (1.1692) Acc D Fake: 3.218% 
Loss D: 1.323 
Loss G: 0.4961 (0.3968) Acc G: 96.782% 
LR: 2.000e-04 

2023-03-02 01:46:18,426 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3836 (0.3464) Acc D Real: 97.481% 
Loss D Fake: 0.9457 (1.1670) Acc D Fake: 3.219% 
Loss D: 1.329 
Loss G: 0.4976 (0.3978) Acc G: 96.781% 
LR: 2.000e-04 

2023-03-02 01:46:18,433 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.3508 (0.3465) Acc D Real: 97.479% 
Loss D Fake: 0.9432 (1.1648) Acc D Fake: 3.220% 
Loss D: 1.294 
Loss G: 0.4993 (0.3988) Acc G: 96.780% 
LR: 2.000e-04 

2023-03-02 01:46:18,440 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3632 (0.3466) Acc D Real: 97.477% 
Loss D Fake: 0.9404 (1.1627) Acc D Fake: 3.221% 
Loss D: 1.304 
Loss G: 0.5012 (0.3998) Acc G: 96.779% 
LR: 2.000e-04 

2023-03-02 01:46:18,447 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3390 (0.3465) Acc D Real: 97.475% 
Loss D Fake: 0.9373 (1.1605) Acc D Fake: 3.222% 
Loss D: 1.276 
Loss G: 0.5033 (0.4008) Acc G: 96.778% 
LR: 2.000e-04 

2023-03-02 01:46:18,455 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.3362 (0.3464) Acc D Real: 97.474% 
Loss D Fake: 0.9337 (1.1584) Acc D Fake: 3.223% 
Loss D: 1.270 
Loss G: 0.5057 (0.4017) Acc G: 96.777% 
LR: 2.000e-04 

2023-03-02 01:46:18,462 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.3536 (0.3465) Acc D Real: 97.476% 
Loss D Fake: 0.9298 (1.1562) Acc D Fake: 3.224% 
Loss D: 1.283 
Loss G: 0.5083 (0.4027) Acc G: 96.776% 
LR: 2.000e-04 

2023-03-02 01:46:18,469 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4022 (0.3470) Acc D Real: 97.475% 
Loss D Fake: 0.9261 (1.1541) Acc D Fake: 3.225% 
Loss D: 1.328 
Loss G: 0.5105 (0.4037) Acc G: 96.775% 
LR: 2.000e-04 

2023-03-02 01:46:18,476 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3702 (0.3472) Acc D Real: 97.474% 
Loss D Fake: 0.9230 (1.1520) Acc D Fake: 3.226% 
Loss D: 1.293 
Loss G: 0.5125 (0.4047) Acc G: 96.774% 
LR: 2.000e-04 

2023-03-02 01:46:18,483 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3598 (0.3474) Acc D Real: 97.475% 
Loss D Fake: 0.9199 (1.1499) Acc D Fake: 3.227% 
Loss D: 1.280 
Loss G: 0.5146 (0.4057) Acc G: 96.773% 
LR: 2.000e-04 

2023-03-02 01:46:18,491 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3838 (0.3477) Acc D Real: 97.474% 
Loss D Fake: 0.9168 (1.1478) Acc D Fake: 3.228% 
Loss D: 1.301 
Loss G: 0.5165 (0.4067) Acc G: 96.772% 
LR: 2.000e-04 

2023-03-02 01:46:18,498 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.4220 (0.3483) Acc D Real: 97.472% 
Loss D Fake: 0.9141 (1.1457) Acc D Fake: 3.229% 
Loss D: 1.336 
Loss G: 0.5180 (0.4077) Acc G: 96.771% 
LR: 2.000e-04 

2023-03-02 01:46:18,507 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3887 (0.3487) Acc D Real: 97.470% 
Loss D Fake: 0.9121 (1.1436) Acc D Fake: 3.230% 
Loss D: 1.301 
Loss G: 0.5193 (0.4087) Acc G: 96.770% 
LR: 2.000e-04 

2023-03-02 01:46:18,515 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3691 (0.3489) Acc D Real: 97.472% 
Loss D Fake: 0.9101 (1.1416) Acc D Fake: 3.231% 
Loss D: 1.279 
Loss G: 0.5207 (0.4097) Acc G: 96.769% 
LR: 2.000e-04 

2023-03-02 01:46:18,523 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.3532 (0.3489) Acc D Real: 97.471% 
Loss D Fake: 0.9080 (1.1395) Acc D Fake: 3.232% 
Loss D: 1.261 
Loss G: 0.5223 (0.4107) Acc G: 96.768% 
LR: 2.000e-04 

2023-03-02 01:46:18,530 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.4237 (0.3496) Acc D Real: 97.471% 
Loss D Fake: 0.9057 (1.1375) Acc D Fake: 3.233% 
Loss D: 1.329 
Loss G: 0.5235 (0.4116) Acc G: 96.767% 
LR: 2.000e-04 

2023-03-02 01:46:18,537 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3590 (0.3496) Acc D Real: 97.474% 
Loss D Fake: 0.9040 (1.1355) Acc D Fake: 3.234% 
Loss D: 1.263 
Loss G: 0.5247 (0.4126) Acc G: 96.766% 
LR: 2.000e-04 

2023-03-02 01:46:18,544 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3780 (0.3499) Acc D Real: 97.474% 
Loss D Fake: 0.9021 (1.1336) Acc D Fake: 3.234% 
Loss D: 1.280 
Loss G: 0.5260 (0.4136) Acc G: 96.766% 
LR: 2.000e-04 

2023-03-02 01:46:18,551 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3886 (0.3502) Acc D Real: 97.474% 
Loss D Fake: 0.9004 (1.1316) Acc D Fake: 3.235% 
Loss D: 1.289 
Loss G: 0.5272 (0.4145) Acc G: 96.765% 
LR: 2.000e-04 

2023-03-02 01:46:18,559 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3744 (0.3504) Acc D Real: 97.473% 
Loss D Fake: 0.8987 (1.1297) Acc D Fake: 3.236% 
Loss D: 1.273 
Loss G: 0.5283 (0.4155) Acc G: 96.764% 
LR: 2.000e-04 

2023-03-02 01:46:18,566 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.4351 (0.3511) Acc D Real: 97.472% 
Loss D Fake: 0.8972 (1.1277) Acc D Fake: 3.237% 
Loss D: 1.332 
Loss G: 0.5291 (0.4164) Acc G: 96.763% 
LR: 2.000e-04 

2023-03-02 01:46:18,575 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3786 (0.3513) Acc D Real: 97.469% 
Loss D Fake: 0.8962 (1.1258) Acc D Fake: 3.238% 
Loss D: 1.275 
Loss G: 0.5298 (0.4173) Acc G: 96.762% 
LR: 2.000e-04 

2023-03-02 01:46:18,583 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.4631 (0.3522) Acc D Real: 97.470% 
Loss D Fake: 0.8955 (1.1240) Acc D Fake: 3.238% 
Loss D: 1.359 
Loss G: 0.5299 (0.4183) Acc G: 96.762% 
LR: 2.000e-04 

2023-03-02 01:46:18,591 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3860 (0.3525) Acc D Real: 97.470% 
Loss D Fake: 0.8956 (1.1221) Acc D Fake: 3.239% 
Loss D: 1.282 
Loss G: 0.5299 (0.4192) Acc G: 96.761% 
LR: 2.000e-04 

2023-03-02 01:46:18,600 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3643 (0.3526) Acc D Real: 97.472% 
Loss D Fake: 0.8954 (1.1203) Acc D Fake: 3.240% 
Loss D: 1.260 
Loss G: 0.5302 (0.4200) Acc G: 96.760% 
LR: 2.000e-04 

2023-03-02 01:46:18,608 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3896 (0.3529) Acc D Real: 97.471% 
Loss D Fake: 0.8949 (1.1185) Acc D Fake: 3.241% 
Loss D: 1.284 
Loss G: 0.5305 (0.4209) Acc G: 96.759% 
LR: 2.000e-04 

2023-03-02 01:46:18,616 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.4261 (0.3535) Acc D Real: 97.472% 
Loss D Fake: 0.8945 (1.1168) Acc D Fake: 3.241% 
Loss D: 1.321 
Loss G: 0.5306 (0.4218) Acc G: 96.759% 
LR: 2.000e-04 

2023-03-02 01:46:18,623 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.4246 (0.3540) Acc D Real: 97.473% 
Loss D Fake: 0.8946 (1.1150) Acc D Fake: 3.242% 
Loss D: 1.319 
Loss G: 0.5304 (0.4226) Acc G: 96.758% 
LR: 2.000e-04 

2023-03-02 01:46:18,630 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3514 (0.3540) Acc D Real: 97.472% 
Loss D Fake: 0.8948 (1.1133) Acc D Fake: 3.243% 
Loss D: 1.246 
Loss G: 0.5306 (0.4235) Acc G: 96.757% 
LR: 2.000e-04 

2023-03-02 01:46:18,637 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3534 (0.3540) Acc D Real: 97.472% 
Loss D Fake: 0.8942 (1.1116) Acc D Fake: 3.244% 
Loss D: 1.248 
Loss G: 0.5311 (0.4243) Acc G: 96.756% 
LR: 2.000e-04 

2023-03-02 01:46:18,645 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3761 (0.3542) Acc D Real: 97.471% 
Loss D Fake: 0.8932 (1.1100) Acc D Fake: 3.244% 
Loss D: 1.269 
Loss G: 0.5319 (0.4251) Acc G: 96.756% 
LR: 2.000e-04 

2023-03-02 01:46:18,653 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3843 (0.3544) Acc D Real: 97.472% 
Loss D Fake: 0.8921 (1.1083) Acc D Fake: 3.245% 
Loss D: 1.276 
Loss G: 0.5327 (0.4259) Acc G: 96.755% 
LR: 2.000e-04 

2023-03-02 01:46:18,660 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3503 (0.3544) Acc D Real: 97.472% 
Loss D Fake: 0.8908 (1.1067) Acc D Fake: 3.246% 
Loss D: 1.241 
Loss G: 0.5338 (0.4268) Acc G: 96.754% 
LR: 2.000e-04 

2023-03-02 01:46:18,668 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3793 (0.3546) Acc D Real: 97.471% 
Loss D Fake: 0.8891 (1.1051) Acc D Fake: 3.246% 
Loss D: 1.268 
Loss G: 0.5349 (0.4276) Acc G: 96.754% 
LR: 2.000e-04 

2023-03-02 01:46:18,675 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.4162 (0.3550) Acc D Real: 97.469% 
Loss D Fake: 0.8876 (1.1034) Acc D Fake: 3.247% 
Loss D: 1.304 
Loss G: 0.5358 (0.4284) Acc G: 96.753% 
LR: 2.000e-04 

2023-03-02 01:46:18,683 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3687 (0.3551) Acc D Real: 97.469% 
Loss D Fake: 0.8865 (1.1018) Acc D Fake: 3.248% 
Loss D: 1.255 
Loss G: 0.5367 (0.4292) Acc G: 96.752% 
LR: 2.000e-04 

2023-03-02 01:46:18,691 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3866 (0.3553) Acc D Real: 97.471% 
Loss D Fake: 0.8852 (1.1003) Acc D Fake: 3.248% 
Loss D: 1.272 
Loss G: 0.5375 (0.4299) Acc G: 96.752% 
LR: 2.000e-04 

2023-03-02 01:46:18,698 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.3668 (0.3554) Acc D Real: 97.471% 
Loss D Fake: 0.8840 (1.0987) Acc D Fake: 3.249% 
Loss D: 1.251 
Loss G: 0.5384 (0.4307) Acc G: 96.751% 
LR: 2.000e-04 

2023-03-02 01:46:18,706 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4047 (0.3558) Acc D Real: 97.470% 
Loss D Fake: 0.8827 (1.0971) Acc D Fake: 3.249% 
Loss D: 1.287 
Loss G: 0.5392 (0.4315) Acc G: 96.751% 
LR: 2.000e-04 

2023-03-02 01:46:18,713 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.4688 (0.3566) Acc D Real: 97.468% 
Loss D Fake: 0.8820 (1.0956) Acc D Fake: 3.250% 
Loss D: 1.351 
Loss G: 0.5392 (0.4323) Acc G: 96.750% 
LR: 2.000e-04 

2023-03-02 01:46:18,721 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.4372 (0.3572) Acc D Real: 97.469% 
Loss D Fake: 0.8825 (1.0941) Acc D Fake: 3.251% 
Loss D: 1.320 
Loss G: 0.5387 (0.4330) Acc G: 96.749% 
LR: 2.000e-04 

2023-03-02 01:46:18,728 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3703 (0.3573) Acc D Real: 97.468% 
Loss D Fake: 0.8833 (1.0926) Acc D Fake: 3.251% 
Loss D: 1.254 
Loss G: 0.5384 (0.4338) Acc G: 96.749% 
LR: 2.000e-04 

2023-03-02 01:46:18,736 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.3965 (0.3575) Acc D Real: 97.470% 
Loss D Fake: 0.8836 (1.0911) Acc D Fake: 3.252% 
Loss D: 1.280 
Loss G: 0.5381 (0.4345) Acc G: 96.748% 
LR: 2.000e-04 

2023-03-02 01:46:18,743 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.3249 (0.3573) Acc D Real: 97.472% 
Loss D Fake: 0.8836 (1.0897) Acc D Fake: 3.252% 
Loss D: 1.209 
Loss G: 0.5385 (0.4352) Acc G: 96.748% 
LR: 2.000e-04 

2023-03-02 01:46:18,751 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.3962 (0.3576) Acc D Real: 97.472% 
Loss D Fake: 0.8828 (1.0883) Acc D Fake: 3.253% 
Loss D: 1.279 
Loss G: 0.5390 (0.4359) Acc G: 96.747% 
LR: 2.000e-04 

2023-03-02 01:46:18,758 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3781 (0.3577) Acc D Real: 97.470% 
Loss D Fake: 0.8821 (1.0869) Acc D Fake: 3.253% 
Loss D: 1.260 
Loss G: 0.5396 (0.4367) Acc G: 96.747% 
LR: 2.000e-04 

2023-03-02 01:46:18,765 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3678 (0.3578) Acc D Real: 97.470% 
Loss D Fake: 0.8812 (1.0855) Acc D Fake: 3.254% 
Loss D: 1.249 
Loss G: 0.5403 (0.4374) Acc G: 96.746% 
LR: 2.000e-04 

2023-03-02 01:46:18,772 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3645 (0.3578) Acc D Real: 97.469% 
Loss D Fake: 0.8800 (1.0841) Acc D Fake: 3.255% 
Loss D: 1.245 
Loss G: 0.5412 (0.4381) Acc G: 96.745% 
LR: 2.000e-04 

2023-03-02 01:46:18,780 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3437 (0.3577) Acc D Real: 97.467% 
Loss D Fake: 0.8785 (1.0827) Acc D Fake: 3.255% 
Loss D: 1.222 
Loss G: 0.5425 (0.4388) Acc G: 96.745% 
LR: 2.000e-04 

2023-03-02 01:46:18,787 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.4332 (0.3582) Acc D Real: 97.468% 
Loss D Fake: 0.8768 (1.0813) Acc D Fake: 3.256% 
Loss D: 1.310 
Loss G: 0.5434 (0.4395) Acc G: 96.744% 
LR: 2.000e-04 

2023-03-02 01:46:18,795 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3277 (0.3580) Acc D Real: 97.468% 
Loss D Fake: 0.8756 (1.0800) Acc D Fake: 3.256% 
Loss D: 1.203 
Loss G: 0.5445 (0.4402) Acc G: 96.744% 
LR: 2.000e-04 

2023-03-02 01:46:18,806 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.4127 (0.3584) Acc D Real: 97.467% 
Loss D Fake: 0.8739 (1.0786) Acc D Fake: 3.257% 
Loss D: 1.287 
Loss G: 0.5455 (0.4409) Acc G: 96.743% 
LR: 2.000e-04 

2023-03-02 01:46:18,814 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3820 (0.3585) Acc D Real: 97.466% 
Loss D Fake: 0.8727 (1.0773) Acc D Fake: 3.257% 
Loss D: 1.255 
Loss G: 0.5464 (0.4415) Acc G: 96.743% 
LR: 2.000e-04 

2023-03-02 01:46:18,822 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4315 (0.3590) Acc D Real: 97.466% 
Loss D Fake: 0.8717 (1.0759) Acc D Fake: 3.258% 
Loss D: 1.303 
Loss G: 0.5469 (0.4422) Acc G: 96.742% 
LR: 2.000e-04 

2023-03-02 01:46:18,829 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3978 (0.3593) Acc D Real: 97.465% 
Loss D Fake: 0.8713 (1.0746) Acc D Fake: 3.258% 
Loss D: 1.269 
Loss G: 0.5471 (0.4429) Acc G: 96.742% 
LR: 2.000e-04 

2023-03-02 01:46:18,836 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.4327 (0.3597) Acc D Real: 97.466% 
Loss D Fake: 0.8712 (1.0733) Acc D Fake: 3.259% 
Loss D: 1.304 
Loss G: 0.5469 (0.4436) Acc G: 96.741% 
LR: 2.000e-04 

2023-03-02 01:46:18,843 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4863 (0.3605) Acc D Real: 97.465% 
Loss D Fake: 0.8720 (1.0720) Acc D Fake: 3.259% 
Loss D: 1.358 
Loss G: 0.5459 (0.4442) Acc G: 96.741% 
LR: 2.000e-04 

2023-03-02 01:46:18,851 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4455 (0.3611) Acc D Real: 97.465% 
Loss D Fake: 0.8739 (1.0708) Acc D Fake: 3.259% 
Loss D: 1.319 
Loss G: 0.5443 (0.4449) Acc G: 96.741% 
LR: 2.000e-04 

2023-03-02 01:46:18,858 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3768 (0.3612) Acc D Real: 97.465% 
Loss D Fake: 0.8759 (1.0695) Acc D Fake: 3.260% 
Loss D: 1.253 
Loss G: 0.5431 (0.4455) Acc G: 96.740% 
LR: 2.000e-04 

2023-03-02 01:46:18,865 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4306 (0.3616) Acc D Real: 97.464% 
Loss D Fake: 0.8776 (1.0683) Acc D Fake: 3.260% 
Loss D: 1.308 
Loss G: 0.5418 (0.4461) Acc G: 96.740% 
LR: 2.000e-04 

2023-03-02 01:46:18,872 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3846 (0.3618) Acc D Real: 97.464% 
Loss D Fake: 0.8793 (1.0672) Acc D Fake: 3.261% 
Loss D: 1.264 
Loss G: 0.5408 (0.4467) Acc G: 96.739% 
LR: 2.000e-04 

2023-03-02 01:46:18,880 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3961 (0.3620) Acc D Real: 97.465% 
Loss D Fake: 0.8806 (1.0660) Acc D Fake: 3.261% 
Loss D: 1.277 
Loss G: 0.5399 (0.4472) Acc G: 96.739% 
LR: 2.000e-04 

2023-03-02 01:46:18,887 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.4028 (0.3622) Acc D Real: 97.465% 
Loss D Fake: 0.8818 (1.0649) Acc D Fake: 3.262% 
Loss D: 1.285 
Loss G: 0.5391 (0.4478) Acc G: 96.738% 
LR: 2.000e-04 

2023-03-02 01:46:18,894 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.3811 (0.3623) Acc D Real: 97.464% 
Loss D Fake: 0.8828 (1.0638) Acc D Fake: 3.262% 
Loss D: 1.264 
Loss G: 0.5386 (0.4484) Acc G: 96.738% 
LR: 2.000e-04 

2023-03-02 01:46:18,902 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.4008 (0.3626) Acc D Real: 97.464% 
Loss D Fake: 0.8834 (1.0627) Acc D Fake: 3.263% 
Loss D: 1.284 
Loss G: 0.5381 (0.4489) Acc G: 96.737% 
LR: 2.000e-04 

2023-03-02 01:46:18,909 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3487 (0.3625) Acc D Real: 97.466% 
Loss D Fake: 0.8838 (1.0616) Acc D Fake: 3.263% 
Loss D: 1.232 
Loss G: 0.5381 (0.4494) Acc G: 96.737% 
LR: 2.000e-04 

2023-03-02 01:46:18,916 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3379 (0.3623) Acc D Real: 97.467% 
Loss D Fake: 0.8834 (1.0605) Acc D Fake: 3.263% 
Loss D: 1.221 
Loss G: 0.5387 (0.4500) Acc G: 96.737% 
LR: 2.000e-04 

2023-03-02 01:46:18,924 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.4044 (0.3626) Acc D Real: 97.467% 
Loss D Fake: 0.8824 (1.0595) Acc D Fake: 3.264% 
Loss D: 1.287 
Loss G: 0.5393 (0.4505) Acc G: 96.736% 
LR: 2.000e-04 

2023-03-02 01:46:18,931 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3631 (0.3626) Acc D Real: 97.466% 
Loss D Fake: 0.8816 (1.0584) Acc D Fake: 3.264% 
Loss D: 1.245 
Loss G: 0.5400 (0.4510) Acc G: 96.736% 
LR: 2.000e-04 

2023-03-02 01:46:18,938 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4012 (0.3628) Acc D Real: 97.467% 
Loss D Fake: 0.8806 (1.0574) Acc D Fake: 3.265% 
Loss D: 1.282 
Loss G: 0.5406 (0.4516) Acc G: 96.735% 
LR: 2.000e-04 

2023-03-02 01:46:18,945 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3729 (0.3629) Acc D Real: 97.468% 
Loss D Fake: 0.8798 (1.0563) Acc D Fake: 3.265% 
Loss D: 1.253 
Loss G: 0.5412 (0.4521) Acc G: 96.735% 
LR: 2.000e-04 

2023-03-02 01:46:18,953 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.4342 (0.3633) Acc D Real: 97.468% 
Loss D Fake: 0.8791 (1.0553) Acc D Fake: 3.266% 
Loss D: 1.313 
Loss G: 0.5414 (0.4526) Acc G: 96.734% 
LR: 2.000e-04 

2023-03-02 01:46:18,960 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3359 (0.3631) Acc D Real: 97.468% 
Loss D Fake: 0.8789 (1.0543) Acc D Fake: 3.266% 
Loss D: 1.215 
Loss G: 0.5419 (0.4531) Acc G: 96.734% 
LR: 2.000e-04 

2023-03-02 01:46:18,967 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3162 (0.3629) Acc D Real: 97.468% 
Loss D Fake: 0.8777 (1.0533) Acc D Fake: 3.266% 
Loss D: 1.194 
Loss G: 0.5431 (0.4536) Acc G: 96.734% 
LR: 2.000e-04 

2023-03-02 01:46:18,975 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3868 (0.3630) Acc D Real: 97.468% 
Loss D Fake: 0.8758 (1.0523) Acc D Fake: 3.267% 
Loss D: 1.263 
Loss G: 0.5443 (0.4542) Acc G: 96.733% 
LR: 2.000e-04 

2023-03-02 01:46:18,982 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3893 (0.3632) Acc D Real: 97.468% 
Loss D Fake: 0.8742 (1.0513) Acc D Fake: 3.267% 
Loss D: 1.264 
Loss G: 0.5454 (0.4547) Acc G: 96.733% 
LR: 2.000e-04 

2023-03-02 01:46:18,989 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.3760 (0.3632) Acc D Real: 97.469% 
Loss D Fake: 0.8729 (1.0502) Acc D Fake: 3.267% 
Loss D: 1.249 
Loss G: 0.5464 (0.4552) Acc G: 96.733% 
LR: 2.000e-04 

2023-03-02 01:46:18,996 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.3992 (0.3634) Acc D Real: 97.470% 
Loss D Fake: 0.8716 (1.0492) Acc D Fake: 3.268% 
Loss D: 1.271 
Loss G: 0.5471 (0.4557) Acc G: 96.732% 
LR: 2.000e-04 

2023-03-02 01:46:19,003 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4140 (0.3637) Acc D Real: 97.470% 
Loss D Fake: 0.8708 (1.0482) Acc D Fake: 3.268% 
Loss D: 1.285 
Loss G: 0.5475 (0.4562) Acc G: 96.732% 
LR: 2.000e-04 

2023-03-02 01:46:19,011 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.3766 (0.3638) Acc D Real: 97.470% 
Loss D Fake: 0.8704 (1.0473) Acc D Fake: 3.269% 
Loss D: 1.247 
Loss G: 0.5478 (0.4567) Acc G: 96.731% 
LR: 2.000e-04 

2023-03-02 01:46:19,018 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3317 (0.3636) Acc D Real: 97.472% 
Loss D Fake: 0.8697 (1.0463) Acc D Fake: 3.269% 
Loss D: 1.201 
Loss G: 0.5487 (0.4572) Acc G: 96.731% 
LR: 2.000e-04 

2023-03-02 01:46:19,026 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3930 (0.3638) Acc D Real: 97.472% 
Loss D Fake: 0.8684 (1.0453) Acc D Fake: 3.269% 
Loss D: 1.261 
Loss G: 0.5494 (0.4577) Acc G: 96.731% 
LR: 2.000e-04 

2023-03-02 01:46:19,034 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.3871 (0.3639) Acc D Real: 97.472% 
Loss D Fake: 0.8674 (1.0443) Acc D Fake: 3.270% 
Loss D: 1.255 
Loss G: 0.5501 (0.4582) Acc G: 96.730% 
LR: 2.000e-04 

2023-03-02 01:46:19,041 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3638 (0.3639) Acc D Real: 97.472% 
Loss D Fake: 0.8665 (1.0434) Acc D Fake: 3.270% 
Loss D: 1.230 
Loss G: 0.5509 (0.4588) Acc G: 96.730% 
LR: 2.000e-04 

2023-03-02 01:46:19,048 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.4053 (0.3641) Acc D Real: 97.472% 
Loss D Fake: 0.8655 (1.0424) Acc D Fake: 3.270% 
Loss D: 1.271 
Loss G: 0.5515 (0.4593) Acc G: 96.730% 
LR: 2.000e-04 

2023-03-02 01:46:19,056 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3903 (0.3643) Acc D Real: 97.472% 
Loss D Fake: 0.8648 (1.0414) Acc D Fake: 3.271% 
Loss D: 1.255 
Loss G: 0.5519 (0.4597) Acc G: 96.729% 
LR: 2.000e-04 

2023-03-02 01:46:19,063 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3128 (0.3640) Acc D Real: 97.474% 
Loss D Fake: 0.8640 (1.0405) Acc D Fake: 3.271% 
Loss D: 1.177 
Loss G: 0.5528 (0.4602) Acc G: 96.729% 
LR: 2.000e-04 

2023-03-02 01:46:19,070 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3978 (0.3642) Acc D Real: 97.473% 
Loss D Fake: 0.8626 (1.0395) Acc D Fake: 3.271% 
Loss D: 1.260 
Loss G: 0.5538 (0.4607) Acc G: 96.729% 
LR: 2.000e-04 

2023-03-02 01:46:19,077 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3920 (0.3643) Acc D Real: 97.474% 
Loss D Fake: 0.8615 (1.0386) Acc D Fake: 3.272% 
Loss D: 1.254 
Loss G: 0.5544 (0.4612) Acc G: 96.728% 
LR: 2.000e-04 

2023-03-02 01:46:19,085 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4244 (0.3646) Acc D Real: 97.474% 
Loss D Fake: 0.8608 (1.0377) Acc D Fake: 3.272% 
Loss D: 1.285 
Loss G: 0.5547 (0.4617) Acc G: 96.728% 
LR: 2.000e-04 

2023-03-02 01:46:19,092 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.3547 (0.3646) Acc D Real: 97.475% 
Loss D Fake: 0.8605 (1.0367) Acc D Fake: 3.272% 
Loss D: 1.215 
Loss G: 0.5551 (0.4622) Acc G: 96.728% 
LR: 2.000e-04 

2023-03-02 01:46:19,099 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.4225 (0.3649) Acc D Real: 97.475% 
Loss D Fake: 0.8600 (1.0358) Acc D Fake: 3.273% 
Loss D: 1.283 
Loss G: 0.5552 (0.4627) Acc G: 96.727% 
LR: 2.000e-04 

2023-03-02 01:46:19,106 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3648 (0.3649) Acc D Real: 97.475% 
Loss D Fake: 0.8599 (1.0349) Acc D Fake: 3.273% 
Loss D: 1.225 
Loss G: 0.5554 (0.4632) Acc G: 96.727% 
LR: 2.000e-04 

2023-03-02 01:46:19,113 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3956 (0.3650) Acc D Real: 97.475% 
Loss D Fake: 0.8596 (1.0340) Acc D Fake: 3.273% 
Loss D: 1.255 
Loss G: 0.5556 (0.4637) Acc G: 96.727% 
LR: 2.000e-04 

2023-03-02 01:46:19,121 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3541 (0.3650) Acc D Real: 97.474% 
Loss D Fake: 0.8593 (1.0331) Acc D Fake: 3.274% 
Loss D: 1.213 
Loss G: 0.5561 (0.4641) Acc G: 96.726% 
LR: 2.000e-04 

2023-03-02 01:46:19,129 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.4764 (0.3655) Acc D Real: 97.474% 
Loss D Fake: 0.8586 (1.0322) Acc D Fake: 3.274% 
Loss D: 1.335 
Loss G: 0.5571 (0.4646) Acc G: 96.726% 
LR: 2.000e-04 

2023-03-02 01:46:19,137 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.4223 (0.3658) Acc D Real: 97.475% 
Loss D Fake: 0.8601 (1.0313) Acc D Fake: 3.274% 
Loss D: 1.282 
Loss G: 0.5564 (0.4651) Acc G: 96.726% 
LR: 2.000e-04 

2023-03-02 01:46:19,144 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3402 (0.3657) Acc D Real: 97.476% 
Loss D Fake: 0.8668 (1.0305) Acc D Fake: 3.274% 
Loss D: 1.207 
Loss G: 0.5529 (0.4655) Acc G: 96.726% 
LR: 2.000e-04 

2023-03-02 01:46:19,152 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3311 (0.3655) Acc D Real: 97.476% 
Loss D Fake: 0.8867 (1.0298) Acc D Fake: 3.275% 
Loss D: 1.218 
Loss G: 0.5339 (0.4659) Acc G: 96.725% 
LR: 2.000e-04 

2023-03-02 01:46:19,159 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3995 (0.3657) Acc D Real: 97.475% 
Loss D Fake: 2.8203 (1.0387) Acc D Fake: 3.275% 
Loss D: 3.220 
Loss G: 0.1122 (0.4641) Acc G: 96.725% 
LR: 2.000e-04 

2023-03-02 01:46:19,167 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3167 (0.3655) Acc D Real: 97.475% 
Loss D Fake: 2.7270 (1.0471) Acc D Fake: 3.275% 
Loss D: 3.044 
Loss G: 0.1124 (0.4623) Acc G: 96.725% 
LR: 2.000e-04 

2023-03-02 01:46:19,175 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.3406 (0.3653) Acc D Real: 97.475% 
Loss D Fake: 2.6956 (1.0553) Acc D Fake: 3.276% 
Loss D: 3.036 
Loss G: 0.1147 (0.4606) Acc G: 96.724% 
LR: 2.000e-04 

2023-03-02 01:46:19,183 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3790 (0.3654) Acc D Real: 97.475% 
Loss D Fake: 2.6303 (1.0631) Acc D Fake: 3.276% 
Loss D: 3.009 
Loss G: 0.1211 (0.4590) Acc G: 96.724% 
LR: 2.000e-04 

2023-03-02 01:46:19,190 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.3467 (0.3653) Acc D Real: 97.475% 
Loss D Fake: 2.5086 (1.0701) Acc D Fake: 3.276% 
Loss D: 2.855 
Loss G: 0.1277 (0.4573) Acc G: 96.724% 
LR: 2.000e-04 

2023-03-02 01:46:19,198 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2943 (0.3650) Acc D Real: 97.475% 
Loss D Fake: 2.4439 (1.0768) Acc D Fake: 3.276% 
Loss D: 2.738 
Loss G: 0.1319 (0.4557) Acc G: 96.724% 
LR: 2.000e-04 

2023-03-02 01:46:19,206 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3552 (0.3649) Acc D Real: 97.475% 
Loss D Fake: 2.3925 (1.0832) Acc D Fake: 3.277% 
Loss D: 2.748 
Loss G: 0.1363 (0.4542) Acc G: 96.723% 
LR: 2.000e-04 

2023-03-02 01:46:19,214 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.3710 (0.3649) Acc D Real: 97.474% 
Loss D Fake: 2.3399 (1.0893) Acc D Fake: 3.277% 
Loss D: 2.711 
Loss G: 0.1412 (0.4527) Acc G: 96.723% 
LR: 2.000e-04 

2023-03-02 01:46:19,221 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.3618 (0.3649) Acc D Real: 97.474% 
Loss D Fake: 2.2860 (1.0951) Acc D Fake: 3.277% 
Loss D: 2.648 
Loss G: 0.1465 (0.4512) Acc G: 96.723% 
LR: 2.000e-04 

2023-03-02 01:46:19,229 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.3435 (0.3648) Acc D Real: 97.475% 
Loss D Fake: 2.2313 (1.1005) Acc D Fake: 3.278% 
Loss D: 2.575 
Loss G: 0.1522 (0.4498) Acc G: 96.722% 
LR: 2.000e-04 

2023-03-02 01:46:19,237 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2961 (0.3645) Acc D Real: 97.475% 
Loss D Fake: 2.1763 (1.1056) Acc D Fake: 3.278% 
Loss D: 2.472 
Loss G: 0.1584 (0.4484) Acc G: 96.722% 
LR: 2.000e-04 

2023-03-02 01:46:19,244 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3472 (0.3644) Acc D Real: 97.476% 
Loss D Fake: 2.1214 (1.1104) Acc D Fake: 3.278% 
Loss D: 2.469 
Loss G: 0.1648 (0.4470) Acc G: 96.722% 
LR: 2.000e-04 

2023-03-02 01:46:19,252 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3451 (0.3643) Acc D Real: 97.475% 
Loss D Fake: 2.0672 (1.1149) Acc D Fake: 3.278% 
Loss D: 2.412 
Loss G: 0.1717 (0.4457) Acc G: 96.722% 
LR: 2.000e-04 

2023-03-02 01:46:19,259 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3044 (0.3640) Acc D Real: 97.474% 
Loss D Fake: 2.0140 (1.1192) Acc D Fake: 3.279% 
Loss D: 2.318 
Loss G: 0.1788 (0.4445) Acc G: 96.721% 
LR: 2.000e-04 

2023-03-02 01:46:19,267 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3401 (0.3639) Acc D Real: 97.474% 
Loss D Fake: 1.9621 (1.1231) Acc D Fake: 3.279% 
Loss D: 2.302 
Loss G: 0.1862 (0.4433) Acc G: 96.721% 
LR: 2.000e-04 

2023-03-02 01:46:19,274 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3009 (0.3636) Acc D Real: 97.473% 
Loss D Fake: 1.9114 (1.1268) Acc D Fake: 3.279% 
Loss D: 2.212 
Loss G: 0.1939 (0.4421) Acc G: 96.721% 
LR: 2.000e-04 

2023-03-02 01:46:19,281 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3223 (0.3635) Acc D Real: 97.472% 
Loss D Fake: 1.8620 (1.1302) Acc D Fake: 3.279% 
Loss D: 2.184 
Loss G: 0.2019 (0.4410) Acc G: 96.721% 
LR: 2.000e-04 

2023-03-02 01:46:19,289 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.3236 (0.3633) Acc D Real: 97.471% 
Loss D Fake: 1.8140 (1.1333) Acc D Fake: 3.280% 
Loss D: 2.138 
Loss G: 0.2101 (0.4399) Acc G: 96.720% 
LR: 2.000e-04 

2023-03-02 01:46:19,297 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3220 (0.3631) Acc D Real: 97.470% 
Loss D Fake: 1.7673 (1.1362) Acc D Fake: 3.287% 
Loss D: 2.089 
Loss G: 0.2185 (0.4389) Acc G: 96.713% 
LR: 2.000e-04 

2023-03-02 01:46:19,304 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3300 (0.3629) Acc D Real: 97.466% 
Loss D Fake: 1.7219 (1.1389) Acc D Fake: 3.295% 
Loss D: 2.052 
Loss G: 0.2273 (0.4380) Acc G: 96.705% 
LR: 2.000e-04 

2023-03-02 01:46:19,312 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3138 (0.3627) Acc D Real: 97.463% 
Loss D Fake: 1.6779 (1.1414) Acc D Fake: 3.303% 
Loss D: 1.992 
Loss G: 0.2362 (0.4370) Acc G: 96.697% 
LR: 2.000e-04 

2023-03-02 01:46:19,319 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.3062 (0.3624) Acc D Real: 97.462% 
Loss D Fake: 1.6353 (1.1436) Acc D Fake: 3.311% 
Loss D: 1.942 
Loss G: 0.2454 (0.4362) Acc G: 96.689% 
LR: 2.000e-04 

2023-03-02 01:46:19,327 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3253 (0.3623) Acc D Real: 97.458% 
Loss D Fake: 1.5939 (1.1456) Acc D Fake: 3.318% 
Loss D: 1.919 
Loss G: 0.2548 (0.4354) Acc G: 96.682% 
LR: 2.000e-04 

2023-03-02 01:46:19,334 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2890 (0.3620) Acc D Real: 97.456% 
Loss D Fake: 1.5538 (1.1475) Acc D Fake: 3.326% 
Loss D: 1.843 
Loss G: 0.2644 (0.4346) Acc G: 96.674% 
LR: 2.000e-04 

2023-03-02 01:46:19,342 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2993 (0.3617) Acc D Real: 97.450% 
Loss D Fake: 1.5151 (1.1491) Acc D Fake: 3.333% 
Loss D: 1.814 
Loss G: 0.2741 (0.4339) Acc G: 96.667% 
LR: 2.000e-04 

2023-03-02 01:46:19,349 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3131 (0.3615) Acc D Real: 97.446% 
Loss D Fake: 1.4777 (1.1506) Acc D Fake: 3.341% 
Loss D: 1.791 
Loss G: 0.2840 (0.4332) Acc G: 96.659% 
LR: 2.000e-04 

2023-03-02 01:46:19,357 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.3203 (0.3613) Acc D Real: 97.446% 
Loss D Fake: 1.4412 (1.1518) Acc D Fake: 3.343% 
Loss D: 1.762 
Loss G: 0.2942 (0.4326) Acc G: 96.657% 
LR: 2.000e-04 

2023-03-02 01:46:19,368 -                train: [    INFO] - 
Epoch: 17/20
2023-03-02 01:46:19,569 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2875 (0.2962) Acc D Real: 96.719% 
Loss D Fake: 1.3712 (1.3885) Acc D Fake: 5.000% 
Loss D: 1.659 
Loss G: 0.3153 (0.3100) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,577 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3045 (0.2990) Acc D Real: 96.684% 
Loss D Fake: 1.3377 (1.3716) Acc D Fake: 5.000% 
Loss D: 1.642 
Loss G: 0.3262 (0.3154) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,584 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.3058 (0.3007) Acc D Real: 96.719% 
Loss D Fake: 1.3052 (1.3550) Acc D Fake: 5.000% 
Loss D: 1.611 
Loss G: 0.3372 (0.3208) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,591 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3036 (0.3013) Acc D Real: 96.677% 
Loss D Fake: 1.2737 (1.3387) Acc D Fake: 5.000% 
Loss D: 1.577 
Loss G: 0.3483 (0.3263) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,598 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2965 (0.3005) Acc D Real: 96.675% 
Loss D Fake: 1.2434 (1.3228) Acc D Fake: 5.000% 
Loss D: 1.540 
Loss G: 0.3595 (0.3319) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,608 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3036 (0.3009) Acc D Real: 96.637% 
Loss D Fake: 1.2143 (1.3073) Acc D Fake: 5.000% 
Loss D: 1.518 
Loss G: 0.3708 (0.3374) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,618 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.3087 (0.3019) Acc D Real: 96.582% 
Loss D Fake: 1.1862 (1.2922) Acc D Fake: 5.000% 
Loss D: 1.495 
Loss G: 0.3822 (0.3430) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,625 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.3030 (0.3020) Acc D Real: 96.522% 
Loss D Fake: 1.1592 (1.2774) Acc D Fake: 5.000% 
Loss D: 1.462 
Loss G: 0.3934 (0.3486) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,633 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2908 (0.3009) Acc D Real: 96.547% 
Loss D Fake: 1.1337 (1.2630) Acc D Fake: 5.000% 
Loss D: 1.425 
Loss G: 0.4046 (0.3542) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,640 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.2967 (0.3005) Acc D Real: 96.548% 
Loss D Fake: 1.1090 (1.2490) Acc D Fake: 5.000% 
Loss D: 1.406 
Loss G: 0.4157 (0.3598) Acc G: 95.000% 
LR: 2.000e-04 

2023-03-02 01:46:19,650 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.3029 (0.3007) Acc D Real: 96.519% 
Loss D Fake: 1.0856 (1.2354) Acc D Fake: 5.139% 
Loss D: 1.388 
Loss G: 0.4269 (0.3654) Acc G: 94.861% 
LR: 2.000e-04 

2023-03-02 01:46:19,657 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.3162 (0.3019) Acc D Real: 96.474% 
Loss D Fake: 1.0630 (1.2222) Acc D Fake: 5.256% 
Loss D: 1.379 
Loss G: 0.4379 (0.3710) Acc G: 94.744% 
LR: 2.000e-04 

2023-03-02 01:46:19,664 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3163 (0.3029) Acc D Real: 96.432% 
Loss D Fake: 1.0412 (1.2092) Acc D Fake: 5.357% 
Loss D: 1.357 
Loss G: 0.4491 (0.3766) Acc G: 94.643% 
LR: 2.000e-04 

2023-03-02 01:46:19,671 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3183 (0.3040) Acc D Real: 96.406% 
Loss D Fake: 1.0204 (1.1966) Acc D Fake: 5.444% 
Loss D: 1.339 
Loss G: 0.4597 (0.3821) Acc G: 94.556% 
LR: 2.000e-04 

2023-03-02 01:46:19,678 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3022 (0.3039) Acc D Real: 96.403% 
Loss D Fake: 1.0012 (1.1844) Acc D Fake: 5.521% 
Loss D: 1.303 
Loss G: 0.4704 (0.3876) Acc G: 94.479% 
LR: 2.000e-04 

2023-03-02 01:46:19,686 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3157 (0.3046) Acc D Real: 96.366% 
Loss D Fake: 0.9823 (1.1725) Acc D Fake: 5.588% 
Loss D: 1.298 
Loss G: 0.4810 (0.3931) Acc G: 94.412% 
LR: 2.000e-04 

2023-03-02 01:46:19,693 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.3331 (0.3061) Acc D Real: 96.334% 
Loss D Fake: 0.9642 (1.1610) Acc D Fake: 5.648% 
Loss D: 1.297 
Loss G: 0.4914 (0.3986) Acc G: 94.352% 
LR: 2.000e-04 

2023-03-02 01:46:19,700 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3102 (0.3064) Acc D Real: 96.305% 
Loss D Fake: 0.9474 (1.1497) Acc D Fake: 5.702% 
Loss D: 1.258 
Loss G: 0.5014 (0.4040) Acc G: 94.298% 
LR: 2.000e-04 

2023-03-02 01:46:19,707 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.3317 (0.3076) Acc D Real: 96.284% 
Loss D Fake: 0.9318 (1.1388) Acc D Fake: 5.750% 
Loss D: 1.264 
Loss G: 0.5107 (0.4093) Acc G: 94.250% 
LR: 2.000e-04 

2023-03-02 01:46:19,714 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.3048 (0.3075) Acc D Real: 96.272% 
Loss D Fake: 0.9175 (1.1283) Acc D Fake: 5.794% 
Loss D: 1.222 
Loss G: 0.5201 (0.4146) Acc G: 94.206% 
LR: 2.000e-04 

2023-03-02 01:46:19,721 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.3131 (0.3077) Acc D Real: 96.257% 
Loss D Fake: 0.9032 (1.1180) Acc D Fake: 5.833% 
Loss D: 1.216 
Loss G: 0.5295 (0.4198) Acc G: 94.167% 
LR: 2.000e-04 

2023-03-02 01:46:19,728 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.3320 (0.3088) Acc D Real: 96.214% 
Loss D Fake: 0.8908 (1.1082) Acc D Fake: 5.870% 
Loss D: 1.223 
Loss G: 0.5359 (0.4249) Acc G: 94.130% 
LR: 2.000e-04 

2023-03-02 01:46:19,735 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.3221 (0.3094) Acc D Real: 96.204% 
Loss D Fake: 0.8867 (1.0989) Acc D Fake: 5.903% 
Loss D: 1.209 
Loss G: 0.5385 (0.4296) Acc G: 94.097% 
LR: 2.000e-04 

2023-03-02 01:46:19,744 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2960 (0.3088) Acc D Real: 96.183% 
Loss D Fake: 0.9070 (1.0913) Acc D Fake: 5.933% 
Loss D: 1.203 
Loss G: 0.5532 (0.4345) Acc G: 94.067% 
LR: 2.000e-04 

2023-03-02 01:46:19,752 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.3074 (0.3088) Acc D Real: 96.150% 
Loss D Fake: 0.8503 (1.0820) Acc D Fake: 5.962% 
Loss D: 1.158 
Loss G: 0.5693 (0.4397) Acc G: 94.038% 
LR: 2.000e-04 

2023-03-02 01:46:19,759 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.3221 (0.3093) Acc D Real: 96.132% 
Loss D Fake: 0.8306 (1.0727) Acc D Fake: 5.988% 
Loss D: 1.153 
Loss G: 0.5819 (0.4450) Acc G: 94.012% 
LR: 2.000e-04 

2023-03-02 01:46:19,767 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3229 (0.3097) Acc D Real: 96.097% 
Loss D Fake: 0.8146 (1.0635) Acc D Fake: 6.012% 
Loss D: 1.137 
Loss G: 0.5934 (0.4503) Acc G: 93.988% 
LR: 2.000e-04 

2023-03-02 01:46:19,775 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3191 (0.3101) Acc D Real: 96.076% 
Loss D Fake: 0.8001 (1.0544) Acc D Fake: 6.034% 
Loss D: 1.119 
Loss G: 0.6045 (0.4556) Acc G: 93.908% 
LR: 2.000e-04 

2023-03-02 01:46:19,782 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3103 (0.3101) Acc D Real: 96.062% 
Loss D Fake: 0.7864 (1.0454) Acc D Fake: 6.111% 
Loss D: 1.097 
Loss G: 0.6155 (0.4609) Acc G: 93.833% 
LR: 2.000e-04 

2023-03-02 01:46:19,790 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.3461 (0.3112) Acc D Real: 96.040% 
Loss D Fake: 0.7733 (1.0367) Acc D Fake: 6.183% 
Loss D: 1.119 
Loss G: 0.6260 (0.4663) Acc G: 93.763% 
LR: 2.000e-04 

2023-03-02 01:46:19,797 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.3078 (0.3111) Acc D Real: 96.021% 
Loss D Fake: 0.7609 (1.0281) Acc D Fake: 6.250% 
Loss D: 1.069 
Loss G: 0.6367 (0.4716) Acc G: 93.698% 
LR: 2.000e-04 

2023-03-02 01:46:19,804 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.3414 (0.3120) Acc D Real: 95.974% 
Loss D Fake: 0.7486 (1.0196) Acc D Fake: 6.313% 
Loss D: 1.090 
Loss G: 0.6476 (0.4769) Acc G: 93.636% 
LR: 2.000e-04 

2023-03-02 01:46:19,811 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.3092 (0.3120) Acc D Real: 95.965% 
Loss D Fake: 0.7361 (1.0112) Acc D Fake: 6.373% 
Loss D: 1.045 
Loss G: 0.6597 (0.4823) Acc G: 93.578% 
LR: 2.000e-04 

2023-03-02 01:46:19,819 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.3046 (0.3118) Acc D Real: 95.952% 
Loss D Fake: 0.7219 (1.0030) Acc D Fake: 6.429% 
Loss D: 1.027 
Loss G: 0.6752 (0.4878) Acc G: 93.286% 
LR: 2.000e-04 

2023-03-02 01:46:19,826 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3394 (0.3125) Acc D Real: 95.874% 
Loss D Fake: 0.7032 (0.9947) Acc D Fake: 6.944% 
Loss D: 1.043 
Loss G: 0.6988 (0.4937) Acc G: 91.806% 
LR: 2.000e-04 

2023-03-02 01:46:19,833 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.3529 (0.3136) Acc D Real: 95.776% 
Loss D Fake: 0.6741 (0.9860) Acc D Fake: 9.189% 
Loss D: 1.027 
Loss G: 0.7400 (0.5003) Acc G: 89.595% 
LR: 2.000e-04 

2023-03-02 01:46:19,840 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.3723 (0.3152) Acc D Real: 95.524% 
Loss D Fake: 0.6320 (0.9767) Acc D Fake: 11.360% 
Loss D: 1.004 
Loss G: 0.7811 (0.5077) Acc G: 87.456% 
LR: 2.000e-04 

2023-03-02 01:46:19,848 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3263 (0.3154) Acc D Real: 95.294% 
Loss D Fake: 0.6031 (0.9671) Acc D Fake: 13.419% 
Loss D: 0.929 
Loss G: 0.8074 (0.5154) Acc G: 85.385% 
LR: 2.000e-04 

2023-03-02 01:46:19,855 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.4121 (0.3179) Acc D Real: 94.758% 
Loss D Fake: 0.5871 (0.9576) Acc D Fake: 15.417% 
Loss D: 0.999 
Loss G: 0.8220 (0.5231) Acc G: 83.417% 
LR: 2.000e-04 

2023-03-02 01:46:19,862 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.3647 (0.3190) Acc D Real: 94.452% 
Loss D Fake: 0.5782 (0.9483) Acc D Fake: 17.317% 
Loss D: 0.943 
Loss G: 0.8316 (0.5306) Acc G: 81.545% 
LR: 2.000e-04 

2023-03-02 01:46:19,869 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3636 (0.3201) Acc D Real: 94.153% 
Loss D Fake: 0.5720 (0.9394) Acc D Fake: 19.127% 
Loss D: 0.936 
Loss G: 0.8389 (0.5379) Acc G: 79.762% 
LR: 2.000e-04 

2023-03-02 01:46:19,877 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.3438 (0.3206) Acc D Real: 93.973% 
Loss D Fake: 0.5670 (0.9307) Acc D Fake: 20.891% 
Loss D: 0.911 
Loss G: 0.8452 (0.5451) Acc G: 78.023% 
LR: 2.000e-04 

2023-03-02 01:46:19,884 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.3986 (0.3224) Acc D Real: 93.567% 
Loss D Fake: 0.5629 (0.9224) Acc D Fake: 22.576% 
Loss D: 0.961 
Loss G: 0.8502 (0.5520) Acc G: 76.364% 
LR: 2.000e-04 

2023-03-02 01:46:19,891 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.3344 (0.3227) Acc D Real: 93.411% 
Loss D Fake: 0.5596 (0.9143) Acc D Fake: 24.185% 
Loss D: 0.894 
Loss G: 0.8548 (0.5587) Acc G: 74.778% 
LR: 2.000e-04 

2023-03-02 01:46:19,899 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.2876 (0.3219) Acc D Real: 93.399% 
Loss D Fake: 0.5563 (0.9065) Acc D Fake: 25.725% 
Loss D: 0.844 
Loss G: 0.8597 (0.5653) Acc G: 73.261% 
LR: 2.000e-04 

2023-03-02 01:46:19,907 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.3639 (0.3228) Acc D Real: 93.201% 
Loss D Fake: 0.5529 (0.8990) Acc D Fake: 27.199% 
Loss D: 0.917 
Loss G: 0.8640 (0.5716) Acc G: 71.809% 
LR: 2.000e-04 

2023-03-02 01:46:19,916 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3522 (0.3234) Acc D Real: 92.999% 
Loss D Fake: 0.5502 (0.8917) Acc D Fake: 28.611% 
Loss D: 0.902 
Loss G: 0.8676 (0.5778) Acc G: 70.417% 
LR: 2.000e-04 

2023-03-02 01:46:19,923 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3353 (0.3236) Acc D Real: 92.872% 
Loss D Fake: 0.5480 (0.8847) Acc D Fake: 29.966% 
Loss D: 0.883 
Loss G: 0.8707 (0.5838) Acc G: 69.082% 
LR: 2.000e-04 

2023-03-02 01:46:19,930 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3188 (0.3235) Acc D Real: 92.755% 
Loss D Fake: 0.5461 (0.8779) Acc D Fake: 31.267% 
Loss D: 0.865 
Loss G: 0.8738 (0.5896) Acc G: 67.800% 
LR: 2.000e-04 

2023-03-02 01:46:19,937 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2748 (0.3226) Acc D Real: 92.745% 
Loss D Fake: 0.5438 (0.8714) Acc D Fake: 32.516% 
Loss D: 0.819 
Loss G: 0.8778 (0.5952) Acc G: 66.569% 
LR: 2.000e-04 

2023-03-02 01:46:19,944 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.3619 (0.3233) Acc D Real: 92.549% 
Loss D Fake: 0.5413 (0.8650) Acc D Fake: 33.718% 
Loss D: 0.903 
Loss G: 0.8811 (0.6007) Acc G: 65.385% 
LR: 2.000e-04 

2023-03-02 01:46:19,952 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.3547 (0.3239) Acc D Real: 92.371% 
Loss D Fake: 0.5395 (0.8589) Acc D Fake: 34.874% 
Loss D: 0.894 
Loss G: 0.8836 (0.6061) Acc G: 64.245% 
LR: 2.000e-04 

2023-03-02 01:46:19,959 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2969 (0.3234) Acc D Real: 92.353% 
Loss D Fake: 0.5382 (0.8530) Acc D Fake: 35.957% 
Loss D: 0.835 
Loss G: 0.8861 (0.6113) Acc G: 63.179% 
LR: 2.000e-04 

2023-03-02 01:46:19,966 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.3198 (0.3234) Acc D Real: 92.286% 
Loss D Fake: 0.5369 (0.8472) Acc D Fake: 37.000% 
Loss D: 0.857 
Loss G: 0.8881 (0.6163) Acc G: 62.152% 
LR: 2.000e-04 

2023-03-02 01:46:19,974 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2644 (0.3223) Acc D Real: 92.288% 
Loss D Fake: 0.5358 (0.8417) Acc D Fake: 38.006% 
Loss D: 0.800 
Loss G: 0.8907 (0.6212) Acc G: 61.161% 
LR: 2.000e-04 

2023-03-02 01:46:19,982 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3457 (0.3227) Acc D Real: 92.135% 
Loss D Fake: 0.5347 (0.8363) Acc D Fake: 38.977% 
Loss D: 0.880 
Loss G: 0.8923 (0.6260) Acc G: 60.205% 
LR: 2.000e-04 

2023-03-02 01:46:19,989 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.3597 (0.3234) Acc D Real: 91.936% 
Loss D Fake: 0.5347 (0.8311) Acc D Fake: 39.914% 
Loss D: 0.894 
Loss G: 0.8923 (0.6305) Acc G: 59.282% 
LR: 2.000e-04 

2023-03-02 01:46:19,997 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.2907 (0.3228) Acc D Real: 91.891% 
Loss D Fake: 0.5358 (0.8261) Acc D Fake: 40.791% 
Loss D: 0.827 
Loss G: 0.8917 (0.6350) Acc G: 58.418% 
LR: 2.000e-04 

2023-03-02 01:46:20,005 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3611 (0.3235) Acc D Real: 91.697% 
Loss D Fake: 0.5373 (0.8213) Acc D Fake: 41.639% 
Loss D: 0.898 
Loss G: 0.8903 (0.6392) Acc G: 57.583% 
LR: 2.000e-04 

2023-03-02 01:46:20,012 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.3304 (0.3236) Acc D Real: 91.543% 
Loss D Fake: 0.5411 (0.8167) Acc D Fake: 42.432% 
Loss D: 0.872 
Loss G: 0.8820 (0.6432) Acc G: 56.803% 
LR: 2.000e-04 

2023-03-02 01:46:20,020 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2640 (0.3226) Acc D Real: 91.543% 
Loss D Fake: 0.5543 (0.8124) Acc D Fake: 43.172% 
Loss D: 0.818 
Loss G: 0.8692 (0.6468) Acc G: 56.102% 
LR: 2.000e-04 

2023-03-02 01:46:20,027 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.2985 (0.3222) Acc D Real: 91.489% 
Loss D Fake: 0.5838 (0.8088) Acc D Fake: 43.783% 
Loss D: 0.882 
Loss G: 0.8629 (0.6503) Acc G: 55.450% 
LR: 2.000e-04 

2023-03-02 01:46:20,035 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.3632 (0.3229) Acc D Real: 91.275% 
Loss D Fake: 0.5847 (0.8053) Acc D Fake: 44.349% 
Loss D: 0.948 
Loss G: 0.8816 (0.6539) Acc G: 54.766% 
LR: 2.000e-04 

2023-03-02 01:46:20,042 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.2837 (0.3223) Acc D Real: 91.237% 
Loss D Fake: 0.5444 (0.8013) Acc D Fake: 45.026% 
Loss D: 0.828 
Loss G: 0.9002 (0.6577) Acc G: 54.077% 
LR: 2.000e-04 

2023-03-02 01:46:20,049 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.2854 (0.3217) Acc D Real: 91.193% 
Loss D Fake: 0.5290 (0.7972) Acc D Fake: 45.732% 
Loss D: 0.814 
Loss G: 0.9128 (0.6615) Acc G: 53.359% 
LR: 2.000e-04 

2023-03-02 01:46:20,056 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.2694 (0.3209) Acc D Real: 91.196% 
Loss D Fake: 0.5193 (0.7930) Acc D Fake: 46.443% 
Loss D: 0.789 
Loss G: 0.9236 (0.6655) Acc G: 52.637% 
LR: 2.000e-04 

2023-03-02 01:46:20,065 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.2782 (0.3203) Acc D Real: 91.172% 
Loss D Fake: 0.5110 (0.7889) Acc D Fake: 47.157% 
Loss D: 0.789 
Loss G: 0.9350 (0.6694) Acc G: 51.936% 
LR: 2.000e-04 

2023-03-02 01:46:20,072 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.2949 (0.3199) Acc D Real: 91.093% 
Loss D Fake: 0.5033 (0.7847) Acc D Fake: 47.874% 
Loss D: 0.798 
Loss G: 0.9449 (0.6734) Acc G: 51.208% 
LR: 2.000e-04 

2023-03-02 01:46:20,080 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.2949 (0.3196) Acc D Real: 91.050% 
Loss D Fake: 0.4973 (0.7806) Acc D Fake: 48.595% 
Loss D: 0.792 
Loss G: 0.9529 (0.6774) Acc G: 50.500% 
LR: 2.000e-04 

2023-03-02 01:46:20,087 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.3121 (0.3195) Acc D Real: 90.950% 
Loss D Fake: 0.4928 (0.7766) Acc D Fake: 49.296% 
Loss D: 0.805 
Loss G: 0.9587 (0.6814) Acc G: 49.789% 
LR: 2.000e-04 

2023-03-02 01:46:20,094 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.2756 (0.3189) Acc D Real: 90.905% 
Loss D Fake: 0.4897 (0.7726) Acc D Fake: 50.000% 
Loss D: 0.765 
Loss G: 0.9631 (0.6853) Acc G: 49.097% 
LR: 2.000e-04 

2023-03-02 01:46:20,101 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.2770 (0.3183) Acc D Real: 90.850% 
Loss D Fake: 0.4872 (0.7687) Acc D Fake: 50.685% 
Loss D: 0.764 
Loss G: 0.9671 (0.6891) Acc G: 48.425% 
LR: 2.000e-04 

2023-03-02 01:46:20,109 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.2777 (0.3177) Acc D Real: 90.819% 
Loss D Fake: 0.4849 (0.7648) Acc D Fake: 51.351% 
Loss D: 0.763 
Loss G: 0.9711 (0.6930) Acc G: 47.770% 
LR: 2.000e-04 

2023-03-02 01:46:20,116 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3321 (0.3179) Acc D Real: 90.680% 
Loss D Fake: 0.4830 (0.7611) Acc D Fake: 52.000% 
Loss D: 0.815 
Loss G: 0.9734 (0.6967) Acc G: 47.133% 
LR: 2.000e-04 

2023-03-02 01:46:20,123 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3325 (0.3181) Acc D Real: 90.563% 
Loss D Fake: 0.4828 (0.7574) Acc D Fake: 52.632% 
Loss D: 0.815 
Loss G: 0.9726 (0.7003) Acc G: 46.513% 
LR: 2.000e-04 

2023-03-02 01:46:20,130 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2714 (0.3175) Acc D Real: 90.520% 
Loss D Fake: 0.4836 (0.7539) Acc D Fake: 53.247% 
Loss D: 0.755 
Loss G: 0.9747 (0.7039) Acc G: 45.909% 
LR: 2.000e-04 

2023-03-02 01:46:20,138 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.3096 (0.3174) Acc D Real: 90.395% 
Loss D Fake: 0.4812 (0.7504) Acc D Fake: 53.846% 
Loss D: 0.791 
Loss G: 0.9799 (0.7074) Acc G: 45.321% 
LR: 2.000e-04 

2023-03-02 01:46:20,145 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.2697 (0.3168) Acc D Real: 90.376% 
Loss D Fake: 0.4772 (0.7469) Acc D Fake: 54.430% 
Loss D: 0.747 
Loss G: 0.9880 (0.7110) Acc G: 44.747% 
LR: 2.000e-04 

2023-03-02 01:46:20,152 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3172 (0.3168) Acc D Real: 90.278% 
Loss D Fake: 0.4728 (0.7435) Acc D Fake: 55.000% 
Loss D: 0.790 
Loss G: 0.9942 (0.7145) Acc G: 44.188% 
LR: 2.000e-04 

2023-03-02 01:46:20,159 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3770 (0.3176) Acc D Real: 90.101% 
Loss D Fake: 0.4701 (0.7401) Acc D Fake: 55.556% 
Loss D: 0.847 
Loss G: 0.9977 (0.7180) Acc G: 43.642% 
LR: 2.000e-04 

2023-03-02 01:46:20,168 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3273 (0.3177) Acc D Real: 89.978% 
Loss D Fake: 0.4685 (0.7368) Acc D Fake: 56.098% 
Loss D: 0.796 
Loss G: 1.0002 (0.7215) Acc G: 43.110% 
LR: 2.000e-04 

2023-03-02 01:46:20,176 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.2195 (0.3165) Acc D Real: 90.009% 
Loss D Fake: 0.4668 (0.7335) Acc D Fake: 56.627% 
Loss D: 0.686 
Loss G: 1.0041 (0.7249) Acc G: 42.590% 
LR: 2.000e-04 

2023-03-02 01:46:20,184 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.2871 (0.3161) Acc D Real: 89.957% 
Loss D Fake: 0.4642 (0.7303) Acc D Fake: 57.143% 
Loss D: 0.751 
Loss G: 1.0087 (0.7282) Acc G: 42.083% 
LR: 2.000e-04 

2023-03-02 01:46:20,193 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3008 (0.3160) Acc D Real: 89.878% 
Loss D Fake: 0.4614 (0.7272) Acc D Fake: 57.647% 
Loss D: 0.762 
Loss G: 1.0137 (0.7316) Acc G: 41.588% 
LR: 2.000e-04 

2023-03-02 01:46:20,201 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.2556 (0.3153) Acc D Real: 89.890% 
Loss D Fake: 0.4583 (0.7240) Acc D Fake: 58.140% 
Loss D: 0.714 
Loss G: 1.0194 (0.7349) Acc G: 41.105% 
LR: 2.000e-04 

2023-03-02 01:46:20,209 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.2775 (0.3148) Acc D Real: 89.843% 
Loss D Fake: 0.4555 (0.7210) Acc D Fake: 58.621% 
Loss D: 0.733 
Loss G: 1.0215 (0.7382) Acc G: 40.632% 
LR: 2.000e-04 

2023-03-02 01:46:20,217 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.2804 (0.3144) Acc D Real: 89.777% 
Loss D Fake: 0.4566 (0.7180) Acc D Fake: 59.091% 
Loss D: 0.737 
Loss G: 1.0154 (0.7414) Acc G: 40.170% 
LR: 2.000e-04 

2023-03-02 01:46:20,225 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.2663 (0.3139) Acc D Real: 89.786% 
Loss D Fake: 0.4620 (0.7151) Acc D Fake: 59.551% 
Loss D: 0.728 
Loss G: 1.0289 (0.7446) Acc G: 39.719% 
LR: 2.000e-04 

2023-03-02 01:46:20,233 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4061 (0.3149) Acc D Real: 89.578% 
Loss D Fake: 0.4483 (0.7121) Acc D Fake: 60.000% 
Loss D: 0.854 
Loss G: 1.0368 (0.7479) Acc G: 39.278% 
LR: 2.000e-04 

2023-03-02 01:46:20,242 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3645 (0.3155) Acc D Real: 89.428% 
Loss D Fake: 0.4466 (0.7092) Acc D Fake: 60.440% 
Loss D: 0.811 
Loss G: 1.0384 (0.7511) Acc G: 38.846% 
LR: 2.000e-04 

2023-03-02 01:46:20,250 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.2882 (0.3152) Acc D Real: 89.383% 
Loss D Fake: 0.4462 (0.7063) Acc D Fake: 60.870% 
Loss D: 0.734 
Loss G: 1.0395 (0.7542) Acc G: 38.424% 
LR: 2.000e-04 

2023-03-02 01:46:20,257 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.2689 (0.3147) Acc D Real: 89.368% 
Loss D Fake: 0.4456 (0.7035) Acc D Fake: 61.290% 
Loss D: 0.715 
Loss G: 1.0405 (0.7573) Acc G: 38.011% 
LR: 2.000e-04 

2023-03-02 01:46:20,264 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3778 (0.3153) Acc D Real: 89.204% 
Loss D Fake: 0.4458 (0.7008) Acc D Fake: 61.702% 
Loss D: 0.824 
Loss G: 1.0381 (0.7603) Acc G: 37.606% 
LR: 2.000e-04 

2023-03-02 01:46:20,272 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.2913 (0.3151) Acc D Real: 89.184% 
Loss D Fake: 0.4490 (0.6981) Acc D Fake: 62.105% 
Loss D: 0.740 
Loss G: 1.0316 (0.7631) Acc G: 37.211% 
LR: 2.000e-04 

2023-03-02 01:46:20,280 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2850 (0.3148) Acc D Real: 89.143% 
Loss D Fake: 0.4526 (0.6956) Acc D Fake: 62.500% 
Loss D: 0.738 
Loss G: 1.0382 (0.7660) Acc G: 36.823% 
LR: 2.000e-04 

2023-03-02 01:46:20,288 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4003 (0.3157) Acc D Real: 88.933% 
Loss D Fake: 0.4443 (0.6930) Acc D Fake: 62.887% 
Loss D: 0.845 
Loss G: 1.0458 (0.7689) Acc G: 36.443% 
LR: 2.000e-04 

2023-03-02 01:46:20,295 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.3508 (0.3160) Acc D Real: 88.789% 
Loss D Fake: 0.4417 (0.6904) Acc D Fake: 63.265% 
Loss D: 0.792 
Loss G: 1.0493 (0.7717) Acc G: 36.071% 
LR: 2.000e-04 

2023-03-02 01:46:20,302 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2935 (0.3158) Acc D Real: 88.732% 
Loss D Fake: 0.4400 (0.6879) Acc D Fake: 63.636% 
Loss D: 0.733 
Loss G: 1.0519 (0.7746) Acc G: 35.707% 
LR: 2.000e-04 

2023-03-02 01:46:20,311 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3383 (0.3160) Acc D Real: 88.616% 
Loss D Fake: 0.4389 (0.6854) Acc D Fake: 64.000% 
Loss D: 0.777 
Loss G: 1.0527 (0.7773) Acc G: 35.350% 
LR: 2.000e-04 

2023-03-02 01:46:20,318 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3226 (0.3161) Acc D Real: 88.526% 
Loss D Fake: 0.4393 (0.6830) Acc D Fake: 64.356% 
Loss D: 0.762 
Loss G: 1.0479 (0.7800) Acc G: 35.000% 
LR: 2.000e-04 

2023-03-02 01:46:20,326 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.2774 (0.3157) Acc D Real: 88.496% 
Loss D Fake: 0.4498 (0.6807) Acc D Fake: 64.706% 
Loss D: 0.727 
Loss G: 1.0445 (0.7826) Acc G: 34.657% 
LR: 2.000e-04 

2023-03-02 01:46:20,333 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.2643 (0.3152) Acc D Real: 88.474% 
Loss D Fake: 0.4396 (0.6783) Acc D Fake: 65.049% 
Loss D: 0.704 
Loss G: 1.0559 (0.7853) Acc G: 34.320% 
LR: 2.000e-04 

2023-03-02 01:46:20,341 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2992 (0.3150) Acc D Real: 88.414% 
Loss D Fake: 0.4353 (0.6760) Acc D Fake: 65.385% 
Loss D: 0.734 
Loss G: 1.0616 (0.7879) Acc G: 33.990% 
LR: 2.000e-04 

2023-03-02 01:46:20,348 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3443 (0.3153) Acc D Real: 88.311% 
Loss D Fake: 0.4334 (0.6737) Acc D Fake: 65.714% 
Loss D: 0.778 
Loss G: 1.0628 (0.7905) Acc G: 33.667% 
LR: 2.000e-04 

2023-03-02 01:46:20,357 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.2482 (0.3147) Acc D Real: 88.316% 
Loss D Fake: 0.4333 (0.6714) Acc D Fake: 66.038% 
Loss D: 0.682 
Loss G: 1.0633 (0.7931) Acc G: 33.349% 
LR: 2.000e-04 

2023-03-02 01:46:20,364 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2233 (0.3138) Acc D Real: 88.346% 
Loss D Fake: 0.4329 (0.6692) Acc D Fake: 66.355% 
Loss D: 0.656 
Loss G: 1.0650 (0.7957) Acc G: 33.037% 
LR: 2.000e-04 

2023-03-02 01:46:20,371 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.2922 (0.3136) Acc D Real: 88.295% 
Loss D Fake: 0.4322 (0.6670) Acc D Fake: 66.667% 
Loss D: 0.724 
Loss G: 1.0651 (0.7981) Acc G: 32.731% 
LR: 2.000e-04 

2023-03-02 01:46:20,379 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3024 (0.3135) Acc D Real: 88.259% 
Loss D Fake: 0.4329 (0.6649) Acc D Fake: 66.972% 
Loss D: 0.735 
Loss G: 1.0649 (0.8006) Acc G: 32.431% 
LR: 2.000e-04 

2023-03-02 01:46:20,386 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3093 (0.3135) Acc D Real: 88.185% 
Loss D Fake: 0.4372 (0.6628) Acc D Fake: 67.273% 
Loss D: 0.747 
Loss G: 1.0570 (0.8029) Acc G: 32.136% 
LR: 2.000e-04 

2023-03-02 01:46:20,393 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3164 (0.3135) Acc D Real: 88.109% 
Loss D Fake: 0.4349 (0.6607) Acc D Fake: 67.568% 
Loss D: 0.751 
Loss G: 1.0743 (0.8054) Acc G: 31.847% 
LR: 2.000e-04 

2023-03-02 01:46:20,400 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.2964 (0.3134) Acc D Real: 88.039% 
Loss D Fake: 0.4245 (0.6586) Acc D Fake: 67.857% 
Loss D: 0.721 
Loss G: 1.0832 (0.8078) Acc G: 31.562% 
LR: 2.000e-04 

2023-03-02 01:46:20,408 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.2812 (0.3131) Acc D Real: 88.018% 
Loss D Fake: 0.4214 (0.6565) Acc D Fake: 68.142% 
Loss D: 0.703 
Loss G: 1.0877 (0.8103) Acc G: 31.283% 
LR: 2.000e-04 

2023-03-02 01:46:20,415 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3054 (0.3130) Acc D Real: 87.949% 
Loss D Fake: 0.4194 (0.6544) Acc D Fake: 68.421% 
Loss D: 0.725 
Loss G: 1.0912 (0.8128) Acc G: 31.009% 
LR: 2.000e-04 

2023-03-02 01:46:20,422 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.2596 (0.3125) Acc D Real: 87.934% 
Loss D Fake: 0.4177 (0.6524) Acc D Fake: 68.696% 
Loss D: 0.677 
Loss G: 1.0943 (0.8152) Acc G: 30.739% 
LR: 2.000e-04 

2023-03-02 01:46:20,429 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.3188 (0.3126) Acc D Real: 87.865% 
Loss D Fake: 0.4162 (0.6503) Acc D Fake: 68.966% 
Loss D: 0.735 
Loss G: 1.0969 (0.8177) Acc G: 30.474% 
LR: 2.000e-04 

2023-03-02 01:46:20,436 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.3881 (0.3132) Acc D Real: 87.745% 
Loss D Fake: 0.4151 (0.6483) Acc D Fake: 69.231% 
Loss D: 0.803 
Loss G: 1.0984 (0.8201) Acc G: 30.214% 
LR: 2.000e-04 

2023-03-02 01:46:20,444 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3401 (0.3135) Acc D Real: 87.653% 
Loss D Fake: 0.4145 (0.6464) Acc D Fake: 69.492% 
Loss D: 0.755 
Loss G: 1.0994 (0.8224) Acc G: 29.958% 
LR: 2.000e-04 

2023-03-02 01:46:20,451 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.3541 (0.3138) Acc D Real: 87.561% 
Loss D Fake: 0.4141 (0.6444) Acc D Fake: 69.748% 
Loss D: 0.768 
Loss G: 1.0998 (0.8248) Acc G: 29.706% 
LR: 2.000e-04 

2023-03-02 01:46:20,459 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2808 (0.3135) Acc D Real: 87.528% 
Loss D Fake: 0.4140 (0.6425) Acc D Fake: 70.000% 
Loss D: 0.695 
Loss G: 1.0999 (0.8271) Acc G: 29.458% 
LR: 2.000e-04 

2023-03-02 01:46:20,466 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2783 (0.3133) Acc D Real: 87.497% 
Loss D Fake: 0.4139 (0.6406) Acc D Fake: 70.248% 
Loss D: 0.692 
Loss G: 1.1002 (0.8293) Acc G: 29.215% 
LR: 2.000e-04 

2023-03-02 01:46:20,473 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.2201 (0.3125) Acc D Real: 87.529% 
Loss D Fake: 0.4137 (0.6387) Acc D Fake: 70.492% 
Loss D: 0.634 
Loss G: 1.1006 (0.8315) Acc G: 28.975% 
LR: 2.000e-04 

2023-03-02 01:46:20,480 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.3090 (0.3125) Acc D Real: 87.487% 
Loss D Fake: 0.4138 (0.6369) Acc D Fake: 70.732% 
Loss D: 0.723 
Loss G: 1.0989 (0.8337) Acc G: 28.740% 
LR: 2.000e-04 

2023-03-02 01:46:20,487 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.2356 (0.3118) Acc D Real: 87.498% 
Loss D Fake: 0.4160 (0.6351) Acc D Fake: 70.968% 
Loss D: 0.652 
Loss G: 1.0920 (0.8358) Acc G: 28.508% 
LR: 2.000e-04 

2023-03-02 01:46:20,494 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.2796 (0.3116) Acc D Real: 87.463% 
Loss D Fake: 0.4206 (0.6334) Acc D Fake: 71.200% 
Loss D: 0.700 
Loss G: 1.1025 (0.8379) Acc G: 28.280% 
LR: 2.000e-04 

2023-03-02 01:46:20,501 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.3426 (0.3118) Acc D Real: 87.362% 
Loss D Fake: 0.4103 (0.6316) Acc D Fake: 71.429% 
Loss D: 0.753 
Loss G: 1.1104 (0.8401) Acc G: 28.056% 
LR: 2.000e-04 

2023-03-02 01:46:20,508 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3613 (0.3122) Acc D Real: 87.260% 
Loss D Fake: 0.4085 (0.6299) Acc D Fake: 71.654% 
Loss D: 0.770 
Loss G: 1.1113 (0.8422) Acc G: 27.835% 
LR: 2.000e-04 

2023-03-02 01:46:20,516 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.2934 (0.3121) Acc D Real: 87.222% 
Loss D Fake: 0.4088 (0.6282) Acc D Fake: 71.875% 
Loss D: 0.702 
Loss G: 1.1111 (0.8443) Acc G: 27.617% 
LR: 2.000e-04 

2023-03-02 01:46:20,523 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.2625 (0.3117) Acc D Real: 87.196% 
Loss D Fake: 0.4089 (0.6265) Acc D Fake: 72.093% 
Loss D: 0.671 
Loss G: 1.1119 (0.8464) Acc G: 27.403% 
LR: 2.000e-04 

2023-03-02 01:46:20,531 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.2011 (0.3108) Acc D Real: 87.231% 
Loss D Fake: 0.4083 (0.6248) Acc D Fake: 72.308% 
Loss D: 0.609 
Loss G: 1.1141 (0.8485) Acc G: 27.192% 
LR: 2.000e-04 

2023-03-02 01:46:20,539 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3223 (0.3109) Acc D Real: 87.151% 
Loss D Fake: 0.4072 (0.6231) Acc D Fake: 72.519% 
Loss D: 0.729 
Loss G: 1.1163 (0.8505) Acc G: 26.985% 
LR: 2.000e-04 

2023-03-02 01:46:20,546 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.1734 (0.3099) Acc D Real: 87.213% 
Loss D Fake: 0.4060 (0.6215) Acc D Fake: 72.727% 
Loss D: 0.579 
Loss G: 1.1198 (0.8525) Acc G: 26.780% 
LR: 2.000e-04 

2023-03-02 01:46:20,554 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3563 (0.3102) Acc D Real: 87.119% 
Loss D Fake: 0.4049 (0.6198) Acc D Fake: 72.932% 
Loss D: 0.761 
Loss G: 1.1189 (0.8545) Acc G: 26.579% 
LR: 2.000e-04 

2023-03-02 01:46:20,561 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3176 (0.3103) Acc D Real: 87.063% 
Loss D Fake: 0.4127 (0.6183) Acc D Fake: 73.134% 
Loss D: 0.730 
Loss G: 0.9736 (0.8554) Acc G: 26.381% 
LR: 2.000e-04 

2023-03-02 01:46:20,568 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.2118 (0.3096) Acc D Real: 87.084% 
Loss D Fake: 0.4955 (0.6174) Acc D Fake: 73.333% 
Loss D: 0.707 
Loss G: 0.9399 (0.8561) Acc G: 26.185% 
LR: 2.000e-04 

2023-03-02 01:46:20,576 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3424 (0.3098) Acc D Real: 86.992% 
Loss D Fake: 0.5008 (0.6165) Acc D Fake: 73.529% 
Loss D: 0.843 
Loss G: 0.9356 (0.8566) Acc G: 25.993% 
LR: 2.000e-04 

2023-03-02 01:46:20,583 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.3216 (0.3099) Acc D Real: 86.919% 
Loss D Fake: 0.5012 (0.6157) Acc D Fake: 73.723% 
Loss D: 0.823 
Loss G: 0.9365 (0.8572) Acc G: 25.803% 
LR: 2.000e-04 

2023-03-02 01:46:20,591 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1939 (0.3090) Acc D Real: 86.958% 
Loss D Fake: 0.4994 (0.6148) Acc D Fake: 73.913% 
Loss D: 0.693 
Loss G: 0.9419 (0.8578) Acc G: 25.616% 
LR: 2.000e-04 

2023-03-02 01:46:20,598 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.2820 (0.3088) Acc D Real: 86.928% 
Loss D Fake: 0.4965 (0.6140) Acc D Fake: 74.101% 
Loss D: 0.778 
Loss G: 0.9429 (0.8585) Acc G: 25.432% 
LR: 2.000e-04 

2023-03-02 01:46:20,605 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2754 (0.3086) Acc D Real: 86.899% 
Loss D Fake: 0.4959 (0.6132) Acc D Fake: 74.286% 
Loss D: 0.771 
Loss G: 0.9452 (0.8591) Acc G: 25.250% 
LR: 2.000e-04 

2023-03-02 01:46:20,612 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3620 (0.3090) Acc D Real: 86.790% 
Loss D Fake: 0.4940 (0.6123) Acc D Fake: 74.468% 
Loss D: 0.856 
Loss G: 0.9479 (0.8597) Acc G: 25.071% 
LR: 2.000e-04 

2023-03-02 01:46:20,620 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.2212 (0.3084) Acc D Real: 86.808% 
Loss D Fake: 0.4919 (0.6115) Acc D Fake: 74.648% 
Loss D: 0.713 
Loss G: 0.9525 (0.8604) Acc G: 24.894% 
LR: 2.000e-04 

2023-03-02 01:46:20,627 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2811 (0.3082) Acc D Real: 86.781% 
Loss D Fake: 0.4889 (0.6106) Acc D Fake: 74.825% 
Loss D: 0.770 
Loss G: 0.9569 (0.8610) Acc G: 24.720% 
LR: 2.000e-04 

2023-03-02 01:46:20,636 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.2295 (0.3076) Acc D Real: 86.795% 
Loss D Fake: 0.4864 (0.6097) Acc D Fake: 75.000% 
Loss D: 0.716 
Loss G: 0.9622 (0.8617) Acc G: 24.549% 
LR: 2.000e-04 

2023-03-02 01:46:20,643 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2510 (0.3072) Acc D Real: 86.794% 
Loss D Fake: 0.4830 (0.6089) Acc D Fake: 75.172% 
Loss D: 0.734 
Loss G: 0.9696 (0.8625) Acc G: 24.379% 
LR: 2.000e-04 

2023-03-02 01:46:20,653 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.2719 (0.3070) Acc D Real: 86.769% 
Loss D Fake: 0.4782 (0.6080) Acc D Fake: 75.342% 
Loss D: 0.750 
Loss G: 0.9821 (0.8633) Acc G: 24.212% 
LR: 2.000e-04 

2023-03-02 01:46:20,661 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3481 (0.3073) Acc D Real: 86.685% 
Loss D Fake: 0.4696 (0.6070) Acc D Fake: 75.510% 
Loss D: 0.818 
Loss G: 1.0111 (0.8643) Acc G: 24.048% 
LR: 2.000e-04 

2023-03-02 01:46:20,668 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3269 (0.3074) Acc D Real: 86.621% 
Loss D Fake: 0.4412 (0.6059) Acc D Fake: 75.676% 
Loss D: 0.768 
Loss G: 1.1742 (0.8664) Acc G: 23.885% 
LR: 2.000e-04 

2023-03-02 01:46:20,676 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3280 (0.3075) Acc D Real: 86.572% 
Loss D Fake: 0.3798 (0.6044) Acc D Fake: 75.839% 
Loss D: 0.708 
Loss G: 1.1832 (0.8685) Acc G: 23.725% 
LR: 2.000e-04 

2023-03-02 01:46:20,683 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.2569 (0.3072) Acc D Real: 86.567% 
Loss D Fake: 0.3774 (0.6029) Acc D Fake: 76.000% 
Loss D: 0.634 
Loss G: 1.1874 (0.8706) Acc G: 23.567% 
LR: 2.000e-04 

2023-03-02 01:46:20,690 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3456 (0.3075) Acc D Real: 86.522% 
Loss D Fake: 0.3757 (0.6014) Acc D Fake: 76.159% 
Loss D: 0.721 
Loss G: 1.1902 (0.8728) Acc G: 23.411% 
LR: 2.000e-04 

2023-03-02 01:46:20,697 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2548 (0.3071) Acc D Real: 86.521% 
Loss D Fake: 0.3744 (0.5999) Acc D Fake: 76.316% 
Loss D: 0.629 
Loss G: 1.1928 (0.8749) Acc G: 23.257% 
LR: 2.000e-04 

2023-03-02 01:46:20,706 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2325 (0.3066) Acc D Real: 86.536% 
Loss D Fake: 0.3732 (0.5984) Acc D Fake: 76.471% 
Loss D: 0.606 
Loss G: 1.1947 (0.8770) Acc G: 23.105% 
LR: 2.000e-04 

2023-03-02 01:46:20,713 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3328 (0.3068) Acc D Real: 86.492% 
Loss D Fake: 0.3729 (0.5969) Acc D Fake: 76.623% 
Loss D: 0.706 
Loss G: 1.1929 (0.8790) Acc G: 22.955% 
LR: 2.000e-04 

2023-03-02 01:46:20,720 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3261 (0.3069) Acc D Real: 86.451% 
Loss D Fake: 0.3749 (0.5955) Acc D Fake: 76.774% 
Loss D: 0.701 
Loss G: 1.1866 (0.8810) Acc G: 22.806% 
LR: 2.000e-04 

2023-03-02 01:46:20,727 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3245 (0.3070) Acc D Real: 86.412% 
Loss D Fake: 0.3778 (0.5941) Acc D Fake: 76.923% 
Loss D: 0.702 
Loss G: 1.1790 (0.8829) Acc G: 22.660% 
LR: 2.000e-04 

2023-03-02 01:46:20,734 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3012 (0.3070) Acc D Real: 86.389% 
Loss D Fake: 0.3794 (0.5927) Acc D Fake: 77.070% 
Loss D: 0.681 
Loss G: 1.1790 (0.8848) Acc G: 22.516% 
LR: 2.000e-04 

2023-03-02 01:46:20,742 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.4027 (0.3076) Acc D Real: 86.318% 
Loss D Fake: 0.3784 (0.5914) Acc D Fake: 77.215% 
Loss D: 0.781 
Loss G: 1.1797 (0.8867) Acc G: 22.373% 
LR: 2.000e-04 

2023-03-02 01:46:20,749 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2968 (0.3075) Acc D Real: 86.296% 
Loss D Fake: 0.3775 (0.5900) Acc D Fake: 77.358% 
Loss D: 0.674 
Loss G: 1.1852 (0.8885) Acc G: 22.233% 
LR: 2.000e-04 

2023-03-02 01:46:20,756 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3550 (0.3078) Acc D Real: 86.236% 
Loss D Fake: 0.3748 (0.5887) Acc D Fake: 77.500% 
Loss D: 0.730 
Loss G: 1.1904 (0.8904) Acc G: 22.094% 
LR: 2.000e-04 

2023-03-02 01:46:20,764 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2011 (0.3072) Acc D Real: 86.274% 
Loss D Fake: 0.3715 (0.5873) Acc D Fake: 77.640% 
Loss D: 0.573 
Loss G: 1.2017 (0.8924) Acc G: 21.957% 
LR: 2.000e-04 

2023-03-02 01:46:20,771 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.3738 (0.3076) Acc D Real: 86.216% 
Loss D Fake: 0.3675 (0.5860) Acc D Fake: 77.778% 
Loss D: 0.741 
Loss G: 1.2058 (0.8943) Acc G: 21.821% 
LR: 2.000e-04 

2023-03-02 01:46:20,778 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.2585 (0.3073) Acc D Real: 86.215% 
Loss D Fake: 0.3669 (0.5846) Acc D Fake: 77.914% 
Loss D: 0.625 
Loss G: 1.2070 (0.8962) Acc G: 21.687% 
LR: 2.000e-04 

2023-03-02 01:46:20,785 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2867 (0.3072) Acc D Real: 86.200% 
Loss D Fake: 0.3666 (0.5833) Acc D Fake: 78.049% 
Loss D: 0.653 
Loss G: 1.2077 (0.8981) Acc G: 21.555% 
LR: 2.000e-04 

2023-03-02 01:46:20,792 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1597 (0.3063) Acc D Real: 86.253% 
Loss D Fake: 0.3662 (0.5820) Acc D Fake: 78.182% 
Loss D: 0.526 
Loss G: 1.2097 (0.9000) Acc G: 21.424% 
LR: 2.000e-04 

2023-03-02 01:46:20,800 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.2376 (0.3059) Acc D Real: 86.258% 
Loss D Fake: 0.3652 (0.5807) Acc D Fake: 78.313% 
Loss D: 0.603 
Loss G: 1.2125 (0.9019) Acc G: 21.295% 
LR: 2.000e-04 

2023-03-02 01:46:20,807 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2521 (0.3055) Acc D Real: 86.260% 
Loss D Fake: 0.3640 (0.5794) Acc D Fake: 78.443% 
Loss D: 0.616 
Loss G: 1.2155 (0.9038) Acc G: 21.168% 
LR: 2.000e-04 

2023-03-02 01:46:20,814 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.2729 (0.3053) Acc D Real: 86.259% 
Loss D Fake: 0.3629 (0.5781) Acc D Fake: 78.571% 
Loss D: 0.636 
Loss G: 1.2180 (0.9056) Acc G: 21.042% 
LR: 2.000e-04 

2023-03-02 01:46:20,821 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.2436 (0.3050) Acc D Real: 86.272% 
Loss D Fake: 0.3620 (0.5768) Acc D Fake: 78.698% 
Loss D: 0.606 
Loss G: 1.2202 (0.9075) Acc G: 20.917% 
LR: 2.000e-04 

2023-03-02 01:46:20,829 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.2519 (0.3047) Acc D Real: 86.272% 
Loss D Fake: 0.3613 (0.5756) Acc D Fake: 78.824% 
Loss D: 0.613 
Loss G: 1.2214 (0.9093) Acc G: 20.794% 
LR: 2.000e-04 

2023-03-02 01:46:20,838 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3117 (0.3047) Acc D Real: 86.238% 
Loss D Fake: 0.3614 (0.5743) Acc D Fake: 78.947% 
Loss D: 0.673 
Loss G: 1.2200 (0.9112) Acc G: 20.673% 
LR: 2.000e-04 

2023-03-02 01:46:20,846 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3215 (0.3048) Acc D Real: 86.204% 
Loss D Fake: 0.3653 (0.5731) Acc D Fake: 79.070% 
Loss D: 0.687 
Loss G: 1.1989 (0.9128) Acc G: 20.552% 
LR: 2.000e-04 

2023-03-02 01:46:20,854 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3756 (0.3052) Acc D Real: 86.120% 
Loss D Fake: 0.3971 (0.5721) Acc D Fake: 79.171% 
Loss D: 0.773 
Loss G: 1.0914 (0.9139) Acc G: 20.472% 
LR: 2.000e-04 

2023-03-02 01:46:20,862 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.1881 (0.3045) Acc D Real: 86.150% 
Loss D Fake: 1.3910 (0.5768) Acc D Fake: 78.793% 
Loss D: 1.579 
Loss G: 0.3440 (0.9106) Acc G: 20.852% 
LR: 2.000e-04 

2023-03-02 01:46:20,870 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.2952 (0.3045) Acc D Real: 86.118% 
Loss D Fake: 1.3059 (0.5809) Acc D Fake: 78.419% 
Loss D: 1.601 
Loss G: 0.8090 (0.9100) Acc G: 20.924% 
LR: 2.000e-04 

2023-03-02 01:46:20,878 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3116 (0.3045) Acc D Real: 86.072% 
Loss D Fake: 0.5397 (0.5807) Acc D Fake: 78.447% 
Loss D: 0.851 
Loss G: 1.2174 (0.9118) Acc G: 20.805% 
LR: 2.000e-04 

2023-03-02 01:46:20,887 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2913 (0.3044) Acc D Real: 86.050% 
Loss D Fake: 0.3573 (0.5794) Acc D Fake: 78.569% 
Loss D: 0.649 
Loss G: 1.2452 (0.9136) Acc G: 20.687% 
LR: 2.000e-04 

2023-03-02 01:46:20,894 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2120 (0.3039) Acc D Real: 86.068% 
Loss D Fake: 0.3482 (0.5782) Acc D Fake: 78.689% 
Loss D: 0.560 
Loss G: 1.2570 (0.9156) Acc G: 20.571% 
LR: 2.000e-04 

2023-03-02 01:46:20,901 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.2332 (0.3035) Acc D Real: 86.082% 
Loss D Fake: 0.3437 (0.5768) Acc D Fake: 78.808% 
Loss D: 0.577 
Loss G: 1.2641 (0.9175) Acc G: 20.456% 
LR: 2.000e-04 

2023-03-02 01:46:20,908 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.2332 (0.3031) Acc D Real: 86.099% 
Loss D Fake: 0.3412 (0.5755) Acc D Fake: 78.926% 
Loss D: 0.574 
Loss G: 1.2668 (0.9195) Acc G: 20.343% 
LR: 2.000e-04 

2023-03-02 01:46:20,916 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3450 (0.3034) Acc D Real: 86.064% 
Loss D Fake: 0.3409 (0.5742) Acc D Fake: 79.042% 
Loss D: 0.686 
Loss G: 1.2643 (0.9214) Acc G: 20.230% 
LR: 2.000e-04 

2023-03-02 01:46:20,923 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3126 (0.3034) Acc D Real: 86.030% 
Loss D Fake: 0.3428 (0.5730) Acc D Fake: 79.158% 
Loss D: 0.655 
Loss G: 1.2597 (0.9232) Acc G: 20.119% 
LR: 2.000e-04 

2023-03-02 01:46:20,930 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2950 (0.3034) Acc D Real: 86.025% 
Loss D Fake: 0.3428 (0.5717) Acc D Fake: 79.271% 
Loss D: 0.638 
Loss G: 1.2675 (0.9251) Acc G: 20.009% 
LR: 2.000e-04 

2023-03-02 01:46:20,938 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3415 (0.3036) Acc D Real: 85.980% 
Loss D Fake: 0.3391 (0.5704) Acc D Fake: 79.384% 
Loss D: 0.681 
Loss G: 1.2706 (0.9270) Acc G: 19.900% 
LR: 2.000e-04 

2023-03-02 01:46:20,945 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3589 (0.3039) Acc D Real: 85.936% 
Loss D Fake: 0.3417 (0.5692) Acc D Fake: 79.495% 
Loss D: 0.701 
Loss G: 1.1941 (0.9284) Acc G: 19.793% 
LR: 2.000e-04 

2023-03-02 01:46:20,952 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1905 (0.3033) Acc D Real: 85.960% 
Loss D Fake: 0.3997 (0.5683) Acc D Fake: 79.606% 
Loss D: 0.590 
Loss G: 1.1539 (0.9296) Acc G: 19.686% 
LR: 2.000e-04 

2023-03-02 01:46:20,960 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3203 (0.3034) Acc D Real: 85.913% 
Loss D Fake: 0.3810 (0.5673) Acc D Fake: 79.715% 
Loss D: 0.701 
Loss G: 1.2705 (0.9315) Acc G: 19.581% 
LR: 2.000e-04 

2023-03-02 01:46:20,968 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.2596 (0.3031) Acc D Real: 85.910% 
Loss D Fake: 0.3364 (0.5661) Acc D Fake: 79.823% 
Loss D: 0.596 
Loss G: 1.2811 (0.9333) Acc G: 19.477% 
LR: 2.000e-04 

2023-03-02 01:46:20,976 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.2226 (0.3027) Acc D Real: 85.928% 
Loss D Fake: 0.3346 (0.5648) Acc D Fake: 79.929% 
Loss D: 0.557 
Loss G: 1.2830 (0.9352) Acc G: 19.374% 
LR: 2.000e-04 

2023-03-02 01:46:20,983 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.2514 (0.3024) Acc D Real: 85.929% 
Loss D Fake: 0.3379 (0.5636) Acc D Fake: 80.035% 
Loss D: 0.589 
Loss G: 1.2472 (0.9368) Acc G: 19.272% 
LR: 2.000e-04 

2023-03-02 01:46:20,990 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.1790 (0.3018) Acc D Real: 85.966% 
Loss D Fake: 0.3818 (0.5627) Acc D Fake: 80.140% 
Loss D: 0.561 
Loss G: 1.2555 (0.9385) Acc G: 19.171% 
LR: 2.000e-04 

2023-03-02 01:46:20,997 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.2284 (0.3014) Acc D Real: 85.985% 
Loss D Fake: 0.3349 (0.5615) Acc D Fake: 80.243% 
Loss D: 0.563 
Loss G: 1.2930 (0.9403) Acc G: 19.071% 
LR: 2.000e-04 

2023-03-02 01:46:21,004 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3930 (0.3019) Acc D Real: 85.933% 
Loss D Fake: 0.3311 (0.5603) Acc D Fake: 80.345% 
Loss D: 0.724 
Loss G: 1.2871 (0.9421) Acc G: 18.972% 
LR: 2.000e-04 

2023-03-02 01:46:21,012 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2506 (0.3016) Acc D Real: 85.935% 
Loss D Fake: 0.3435 (0.5592) Acc D Fake: 80.447% 
Loss D: 0.594 
Loss G: 1.2496 (0.9437) Acc G: 18.875% 
LR: 2.000e-04 

2023-03-02 01:46:21,019 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.2960 (0.3016) Acc D Real: 85.919% 
Loss D Fake: 0.3688 (0.5582) Acc D Fake: 80.547% 
Loss D: 0.665 
Loss G: 1.2697 (0.9454) Acc G: 18.778% 
LR: 2.000e-04 

2023-03-02 01:46:21,026 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.2356 (0.3013) Acc D Real: 85.929% 
Loss D Fake: 0.3337 (0.5571) Acc D Fake: 80.646% 
Loss D: 0.569 
Loss G: 1.2920 (0.9471) Acc G: 18.682% 
LR: 2.000e-04 

2023-03-02 01:46:21,034 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3023 (0.3013) Acc D Real: 85.912% 
Loss D Fake: 0.3326 (0.5559) Acc D Fake: 80.745% 
Loss D: 0.635 
Loss G: 1.2862 (0.9489) Acc G: 18.587% 
LR: 2.000e-04 

2023-03-02 01:46:21,041 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2969 (0.3012) Acc D Real: 85.903% 
Loss D Fake: 0.3386 (0.5548) Acc D Fake: 80.842% 
Loss D: 0.636 
Loss G: 1.2685 (0.9505) Acc G: 18.493% 
LR: 2.000e-04 

2023-03-02 01:46:21,049 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2544 (0.3010) Acc D Real: 85.910% 
Loss D Fake: 0.3526 (0.5538) Acc D Fake: 80.938% 
Loss D: 0.607 
Loss G: 1.2186 (0.9518) Acc G: 18.400% 
LR: 2.000e-04 

2023-03-02 01:46:21,056 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.2011 (0.3005) Acc D Real: 85.942% 
Loss D Fake: 0.4116 (0.5531) Acc D Fake: 80.992% 
Loss D: 0.613 
Loss G: 0.3943 (0.9490) Acc G: 18.742% 
LR: 2.000e-04 

2023-03-02 01:46:21,063 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.1610 (0.2998) Acc D Real: 85.984% 
Loss D Fake: 1.2080 (0.5564) Acc D Fake: 80.655% 
Loss D: 1.369 
Loss G: 0.3856 (0.9462) Acc G: 19.080% 
LR: 2.000e-04 

2023-03-02 01:46:21,071 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1880 (0.2993) Acc D Real: 86.017% 
Loss D Fake: 1.2097 (0.5596) Acc D Fake: 80.322% 
Loss D: 1.398 
Loss G: 0.3861 (0.9435) Acc G: 19.414% 
LR: 2.000e-04 

2023-03-02 01:46:21,079 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2173 (0.2988) Acc D Real: 86.039% 
Loss D Fake: 1.2022 (0.5628) Acc D Fake: 79.992% 
Loss D: 1.420 
Loss G: 0.3899 (0.9407) Acc G: 19.745% 
LR: 2.000e-04 

2023-03-02 01:46:21,086 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.2483 (0.2986) Acc D Real: 86.048% 
Loss D Fake: 1.1894 (0.5658) Acc D Fake: 79.665% 
Loss D: 1.438 
Loss G: 0.3957 (0.9381) Acc G: 20.074% 
LR: 2.000e-04 

2023-03-02 01:46:21,093 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.1994 (0.2981) Acc D Real: 86.077% 
Loss D Fake: 1.1732 (0.5688) Acc D Fake: 79.341% 
Loss D: 1.373 
Loss G: 0.4032 (0.9355) Acc G: 20.398% 
LR: 2.000e-04 

2023-03-02 01:46:21,100 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.1801 (0.2975) Acc D Real: 86.104% 
Loss D Fake: 1.1534 (0.5716) Acc D Fake: 79.021% 
Loss D: 1.334 
Loss G: 0.4128 (0.9329) Acc G: 20.720% 
LR: 2.000e-04 

2023-03-02 01:46:21,108 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.2391 (0.2973) Acc D Real: 86.127% 
Loss D Fake: 1.1304 (0.5743) Acc D Fake: 78.704% 
Loss D: 1.370 
Loss G: 0.4242 (0.9305) Acc G: 21.039% 
LR: 2.000e-04 

2023-03-02 01:46:21,116 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2072 (0.2968) Acc D Real: 86.156% 
Loss D Fake: 1.1043 (0.5769) Acc D Fake: 78.389% 
Loss D: 1.312 
Loss G: 0.4402 (0.9281) Acc G: 21.354% 
LR: 2.000e-04 

2023-03-02 01:46:21,124 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1897 (0.2963) Acc D Real: 86.188% 
Loss D Fake: 1.0608 (0.5792) Acc D Fake: 78.086% 
Loss D: 1.251 
Loss G: 0.6231 (0.9266) Acc G: 21.475% 
LR: 2.000e-04 

2023-03-02 01:46:21,131 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.1859 (0.2958) Acc D Real: 86.224% 
Loss D Fake: 1.3738 (0.5830) Acc D Fake: 77.952% 
Loss D: 1.560 
Loss G: 0.6309 (0.9252) Acc G: 21.603% 
LR: 2.000e-04 

2023-03-02 01:46:21,138 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.1834 (0.2953) Acc D Real: 86.260% 
Loss D Fake: 1.2492 (0.5861) Acc D Fake: 77.844% 
Loss D: 1.433 
Loss G: 0.7996 (0.9246) Acc G: 21.666% 
LR: 2.000e-04 

2023-03-02 01:46:21,146 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.1843 (0.2947) Acc D Real: 86.289% 
Loss D Fake: 0.7036 (0.5867) Acc D Fake: 77.838% 
Loss D: 0.888 
Loss G: 1.3233 (0.9265) Acc G: 21.564% 
LR: 2.000e-04 

2023-03-02 01:46:21,153 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2055 (0.2943) Acc D Real: 86.313% 
Loss D Fake: 0.2995 (0.5853) Acc D Fake: 77.942% 
Loss D: 0.505 
Loss G: 1.3968 (0.9287) Acc G: 21.463% 
LR: 2.000e-04 

2023-03-02 01:46:21,160 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.2977 (0.2943) Acc D Real: 86.299% 
Loss D Fake: 0.3082 (0.5841) Acc D Fake: 78.045% 
Loss D: 0.606 
Loss G: 1.3517 (0.9307) Acc G: 21.362% 
LR: 2.000e-04 

2023-03-02 01:46:21,167 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.3553 (0.2946) Acc D Real: 86.289% 
Loss D Fake: 0.3134 (0.5828) Acc D Fake: 78.147% 
Loss D: 0.669 
Loss G: 1.3533 (0.9327) Acc G: 21.263% 
LR: 2.000e-04 

2023-03-02 01:46:21,175 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3293 (0.2948) Acc D Real: 86.283% 
Loss D Fake: 0.3139 (0.5815) Acc D Fake: 78.248% 
Loss D: 0.643 
Loss G: 1.3543 (0.9346) Acc G: 21.165% 
LR: 2.000e-04 

2023-03-02 01:46:21,182 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2554 (0.2946) Acc D Real: 86.296% 
Loss D Fake: 0.3160 (0.5803) Acc D Fake: 78.349% 
Loss D: 0.571 
Loss G: 1.3533 (0.9366) Acc G: 21.067% 
LR: 2.000e-04 

2023-03-02 01:46:21,190 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.3583 (0.2949) Acc D Real: 86.280% 
Loss D Fake: 0.3187 (0.5791) Acc D Fake: 78.448% 
Loss D: 0.677 
Loss G: 1.3558 (0.9385) Acc G: 20.970% 
LR: 2.000e-04 

2023-03-02 01:46:21,197 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.3120 (0.2950) Acc D Real: 86.278% 
Loss D Fake: 0.3154 (0.5779) Acc D Fake: 78.546% 
Loss D: 0.627 
Loss G: 1.3702 (0.9404) Acc G: 20.875% 
LR: 2.000e-04 

2023-03-02 01:46:21,205 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3883 (0.2954) Acc D Real: 86.264% 
Loss D Fake: 0.3083 (0.5767) Acc D Fake: 78.644% 
Loss D: 0.697 
Loss G: 1.3831 (0.9425) Acc G: 20.780% 
LR: 2.000e-04 

2023-03-02 01:46:21,212 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2443 (0.2952) Acc D Real: 86.284% 
Loss D Fake: 0.3035 (0.5755) Acc D Fake: 78.741% 
Loss D: 0.548 
Loss G: 1.3923 (0.9445) Acc G: 20.686% 
LR: 2.000e-04 

2023-03-02 01:46:21,220 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2453 (0.2949) Acc D Real: 86.306% 
Loss D Fake: 0.3002 (0.5742) Acc D Fake: 78.836% 
Loss D: 0.546 
Loss G: 1.3998 (0.9465) Acc G: 20.593% 
LR: 2.000e-04 

2023-03-02 01:46:21,227 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.3527 (0.2952) Acc D Real: 86.288% 
Loss D Fake: 0.2977 (0.5730) Acc D Fake: 78.931% 
Loss D: 0.650 
Loss G: 1.4046 (0.9486) Acc G: 20.500% 
LR: 2.000e-04 

2023-03-02 01:46:21,235 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.3908 (0.2956) Acc D Real: 86.255% 
Loss D Fake: 0.2964 (0.5717) Acc D Fake: 79.025% 
Loss D: 0.687 
Loss G: 1.4063 (0.9506) Acc G: 20.409% 
LR: 2.000e-04 

2023-03-02 01:46:21,242 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.2032 (0.2952) Acc D Real: 86.281% 
Loss D Fake: 0.2958 (0.5705) Acc D Fake: 79.119% 
Loss D: 0.499 
Loss G: 1.4080 (0.9527) Acc G: 20.318% 
LR: 2.000e-04 

2023-03-02 01:46:21,249 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2155 (0.2949) Acc D Real: 86.289% 
Loss D Fake: 0.2950 (0.5693) Acc D Fake: 79.142% 
Loss D: 0.511 
Loss G: 1.4102 (0.9547) Acc G: 20.296% 
LR: 2.000e-04 

2023-03-02 01:46:21,260 -                train: [    INFO] - 
Epoch: 18/20
2023-03-02 01:46:21,430 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2715 (0.2898) Acc D Real: 86.901% 
Loss D Fake: 0.2938 (0.2940) Acc D Fake: 100.000% 
Loss D: 0.565 
Loss G: 1.4126 (1.4121) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,437 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.3217 (0.3004) Acc D Real: 87.135% 
Loss D Fake: 0.2936 (0.2939) Acc D Fake: 100.000% 
Loss D: 0.615 
Loss G: 1.4125 (1.4122) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,444 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2486 (0.2875) Acc D Real: 87.591% 
Loss D Fake: 0.2937 (0.2938) Acc D Fake: 100.000% 
Loss D: 0.542 
Loss G: 1.4122 (1.4122) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,460 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2395 (0.2779) Acc D Real: 88.219% 
Loss D Fake: 0.2937 (0.2938) Acc D Fake: 100.000% 
Loss D: 0.533 
Loss G: 1.4124 (1.4123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,467 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.2644 (0.2756) Acc D Real: 88.177% 
Loss D Fake: 0.2936 (0.2938) Acc D Fake: 100.000% 
Loss D: 0.558 
Loss G: 1.4124 (1.4123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,474 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3013 (0.2793) Acc D Real: 87.708% 
Loss D Fake: 0.2936 (0.2938) Acc D Fake: 100.000% 
Loss D: 0.595 
Loss G: 1.4120 (1.4123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,481 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2163 (0.2714) Acc D Real: 87.930% 
Loss D Fake: 0.2948 (0.2939) Acc D Fake: 100.000% 
Loss D: 0.511 
Loss G: 1.4014 (1.4109) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,488 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2092 (0.2645) Acc D Real: 88.328% 
Loss D Fake: 0.3006 (0.2946) Acc D Fake: 100.000% 
Loss D: 0.510 
Loss G: 1.3773 (1.4072) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,495 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1542 (0.2535) Acc D Real: 89.010% 
Loss D Fake: 0.3140 (0.2966) Acc D Fake: 100.000% 
Loss D: 0.468 
Loss G: 1.2022 (1.3867) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,501 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.3608 (0.2632) Acc D Real: 87.969% 
Loss D Fake: 0.3901 (0.3051) Acc D Fake: 100.000% 
Loss D: 0.751 
Loss G: 1.1390 (1.3642) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,508 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2594 (0.2629) Acc D Real: 88.108% 
Loss D Fake: 0.3940 (0.3125) Acc D Fake: 100.000% 
Loss D: 0.653 
Loss G: 1.1393 (1.3454) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,515 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2808 (0.2643) Acc D Real: 88.013% 
Loss D Fake: 0.3907 (0.3185) Acc D Fake: 100.000% 
Loss D: 0.672 
Loss G: 1.1687 (1.3318) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,522 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.2382 (0.2624) Acc D Real: 87.898% 
Loss D Fake: 0.3178 (0.3184) Acc D Fake: 100.000% 
Loss D: 0.556 
Loss G: 1.3974 (1.3365) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,529 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.2551 (0.2619) Acc D Real: 87.913% 
Loss D Fake: 0.2953 (0.3169) Acc D Fake: 100.000% 
Loss D: 0.550 
Loss G: 1.4159 (1.3418) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,536 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3584 (0.2680) Acc D Real: 87.562% 
Loss D Fake: 0.2925 (0.3154) Acc D Fake: 100.000% 
Loss D: 0.651 
Loss G: 1.4209 (1.3467) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,543 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3701 (0.2740) Acc D Real: 87.089% 
Loss D Fake: 0.2918 (0.3140) Acc D Fake: 100.000% 
Loss D: 0.662 
Loss G: 1.4214 (1.3511) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,550 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2872 (0.2747) Acc D Real: 87.066% 
Loss D Fake: 0.2920 (0.3128) Acc D Fake: 100.000% 
Loss D: 0.579 
Loss G: 1.4207 (1.3550) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,557 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3442 (0.2784) Acc D Real: 86.812% 
Loss D Fake: 0.2924 (0.3117) Acc D Fake: 100.000% 
Loss D: 0.637 
Loss G: 1.4190 (1.3584) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,564 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1450 (0.2717) Acc D Real: 87.201% 
Loss D Fake: 0.2928 (0.3107) Acc D Fake: 100.000% 
Loss D: 0.438 
Loss G: 1.4188 (1.3614) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,571 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.2663 (0.2714) Acc D Real: 87.247% 
Loss D Fake: 0.2928 (0.3099) Acc D Fake: 100.000% 
Loss D: 0.559 
Loss G: 1.4184 (1.3641) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,577 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.2233 (0.2693) Acc D Real: 87.434% 
Loss D Fake: 0.2930 (0.3091) Acc D Fake: 100.000% 
Loss D: 0.516 
Loss G: 1.4183 (1.3666) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,584 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1558 (0.2643) Acc D Real: 87.740% 
Loss D Fake: 0.2929 (0.3084) Acc D Fake: 100.000% 
Loss D: 0.449 
Loss G: 1.4194 (1.3689) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,591 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.2799 (0.2650) Acc D Real: 87.650% 
Loss D Fake: 0.2927 (0.3078) Acc D Fake: 100.000% 
Loss D: 0.573 
Loss G: 1.4199 (1.3710) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,598 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1795 (0.2616) Acc D Real: 87.817% 
Loss D Fake: 0.2926 (0.3072) Acc D Fake: 100.000% 
Loss D: 0.472 
Loss G: 1.4211 (1.3730) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,605 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2782 (0.2622) Acc D Real: 87.730% 
Loss D Fake: 0.2924 (0.3066) Acc D Fake: 100.000% 
Loss D: 0.571 
Loss G: 1.4217 (1.3749) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,612 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1428 (0.2578) Acc D Real: 88.011% 
Loss D Fake: 0.2923 (0.3061) Acc D Fake: 100.000% 
Loss D: 0.435 
Loss G: 1.4232 (1.3767) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,619 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.2694 (0.2582) Acc D Real: 87.887% 
Loss D Fake: 0.2919 (0.3056) Acc D Fake: 100.000% 
Loss D: 0.561 
Loss G: 1.4247 (1.3784) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,627 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.2788 (0.2589) Acc D Real: 87.861% 
Loss D Fake: 0.2916 (0.3051) Acc D Fake: 100.000% 
Loss D: 0.570 
Loss G: 1.4255 (1.3800) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,634 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.2041 (0.2571) Acc D Real: 87.946% 
Loss D Fake: 0.2915 (0.3046) Acc D Fake: 100.000% 
Loss D: 0.496 
Loss G: 1.4264 (1.3815) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,641 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2799 (0.2578) Acc D Real: 87.880% 
Loss D Fake: 0.2913 (0.3042) Acc D Fake: 100.000% 
Loss D: 0.571 
Loss G: 1.4271 (1.3830) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,649 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1768 (0.2553) Acc D Real: 88.016% 
Loss D Fake: 0.2911 (0.3038) Acc D Fake: 100.000% 
Loss D: 0.468 
Loss G: 1.4283 (1.3844) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,656 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.1521 (0.2522) Acc D Real: 88.158% 
Loss D Fake: 0.2908 (0.3034) Acc D Fake: 100.000% 
Loss D: 0.443 
Loss G: 1.4295 (1.3858) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,664 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1886 (0.2503) Acc D Real: 88.312% 
Loss D Fake: 0.2907 (0.3030) Acc D Fake: 100.000% 
Loss D: 0.479 
Loss G: 1.4308 (1.3871) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,671 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.2648 (0.2507) Acc D Real: 88.268% 
Loss D Fake: 0.2905 (0.3027) Acc D Fake: 100.000% 
Loss D: 0.555 
Loss G: 1.4317 (1.3884) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,679 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1732 (0.2485) Acc D Real: 88.397% 
Loss D Fake: 0.2904 (0.3023) Acc D Fake: 100.000% 
Loss D: 0.464 
Loss G: 1.4330 (1.3896) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,686 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2685 (0.2491) Acc D Real: 88.363% 
Loss D Fake: 0.2902 (0.3020) Acc D Fake: 100.000% 
Loss D: 0.559 
Loss G: 1.4330 (1.3908) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,693 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2094 (0.2480) Acc D Real: 88.502% 
Loss D Fake: 0.2907 (0.3017) Acc D Fake: 100.000% 
Loss D: 0.500 
Loss G: 1.4319 (1.3919) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,700 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.3106 (0.2496) Acc D Real: 88.377% 
Loss D Fake: 0.2915 (0.3014) Acc D Fake: 100.000% 
Loss D: 0.602 
Loss G: 1.4291 (1.3928) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,708 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.2072 (0.2486) Acc D Real: 88.419% 
Loss D Fake: 0.2927 (0.3012) Acc D Fake: 100.000% 
Loss D: 0.500 
Loss G: 1.4262 (1.3937) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,715 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.2093 (0.2476) Acc D Real: 88.427% 
Loss D Fake: 0.2937 (0.3010) Acc D Fake: 100.000% 
Loss D: 0.503 
Loss G: 1.4234 (1.3944) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,722 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.2143 (0.2468) Acc D Real: 88.408% 
Loss D Fake: 0.2949 (0.3009) Acc D Fake: 100.000% 
Loss D: 0.509 
Loss G: 1.4183 (1.3950) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,730 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1318 (0.2442) Acc D Real: 88.560% 
Loss D Fake: 0.2968 (0.3008) Acc D Fake: 100.000% 
Loss D: 0.429 
Loss G: 1.4142 (1.3954) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,737 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.2180 (0.2436) Acc D Real: 88.633% 
Loss D Fake: 0.2976 (0.3007) Acc D Fake: 100.000% 
Loss D: 0.516 
Loss G: 1.4226 (1.3960) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,744 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.2074 (0.2428) Acc D Real: 88.675% 
Loss D Fake: 0.3077 (0.3009) Acc D Fake: 100.000% 
Loss D: 0.515 
Loss G: 1.4431 (1.3971) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,752 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.1390 (0.2405) Acc D Real: 88.790% 
Loss D Fake: 0.2853 (0.3005) Acc D Fake: 100.000% 
Loss D: 0.424 
Loss G: 1.4574 (1.3984) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,759 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1461 (0.2385) Acc D Real: 88.916% 
Loss D Fake: 0.2822 (0.3001) Acc D Fake: 100.000% 
Loss D: 0.428 
Loss G: 1.4650 (1.3998) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,767 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.1667 (0.2370) Acc D Real: 88.993% 
Loss D Fake: 0.2801 (0.2997) Acc D Fake: 100.000% 
Loss D: 0.447 
Loss G: 1.4710 (1.4013) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,774 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.1905 (0.2361) Acc D Real: 89.088% 
Loss D Fake: 0.2783 (0.2993) Acc D Fake: 100.000% 
Loss D: 0.469 
Loss G: 1.4762 (1.4028) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,781 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.2433 (0.2362) Acc D Real: 89.073% 
Loss D Fake: 0.2768 (0.2988) Acc D Fake: 100.000% 
Loss D: 0.520 
Loss G: 1.4805 (1.4044) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,788 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.1903 (0.2353) Acc D Real: 89.175% 
Loss D Fake: 0.2756 (0.2984) Acc D Fake: 100.000% 
Loss D: 0.466 
Loss G: 1.4844 (1.4059) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,796 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.2666 (0.2359) Acc D Real: 89.144% 
Loss D Fake: 0.2751 (0.2979) Acc D Fake: 100.000% 
Loss D: 0.542 
Loss G: 1.4828 (1.4074) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,803 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2787 (0.2367) Acc D Real: 89.053% 
Loss D Fake: 0.2770 (0.2975) Acc D Fake: 100.000% 
Loss D: 0.556 
Loss G: 1.4784 (1.4088) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,810 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.2158 (0.2363) Acc D Real: 89.063% 
Loss D Fake: 0.2793 (0.2972) Acc D Fake: 100.000% 
Loss D: 0.495 
Loss G: 1.4738 (1.4100) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,817 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.2336 (0.2363) Acc D Real: 89.063% 
Loss D Fake: 0.2813 (0.2969) Acc D Fake: 100.000% 
Loss D: 0.515 
Loss G: 1.4709 (1.4111) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,825 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.2905 (0.2372) Acc D Real: 89.001% 
Loss D Fake: 0.2824 (0.2966) Acc D Fake: 100.000% 
Loss D: 0.573 
Loss G: 1.4700 (1.4121) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,832 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.2557 (0.2376) Acc D Real: 88.991% 
Loss D Fake: 0.2824 (0.2964) Acc D Fake: 100.000% 
Loss D: 0.538 
Loss G: 1.4716 (1.4132) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,840 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.1733 (0.2365) Acc D Real: 89.039% 
Loss D Fake: 0.2811 (0.2961) Acc D Fake: 100.000% 
Loss D: 0.454 
Loss G: 1.4763 (1.4143) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,847 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.1589 (0.2351) Acc D Real: 89.100% 
Loss D Fake: 0.2789 (0.2958) Acc D Fake: 100.000% 
Loss D: 0.438 
Loss G: 1.4825 (1.4154) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,854 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.1926 (0.2344) Acc D Real: 89.175% 
Loss D Fake: 0.2766 (0.2955) Acc D Fake: 100.000% 
Loss D: 0.469 
Loss G: 1.4887 (1.4166) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,861 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.1376 (0.2328) Acc D Real: 89.270% 
Loss D Fake: 0.2743 (0.2952) Acc D Fake: 100.000% 
Loss D: 0.412 
Loss G: 1.4951 (1.4179) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,869 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.2969 (0.2339) Acc D Real: 89.179% 
Loss D Fake: 0.2723 (0.2948) Acc D Fake: 100.000% 
Loss D: 0.569 
Loss G: 1.5001 (1.4192) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,876 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3428 (0.2356) Acc D Real: 89.041% 
Loss D Fake: 0.2709 (0.2944) Acc D Fake: 100.000% 
Loss D: 0.614 
Loss G: 1.5030 (1.4206) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,883 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.2648 (0.2361) Acc D Real: 89.033% 
Loss D Fake: 0.2701 (0.2940) Acc D Fake: 100.000% 
Loss D: 0.535 
Loss G: 1.5047 (1.4219) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,891 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3416 (0.2377) Acc D Real: 88.930% 
Loss D Fake: 0.2697 (0.2937) Acc D Fake: 100.000% 
Loss D: 0.611 
Loss G: 1.5048 (1.4232) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,898 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.1239 (0.2360) Acc D Real: 89.035% 
Loss D Fake: 0.2694 (0.2933) Acc D Fake: 100.000% 
Loss D: 0.393 
Loss G: 1.5059 (1.4244) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,905 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.1995 (0.2354) Acc D Real: 89.078% 
Loss D Fake: 0.2689 (0.2929) Acc D Fake: 100.000% 
Loss D: 0.468 
Loss G: 1.5077 (1.4257) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,913 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.2515 (0.2357) Acc D Real: 89.089% 
Loss D Fake: 0.2683 (0.2926) Acc D Fake: 100.000% 
Loss D: 0.520 
Loss G: 1.5089 (1.4269) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,920 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.2300 (0.2356) Acc D Real: 89.086% 
Loss D Fake: 0.2680 (0.2922) Acc D Fake: 100.000% 
Loss D: 0.498 
Loss G: 1.5100 (1.4281) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,927 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.2092 (0.2352) Acc D Real: 89.126% 
Loss D Fake: 0.2676 (0.2919) Acc D Fake: 100.000% 
Loss D: 0.477 
Loss G: 1.5111 (1.4293) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,935 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.1757 (0.2344) Acc D Real: 89.196% 
Loss D Fake: 0.2673 (0.2915) Acc D Fake: 100.000% 
Loss D: 0.443 
Loss G: 1.5126 (1.4305) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,942 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.1870 (0.2337) Acc D Real: 89.251% 
Loss D Fake: 0.2668 (0.2912) Acc D Fake: 100.000% 
Loss D: 0.454 
Loss G: 1.5144 (1.4316) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,950 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.2504 (0.2339) Acc D Real: 89.235% 
Loss D Fake: 0.2664 (0.2908) Acc D Fake: 100.000% 
Loss D: 0.517 
Loss G: 1.5157 (1.4328) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,957 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.1827 (0.2332) Acc D Real: 89.275% 
Loss D Fake: 0.2660 (0.2905) Acc D Fake: 100.000% 
Loss D: 0.449 
Loss G: 1.5173 (1.4339) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,964 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.2188 (0.2330) Acc D Real: 89.272% 
Loss D Fake: 0.2656 (0.2902) Acc D Fake: 100.000% 
Loss D: 0.484 
Loss G: 1.5190 (1.4350) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,971 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.2490 (0.2333) Acc D Real: 89.242% 
Loss D Fake: 0.2651 (0.2898) Acc D Fake: 100.000% 
Loss D: 0.514 
Loss G: 1.5204 (1.4362) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,979 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.1780 (0.2325) Acc D Real: 89.274% 
Loss D Fake: 0.2647 (0.2895) Acc D Fake: 100.000% 
Loss D: 0.443 
Loss G: 1.5222 (1.4373) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,986 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.1062 (0.2309) Acc D Real: 89.362% 
Loss D Fake: 0.2641 (0.2892) Acc D Fake: 100.000% 
Loss D: 0.370 
Loss G: 1.5253 (1.4384) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:21,993 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3736 (0.2327) Acc D Real: 89.203% 
Loss D Fake: 0.2635 (0.2889) Acc D Fake: 100.000% 
Loss D: 0.637 
Loss G: 1.5257 (1.4395) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,001 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.1225 (0.2313) Acc D Real: 89.273% 
Loss D Fake: 0.2636 (0.2886) Acc D Fake: 100.000% 
Loss D: 0.386 
Loss G: 1.5263 (1.4406) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,008 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.2028 (0.2310) Acc D Real: 89.273% 
Loss D Fake: 0.2635 (0.2882) Acc D Fake: 100.000% 
Loss D: 0.466 
Loss G: 1.5275 (1.4417) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,016 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.2575 (0.2313) Acc D Real: 89.224% 
Loss D Fake: 0.2633 (0.2879) Acc D Fake: 100.000% 
Loss D: 0.521 
Loss G: 1.5275 (1.4427) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,024 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.2869 (0.2320) Acc D Real: 89.177% 
Loss D Fake: 0.2636 (0.2876) Acc D Fake: 100.000% 
Loss D: 0.551 
Loss G: 1.5261 (1.4437) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,031 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.2590 (0.2323) Acc D Real: 89.105% 
Loss D Fake: 0.2645 (0.2874) Acc D Fake: 100.000% 
Loss D: 0.524 
Loss G: 1.4358 (1.4436) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,039 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.1464 (0.2313) Acc D Real: 89.167% 
Loss D Fake: 0.4424 (0.2892) Acc D Fake: 99.879% 
Loss D: 0.589 
Loss G: 1.3572 (1.4426) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,046 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1276 (0.2301) Acc D Real: 89.233% 
Loss D Fake: 0.2651 (0.2889) Acc D Fake: 99.880% 
Loss D: 0.393 
Loss G: 1.5271 (1.4436) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,053 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.1988 (0.2297) Acc D Real: 89.243% 
Loss D Fake: 0.2633 (0.2886) Acc D Fake: 99.881% 
Loss D: 0.462 
Loss G: 1.5322 (1.4446) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,061 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.2053 (0.2295) Acc D Real: 89.239% 
Loss D Fake: 0.2626 (0.2883) Acc D Fake: 99.883% 
Loss D: 0.468 
Loss G: 1.5349 (1.4456) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,068 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.1502 (0.2286) Acc D Real: 89.291% 
Loss D Fake: 0.2622 (0.2880) Acc D Fake: 99.884% 
Loss D: 0.412 
Loss G: 1.5373 (1.4467) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,075 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.1865 (0.2281) Acc D Real: 89.314% 
Loss D Fake: 0.2617 (0.2877) Acc D Fake: 99.885% 
Loss D: 0.448 
Loss G: 1.5398 (1.4477) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,083 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.1519 (0.2273) Acc D Real: 89.369% 
Loss D Fake: 0.2612 (0.2874) Acc D Fake: 99.887% 
Loss D: 0.413 
Loss G: 1.5425 (1.4487) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,090 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.2449 (0.2274) Acc D Real: 89.342% 
Loss D Fake: 0.2608 (0.2872) Acc D Fake: 99.888% 
Loss D: 0.506 
Loss G: 1.5439 (1.4498) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,097 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.1371 (0.2265) Acc D Real: 89.392% 
Loss D Fake: 0.2606 (0.2869) Acc D Fake: 99.889% 
Loss D: 0.398 
Loss G: 1.5453 (1.4508) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,104 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.1497 (0.2257) Acc D Real: 89.423% 
Loss D Fake: 0.2603 (0.2866) Acc D Fake: 99.890% 
Loss D: 0.410 
Loss G: 1.5474 (1.4518) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,112 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.1109 (0.2245) Acc D Real: 89.483% 
Loss D Fake: 0.2597 (0.2863) Acc D Fake: 99.891% 
Loss D: 0.371 
Loss G: 1.5503 (1.4529) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,120 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2199 (0.2244) Acc D Real: 89.495% 
Loss D Fake: 0.2590 (0.2860) Acc D Fake: 99.893% 
Loss D: 0.479 
Loss G: 1.5529 (1.4539) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,127 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.1039 (0.2232) Acc D Real: 89.562% 
Loss D Fake: 0.2584 (0.2857) Acc D Fake: 99.894% 
Loss D: 0.362 
Loss G: 1.5561 (1.4550) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,134 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.1500 (0.2224) Acc D Real: 89.611% 
Loss D Fake: 0.2575 (0.2854) Acc D Fake: 99.895% 
Loss D: 0.407 
Loss G: 1.5596 (1.4560) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,141 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2462 (0.2227) Acc D Real: 89.594% 
Loss D Fake: 0.2567 (0.2852) Acc D Fake: 99.896% 
Loss D: 0.503 
Loss G: 1.5620 (1.4571) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,148 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.1594 (0.2220) Acc D Real: 89.641% 
Loss D Fake: 0.2562 (0.2849) Acc D Fake: 99.897% 
Loss D: 0.416 
Loss G: 1.5644 (1.4582) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,156 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.1830 (0.2216) Acc D Real: 89.662% 
Loss D Fake: 0.2557 (0.2846) Acc D Fake: 99.898% 
Loss D: 0.439 
Loss G: 1.5661 (1.4592) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,163 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.1689 (0.2211) Acc D Real: 89.704% 
Loss D Fake: 0.2553 (0.2843) Acc D Fake: 99.899% 
Loss D: 0.424 
Loss G: 1.5678 (1.4603) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,171 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.1690 (0.2206) Acc D Real: 89.717% 
Loss D Fake: 0.2551 (0.2840) Acc D Fake: 99.900% 
Loss D: 0.424 
Loss G: 1.5369 (1.4611) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,178 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.1399 (0.2198) Acc D Real: 89.754% 
Loss D Fake: 0.2873 (0.2840) Acc D Fake: 99.901% 
Loss D: 0.427 
Loss G: 1.5771 (1.4622) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,185 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.1496 (0.2192) Acc D Real: 89.783% 
Loss D Fake: 0.2516 (0.2837) Acc D Fake: 99.902% 
Loss D: 0.401 
Loss G: 1.5866 (1.4634) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,193 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.1398 (0.2184) Acc D Real: 89.830% 
Loss D Fake: 0.2495 (0.2834) Acc D Fake: 99.903% 
Loss D: 0.389 
Loss G: 1.5943 (1.4646) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,200 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2599 (0.2188) Acc D Real: 89.809% 
Loss D Fake: 0.2478 (0.2831) Acc D Fake: 99.904% 
Loss D: 0.508 
Loss G: 1.5998 (1.4659) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,207 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.1517 (0.2182) Acc D Real: 89.842% 
Loss D Fake: 0.2467 (0.2827) Acc D Fake: 99.905% 
Loss D: 0.398 
Loss G: 1.6044 (1.4671) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,215 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.1231 (0.2173) Acc D Real: 89.896% 
Loss D Fake: 0.2455 (0.2824) Acc D Fake: 99.905% 
Loss D: 0.369 
Loss G: 1.6091 (1.4684) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,222 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.1710 (0.2169) Acc D Real: 89.920% 
Loss D Fake: 0.2445 (0.2821) Acc D Fake: 99.906% 
Loss D: 0.415 
Loss G: 1.6134 (1.4698) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,229 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.2108 (0.2168) Acc D Real: 89.916% 
Loss D Fake: 0.2437 (0.2817) Acc D Fake: 99.907% 
Loss D: 0.454 
Loss G: 1.6164 (1.4711) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,236 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.1772 (0.2165) Acc D Real: 89.927% 
Loss D Fake: 0.2439 (0.2814) Acc D Fake: 99.908% 
Loss D: 0.421 
Loss G: 1.6082 (1.4723) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:22,244 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.2140 (0.2165) Acc D Real: 89.923% 
Loss D Fake: 0.3052 (0.2816) Acc D Fake: 99.909% 
Loss D: 0.519 
Loss G: 1.1611 (1.4696) Acc G: 0.124% 
LR: 2.000e-04 

2023-03-02 01:46:22,251 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2063 (0.2164) Acc D Real: 89.918% 
Loss D Fake: 1.3007 (0.2905) Acc D Fake: 99.544% 
Loss D: 1.507 
Loss G: 1.6172 (1.4708) Acc G: 0.122% 
LR: 2.000e-04 

2023-03-02 01:46:22,258 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.1873 (0.2161) Acc D Real: 89.922% 
Loss D Fake: 0.2406 (0.2901) Acc D Fake: 99.548% 
Loss D: 0.428 
Loss G: 1.6319 (1.4722) Acc G: 0.121% 
LR: 2.000e-04 

2023-03-02 01:46:22,266 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.2167 (0.2161) Acc D Real: 89.915% 
Loss D Fake: 0.2388 (0.2896) Acc D Fake: 99.552% 
Loss D: 0.456 
Loss G: 1.6336 (1.4736) Acc G: 0.120% 
LR: 2.000e-04 

2023-03-02 01:46:22,273 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2332 (0.2163) Acc D Real: 89.904% 
Loss D Fake: 0.2390 (0.2892) Acc D Fake: 99.556% 
Loss D: 0.472 
Loss G: 1.6306 (1.4750) Acc G: 0.119% 
LR: 2.000e-04 

2023-03-02 01:46:22,280 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3258 (0.2172) Acc D Real: 89.845% 
Loss D Fake: 0.2401 (0.2888) Acc D Fake: 99.559% 
Loss D: 0.566 
Loss G: 1.6244 (1.4762) Acc G: 0.118% 
LR: 2.000e-04 

2023-03-02 01:46:22,287 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.1244 (0.2164) Acc D Real: 89.889% 
Loss D Fake: 0.2420 (0.2884) Acc D Fake: 99.563% 
Loss D: 0.366 
Loss G: 1.6180 (1.4774) Acc G: 0.117% 
LR: 2.000e-04 

2023-03-02 01:46:22,295 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2117 (0.2164) Acc D Real: 89.888% 
Loss D Fake: 0.2437 (0.2880) Acc D Fake: 99.567% 
Loss D: 0.455 
Loss G: 1.6122 (1.4786) Acc G: 0.116% 
LR: 2.000e-04 

2023-03-02 01:46:22,302 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2560 (0.2167) Acc D Real: 89.867% 
Loss D Fake: 0.2452 (0.2877) Acc D Fake: 99.570% 
Loss D: 0.501 
Loss G: 1.6074 (1.4796) Acc G: 0.115% 
LR: 2.000e-04 

2023-03-02 01:46:22,310 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.1988 (0.2166) Acc D Real: 89.876% 
Loss D Fake: 0.2471 (0.2873) Acc D Fake: 99.574% 
Loss D: 0.446 
Loss G: 1.5970 (1.4806) Acc G: 0.114% 
LR: 2.000e-04 

2023-03-02 01:46:22,317 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2415 (0.2168) Acc D Real: 89.877% 
Loss D Fake: 0.2515 (0.2871) Acc D Fake: 99.577% 
Loss D: 0.493 
Loss G: 1.5859 (1.4814) Acc G: 0.113% 
LR: 2.000e-04 

2023-03-02 01:46:22,324 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.2750 (0.2172) Acc D Real: 89.848% 
Loss D Fake: 0.2546 (0.2868) Acc D Fake: 99.581% 
Loss D: 0.530 
Loss G: 1.5822 (1.4823) Acc G: 0.113% 
LR: 2.000e-04 

2023-03-02 01:46:22,331 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3313 (0.2181) Acc D Real: 89.806% 
Loss D Fake: 0.2541 (0.2865) Acc D Fake: 99.584% 
Loss D: 0.585 
Loss G: 1.5853 (1.4831) Acc G: 0.112% 
LR: 2.000e-04 

2023-03-02 01:46:22,339 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.1649 (0.2177) Acc D Real: 89.833% 
Loss D Fake: 0.2515 (0.2862) Acc D Fake: 99.587% 
Loss D: 0.416 
Loss G: 1.5929 (1.4840) Acc G: 0.111% 
LR: 2.000e-04 

2023-03-02 01:46:22,346 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.3139 (0.2185) Acc D Real: 89.782% 
Loss D Fake: 0.2484 (0.2860) Acc D Fake: 99.591% 
Loss D: 0.562 
Loss G: 1.5996 (1.4849) Acc G: 0.110% 
LR: 2.000e-04 

2023-03-02 01:46:22,353 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.1929 (0.2183) Acc D Real: 89.810% 
Loss D Fake: 0.2459 (0.2856) Acc D Fake: 99.594% 
Loss D: 0.439 
Loss G: 1.6051 (1.4858) Acc G: 0.109% 
LR: 2.000e-04 

2023-03-02 01:46:22,360 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3723 (0.2195) Acc D Real: 89.744% 
Loss D Fake: 0.2445 (0.2853) Acc D Fake: 99.597% 
Loss D: 0.617 
Loss G: 1.6050 (1.4867) Acc G: 0.108% 
LR: 2.000e-04 

2023-03-02 01:46:22,368 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.2891 (0.2200) Acc D Real: 89.713% 
Loss D Fake: 0.2447 (0.2850) Acc D Fake: 99.600% 
Loss D: 0.534 
Loss G: 1.6035 (1.4876) Acc G: 0.107% 
LR: 2.000e-04 

2023-03-02 01:46:22,375 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.2383 (0.2201) Acc D Real: 89.699% 
Loss D Fake: 0.2448 (0.2847) Acc D Fake: 99.603% 
Loss D: 0.483 
Loss G: 1.6026 (1.4885) Acc G: 0.107% 
LR: 2.000e-04 

2023-03-02 01:46:22,382 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.2459 (0.2203) Acc D Real: 89.695% 
Loss D Fake: 0.2447 (0.2844) Acc D Fake: 99.606% 
Loss D: 0.491 
Loss G: 1.6022 (1.4894) Acc G: 0.106% 
LR: 2.000e-04 

2023-03-02 01:46:22,389 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3751 (0.2215) Acc D Real: 89.615% 
Loss D Fake: 0.2448 (0.2841) Acc D Fake: 99.609% 
Loss D: 0.620 
Loss G: 1.5997 (1.4902) Acc G: 0.105% 
LR: 2.000e-04 

2023-03-02 01:46:22,396 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2588 (0.2218) Acc D Real: 89.601% 
Loss D Fake: 0.2446 (0.2838) Acc D Fake: 99.612% 
Loss D: 0.503 
Loss G: 1.6017 (1.4910) Acc G: 0.104% 
LR: 2.000e-04 

2023-03-02 01:46:22,403 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.3356 (0.2226) Acc D Real: 89.547% 
Loss D Fake: 0.2444 (0.2835) Acc D Fake: 99.615% 
Loss D: 0.580 
Loss G: 1.5980 (1.4918) Acc G: 0.103% 
LR: 2.000e-04 

2023-03-02 01:46:22,411 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.2290 (0.2227) Acc D Real: 89.539% 
Loss D Fake: 0.2459 (0.2832) Acc D Fake: 99.618% 
Loss D: 0.475 
Loss G: 1.5921 (1.4926) Acc G: 0.103% 
LR: 2.000e-04 

2023-03-02 01:46:22,418 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1912 (0.2224) Acc D Real: 89.546% 
Loss D Fake: 0.2476 (0.2830) Acc D Fake: 99.621% 
Loss D: 0.439 
Loss G: 1.5865 (1.4932) Acc G: 0.102% 
LR: 2.000e-04 

2023-03-02 01:46:22,425 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.1971 (0.2223) Acc D Real: 89.543% 
Loss D Fake: 0.2492 (0.2827) Acc D Fake: 99.623% 
Loss D: 0.446 
Loss G: 1.5817 (1.4939) Acc G: 0.101% 
LR: 2.000e-04 

2023-03-02 01:46:22,432 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.1744 (0.2219) Acc D Real: 89.557% 
Loss D Fake: 0.2506 (0.2825) Acc D Fake: 99.626% 
Loss D: 0.425 
Loss G: 1.5782 (1.4945) Acc G: 0.100% 
LR: 2.000e-04 

2023-03-02 01:46:22,440 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2230 (0.2219) Acc D Real: 89.550% 
Loss D Fake: 0.2518 (0.2823) Acc D Fake: 99.629% 
Loss D: 0.475 
Loss G: 1.5751 (1.4951) Acc G: 0.100% 
LR: 2.000e-04 

2023-03-02 01:46:22,447 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.1933 (0.2217) Acc D Real: 89.551% 
Loss D Fake: 0.2537 (0.2821) Acc D Fake: 99.631% 
Loss D: 0.447 
Loss G: 1.5614 (1.4955) Acc G: 0.099% 
LR: 2.000e-04 

2023-03-02 01:46:22,454 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.1221 (0.2210) Acc D Real: 89.590% 
Loss D Fake: 0.2911 (0.2821) Acc D Fake: 99.634% 
Loss D: 0.413 
Loss G: 0.4634 (1.4883) Acc G: 0.542% 
LR: 2.000e-04 

2023-03-02 01:46:22,462 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.1106 (0.2202) Acc D Real: 89.633% 
Loss D Fake: 2.3936 (0.2969) Acc D Fake: 99.147% 
Loss D: 2.504 
Loss G: 0.3489 (1.4803) Acc G: 1.028% 
LR: 2.000e-04 

2023-03-02 01:46:22,470 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1925 (0.2201) Acc D Real: 89.636% 
Loss D Fake: 2.3792 (0.3114) Acc D Fake: 98.667% 
Loss D: 2.572 
Loss G: 0.4502 (1.4731) Acc G: 1.461% 
LR: 2.000e-04 

2023-03-02 01:46:22,477 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.1366 (0.2195) Acc D Real: 89.668% 
Loss D Fake: 1.7963 (0.3216) Acc D Fake: 98.300% 
Loss D: 1.933 
Loss G: 1.5943 (1.4740) Acc G: 1.451% 
LR: 2.000e-04 

2023-03-02 01:46:22,485 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.2009 (0.2194) Acc D Real: 89.663% 
Loss D Fake: 0.2457 (0.3211) Acc D Fake: 98.311% 
Loss D: 0.447 
Loss G: 1.6103 (1.4749) Acc G: 1.441% 
LR: 2.000e-04 

2023-03-02 01:46:22,492 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.1924 (0.2192) Acc D Real: 89.672% 
Loss D Fake: 0.2432 (0.3206) Acc D Fake: 98.323% 
Loss D: 0.436 
Loss G: 1.6133 (1.4759) Acc G: 1.431% 
LR: 2.000e-04 

2023-03-02 01:46:22,500 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.2310 (0.2193) Acc D Real: 89.663% 
Loss D Fake: 0.2437 (0.3200) Acc D Fake: 98.334% 
Loss D: 0.475 
Loss G: 1.6081 (1.4767) Acc G: 1.421% 
LR: 2.000e-04 

2023-03-02 01:46:22,507 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.2413 (0.2194) Acc D Real: 89.651% 
Loss D Fake: 0.2469 (0.3196) Acc D Fake: 98.345% 
Loss D: 0.488 
Loss G: 1.5932 (1.4775) Acc G: 1.412% 
LR: 2.000e-04 

2023-03-02 01:46:22,514 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.2137 (0.2194) Acc D Real: 89.647% 
Loss D Fake: 0.2556 (0.3191) Acc D Fake: 98.356% 
Loss D: 0.469 
Loss G: 1.5510 (1.4780) Acc G: 1.402% 
LR: 2.000e-04 

2023-03-02 01:46:22,522 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.1215 (0.2187) Acc D Real: 89.685% 
Loss D Fake: 1.1617 (0.3247) Acc D Fake: 97.848% 
Loss D: 1.283 
Loss G: 1.5796 (1.4787) Acc G: 1.393% 
LR: 2.000e-04 

2023-03-02 01:46:22,530 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2212 (0.2187) Acc D Real: 89.685% 
Loss D Fake: 0.2463 (0.3242) Acc D Fake: 97.863% 
Loss D: 0.467 
Loss G: 1.6190 (1.4796) Acc G: 1.384% 
LR: 2.000e-04 

2023-03-02 01:46:22,537 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.2043 (0.2186) Acc D Real: 89.693% 
Loss D Fake: 0.2380 (0.3236) Acc D Fake: 97.876% 
Loss D: 0.442 
Loss G: 1.6372 (1.4806) Acc G: 1.375% 
LR: 2.000e-04 

2023-03-02 01:46:22,545 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.3233 (0.2193) Acc D Real: 89.651% 
Loss D Fake: 0.2337 (0.3230) Acc D Fake: 97.890% 
Loss D: 0.557 
Loss G: 1.6480 (1.4817) Acc G: 1.366% 
LR: 2.000e-04 

2023-03-02 01:46:22,552 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.2966 (0.2198) Acc D Real: 89.622% 
Loss D Fake: 0.2312 (0.3224) Acc D Fake: 97.904% 
Loss D: 0.528 
Loss G: 1.6546 (1.4828) Acc G: 1.357% 
LR: 2.000e-04 

2023-03-02 01:46:22,560 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.1933 (0.2196) Acc D Real: 89.631% 
Loss D Fake: 0.2295 (0.3219) Acc D Fake: 97.917% 
Loss D: 0.423 
Loss G: 1.6595 (1.4840) Acc G: 1.348% 
LR: 2.000e-04 

2023-03-02 01:46:22,567 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3648 (0.2206) Acc D Real: 89.573% 
Loss D Fake: 0.2283 (0.3213) Acc D Fake: 97.931% 
Loss D: 0.593 
Loss G: 1.6620 (1.4851) Acc G: 1.340% 
LR: 2.000e-04 

2023-03-02 01:46:22,575 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.2418 (0.2207) Acc D Real: 89.561% 
Loss D Fake: 0.2278 (0.3207) Acc D Fake: 97.944% 
Loss D: 0.470 
Loss G: 1.6631 (1.4862) Acc G: 1.331% 
LR: 2.000e-04 

2023-03-02 01:46:22,582 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2522 (0.2209) Acc D Real: 89.540% 
Loss D Fake: 0.2275 (0.3201) Acc D Fake: 97.957% 
Loss D: 0.480 
Loss G: 1.6637 (1.4874) Acc G: 1.323% 
LR: 2.000e-04 

2023-03-02 01:46:22,590 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3679 (0.2218) Acc D Real: 89.493% 
Loss D Fake: 0.2274 (0.3195) Acc D Fake: 97.969% 
Loss D: 0.595 
Loss G: 1.6624 (1.4884) Acc G: 1.315% 
LR: 2.000e-04 

2023-03-02 01:46:22,597 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2730 (0.2221) Acc D Real: 89.476% 
Loss D Fake: 0.2278 (0.3189) Acc D Fake: 97.982% 
Loss D: 0.501 
Loss G: 1.6600 (1.4895) Acc G: 1.307% 
LR: 2.000e-04 

2023-03-02 01:46:22,605 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.2775 (0.2225) Acc D Real: 89.454% 
Loss D Fake: 0.2283 (0.3184) Acc D Fake: 97.994% 
Loss D: 0.506 
Loss G: 1.6571 (1.4905) Acc G: 1.299% 
LR: 2.000e-04 

2023-03-02 01:46:22,612 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3109 (0.2230) Acc D Real: 89.423% 
Loss D Fake: 0.2290 (0.3178) Acc D Fake: 98.007% 
Loss D: 0.540 
Loss G: 1.6536 (1.4915) Acc G: 1.291% 
LR: 2.000e-04 

2023-03-02 01:46:22,620 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2924 (0.2234) Acc D Real: 89.399% 
Loss D Fake: 0.2298 (0.3173) Acc D Fake: 98.019% 
Loss D: 0.522 
Loss G: 1.6497 (1.4925) Acc G: 1.283% 
LR: 2.000e-04 

2023-03-02 01:46:22,628 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.3538 (0.2242) Acc D Real: 89.343% 
Loss D Fake: 0.2307 (0.3168) Acc D Fake: 98.031% 
Loss D: 0.584 
Loss G: 1.6452 (1.4934) Acc G: 1.275% 
LR: 2.000e-04 

2023-03-02 01:46:22,635 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3887 (0.2252) Acc D Real: 89.277% 
Loss D Fake: 0.2318 (0.3162) Acc D Fake: 98.043% 
Loss D: 0.620 
Loss G: 1.6395 (1.4943) Acc G: 1.267% 
LR: 2.000e-04 

2023-03-02 01:46:22,643 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.2697 (0.2255) Acc D Real: 89.256% 
Loss D Fake: 0.2332 (0.3158) Acc D Fake: 98.055% 
Loss D: 0.503 
Loss G: 1.6339 (1.4952) Acc G: 1.260% 
LR: 2.000e-04 

2023-03-02 01:46:22,650 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.2245 (0.2255) Acc D Real: 89.252% 
Loss D Fake: 0.2344 (0.3153) Acc D Fake: 98.066% 
Loss D: 0.459 
Loss G: 1.6292 (1.4960) Acc G: 1.252% 
LR: 2.000e-04 

2023-03-02 01:46:22,658 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.2235 (0.2255) Acc D Real: 89.252% 
Loss D Fake: 0.2354 (0.3148) Acc D Fake: 98.078% 
Loss D: 0.459 
Loss G: 1.6252 (1.4967) Acc G: 1.245% 
LR: 2.000e-04 

2023-03-02 01:46:22,665 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.1841 (0.2252) Acc D Real: 89.262% 
Loss D Fake: 0.2362 (0.3143) Acc D Fake: 98.089% 
Loss D: 0.420 
Loss G: 1.6221 (1.4975) Acc G: 1.237% 
LR: 2.000e-04 

2023-03-02 01:46:22,673 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.2772 (0.2255) Acc D Real: 89.236% 
Loss D Fake: 0.2370 (0.3139) Acc D Fake: 98.100% 
Loss D: 0.514 
Loss G: 1.6190 (1.4982) Acc G: 1.230% 
LR: 2.000e-04 

2023-03-02 01:46:22,680 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3469 (0.2262) Acc D Real: 89.184% 
Loss D Fake: 0.2378 (0.3134) Acc D Fake: 98.111% 
Loss D: 0.585 
Loss G: 1.6153 (1.4988) Acc G: 1.223% 
LR: 2.000e-04 

2023-03-02 01:46:22,688 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.3222 (0.2268) Acc D Real: 89.147% 
Loss D Fake: 0.2389 (0.3130) Acc D Fake: 98.122% 
Loss D: 0.561 
Loss G: 1.6109 (1.4995) Acc G: 1.216% 
LR: 2.000e-04 

2023-03-02 01:46:22,695 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.3715 (0.2276) Acc D Real: 89.087% 
Loss D Fake: 0.2401 (0.3126) Acc D Fake: 98.133% 
Loss D: 0.612 
Loss G: 1.6053 (1.5001) Acc G: 1.209% 
LR: 2.000e-04 

2023-03-02 01:46:22,703 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.2381 (0.2277) Acc D Real: 89.080% 
Loss D Fake: 0.2415 (0.3122) Acc D Fake: 98.143% 
Loss D: 0.480 
Loss G: 1.6000 (1.5007) Acc G: 1.202% 
LR: 2.000e-04 

2023-03-02 01:46:22,710 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.2146 (0.2276) Acc D Real: 89.076% 
Loss D Fake: 0.2428 (0.3118) Acc D Fake: 98.154% 
Loss D: 0.457 
Loss G: 1.5957 (1.5012) Acc G: 1.195% 
LR: 2.000e-04 

2023-03-02 01:46:22,718 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2271 (0.2276) Acc D Real: 89.073% 
Loss D Fake: 0.2438 (0.3114) Acc D Fake: 98.164% 
Loss D: 0.471 
Loss G: 1.5922 (1.5017) Acc G: 1.189% 
LR: 2.000e-04 

2023-03-02 01:46:22,725 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2973 (0.2280) Acc D Real: 89.046% 
Loss D Fake: 0.2447 (0.3110) Acc D Fake: 98.175% 
Loss D: 0.542 
Loss G: 1.5885 (1.5022) Acc G: 1.182% 
LR: 2.000e-04 

2023-03-02 01:46:22,733 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.2810 (0.2283) Acc D Real: 89.024% 
Loss D Fake: 0.2457 (0.3107) Acc D Fake: 98.185% 
Loss D: 0.527 
Loss G: 1.5846 (1.5027) Acc G: 1.175% 
LR: 2.000e-04 

2023-03-02 01:46:22,740 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.2204 (0.2283) Acc D Real: 89.022% 
Loss D Fake: 0.2467 (0.3103) Acc D Fake: 98.195% 
Loss D: 0.467 
Loss G: 1.5810 (1.5031) Acc G: 1.169% 
LR: 2.000e-04 

2023-03-02 01:46:22,748 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3172 (0.2287) Acc D Real: 88.980% 
Loss D Fake: 0.2477 (0.3100) Acc D Fake: 98.205% 
Loss D: 0.565 
Loss G: 1.5773 (1.5035) Acc G: 1.162% 
LR: 2.000e-04 

2023-03-02 01:46:22,755 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.4019 (0.2297) Acc D Real: 88.914% 
Loss D Fake: 0.2487 (0.3096) Acc D Fake: 98.215% 
Loss D: 0.651 
Loss G: 1.5725 (1.5039) Acc G: 1.156% 
LR: 2.000e-04 

2023-03-02 01:46:22,763 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2338 (0.2297) Acc D Real: 88.909% 
Loss D Fake: 0.2500 (0.3093) Acc D Fake: 98.225% 
Loss D: 0.484 
Loss G: 1.5680 (1.5043) Acc G: 1.150% 
LR: 2.000e-04 

2023-03-02 01:46:22,770 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.1589 (0.2293) Acc D Real: 88.929% 
Loss D Fake: 0.2510 (0.3090) Acc D Fake: 98.234% 
Loss D: 0.410 
Loss G: 1.5650 (1.5046) Acc G: 1.143% 
LR: 2.000e-04 

2023-03-02 01:46:22,778 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2028 (0.2292) Acc D Real: 88.937% 
Loss D Fake: 0.2517 (0.3087) Acc D Fake: 98.244% 
Loss D: 0.455 
Loss G: 1.5628 (1.5049) Acc G: 1.137% 
LR: 2.000e-04 

2023-03-02 01:46:22,785 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.1130 (0.2286) Acc D Real: 88.967% 
Loss D Fake: 0.2522 (0.3084) Acc D Fake: 98.253% 
Loss D: 0.365 
Loss G: 1.5621 (1.5052) Acc G: 1.131% 
LR: 2.000e-04 

2023-03-02 01:46:22,793 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.1607 (0.2282) Acc D Real: 88.984% 
Loss D Fake: 0.2522 (0.3081) Acc D Fake: 98.263% 
Loss D: 0.413 
Loss G: 1.5626 (1.5055) Acc G: 1.125% 
LR: 2.000e-04 

2023-03-02 01:46:22,800 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3165 (0.2287) Acc D Real: 88.951% 
Loss D Fake: 0.2522 (0.3078) Acc D Fake: 98.272% 
Loss D: 0.569 
Loss G: 1.5624 (1.5058) Acc G: 1.119% 
LR: 2.000e-04 

2023-03-02 01:46:22,808 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.2340 (0.2287) Acc D Real: 88.946% 
Loss D Fake: 0.2524 (0.3075) Acc D Fake: 98.281% 
Loss D: 0.486 
Loss G: 1.5618 (1.5061) Acc G: 1.113% 
LR: 2.000e-04 

2023-03-02 01:46:22,816 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.3459 (0.2293) Acc D Real: 88.901% 
Loss D Fake: 0.2526 (0.3072) Acc D Fake: 98.290% 
Loss D: 0.599 
Loss G: 1.5602 (1.5064) Acc G: 1.107% 
LR: 2.000e-04 

2023-03-02 01:46:22,823 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.2948 (0.2297) Acc D Real: 88.872% 
Loss D Fake: 0.2532 (0.3069) Acc D Fake: 98.299% 
Loss D: 0.548 
Loss G: 1.5579 (1.5067) Acc G: 1.101% 
LR: 2.000e-04 

2023-03-02 01:46:22,831 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.1758 (0.2294) Acc D Real: 88.885% 
Loss D Fake: 0.2538 (0.3066) Acc D Fake: 98.308% 
Loss D: 0.430 
Loss G: 1.5561 (1.5069) Acc G: 1.096% 
LR: 2.000e-04 

2023-03-02 01:46:22,838 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.4105 (0.2303) Acc D Real: 88.815% 
Loss D Fake: 0.2544 (0.3064) Acc D Fake: 98.317% 
Loss D: 0.665 
Loss G: 1.5530 (1.5072) Acc G: 1.090% 
LR: 2.000e-04 

2023-03-02 01:46:22,845 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.1918 (0.2301) Acc D Real: 88.822% 
Loss D Fake: 0.2553 (0.3061) Acc D Fake: 98.325% 
Loss D: 0.447 
Loss G: 1.5504 (1.5074) Acc G: 1.084% 
LR: 2.000e-04 

2023-03-02 01:46:22,853 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.3055 (0.2305) Acc D Real: 88.787% 
Loss D Fake: 0.2560 (0.3058) Acc D Fake: 98.334% 
Loss D: 0.562 
Loss G: 1.5469 (1.5076) Acc G: 1.079% 
LR: 2.000e-04 

2023-03-02 01:46:22,860 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.2281 (0.2305) Acc D Real: 88.778% 
Loss D Fake: 0.2571 (0.3056) Acc D Fake: 98.342% 
Loss D: 0.485 
Loss G: 1.5436 (1.5078) Acc G: 1.073% 
LR: 2.000e-04 

2023-03-02 01:46:22,868 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3289 (0.2310) Acc D Real: 88.735% 
Loss D Fake: 0.2581 (0.3054) Acc D Fake: 98.351% 
Loss D: 0.587 
Loss G: 1.5401 (1.5079) Acc G: 1.068% 
LR: 2.000e-04 

2023-03-02 01:46:22,876 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2763 (0.2312) Acc D Real: 88.714% 
Loss D Fake: 0.2592 (0.3051) Acc D Fake: 98.359% 
Loss D: 0.535 
Loss G: 1.5366 (1.5081) Acc G: 1.062% 
LR: 2.000e-04 

2023-03-02 01:46:22,883 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.1614 (0.2309) Acc D Real: 88.734% 
Loss D Fake: 0.2601 (0.3049) Acc D Fake: 98.367% 
Loss D: 0.421 
Loss G: 1.5342 (1.5082) Acc G: 1.057% 
LR: 2.000e-04 

2023-03-02 01:46:22,891 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.1906 (0.2307) Acc D Real: 88.744% 
Loss D Fake: 0.2607 (0.3047) Acc D Fake: 98.376% 
Loss D: 0.451 
Loss G: 1.5327 (1.5083) Acc G: 1.052% 
LR: 2.000e-04 

2023-03-02 01:46:22,899 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2593 (0.2308) Acc D Real: 88.726% 
Loss D Fake: 0.2611 (0.3045) Acc D Fake: 98.384% 
Loss D: 0.520 
Loss G: 1.5317 (1.5084) Acc G: 1.047% 
LR: 2.000e-04 

2023-03-02 01:46:22,907 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.1394 (0.2304) Acc D Real: 88.754% 
Loss D Fake: 0.2614 (0.3042) Acc D Fake: 98.392% 
Loss D: 0.401 
Loss G: 1.5315 (1.5086) Acc G: 1.041% 
LR: 2.000e-04 

2023-03-02 01:46:22,914 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.1521 (0.2300) Acc D Real: 88.775% 
Loss D Fake: 0.2614 (0.3040) Acc D Fake: 98.400% 
Loss D: 0.414 
Loss G: 1.5322 (1.5087) Acc G: 1.036% 
LR: 2.000e-04 

2023-03-02 01:46:22,922 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.2399 (0.2300) Acc D Real: 88.765% 
Loss D Fake: 0.2614 (0.3038) Acc D Fake: 98.407% 
Loss D: 0.501 
Loss G: 1.5328 (1.5088) Acc G: 1.031% 
LR: 2.000e-04 

2023-03-02 01:46:22,929 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2664 (0.2302) Acc D Real: 88.748% 
Loss D Fake: 0.2614 (0.3036) Acc D Fake: 98.415% 
Loss D: 0.528 
Loss G: 1.5329 (1.5089) Acc G: 1.026% 
LR: 2.000e-04 

2023-03-02 01:46:22,937 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.2861 (0.2305) Acc D Real: 88.724% 
Loss D Fake: 0.2616 (0.3034) Acc D Fake: 98.423% 
Loss D: 0.548 
Loss G: 1.5317 (1.5090) Acc G: 1.021% 
LR: 2.000e-04 

2023-03-02 01:46:22,944 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.0932 (0.2298) Acc D Real: 88.762% 
Loss D Fake: 0.2620 (0.3032) Acc D Fake: 98.430% 
Loss D: 0.355 
Loss G: 1.5317 (1.5091) Acc G: 1.016% 
LR: 2.000e-04 

2023-03-02 01:46:22,951 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2230 (0.2298) Acc D Real: 88.754% 
Loss D Fake: 0.2621 (0.3030) Acc D Fake: 98.438% 
Loss D: 0.485 
Loss G: 1.5323 (1.5092) Acc G: 1.011% 
LR: 2.000e-04 

2023-03-02 01:46:22,959 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.1200 (0.2292) Acc D Real: 88.780% 
Loss D Fake: 0.2620 (0.3028) Acc D Fake: 98.445% 
Loss D: 0.382 
Loss G: 1.5339 (1.5094) Acc G: 1.007% 
LR: 2.000e-04 

2023-03-02 01:46:22,966 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.1681 (0.2290) Acc D Real: 88.792% 
Loss D Fake: 0.2616 (0.3026) Acc D Fake: 98.453% 
Loss D: 0.430 
Loss G: 1.5360 (1.5095) Acc G: 1.002% 
LR: 2.000e-04 

2023-03-02 01:46:22,973 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.1304 (0.2285) Acc D Real: 88.817% 
Loss D Fake: 0.2611 (0.3024) Acc D Fake: 98.460% 
Loss D: 0.391 
Loss G: 1.5390 (1.5096) Acc G: 0.997% 
LR: 2.000e-04 

2023-03-02 01:46:22,981 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.2546 (0.2286) Acc D Real: 88.804% 
Loss D Fake: 0.2604 (0.3022) Acc D Fake: 98.467% 
Loss D: 0.515 
Loss G: 1.5413 (1.5098) Acc G: 0.992% 
LR: 2.000e-04 

2023-03-02 01:46:22,988 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2545 (0.2287) Acc D Real: 88.791% 
Loss D Fake: 0.2600 (0.3020) Acc D Fake: 98.475% 
Loss D: 0.515 
Loss G: 1.5429 (1.5099) Acc G: 0.988% 
LR: 2.000e-04 

2023-03-02 01:46:22,995 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.2260 (0.2287) Acc D Real: 88.787% 
Loss D Fake: 0.2598 (0.3018) Acc D Fake: 98.482% 
Loss D: 0.486 
Loss G: 1.5437 (1.5101) Acc G: 0.983% 
LR: 2.000e-04 

2023-03-02 01:46:23,002 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.1836 (0.2285) Acc D Real: 88.793% 
Loss D Fake: 0.2598 (0.3016) Acc D Fake: 98.489% 
Loss D: 0.443 
Loss G: 1.5447 (1.5103) Acc G: 0.978% 
LR: 2.000e-04 

2023-03-02 01:46:23,009 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.1978 (0.2284) Acc D Real: 88.797% 
Loss D Fake: 0.2596 (0.3014) Acc D Fake: 98.496% 
Loss D: 0.457 
Loss G: 1.5458 (1.5104) Acc G: 0.974% 
LR: 2.000e-04 

2023-03-02 01:46:23,016 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2517 (0.2285) Acc D Real: 88.781% 
Loss D Fake: 0.2595 (0.3012) Acc D Fake: 98.503% 
Loss D: 0.511 
Loss G: 1.5465 (1.5106) Acc G: 0.969% 
LR: 2.000e-04 

2023-03-02 01:46:23,024 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.1820 (0.2283) Acc D Real: 88.788% 
Loss D Fake: 0.2595 (0.3011) Acc D Fake: 98.510% 
Loss D: 0.441 
Loss G: 1.5475 (1.5108) Acc G: 0.965% 
LR: 2.000e-04 

2023-03-02 01:46:23,031 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.0766 (0.2276) Acc D Real: 88.825% 
Loss D Fake: 0.2591 (0.3009) Acc D Fake: 98.516% 
Loss D: 0.336 
Loss G: 1.5501 (1.5109) Acc G: 0.961% 
LR: 2.000e-04 

2023-03-02 01:46:23,038 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.1631 (0.2273) Acc D Real: 88.837% 
Loss D Fake: 0.2584 (0.3007) Acc D Fake: 98.523% 
Loss D: 0.422 
Loss G: 1.5534 (1.5111) Acc G: 0.956% 
LR: 2.000e-04 

2023-03-02 01:46:23,045 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2386 (0.2273) Acc D Real: 88.825% 
Loss D Fake: 0.2578 (0.3005) Acc D Fake: 98.530% 
Loss D: 0.496 
Loss G: 1.5560 (1.5113) Acc G: 0.952% 
LR: 2.000e-04 

2023-03-02 01:46:23,052 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2601 (0.2275) Acc D Real: 88.812% 
Loss D Fake: 0.2573 (0.3003) Acc D Fake: 98.537% 
Loss D: 0.517 
Loss G: 1.5577 (1.5115) Acc G: 0.948% 
LR: 2.000e-04 

2023-03-02 01:46:23,059 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2163 (0.2274) Acc D Real: 88.806% 
Loss D Fake: 0.2570 (0.3001) Acc D Fake: 98.543% 
Loss D: 0.473 
Loss G: 1.5592 (1.5118) Acc G: 0.943% 
LR: 2.000e-04 

2023-03-02 01:46:23,067 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.1461 (0.2271) Acc D Real: 88.825% 
Loss D Fake: 0.2567 (0.2999) Acc D Fake: 98.550% 
Loss D: 0.403 
Loss G: 1.5613 (1.5120) Acc G: 0.939% 
LR: 2.000e-04 

2023-03-02 01:46:23,074 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.1787 (0.2268) Acc D Real: 88.832% 
Loss D Fake: 0.2561 (0.2997) Acc D Fake: 98.556% 
Loss D: 0.435 
Loss G: 1.5636 (1.5122) Acc G: 0.935% 
LR: 2.000e-04 

2023-03-02 01:46:23,081 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.2808 (0.2271) Acc D Real: 88.827% 
Loss D Fake: 0.2557 (0.2995) Acc D Fake: 98.558% 
Loss D: 0.536 
Loss G: 1.5652 (1.5124) Acc G: 0.934% 
LR: 2.000e-04 

2023-03-02 01:46:23,091 -                train: [    INFO] - 
Epoch: 19/20
2023-03-02 01:46:23,262 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.2062 (0.1761) Acc D Real: 91.719% 
Loss D Fake: 0.2549 (0.2551) Acc D Fake: 100.000% 
Loss D: 0.461 
Loss G: 1.5688 (1.5680) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,270 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.1916 (0.1812) Acc D Real: 91.181% 
Loss D Fake: 0.2546 (0.2549) Acc D Fake: 100.000% 
Loss D: 0.446 
Loss G: 1.5705 (1.5688) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,278 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.1617 (0.1764) Acc D Real: 91.419% 
Loss D Fake: 0.2541 (0.2547) Acc D Fake: 100.000% 
Loss D: 0.416 
Loss G: 1.5726 (1.5698) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,295 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.2117 (0.1834) Acc D Real: 90.969% 
Loss D Fake: 0.2537 (0.2545) Acc D Fake: 100.000% 
Loss D: 0.465 
Loss G: 1.5744 (1.5707) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,302 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.1461 (0.1772) Acc D Real: 91.293% 
Loss D Fake: 0.2532 (0.2543) Acc D Fake: 100.000% 
Loss D: 0.399 
Loss G: 1.5767 (1.5717) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,309 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.1305 (0.1705) Acc D Real: 91.734% 
Loss D Fake: 0.2526 (0.2541) Acc D Fake: 100.000% 
Loss D: 0.383 
Loss G: 1.5796 (1.5728) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,317 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.1257 (0.1649) Acc D Real: 92.064% 
Loss D Fake: 0.2519 (0.2538) Acc D Fake: 100.000% 
Loss D: 0.378 
Loss G: 1.5831 (1.5741) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,324 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2848 (0.1782) Acc D Real: 91.175% 
Loss D Fake: 0.2512 (0.2535) Acc D Fake: 100.000% 
Loss D: 0.536 
Loss G: 1.5856 (1.5754) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,331 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.2264 (0.1831) Acc D Real: 90.792% 
Loss D Fake: 0.2507 (0.2532) Acc D Fake: 100.000% 
Loss D: 0.477 
Loss G: 1.5875 (1.5766) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,338 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.1999 (0.1846) Acc D Real: 90.587% 
Loss D Fake: 0.2503 (0.2530) Acc D Fake: 100.000% 
Loss D: 0.450 
Loss G: 1.5890 (1.5777) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,345 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.1344 (0.1804) Acc D Real: 90.864% 
Loss D Fake: 0.2501 (0.2527) Acc D Fake: 100.000% 
Loss D: 0.384 
Loss G: 1.5908 (1.5788) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,352 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.1039 (0.1745) Acc D Real: 91.242% 
Loss D Fake: 0.2497 (0.2525) Acc D Fake: 100.000% 
Loss D: 0.354 
Loss G: 1.5935 (1.5800) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,359 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.1914 (0.1757) Acc D Real: 91.138% 
Loss D Fake: 0.2491 (0.2522) Acc D Fake: 100.000% 
Loss D: 0.441 
Loss G: 1.5961 (1.5811) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,366 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.1623 (0.1748) Acc D Real: 91.198% 
Loss D Fake: 0.2487 (0.2520) Acc D Fake: 100.000% 
Loss D: 0.411 
Loss G: 1.5985 (1.5823) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,373 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3061 (0.1830) Acc D Real: 90.684% 
Loss D Fake: 0.2484 (0.2518) Acc D Fake: 100.000% 
Loss D: 0.555 
Loss G: 1.5995 (1.5833) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,381 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.1423 (0.1806) Acc D Real: 90.803% 
Loss D Fake: 0.2483 (0.2516) Acc D Fake: 100.000% 
Loss D: 0.391 
Loss G: 1.6006 (1.5844) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,387 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.1801 (0.1806) Acc D Real: 90.755% 
Loss D Fake: 0.2482 (0.2514) Acc D Fake: 100.000% 
Loss D: 0.428 
Loss G: 1.6020 (1.5853) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,394 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.1930 (0.1813) Acc D Real: 90.694% 
Loss D Fake: 0.2482 (0.2512) Acc D Fake: 100.000% 
Loss D: 0.441 
Loss G: 1.6018 (1.5862) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,401 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.1485 (0.1796) Acc D Real: 90.807% 
Loss D Fake: 0.2489 (0.2511) Acc D Fake: 100.000% 
Loss D: 0.397 
Loss G: 1.6015 (1.5870) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,408 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.1074 (0.1762) Acc D Real: 91.019% 
Loss D Fake: 0.2494 (0.2510) Acc D Fake: 100.000% 
Loss D: 0.357 
Loss G: 1.6023 (1.5877) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,415 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1184 (0.1736) Acc D Real: 91.203% 
Loss D Fake: 0.2496 (0.2510) Acc D Fake: 100.000% 
Loss D: 0.368 
Loss G: 1.6039 (1.5884) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,422 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.1335 (0.1718) Acc D Real: 91.293% 
Loss D Fake: 0.2495 (0.2509) Acc D Fake: 100.000% 
Loss D: 0.383 
Loss G: 1.6061 (1.5892) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,429 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.1743 (0.1719) Acc D Real: 91.324% 
Loss D Fake: 0.2493 (0.2508) Acc D Fake: 100.000% 
Loss D: 0.424 
Loss G: 1.6083 (1.5900) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,436 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.1201 (0.1698) Acc D Real: 91.467% 
Loss D Fake: 0.2491 (0.2508) Acc D Fake: 100.000% 
Loss D: 0.369 
Loss G: 1.6111 (1.5908) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,443 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.1078 (0.1675) Acc D Real: 91.617% 
Loss D Fake: 0.2486 (0.2507) Acc D Fake: 100.000% 
Loss D: 0.356 
Loss G: 1.6148 (1.5918) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,450 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.1518 (0.1669) Acc D Real: 91.657% 
Loss D Fake: 0.2479 (0.2506) Acc D Fake: 100.000% 
Loss D: 0.400 
Loss G: 1.6188 (1.5928) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,457 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.1326 (0.1657) Acc D Real: 91.717% 
Loss D Fake: 0.2471 (0.2505) Acc D Fake: 100.000% 
Loss D: 0.380 
Loss G: 1.6231 (1.5939) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,464 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.1611 (0.1655) Acc D Real: 91.749% 
Loss D Fake: 0.2462 (0.2503) Acc D Fake: 100.000% 
Loss D: 0.407 
Loss G: 1.6274 (1.5950) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,472 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.1717 (0.1657) Acc D Real: 91.757% 
Loss D Fake: 0.2453 (0.2501) Acc D Fake: 100.000% 
Loss D: 0.417 
Loss G: 1.6311 (1.5962) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,479 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.1050 (0.1638) Acc D Real: 91.880% 
Loss D Fake: 0.2446 (0.2500) Acc D Fake: 100.000% 
Loss D: 0.350 
Loss G: 1.6353 (1.5975) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,487 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1036 (0.1619) Acc D Real: 92.005% 
Loss D Fake: 0.2436 (0.2498) Acc D Fake: 100.000% 
Loss D: 0.347 
Loss G: 1.6402 (1.5988) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,495 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2577 (0.1648) Acc D Real: 91.848% 
Loss D Fake: 0.2426 (0.2495) Acc D Fake: 100.000% 
Loss D: 0.500 
Loss G: 1.6440 (1.6002) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,502 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.1428 (0.1641) Acc D Real: 91.884% 
Loss D Fake: 0.2418 (0.2493) Acc D Fake: 100.000% 
Loss D: 0.385 
Loss G: 1.6476 (1.6016) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,510 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.0893 (0.1620) Acc D Real: 92.013% 
Loss D Fake: 0.2410 (0.2491) Acc D Fake: 100.000% 
Loss D: 0.330 
Loss G: 1.6521 (1.6030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,517 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.1271 (0.1610) Acc D Real: 92.060% 
Loss D Fake: 0.2399 (0.2488) Acc D Fake: 100.000% 
Loss D: 0.367 
Loss G: 1.6570 (1.6045) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,524 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.1509 (0.1608) Acc D Real: 92.044% 
Loss D Fake: 0.2388 (0.2486) Acc D Fake: 100.000% 
Loss D: 0.390 
Loss G: 1.6616 (1.6061) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,532 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2545 (0.1632) Acc D Real: 91.902% 
Loss D Fake: 0.2380 (0.2483) Acc D Fake: 100.000% 
Loss D: 0.492 
Loss G: 1.6646 (1.6076) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,539 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.1253 (0.1622) Acc D Real: 91.942% 
Loss D Fake: 0.2375 (0.2480) Acc D Fake: 100.000% 
Loss D: 0.363 
Loss G: 1.6677 (1.6091) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,547 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.1799 (0.1627) Acc D Real: 91.915% 
Loss D Fake: 0.2368 (0.2477) Acc D Fake: 100.000% 
Loss D: 0.417 
Loss G: 1.6709 (1.6107) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,554 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.2139 (0.1639) Acc D Real: 91.852% 
Loss D Fake: 0.2362 (0.2474) Acc D Fake: 100.000% 
Loss D: 0.450 
Loss G: 1.6735 (1.6122) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,562 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.1570 (0.1638) Acc D Real: 91.855% 
Loss D Fake: 0.2357 (0.2472) Acc D Fake: 100.000% 
Loss D: 0.393 
Loss G: 1.6759 (1.6137) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,569 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.1718 (0.1640) Acc D Real: 91.850% 
Loss D Fake: 0.2353 (0.2469) Acc D Fake: 100.000% 
Loss D: 0.407 
Loss G: 1.6781 (1.6152) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,576 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.2151 (0.1651) Acc D Real: 91.776% 
Loss D Fake: 0.2349 (0.2466) Acc D Fake: 100.000% 
Loss D: 0.450 
Loss G: 1.6799 (1.6167) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,584 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.1405 (0.1646) Acc D Real: 91.787% 
Loss D Fake: 0.2345 (0.2463) Acc D Fake: 100.000% 
Loss D: 0.375 
Loss G: 1.6821 (1.6182) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,591 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.1887 (0.1651) Acc D Real: 91.770% 
Loss D Fake: 0.2341 (0.2461) Acc D Fake: 100.000% 
Loss D: 0.423 
Loss G: 1.6841 (1.6196) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,599 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.1765 (0.1653) Acc D Real: 91.746% 
Loss D Fake: 0.2336 (0.2458) Acc D Fake: 100.000% 
Loss D: 0.410 
Loss G: 1.6862 (1.6210) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,606 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.1014 (0.1640) Acc D Real: 91.836% 
Loss D Fake: 0.2332 (0.2455) Acc D Fake: 100.000% 
Loss D: 0.335 
Loss G: 1.6890 (1.6224) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,613 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.2516 (0.1658) Acc D Real: 91.729% 
Loss D Fake: 0.2326 (0.2453) Acc D Fake: 100.000% 
Loss D: 0.484 
Loss G: 1.6909 (1.6238) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,622 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.1240 (0.1650) Acc D Real: 91.756% 
Loss D Fake: 0.2435 (0.2452) Acc D Fake: 100.000% 
Loss D: 0.367 
Loss G: 0.2742 (1.5968) Acc G: 1.733% 
LR: 2.000e-04 

2023-03-02 01:46:23,629 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.1553 (0.1648) Acc D Real: 91.773% 
Loss D Fake: 1.6103 (0.2720) Acc D Fake: 98.268% 
Loss D: 1.766 
Loss G: 0.2708 (1.5708) Acc G: 3.431% 
LR: 2.000e-04 

2023-03-02 01:46:23,637 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.0915 (0.1634) Acc D Real: 91.859% 
Loss D Fake: 1.5937 (0.2974) Acc D Fake: 96.603% 
Loss D: 1.685 
Loss G: 0.2772 (1.5459) Acc G: 5.096% 
LR: 2.000e-04 

2023-03-02 01:46:23,646 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.1055 (0.1623) Acc D Real: 91.929% 
Loss D Fake: 1.5640 (0.3213) Acc D Fake: 94.969% 
Loss D: 1.669 
Loss G: 0.2869 (1.5222) Acc G: 6.698% 
LR: 2.000e-04 

2023-03-02 01:46:23,654 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.1374 (0.1618) Acc D Real: 91.976% 
Loss D Fake: 1.5273 (0.3437) Acc D Fake: 93.395% 
Loss D: 1.665 
Loss G: 0.2986 (1.4995) Acc G: 8.241% 
LR: 2.000e-04 

2023-03-02 01:46:23,662 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.1309 (0.1612) Acc D Real: 92.015% 
Loss D Fake: 1.4865 (0.3644) Acc D Fake: 91.879% 
Loss D: 1.617 
Loss G: 0.3121 (1.4779) Acc G: 9.667% 
LR: 2.000e-04 

2023-03-02 01:46:23,669 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.0972 (0.1601) Acc D Real: 92.082% 
Loss D Fake: 1.4422 (0.3837) Acc D Fake: 90.506% 
Loss D: 1.539 
Loss G: 0.3273 (1.4574) Acc G: 11.012% 
LR: 2.000e-04 

2023-03-02 01:46:23,677 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.1512 (0.1599) Acc D Real: 92.109% 
Loss D Fake: 1.3951 (0.4014) Acc D Fake: 89.211% 
Loss D: 1.546 
Loss G: 0.3438 (1.4379) Acc G: 12.281% 
LR: 2.000e-04 

2023-03-02 01:46:23,685 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.2013 (0.1607) Acc D Real: 92.053% 
Loss D Fake: 1.3471 (0.4177) Acc D Fake: 87.960% 
Loss D: 1.548 
Loss G: 0.3614 (1.4193) Acc G: 13.477% 
LR: 2.000e-04 

2023-03-02 01:46:23,692 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.1924 (0.1612) Acc D Real: 92.033% 
Loss D Fake: 1.2992 (0.4327) Acc D Fake: 86.780% 
Loss D: 1.492 
Loss G: 0.3799 (1.4017) Acc G: 14.633% 
LR: 2.000e-04 

2023-03-02 01:46:23,701 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.1482 (0.1610) Acc D Real: 92.046% 
Loss D Fake: 1.2503 (0.4463) Acc D Fake: 85.667% 
Loss D: 1.399 
Loss G: 0.4003 (1.3850) Acc G: 15.722% 
LR: 2.000e-04 

2023-03-02 01:46:23,708 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.1794 (0.1613) Acc D Real: 92.019% 
Loss D Fake: 1.1988 (0.4586) Acc D Fake: 84.590% 
Loss D: 1.378 
Loss G: 0.4228 (1.3692) Acc G: 16.749% 
LR: 2.000e-04 

2023-03-02 01:46:23,716 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.1841 (0.1616) Acc D Real: 91.983% 
Loss D Fake: 1.1439 (0.4697) Acc D Fake: 83.575% 
Loss D: 1.328 
Loss G: 0.4488 (1.3544) Acc G: 17.715% 
LR: 2.000e-04 

2023-03-02 01:46:23,724 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.0753 (0.1603) Acc D Real: 92.063% 
Loss D Fake: 1.0786 (0.4794) Acc D Fake: 82.619% 
Loss D: 1.154 
Loss G: 0.4841 (1.3406) Acc G: 18.624% 
LR: 2.000e-04 

2023-03-02 01:46:23,732 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.1395 (0.1600) Acc D Real: 92.091% 
Loss D Fake: 1.0006 (0.4875) Acc D Fake: 81.719% 
Loss D: 1.140 
Loss G: 0.5201 (1.3277) Acc G: 19.479% 
LR: 2.000e-04 

2023-03-02 01:46:23,740 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.2762 (0.1617) Acc D Real: 92.007% 
Loss D Fake: 0.9437 (0.4945) Acc D Fake: 80.897% 
Loss D: 1.220 
Loss G: 0.5524 (1.3158) Acc G: 20.282% 
LR: 2.000e-04 

2023-03-02 01:46:23,747 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.2086 (0.1625) Acc D Real: 91.986% 
Loss D Fake: 0.8934 (0.5006) Acc D Fake: 80.126% 
Loss D: 1.102 
Loss G: 0.5867 (1.3048) Acc G: 21.010% 
LR: 2.000e-04 

2023-03-02 01:46:23,755 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.2715 (0.1641) Acc D Real: 91.885% 
Loss D Fake: 0.8440 (0.5057) Acc D Fake: 79.428% 
Loss D: 1.115 
Loss G: 0.6229 (1.2946) Acc G: 21.667% 
LR: 2.000e-04 

2023-03-02 01:46:23,763 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.2428 (0.1652) Acc D Real: 91.821% 
Loss D Fake: 0.7968 (0.5100) Acc D Fake: 78.799% 
Loss D: 1.040 
Loss G: 0.6614 (1.2853) Acc G: 22.230% 
LR: 2.000e-04 

2023-03-02 01:46:23,771 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.2811 (0.1669) Acc D Real: 91.707% 
Loss D Fake: 0.7506 (0.5135) Acc D Fake: 78.261% 
Loss D: 1.032 
Loss G: 0.7038 (1.2768) Acc G: 22.705% 
LR: 2.000e-04 

2023-03-02 01:46:23,778 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.1618 (0.1668) Acc D Real: 91.702% 
Loss D Fake: 0.7009 (0.5161) Acc D Fake: 77.833% 
Loss D: 0.863 
Loss G: 0.7627 (1.2695) Acc G: 23.024% 
LR: 2.000e-04 

2023-03-02 01:46:23,786 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2636 (0.1682) Acc D Real: 91.563% 
Loss D Fake: 0.6358 (0.5178) Acc D Fake: 77.582% 
Loss D: 0.899 
Loss G: 0.8511 (1.2636) Acc G: 23.099% 
LR: 2.000e-04 

2023-03-02 01:46:23,793 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3428 (0.1706) Acc D Real: 91.348% 
Loss D Fake: 0.5414 (0.5181) Acc D Fake: 77.662% 
Loss D: 0.884 
Loss G: 0.9986 (1.2599) Acc G: 22.778% 
LR: 2.000e-04 

2023-03-02 01:46:23,801 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3687 (0.1733) Acc D Real: 91.109% 
Loss D Fake: 0.4343 (0.5170) Acc D Fake: 77.968% 
Loss D: 0.803 
Loss G: 1.1338 (1.2582) Acc G: 22.466% 
LR: 2.000e-04 

2023-03-02 01:46:23,808 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3934 (0.1763) Acc D Real: 90.838% 
Loss D Fake: 0.3825 (0.5152) Acc D Fake: 78.266% 
Loss D: 0.776 
Loss G: 1.1757 (1.2571) Acc G: 22.162% 
LR: 2.000e-04 

2023-03-02 01:46:23,816 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3374 (0.1785) Acc D Real: 90.636% 
Loss D Fake: 0.3721 (0.5133) Acc D Fake: 78.556% 
Loss D: 0.709 
Loss G: 1.1934 (1.2562) Acc G: 21.867% 
LR: 2.000e-04 

2023-03-02 01:46:23,823 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.3691 (0.1810) Acc D Real: 90.401% 
Loss D Fake: 0.3663 (0.5113) Acc D Fake: 78.838% 
Loss D: 0.735 
Loss G: 1.2049 (1.2556) Acc G: 21.579% 
LR: 2.000e-04 

2023-03-02 01:46:23,831 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.3440 (0.1831) Acc D Real: 90.183% 
Loss D Fake: 0.3625 (0.5094) Acc D Fake: 79.113% 
Loss D: 0.706 
Loss G: 1.2125 (1.2550) Acc G: 21.299% 
LR: 2.000e-04 

2023-03-02 01:46:23,838 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4539 (0.1866) Acc D Real: 89.846% 
Loss D Fake: 0.3605 (0.5075) Acc D Fake: 79.380% 
Loss D: 0.814 
Loss G: 1.2147 (1.2545) Acc G: 21.026% 
LR: 2.000e-04 

2023-03-02 01:46:23,846 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.3604 (0.1888) Acc D Real: 89.630% 
Loss D Fake: 0.3602 (0.5056) Acc D Fake: 79.641% 
Loss D: 0.721 
Loss G: 1.2147 (1.2540) Acc G: 20.759% 
LR: 2.000e-04 

2023-03-02 01:46:23,853 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.4202 (0.1917) Acc D Real: 89.345% 
Loss D Fake: 0.3606 (0.5038) Acc D Fake: 79.896% 
Loss D: 0.781 
Loss G: 1.2125 (1.2535) Acc G: 20.500% 
LR: 2.000e-04 

2023-03-02 01:46:23,861 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.3828 (0.1940) Acc D Real: 89.104% 
Loss D Fake: 0.3618 (0.5021) Acc D Fake: 80.144% 
Loss D: 0.745 
Loss G: 1.2088 (1.2529) Acc G: 20.247% 
LR: 2.000e-04 

2023-03-02 01:46:23,868 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3808 (0.1963) Acc D Real: 88.892% 
Loss D Fake: 0.3635 (0.5004) Acc D Fake: 80.386% 
Loss D: 0.744 
Loss G: 1.2046 (1.2523) Acc G: 20.000% 
LR: 2.000e-04 

2023-03-02 01:46:23,876 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3876 (0.1986) Acc D Real: 88.667% 
Loss D Fake: 0.3653 (0.4988) Acc D Fake: 80.622% 
Loss D: 0.753 
Loss G: 1.1996 (1.2517) Acc G: 19.759% 
LR: 2.000e-04 

2023-03-02 01:46:23,883 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.2749 (0.1995) Acc D Real: 88.550% 
Loss D Fake: 0.3672 (0.4972) Acc D Fake: 80.853% 
Loss D: 0.642 
Loss G: 1.1957 (1.2510) Acc G: 19.524% 
LR: 2.000e-04 

2023-03-02 01:46:23,891 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.2407 (0.2000) Acc D Real: 88.468% 
Loss D Fake: 0.3685 (0.4957) Acc D Fake: 81.078% 
Loss D: 0.609 
Loss G: 1.1938 (1.2503) Acc G: 19.294% 
LR: 2.000e-04 

2023-03-02 01:46:23,898 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.4156 (0.2025) Acc D Real: 88.224% 
Loss D Fake: 0.3693 (0.4942) Acc D Fake: 81.298% 
Loss D: 0.785 
Loss G: 1.1907 (1.2497) Acc G: 19.070% 
LR: 2.000e-04 

2023-03-02 01:46:23,906 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.5334 (0.2063) Acc D Real: 87.872% 
Loss D Fake: 0.3712 (0.4928) Acc D Fake: 81.513% 
Loss D: 0.905 
Loss G: 1.1839 (1.2489) Acc G: 18.851% 
LR: 2.000e-04 

2023-03-02 01:46:23,913 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3581 (0.2080) Acc D Real: 87.696% 
Loss D Fake: 0.3743 (0.4914) Acc D Fake: 81.723% 
Loss D: 0.732 
Loss G: 1.1769 (1.2481) Acc G: 18.636% 
LR: 2.000e-04 

2023-03-02 01:46:23,921 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3825 (0.2100) Acc D Real: 87.495% 
Loss D Fake: 0.3772 (0.4902) Acc D Fake: 81.929% 
Loss D: 0.760 
Loss G: 1.1699 (1.2472) Acc G: 18.427% 
LR: 2.000e-04 

2023-03-02 01:46:23,928 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.3063 (0.2111) Acc D Real: 87.366% 
Loss D Fake: 0.3801 (0.4889) Acc D Fake: 82.130% 
Loss D: 0.686 
Loss G: 1.1639 (1.2463) Acc G: 18.222% 
LR: 2.000e-04 

2023-03-02 01:46:23,936 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3402 (0.2125) Acc D Real: 87.220% 
Loss D Fake: 0.3825 (0.4878) Acc D Fake: 82.326% 
Loss D: 0.723 
Loss G: 1.1587 (1.2453) Acc G: 18.022% 
LR: 2.000e-04 

2023-03-02 01:46:23,944 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.3744 (0.2142) Acc D Real: 87.033% 
Loss D Fake: 0.3847 (0.4866) Acc D Fake: 82.518% 
Loss D: 0.759 
Loss G: 1.1533 (1.2443) Acc G: 17.826% 
LR: 2.000e-04 

2023-03-02 01:46:23,952 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3542 (0.2157) Acc D Real: 86.858% 
Loss D Fake: 0.3872 (0.4856) Acc D Fake: 82.706% 
Loss D: 0.741 
Loss G: 1.1476 (1.2433) Acc G: 17.634% 
LR: 2.000e-04 

2023-03-02 01:46:23,959 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.5780 (0.2196) Acc D Real: 86.492% 
Loss D Fake: 0.3903 (0.4846) Acc D Fake: 82.890% 
Loss D: 0.968 
Loss G: 1.1381 (1.2422) Acc G: 17.447% 
LR: 2.000e-04 

2023-03-02 01:46:23,967 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.3469 (0.2209) Acc D Real: 86.332% 
Loss D Fake: 0.3950 (0.4836) Acc D Fake: 83.070% 
Loss D: 0.742 
Loss G: 1.1279 (1.2410) Acc G: 17.263% 
LR: 2.000e-04 

2023-03-02 01:46:23,974 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2172 (0.2209) Acc D Real: 86.306% 
Loss D Fake: 0.3992 (0.4827) Acc D Fake: 83.247% 
Loss D: 0.616 
Loss G: 1.1203 (1.2397) Acc G: 17.083% 
LR: 2.000e-04 

2023-03-02 01:46:23,982 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.4483 (0.2232) Acc D Real: 86.064% 
Loss D Fake: 0.4026 (0.4819) Acc D Fake: 83.419% 
Loss D: 0.851 
Loss G: 1.1122 (1.2384) Acc G: 16.907% 
LR: 2.000e-04 

2023-03-02 01:46:23,989 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2887 (0.2239) Acc D Real: 85.964% 
Loss D Fake: 0.4064 (0.4811) Acc D Fake: 83.588% 
Loss D: 0.695 
Loss G: 1.1046 (1.2370) Acc G: 16.735% 
LR: 2.000e-04 

2023-03-02 01:46:23,997 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2961 (0.2246) Acc D Real: 85.863% 
Loss D Fake: 0.4096 (0.4804) Acc D Fake: 83.754% 
Loss D: 0.706 
Loss G: 1.0986 (1.2356) Acc G: 16.566% 
LR: 2.000e-04 

2023-03-02 01:46:24,005 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.2482 (0.2249) Acc D Real: 85.806% 
Loss D Fake: 0.4121 (0.4797) Acc D Fake: 83.917% 
Loss D: 0.660 
Loss G: 1.0943 (1.2342) Acc G: 16.400% 
LR: 2.000e-04 

2023-03-02 01:46:24,012 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.2684 (0.2253) Acc D Real: 85.736% 
Loss D Fake: 0.4145 (0.4791) Acc D Fake: 84.076% 
Loss D: 0.683 
Loss G: 1.0875 (1.2328) Acc G: 16.238% 
LR: 2.000e-04 

2023-03-02 01:46:24,021 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.3024 (0.2261) Acc D Real: 85.639% 
Loss D Fake: 0.4181 (0.4785) Acc D Fake: 84.232% 
Loss D: 0.721 
Loss G: 1.0802 (1.2313) Acc G: 16.078% 
LR: 2.000e-04 

2023-03-02 01:46:24,028 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4160 (0.2279) Acc D Real: 85.437% 
Loss D Fake: 0.4229 (0.4780) Acc D Fake: 84.385% 
Loss D: 0.839 
Loss G: 1.0645 (1.2296) Acc G: 15.922% 
LR: 2.000e-04 

2023-03-02 01:46:24,036 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.3250 (0.2288) Acc D Real: 85.323% 
Loss D Fake: 0.4374 (0.4776) Acc D Fake: 84.535% 
Loss D: 0.762 
Loss G: 1.0084 (1.2275) Acc G: 15.769% 
LR: 2.000e-04 

2023-03-02 01:46:24,044 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.2457 (0.2290) Acc D Real: 85.285% 
Loss D Fake: 0.4983 (0.4778) Acc D Fake: 84.683% 
Loss D: 0.744 
Loss G: 0.9757 (1.2251) Acc G: 15.619% 
LR: 2.000e-04 

2023-03-02 01:46:24,054 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.2295 (0.2290) Acc D Real: 85.254% 
Loss D Fake: 0.4712 (0.4777) Acc D Fake: 84.827% 
Loss D: 0.701 
Loss G: 1.0573 (1.2235) Acc G: 15.472% 
LR: 2.000e-04 

2023-03-02 01:46:24,062 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.3299 (0.2299) Acc D Real: 85.151% 
Loss D Fake: 0.4246 (0.4772) Acc D Fake: 84.969% 
Loss D: 0.754 
Loss G: 1.0775 (1.2222) Acc G: 15.327% 
LR: 2.000e-04 

2023-03-02 01:46:24,069 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.4727 (0.2322) Acc D Real: 84.939% 
Loss D Fake: 0.4194 (0.4767) Acc D Fake: 85.108% 
Loss D: 0.892 
Loss G: 1.0832 (1.2209) Acc G: 15.185% 
LR: 2.000e-04 

2023-03-02 01:46:24,077 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3355 (0.2331) Acc D Real: 84.851% 
Loss D Fake: 0.4182 (0.4761) Acc D Fake: 85.245% 
Loss D: 0.754 
Loss G: 1.0834 (1.2196) Acc G: 15.046% 
LR: 2.000e-04 

2023-03-02 01:46:24,084 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.4480 (0.2351) Acc D Real: 84.668% 
Loss D Fake: 0.4192 (0.4756) Acc D Fake: 85.379% 
Loss D: 0.867 
Loss G: 1.0791 (1.2183) Acc G: 14.909% 
LR: 2.000e-04 

2023-03-02 01:46:24,092 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.5424 (0.2379) Acc D Real: 84.388% 
Loss D Fake: 0.4233 (0.4751) Acc D Fake: 85.511% 
Loss D: 0.966 
Loss G: 1.0648 (1.2170) Acc G: 14.775% 
LR: 2.000e-04 

2023-03-02 01:46:24,099 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3595 (0.2389) Acc D Real: 84.293% 
Loss D Fake: 0.4334 (0.4748) Acc D Fake: 85.640% 
Loss D: 0.793 
Loss G: 1.0382 (1.2154) Acc G: 14.643% 
LR: 2.000e-04 

2023-03-02 01:46:24,107 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.3853 (0.2402) Acc D Real: 84.168% 
Loss D Fake: 0.4689 (0.4747) Acc D Fake: 85.767% 
Loss D: 0.854 
Loss G: 1.0437 (1.2138) Acc G: 14.513% 
LR: 2.000e-04 

2023-03-02 01:46:24,114 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.2590 (0.2404) Acc D Real: 84.152% 
Loss D Fake: 0.4335 (0.4744) Acc D Fake: 85.892% 
Loss D: 0.692 
Loss G: 1.0592 (1.2125) Acc G: 14.386% 
LR: 2.000e-04 

2023-03-02 01:46:24,122 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.2880 (0.2408) Acc D Real: 84.106% 
Loss D Fake: 0.4286 (0.4740) Acc D Fake: 86.014% 
Loss D: 0.717 
Loss G: 1.0656 (1.2112) Acc G: 14.261% 
LR: 2.000e-04 

2023-03-02 01:46:24,129 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.2798 (0.2412) Acc D Real: 84.078% 
Loss D Fake: 0.4262 (0.4735) Acc D Fake: 86.135% 
Loss D: 0.706 
Loss G: 1.0698 (1.2100) Acc G: 14.138% 
LR: 2.000e-04 

2023-03-02 01:46:24,137 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2303 (0.2411) Acc D Real: 84.075% 
Loss D Fake: 0.4243 (0.4731) Acc D Fake: 86.254% 
Loss D: 0.655 
Loss G: 1.0732 (1.2088) Acc G: 14.017% 
LR: 2.000e-04 

2023-03-02 01:46:24,144 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.3616 (0.2421) Acc D Real: 83.977% 
Loss D Fake: 0.4234 (0.4727) Acc D Fake: 86.370% 
Loss D: 0.785 
Loss G: 1.0723 (1.2077) Acc G: 13.898% 
LR: 2.000e-04 

2023-03-02 01:46:24,152 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.2467 (0.2421) Acc D Real: 83.974% 
Loss D Fake: 0.4247 (0.4723) Acc D Fake: 86.485% 
Loss D: 0.671 
Loss G: 1.0693 (1.2065) Acc G: 13.782% 
LR: 2.000e-04 

2023-03-02 01:46:24,159 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.3298 (0.2429) Acc D Real: 83.909% 
Loss D Fake: 0.4268 (0.4719) Acc D Fake: 86.597% 
Loss D: 0.757 
Loss G: 1.0631 (1.2053) Acc G: 13.667% 
LR: 2.000e-04 

2023-03-02 01:46:24,166 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.3222 (0.2435) Acc D Real: 83.827% 
Loss D Fake: 0.4312 (0.4716) Acc D Fake: 86.708% 
Loss D: 0.753 
Loss G: 1.0522 (1.2040) Acc G: 13.554% 
LR: 2.000e-04 

2023-03-02 01:46:24,174 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.3545 (0.2444) Acc D Real: 83.722% 
Loss D Fake: 0.4397 (0.4713) Acc D Fake: 86.817% 
Loss D: 0.794 
Loss G: 1.0350 (1.2027) Acc G: 13.443% 
LR: 2.000e-04 

2023-03-02 01:46:24,181 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2786 (0.2447) Acc D Real: 83.684% 
Loss D Fake: 0.4568 (0.4712) Acc D Fake: 86.924% 
Loss D: 0.735 
Loss G: 1.0648 (1.2015) Acc G: 13.333% 
LR: 2.000e-04 

2023-03-02 01:46:24,189 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.3716 (0.2457) Acc D Real: 83.585% 
Loss D Fake: 0.4213 (0.4708) Acc D Fake: 87.030% 
Loss D: 0.793 
Loss G: 1.0860 (1.2006) Acc G: 13.226% 
LR: 2.000e-04 

2023-03-02 01:46:24,196 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3284 (0.2464) Acc D Real: 83.517% 
Loss D Fake: 0.4144 (0.4704) Acc D Fake: 87.133% 
Loss D: 0.743 
Loss G: 1.0968 (1.1998) Acc G: 13.120% 
LR: 2.000e-04 

2023-03-02 01:46:24,204 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.2988 (0.2468) Acc D Real: 83.462% 
Loss D Fake: 0.4104 (0.4699) Acc D Fake: 87.235% 
Loss D: 0.709 
Loss G: 1.1034 (1.1990) Acc G: 13.016% 
LR: 2.000e-04 

2023-03-02 01:46:24,211 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.2721 (0.2470) Acc D Real: 83.432% 
Loss D Fake: 0.4081 (0.4694) Acc D Fake: 87.336% 
Loss D: 0.680 
Loss G: 1.1070 (1.1983) Acc G: 12.913% 
LR: 2.000e-04 

2023-03-02 01:46:24,219 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.3476 (0.2478) Acc D Real: 83.331% 
Loss D Fake: 0.4070 (0.4689) Acc D Fake: 87.435% 
Loss D: 0.755 
Loss G: 1.1081 (1.1976) Acc G: 12.812% 
LR: 2.000e-04 

2023-03-02 01:46:24,226 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3340 (0.2485) Acc D Real: 83.257% 
Loss D Fake: 0.4080 (0.4684) Acc D Fake: 87.532% 
Loss D: 0.742 
Loss G: 1.1012 (1.1968) Acc G: 12.713% 
LR: 2.000e-04 

2023-03-02 01:46:24,233 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3876 (0.2495) Acc D Real: 83.160% 
Loss D Fake: 0.4131 (0.4680) Acc D Fake: 87.628% 
Loss D: 0.801 
Loss G: 1.0870 (1.1960) Acc G: 12.615% 
LR: 2.000e-04 

2023-03-02 01:46:24,241 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3721 (0.2505) Acc D Real: 83.053% 
Loss D Fake: 0.4242 (0.4677) Acc D Fake: 87.723% 
Loss D: 0.796 
Loss G: 0.9122 (1.1938) Acc G: 12.659% 
LR: 2.000e-04 

2023-03-02 01:46:24,249 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.3404 (0.2511) Acc D Real: 83.009% 
Loss D Fake: 0.7021 (0.4694) Acc D Fake: 87.462% 
Loss D: 1.042 
Loss G: 0.7173 (1.1902) Acc G: 12.942% 
LR: 2.000e-04 

2023-03-02 01:46:24,257 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.2918 (0.2514) Acc D Real: 82.987% 
Loss D Fake: 0.7451 (0.4715) Acc D Fake: 87.155% 
Loss D: 1.037 
Loss G: 0.6980 (1.1865) Acc G: 13.246% 
LR: 2.000e-04 

2023-03-02 01:46:24,264 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.3949 (0.2525) Acc D Real: 82.910% 
Loss D Fake: 0.7457 (0.4736) Acc D Fake: 86.847% 
Loss D: 1.141 
Loss G: 0.7099 (1.1830) Acc G: 13.532% 
LR: 2.000e-04 

2023-03-02 01:46:24,272 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.2475 (0.2525) Acc D Real: 82.922% 
Loss D Fake: 0.7196 (0.4754) Acc D Fake: 86.574% 
Loss D: 0.967 
Loss G: 0.7706 (1.1799) Acc G: 13.741% 
LR: 2.000e-04 

2023-03-02 01:46:24,280 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.2249 (0.2523) Acc D Real: 82.933% 
Loss D Fake: 0.6971 (0.4770) Acc D Fake: 86.329% 
Loss D: 0.922 
Loss G: 0.7108 (1.1764) Acc G: 14.020% 
LR: 2.000e-04 

2023-03-02 01:46:24,287 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.2011 (0.2519) Acc D Real: 82.957% 
Loss D Fake: 0.7482 (0.4790) Acc D Fake: 86.015% 
Loss D: 0.949 
Loss G: 0.6828 (1.1728) Acc G: 14.331% 
LR: 2.000e-04 

2023-03-02 01:46:24,295 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.2939 (0.2522) Acc D Real: 82.962% 
Loss D Fake: 0.7565 (0.4810) Acc D Fake: 85.694% 
Loss D: 1.050 
Loss G: 0.6785 (1.1693) Acc G: 14.650% 
LR: 2.000e-04 

2023-03-02 01:46:24,303 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.4294 (0.2535) Acc D Real: 82.905% 
Loss D Fake: 0.7546 (0.4830) Acc D Fake: 85.365% 
Loss D: 1.184 
Loss G: 0.6781 (1.1657) Acc G: 14.976% 
LR: 2.000e-04 

2023-03-02 01:46:24,311 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.2070 (0.2532) Acc D Real: 82.955% 
Loss D Fake: 0.7502 (0.4849) Acc D Fake: 85.041% 
Loss D: 0.957 
Loss G: 0.6845 (1.1623) Acc G: 15.286% 
LR: 2.000e-04 

2023-03-02 01:46:24,318 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3516 (0.2538) Acc D Real: 82.916% 
Loss D Fake: 0.7398 (0.4867) Acc D Fake: 84.734% 
Loss D: 1.091 
Loss G: 0.6940 (1.1590) Acc G: 15.591% 
LR: 2.000e-04 

2023-03-02 01:46:24,325 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.3416 (0.2545) Acc D Real: 82.909% 
Loss D Fake: 0.7287 (0.4884) Acc D Fake: 84.442% 
Loss D: 1.070 
Loss G: 0.7033 (1.1558) Acc G: 15.880% 
LR: 2.000e-04 

2023-03-02 01:46:24,333 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2531 (0.2545) Acc D Real: 82.920% 
Loss D Fake: 0.7173 (0.4900) Acc D Fake: 84.166% 
Loss D: 0.970 
Loss G: 0.7163 (1.1527) Acc G: 16.154% 
LR: 2.000e-04 

2023-03-02 01:46:24,341 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.2867 (0.2547) Acc D Real: 82.920% 
Loss D Fake: 0.7032 (0.4915) Acc D Fake: 83.906% 
Loss D: 0.990 
Loss G: 0.7289 (1.1497) Acc G: 16.400% 
LR: 2.000e-04 

2023-03-02 01:46:24,348 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.1657 (0.2541) Acc D Real: 82.965% 
Loss D Fake: 0.6887 (0.4928) Acc D Fake: 83.661% 
Loss D: 0.854 
Loss G: 0.7474 (1.1470) Acc G: 16.632% 
LR: 2.000e-04 

2023-03-02 01:46:24,355 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.3319 (0.2546) Acc D Real: 82.929% 
Loss D Fake: 0.6692 (0.4940) Acc D Fake: 83.441% 
Loss D: 1.001 
Loss G: 0.7679 (1.1444) Acc G: 16.826% 
LR: 2.000e-04 

2023-03-02 01:46:24,363 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.3601 (0.2553) Acc D Real: 82.866% 
Loss D Fake: 0.6506 (0.4951) Acc D Fake: 83.259% 
Loss D: 1.011 
Loss G: 0.7878 (1.1419) Acc G: 16.995% 
LR: 2.000e-04 

2023-03-02 01:46:24,371 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.3319 (0.2558) Acc D Real: 82.822% 
Loss D Fake: 0.6334 (0.4960) Acc D Fake: 83.102% 
Loss D: 0.965 
Loss G: 0.8047 (1.1397) Acc G: 17.128% 
LR: 2.000e-04 

2023-03-02 01:46:24,378 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3225 (0.2563) Acc D Real: 82.770% 
Loss D Fake: 0.6188 (0.4969) Acc D Fake: 82.969% 
Loss D: 0.941 
Loss G: 0.8211 (1.1375) Acc G: 17.237% 
LR: 2.000e-04 

2023-03-02 01:46:24,386 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3499 (0.2569) Acc D Real: 82.727% 
Loss D Fake: 0.6022 (0.4976) Acc D Fake: 82.872% 
Loss D: 0.952 
Loss G: 0.8445 (1.1356) Acc G: 17.300% 
LR: 2.000e-04 

2023-03-02 01:46:24,393 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.3113 (0.2573) Acc D Real: 82.678% 
Loss D Fake: 0.5793 (0.4981) Acc D Fake: 82.831% 
Loss D: 0.891 
Loss G: 0.8762 (1.1339) Acc G: 17.307% 
LR: 2.000e-04 

2023-03-02 01:46:24,401 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2742 (0.2574) Acc D Real: 82.648% 
Loss D Fake: 0.5486 (0.4984) Acc D Fake: 82.856% 
Loss D: 0.823 
Loss G: 0.9236 (1.1325) Acc G: 17.215% 
LR: 2.000e-04 

2023-03-02 01:46:24,410 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.3399 (0.2579) Acc D Real: 82.587% 
Loss D Fake: 0.5085 (0.4985) Acc D Fake: 82.968% 
Loss D: 0.848 
Loss G: 0.9726 (1.1314) Acc G: 17.102% 
LR: 2.000e-04 

2023-03-02 01:46:24,417 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.4709 (0.2593) Acc D Real: 82.460% 
Loss D Fake: 0.4761 (0.4984) Acc D Fake: 83.079% 
Loss D: 0.947 
Loss G: 1.0420 (1.1308) Acc G: 16.991% 
LR: 2.000e-04 

2023-03-02 01:46:24,425 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.3612 (0.2600) Acc D Real: 82.383% 
Loss D Fake: 0.4182 (0.4978) Acc D Fake: 83.188% 
Loss D: 0.779 
Loss G: 1.1562 (1.1310) Acc G: 16.882% 
LR: 2.000e-04 

2023-03-02 01:46:24,433 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3913 (0.2608) Acc D Real: 82.294% 
Loss D Fake: 0.3799 (0.4971) Acc D Fake: 83.296% 
Loss D: 0.771 
Loss G: 1.1728 (1.1313) Acc G: 16.774% 
LR: 2.000e-04 

2023-03-02 01:46:24,440 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.3060 (0.2611) Acc D Real: 82.242% 
Loss D Fake: 0.3761 (0.4963) Acc D Fake: 83.402% 
Loss D: 0.682 
Loss G: 1.1784 (1.1316) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:46:24,448 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3392 (0.2616) Acc D Real: 82.176% 
Loss D Fake: 0.3746 (0.4956) Acc D Fake: 83.507% 
Loss D: 0.714 
Loss G: 1.1805 (1.1319) Acc G: 16.561% 
LR: 2.000e-04 

2023-03-02 01:46:24,455 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.3657 (0.2622) Acc D Real: 82.108% 
Loss D Fake: 0.3742 (0.4948) Acc D Fake: 83.611% 
Loss D: 0.740 
Loss G: 1.1805 (1.1322) Acc G: 16.457% 
LR: 2.000e-04 

2023-03-02 01:46:24,463 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.3881 (0.2630) Acc D Real: 82.041% 
Loss D Fake: 0.3752 (0.4940) Acc D Fake: 83.713% 
Loss D: 0.763 
Loss G: 1.1749 (1.1325) Acc G: 16.354% 
LR: 2.000e-04 

2023-03-02 01:46:24,470 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.3429 (0.2635) Acc D Real: 81.981% 
Loss D Fake: 0.3785 (0.4933) Acc D Fake: 83.814% 
Loss D: 0.721 
Loss G: 1.1657 (1.1327) Acc G: 16.253% 
LR: 2.000e-04 

2023-03-02 01:46:24,477 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.2359 (0.2634) Acc D Real: 81.974% 
Loss D Fake: 0.3827 (0.4926) Acc D Fake: 83.914% 
Loss D: 0.619 
Loss G: 1.1558 (1.1328) Acc G: 16.152% 
LR: 2.000e-04 

2023-03-02 01:46:24,484 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.3558 (0.2639) Acc D Real: 81.900% 
Loss D Fake: 0.3876 (0.4920) Acc D Fake: 84.013% 
Loss D: 0.743 
Loss G: 1.1417 (1.1329) Acc G: 16.053% 
LR: 2.000e-04 

2023-03-02 01:46:24,492 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2794 (0.2640) Acc D Real: 81.876% 
Loss D Fake: 0.3971 (0.4914) Acc D Fake: 84.110% 
Loss D: 0.676 
Loss G: 1.0981 (1.1327) Acc G: 15.955% 
LR: 2.000e-04 

2023-03-02 01:46:24,500 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.1589 (0.2634) Acc D Real: 81.924% 
Loss D Fake: 0.4367 (0.4911) Acc D Fake: 84.207% 
Loss D: 0.596 
Loss G: 1.0984 (1.1324) Acc G: 15.859% 
LR: 2.000e-04 

2023-03-02 01:46:24,508 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.3036 (0.2636) Acc D Real: 81.888% 
Loss D Fake: 0.3991 (0.4905) Acc D Fake: 84.302% 
Loss D: 0.703 
Loss G: 1.1405 (1.1325) Acc G: 15.763% 
LR: 2.000e-04 

2023-03-02 01:46:24,517 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.4004 (0.2644) Acc D Real: 81.798% 
Loss D Fake: 0.3891 (0.4899) Acc D Fake: 84.396% 
Loss D: 0.789 
Loss G: 1.1497 (1.1326) Acc G: 15.669% 
LR: 2.000e-04 

2023-03-02 01:46:24,525 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3085 (0.2647) Acc D Real: 81.771% 
Loss D Fake: 0.3867 (0.4893) Acc D Fake: 84.489% 
Loss D: 0.695 
Loss G: 1.1531 (1.1327) Acc G: 15.575% 
LR: 2.000e-04 

2023-03-02 01:46:24,533 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.3956 (0.2655) Acc D Real: 81.691% 
Loss D Fake: 0.3859 (0.4887) Acc D Fake: 84.581% 
Loss D: 0.782 
Loss G: 1.1535 (1.1328) Acc G: 15.483% 
LR: 2.000e-04 

2023-03-02 01:46:24,541 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.3703 (0.2661) Acc D Real: 81.637% 
Loss D Fake: 0.3864 (0.4881) Acc D Fake: 84.671% 
Loss D: 0.757 
Loss G: 1.1512 (1.1330) Acc G: 15.392% 
LR: 2.000e-04 

2023-03-02 01:46:24,549 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.3127 (0.2664) Acc D Real: 81.606% 
Loss D Fake: 0.3878 (0.4875) Acc D Fake: 84.761% 
Loss D: 0.700 
Loss G: 1.1474 (1.1330) Acc G: 15.302% 
LR: 2.000e-04 

2023-03-02 01:46:24,558 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3745 (0.2670) Acc D Real: 81.536% 
Loss D Fake: 0.3899 (0.4869) Acc D Fake: 84.850% 
Loss D: 0.764 
Loss G: 1.1412 (1.1331) Acc G: 15.213% 
LR: 2.000e-04 

2023-03-02 01:46:24,568 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.2736 (0.2670) Acc D Real: 81.519% 
Loss D Fake: 0.3931 (0.4864) Acc D Fake: 84.937% 
Loss D: 0.667 
Loss G: 1.1344 (1.1331) Acc G: 15.125% 
LR: 2.000e-04 

2023-03-02 01:46:24,577 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.2714 (0.2671) Acc D Real: 81.504% 
Loss D Fake: 0.3963 (0.4859) Acc D Fake: 85.024% 
Loss D: 0.668 
Loss G: 1.1272 (1.1331) Acc G: 15.038% 
LR: 2.000e-04 

2023-03-02 01:46:24,587 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3817 (0.2677) Acc D Real: 81.449% 
Loss D Fake: 0.4017 (0.4854) Acc D Fake: 85.109% 
Loss D: 0.783 
Loss G: 1.1167 (1.1330) Acc G: 14.952% 
LR: 2.000e-04 

2023-03-02 01:46:24,596 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3573 (0.2682) Acc D Real: 81.403% 
Loss D Fake: 0.4060 (0.4849) Acc D Fake: 85.194% 
Loss D: 0.763 
Loss G: 1.1256 (1.1329) Acc G: 14.867% 
LR: 2.000e-04 

2023-03-02 01:46:24,606 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2634 (0.2682) Acc D Real: 81.399% 
Loss D Fake: 0.3963 (0.4844) Acc D Fake: 85.277% 
Loss D: 0.660 
Loss G: 1.1340 (1.1329) Acc G: 14.783% 
LR: 2.000e-04 

2023-03-02 01:46:24,615 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2695 (0.2682) Acc D Real: 81.400% 
Loss D Fake: 0.3937 (0.4839) Acc D Fake: 85.360% 
Loss D: 0.663 
Loss G: 1.1384 (1.1330) Acc G: 14.700% 
LR: 2.000e-04 

2023-03-02 01:46:24,625 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.4720 (0.2693) Acc D Real: 81.289% 
Loss D Fake: 0.3932 (0.4834) Acc D Fake: 85.442% 
Loss D: 0.865 
Loss G: 1.1342 (1.1330) Acc G: 14.618% 
LR: 2.000e-04 

2023-03-02 01:46:24,633 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.2984 (0.2695) Acc D Real: 81.271% 
Loss D Fake: 0.3966 (0.4829) Acc D Fake: 85.523% 
Loss D: 0.695 
Loss G: 1.1256 (1.1329) Acc G: 14.537% 
LR: 2.000e-04 

2023-03-02 01:46:24,641 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.3716 (0.2701) Acc D Real: 81.212% 
Loss D Fake: 0.4040 (0.4825) Acc D Fake: 85.603% 
Loss D: 0.776 
Loss G: 1.0756 (1.1326) Acc G: 14.457% 
LR: 2.000e-04 

2023-03-02 01:46:24,649 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.3557 (0.2705) Acc D Real: 81.173% 
Loss D Fake: 0.4666 (0.4824) Acc D Fake: 85.682% 
Loss D: 0.822 
Loss G: 1.1128 (1.1325) Acc G: 14.377% 
LR: 2.000e-04 

2023-03-02 01:46:24,656 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2465 (0.2704) Acc D Real: 81.181% 
Loss D Fake: 0.3990 (0.4820) Acc D Fake: 85.760% 
Loss D: 0.646 
Loss G: 1.1312 (1.1325) Acc G: 14.299% 
LR: 2.000e-04 

2023-03-02 01:46:24,664 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.3004 (0.2706) Acc D Real: 81.161% 
Loss D Fake: 0.3948 (0.4815) Acc D Fake: 85.838% 
Loss D: 0.695 
Loss G: 1.1359 (1.1325) Acc G: 14.221% 
LR: 2.000e-04 

2023-03-02 01:46:24,671 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.3225 (0.2708) Acc D Real: 81.133% 
Loss D Fake: 0.3939 (0.4810) Acc D Fake: 85.914% 
Loss D: 0.716 
Loss G: 1.1363 (1.1325) Acc G: 14.144% 
LR: 2.000e-04 

2023-03-02 01:46:24,679 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.3428 (0.2712) Acc D Real: 81.095% 
Loss D Fake: 0.3944 (0.4806) Acc D Fake: 85.990% 
Loss D: 0.737 
Loss G: 1.1344 (1.1325) Acc G: 14.068% 
LR: 2.000e-04 

2023-03-02 01:46:24,686 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.2604 (0.2712) Acc D Real: 81.092% 
Loss D Fake: 0.3957 (0.4801) Acc D Fake: 86.065% 
Loss D: 0.656 
Loss G: 1.1307 (1.1325) Acc G: 13.993% 
LR: 2.000e-04 

2023-03-02 01:46:24,694 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.2735 (0.2712) Acc D Real: 81.090% 
Loss D Fake: 0.3976 (0.4797) Acc D Fake: 86.139% 
Loss D: 0.671 
Loss G: 1.1264 (1.1325) Acc G: 13.918% 
LR: 2.000e-04 

2023-03-02 01:46:24,701 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.3557 (0.2716) Acc D Real: 81.048% 
Loss D Fake: 0.4005 (0.4792) Acc D Fake: 86.212% 
Loss D: 0.756 
Loss G: 1.1150 (1.1324) Acc G: 13.845% 
LR: 2.000e-04 

2023-03-02 01:46:24,708 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4099 (0.2724) Acc D Real: 80.994% 
Loss D Fake: 0.4225 (0.4789) Acc D Fake: 86.285% 
Loss D: 0.832 
Loss G: 1.1214 (1.1323) Acc G: 13.772% 
LR: 2.000e-04 

2023-03-02 01:46:24,717 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.2799 (0.2724) Acc D Real: 80.991% 
Loss D Fake: 0.3972 (0.4785) Acc D Fake: 86.357% 
Loss D: 0.677 
Loss G: 1.1350 (1.1324) Acc G: 13.700% 
LR: 2.000e-04 

2023-03-02 01:46:24,724 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.2816 (0.2724) Acc D Real: 80.986% 
Loss D Fake: 0.3933 (0.4781) Acc D Fake: 86.428% 
Loss D: 0.675 
Loss G: 1.1407 (1.1324) Acc G: 13.628% 
LR: 2.000e-04 

2023-03-02 01:46:24,732 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.1944 (0.2720) Acc D Real: 81.028% 
Loss D Fake: 0.3914 (0.4776) Acc D Fake: 86.498% 
Loss D: 0.586 
Loss G: 1.1450 (1.1325) Acc G: 13.558% 
LR: 2.000e-04 

2023-03-02 01:46:24,739 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.2986 (0.2722) Acc D Real: 81.019% 
Loss D Fake: 0.3899 (0.4772) Acc D Fake: 86.568% 
Loss D: 0.689 
Loss G: 1.1470 (1.1325) Acc G: 13.488% 
LR: 2.000e-04 

2023-03-02 01:46:24,747 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.2178 (0.2719) Acc D Real: 81.050% 
Loss D Fake: 0.3893 (0.4767) Acc D Fake: 86.636% 
Loss D: 0.607 
Loss G: 1.1490 (1.1326) Acc G: 13.419% 
LR: 2.000e-04 

2023-03-02 01:46:24,754 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.1747 (0.2714) Acc D Real: 81.086% 
Loss D Fake: 0.3882 (0.4763) Acc D Fake: 86.705% 
Loss D: 0.563 
Loss G: 1.1523 (1.1327) Acc G: 13.350% 
LR: 2.000e-04 

2023-03-02 01:46:24,762 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3869 (0.2720) Acc D Real: 81.039% 
Loss D Fake: 0.3870 (0.4758) Acc D Fake: 86.772% 
Loss D: 0.774 
Loss G: 1.1535 (1.1328) Acc G: 13.283% 
LR: 2.000e-04 

2023-03-02 01:46:24,769 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.2272 (0.2718) Acc D Real: 81.056% 
Loss D Fake: 0.3871 (0.4754) Acc D Fake: 86.839% 
Loss D: 0.614 
Loss G: 1.1527 (1.1329) Acc G: 13.215% 
LR: 2.000e-04 

2023-03-02 01:46:24,777 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.2095 (0.2715) Acc D Real: 81.092% 
Loss D Fake: 0.3875 (0.4749) Acc D Fake: 86.905% 
Loss D: 0.597 
Loss G: 1.1520 (1.1330) Acc G: 13.149% 
LR: 2.000e-04 

2023-03-02 01:46:24,784 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3736 (0.2720) Acc D Real: 81.041% 
Loss D Fake: 0.3886 (0.4745) Acc D Fake: 86.971% 
Loss D: 0.762 
Loss G: 1.1451 (1.1331) Acc G: 13.083% 
LR: 2.000e-04 

2023-03-02 01:46:24,792 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.3186 (0.2722) Acc D Real: 81.022% 
Loss D Fake: 0.3995 (0.4741) Acc D Fake: 87.035% 
Loss D: 0.718 
Loss G: 0.8592 (1.1317) Acc G: 13.167% 
LR: 2.000e-04 

2023-03-02 01:46:24,800 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.2363 (0.2720) Acc D Real: 81.039% 
Loss D Fake: 0.7011 (0.4752) Acc D Fake: 86.869% 
Loss D: 0.937 
Loss G: 0.7799 (1.1300) Acc G: 13.309% 
LR: 2.000e-04 

2023-03-02 01:46:24,807 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.3136 (0.2722) Acc D Real: 81.040% 
Loss D Fake: 0.6528 (0.4761) Acc D Fake: 86.744% 
Loss D: 0.966 
Loss G: 1.1497 (1.1301) Acc G: 13.243% 
LR: 2.000e-04 

2023-03-02 01:46:24,815 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.1621 (0.2717) Acc D Real: 81.100% 
Loss D Fake: 0.3797 (0.4756) Acc D Fake: 86.809% 
Loss D: 0.542 
Loss G: 1.1841 (1.1304) Acc G: 13.178% 
LR: 2.000e-04 

2023-03-02 01:46:24,822 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.3498 (0.2721) Acc D Real: 81.063% 
Loss D Fake: 0.3701 (0.4751) Acc D Fake: 86.874% 
Loss D: 0.720 
Loss G: 1.2012 (1.1307) Acc G: 13.114% 
LR: 2.000e-04 

2023-03-02 01:46:24,830 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.1892 (0.2717) Acc D Real: 81.097% 
Loss D Fake: 0.3650 (0.4746) Acc D Fake: 86.937% 
Loss D: 0.554 
Loss G: 1.2103 (1.1311) Acc G: 13.050% 
LR: 2.000e-04 

2023-03-02 01:46:24,837 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.4837 (0.2727) Acc D Real: 81.009% 
Loss D Fake: 0.3627 (0.4741) Acc D Fake: 87.001% 
Loss D: 0.846 
Loss G: 1.2123 (1.1315) Acc G: 12.987% 
LR: 2.000e-04 

2023-03-02 01:46:24,845 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.4121 (0.2734) Acc D Real: 80.945% 
Loss D Fake: 0.3629 (0.4735) Acc D Fake: 87.063% 
Loss D: 0.775 
Loss G: 1.2095 (1.1318) Acc G: 12.925% 
LR: 2.000e-04 

2023-03-02 01:46:24,852 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.2623 (0.2733) Acc D Real: 80.948% 
Loss D Fake: 0.3644 (0.4730) Acc D Fake: 87.125% 
Loss D: 0.627 
Loss G: 1.2063 (1.1322) Acc G: 12.863% 
LR: 2.000e-04 

2023-03-02 01:46:24,860 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.4085 (0.2739) Acc D Real: 80.893% 
Loss D Fake: 0.3658 (0.4725) Acc D Fake: 87.186% 
Loss D: 0.774 
Loss G: 1.2015 (1.1325) Acc G: 12.802% 
LR: 2.000e-04 

2023-03-02 01:46:24,867 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.3754 (0.2744) Acc D Real: 80.848% 
Loss D Fake: 0.3682 (0.4720) Acc D Fake: 87.247% 
Loss D: 0.744 
Loss G: 1.1947 (1.1328) Acc G: 12.741% 
LR: 2.000e-04 

2023-03-02 01:46:24,875 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.4096 (0.2751) Acc D Real: 80.791% 
Loss D Fake: 0.3717 (0.4715) Acc D Fake: 87.307% 
Loss D: 0.781 
Loss G: 1.1835 (1.1331) Acc G: 12.681% 
LR: 2.000e-04 

2023-03-02 01:46:24,882 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.3672 (0.2755) Acc D Real: 80.746% 
Loss D Fake: 0.3775 (0.4711) Acc D Fake: 87.367% 
Loss D: 0.745 
Loss G: 1.1662 (1.1332) Acc G: 12.621% 
LR: 2.000e-04 

2023-03-02 01:46:24,890 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3222 (0.2757) Acc D Real: 80.728% 
Loss D Fake: 0.3858 (0.4707) Acc D Fake: 87.426% 
Loss D: 0.708 
Loss G: 1.1455 (1.1333) Acc G: 12.562% 
LR: 2.000e-04 

2023-03-02 01:46:24,897 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.2947 (0.2758) Acc D Real: 80.708% 
Loss D Fake: 0.3963 (0.4703) Acc D Fake: 87.484% 
Loss D: 0.691 
Loss G: 1.1128 (1.1332) Acc G: 12.504% 
LR: 2.000e-04 

2023-03-02 01:46:24,904 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.3540 (0.2762) Acc D Real: 80.681% 
Loss D Fake: 0.4903 (0.4704) Acc D Fake: 87.488% 
Loss D: 0.844 
Loss G: 1.1292 (1.1332) Acc G: 12.446% 
LR: 2.000e-04 

2023-03-02 01:46:24,912 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2977 (0.2763) Acc D Real: 80.676% 
Loss D Fake: 0.3926 (0.4701) Acc D Fake: 87.546% 
Loss D: 0.690 
Loss G: 1.1458 (1.1332) Acc G: 12.389% 
LR: 2.000e-04 

2023-03-02 01:46:24,919 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2835 (0.2763) Acc D Real: 80.663% 
Loss D Fake: 0.3894 (0.4697) Acc D Fake: 87.603% 
Loss D: 0.673 
Loss G: 1.1496 (1.1333) Acc G: 12.332% 
LR: 2.000e-04 

2023-03-02 01:46:24,927 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2583 (0.2762) Acc D Real: 80.662% 
Loss D Fake: 0.3884 (0.4693) Acc D Fake: 87.660% 
Loss D: 0.647 
Loss G: 1.1516 (1.1334) Acc G: 12.275% 
LR: 2.000e-04 

2023-03-02 01:46:24,934 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.3558 (0.2766) Acc D Real: 80.623% 
Loss D Fake: 0.3878 (0.4690) Acc D Fake: 87.716% 
Loss D: 0.744 
Loss G: 1.1519 (1.1335) Acc G: 12.220% 
LR: 2.000e-04 

2023-03-02 01:46:24,941 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.4716 (0.2775) Acc D Real: 80.542% 
Loss D Fake: 0.3883 (0.4686) Acc D Fake: 87.771% 
Loss D: 0.860 
Loss G: 1.1486 (1.1335) Acc G: 12.164% 
LR: 2.000e-04 

2023-03-02 01:46:24,949 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.3950 (0.2780) Acc D Real: 80.501% 
Loss D Fake: 0.3902 (0.4682) Acc D Fake: 87.826% 
Loss D: 0.785 
Loss G: 1.1438 (1.1336) Acc G: 12.110% 
LR: 2.000e-04 

2023-03-02 01:46:24,956 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.4211 (0.2786) Acc D Real: 80.439% 
Loss D Fake: 0.3928 (0.4679) Acc D Fake: 87.881% 
Loss D: 0.814 
Loss G: 1.1366 (1.1336) Acc G: 12.055% 
LR: 2.000e-04 

2023-03-02 01:46:24,964 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.4995 (0.2796) Acc D Real: 80.340% 
Loss D Fake: 0.3967 (0.4676) Acc D Fake: 87.935% 
Loss D: 0.896 
Loss G: 1.1262 (1.1336) Acc G: 12.001% 
LR: 2.000e-04 

2023-03-02 01:46:24,971 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.3321 (0.2799) Acc D Real: 80.314% 
Loss D Fake: 0.4016 (0.4673) Acc D Fake: 87.989% 
Loss D: 0.734 
Loss G: 1.1165 (1.1335) Acc G: 11.948% 
LR: 2.000e-04 

2023-03-02 01:46:24,979 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.1898 (0.2795) Acc D Real: 80.320% 
Loss D Fake: 0.4053 (0.4670) Acc D Fake: 88.002% 
Loss D: 0.595 
Loss G: 1.1109 (1.1334) Acc G: 11.935% 
LR: 2.000e-04 

2023-03-02 01:46:24,989 -                train: [    INFO] - 
Epoch: 20/20
2023-03-02 01:46:25,162 -                train: [    INFO] - TRAIN Iteration: [   2/226] 
Loss D Real: 0.3607 (0.3048) Acc D Real: 75.156% 
Loss D Fake: 0.4084 (0.4077) Acc D Fake: 100.000% 
Loss D: 0.769 
Loss G: 1.1050 (1.1065) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,169 -                train: [    INFO] - TRAIN Iteration: [   3/226] 
Loss D Real: 0.2764 (0.2953) Acc D Real: 75.781% 
Loss D Fake: 0.4099 (0.4084) Acc D Fake: 100.000% 
Loss D: 0.686 
Loss G: 1.1022 (1.1051) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,176 -                train: [    INFO] - TRAIN Iteration: [   4/226] 
Loss D Real: 0.2178 (0.2760) Acc D Real: 77.721% 
Loss D Fake: 0.4110 (0.4091) Acc D Fake: 100.000% 
Loss D: 0.629 
Loss G: 1.1001 (1.1038) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,203 -                train: [    INFO] - TRAIN Iteration: [   5/226] 
Loss D Real: 0.3113 (0.2830) Acc D Real: 77.198% 
Loss D Fake: 0.4123 (0.4097) Acc D Fake: 100.000% 
Loss D: 0.724 
Loss G: 1.0964 (1.1023) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,210 -                train: [    INFO] - TRAIN Iteration: [   6/226] 
Loss D Real: 0.4312 (0.3077) Acc D Real: 75.061% 
Loss D Fake: 0.4157 (0.4107) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 1.0833 (1.0992) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,217 -                train: [    INFO] - TRAIN Iteration: [   7/226] 
Loss D Real: 0.3348 (0.3116) Acc D Real: 74.874% 
Loss D Fake: 0.4241 (0.4126) Acc D Fake: 100.000% 
Loss D: 0.759 
Loss G: 1.0638 (1.0941) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:46:25,224 -                train: [    INFO] - TRAIN Iteration: [   8/226] 
Loss D Real: 0.2209 (0.3003) Acc D Real: 76.302% 
Loss D Fake: 0.4368 (0.4157) Acc D Fake: 100.000% 
Loss D: 0.658 
Loss G: 0.9072 (1.0707) Acc G: 1.667% 
LR: 2.000e-04 

2023-03-02 01:46:25,231 -                train: [    INFO] - TRAIN Iteration: [   9/226] 
Loss D Real: 0.2845 (0.2985) Acc D Real: 76.395% 
Loss D Fake: 0.7391 (0.4516) Acc D Fake: 95.556% 
Loss D: 1.024 
Loss G: 1.0262 (1.0658) Acc G: 1.481% 
LR: 2.000e-04 

2023-03-02 01:46:25,239 -                train: [    INFO] - TRAIN Iteration: [  10/226] 
Loss D Real: 0.1990 (0.2885) Acc D Real: 77.490% 
Loss D Fake: 0.4291 (0.4494) Acc D Fake: 96.000% 
Loss D: 0.628 
Loss G: 1.0791 (1.0671) Acc G: 1.333% 
LR: 2.000e-04 

2023-03-02 01:46:25,246 -                train: [    INFO] - TRAIN Iteration: [  11/226] 
Loss D Real: 0.2630 (0.2862) Acc D Real: 77.798% 
Loss D Fake: 0.4166 (0.4464) Acc D Fake: 96.364% 
Loss D: 0.680 
Loss G: 1.0978 (1.0699) Acc G: 1.212% 
LR: 2.000e-04 

2023-03-02 01:46:25,253 -                train: [    INFO] - TRAIN Iteration: [  12/226] 
Loss D Real: 0.2623 (0.2842) Acc D Real: 78.095% 
Loss D Fake: 0.4094 (0.4433) Acc D Fake: 96.667% 
Loss D: 0.672 
Loss G: 1.1107 (1.0733) Acc G: 1.111% 
LR: 2.000e-04 

2023-03-02 01:46:25,260 -                train: [    INFO] - TRAIN Iteration: [  13/226] 
Loss D Real: 0.2232 (0.2795) Acc D Real: 78.454% 
Loss D Fake: 0.4044 (0.4403) Acc D Fake: 96.923% 
Loss D: 0.628 
Loss G: 1.1192 (1.0768) Acc G: 1.026% 
LR: 2.000e-04 

2023-03-02 01:46:25,267 -                train: [    INFO] - TRAIN Iteration: [  14/226] 
Loss D Real: 0.3183 (0.2823) Acc D Real: 78.289% 
Loss D Fake: 0.4018 (0.4376) Acc D Fake: 97.143% 
Loss D: 0.720 
Loss G: 1.1213 (1.0800) Acc G: 0.952% 
LR: 2.000e-04 

2023-03-02 01:46:25,274 -                train: [    INFO] - TRAIN Iteration: [  15/226] 
Loss D Real: 0.3345 (0.2858) Acc D Real: 78.094% 
Loss D Fake: 0.4018 (0.4352) Acc D Fake: 97.333% 
Loss D: 0.736 
Loss G: 1.1202 (1.0827) Acc G: 0.889% 
LR: 2.000e-04 

2023-03-02 01:46:25,281 -                train: [    INFO] - TRAIN Iteration: [  16/226] 
Loss D Real: 0.3366 (0.2890) Acc D Real: 77.878% 
Loss D Fake: 0.4027 (0.4331) Acc D Fake: 97.500% 
Loss D: 0.739 
Loss G: 1.1176 (1.0849) Acc G: 0.833% 
LR: 2.000e-04 

2023-03-02 01:46:25,289 -                train: [    INFO] - TRAIN Iteration: [  17/226] 
Loss D Real: 0.3071 (0.2900) Acc D Real: 77.825% 
Loss D Fake: 0.4047 (0.4315) Acc D Fake: 97.647% 
Loss D: 0.712 
Loss G: 1.1103 (1.0864) Acc G: 0.784% 
LR: 2.000e-04 

2023-03-02 01:46:25,297 -                train: [    INFO] - TRAIN Iteration: [  18/226] 
Loss D Real: 0.2294 (0.2867) Acc D Real: 78.328% 
Loss D Fake: 0.4093 (0.4302) Acc D Fake: 97.778% 
Loss D: 0.639 
Loss G: 1.0981 (1.0870) Acc G: 0.741% 
LR: 2.000e-04 

2023-03-02 01:46:25,305 -                train: [    INFO] - TRAIN Iteration: [  19/226] 
Loss D Real: 0.3806 (0.2916) Acc D Real: 77.906% 
Loss D Fake: 0.4166 (0.4295) Acc D Fake: 97.895% 
Loss D: 0.797 
Loss G: 1.0788 (1.0866) Acc G: 0.702% 
LR: 2.000e-04 

2023-03-02 01:46:25,313 -                train: [    INFO] - TRAIN Iteration: [  20/226] 
Loss D Real: 0.2026 (0.2871) Acc D Real: 78.289% 
Loss D Fake: 0.4301 (0.4295) Acc D Fake: 98.000% 
Loss D: 0.633 
Loss G: 1.0650 (1.0855) Acc G: 0.667% 
LR: 2.000e-04 

2023-03-02 01:46:25,319 -                train: [    INFO] - TRAIN Iteration: [  21/226] 
Loss D Real: 0.2592 (0.2858) Acc D Real: 78.465% 
Loss D Fake: 0.4280 (0.4295) Acc D Fake: 98.095% 
Loss D: 0.687 
Loss G: 1.0819 (1.0853) Acc G: 0.635% 
LR: 2.000e-04 

2023-03-02 01:46:25,326 -                train: [    INFO] - TRAIN Iteration: [  22/226] 
Loss D Real: 0.1738 (0.2807) Acc D Real: 78.911% 
Loss D Fake: 0.4154 (0.4288) Acc D Fake: 98.182% 
Loss D: 0.589 
Loss G: 1.1015 (1.0861) Acc G: 0.606% 
LR: 2.000e-04 

2023-03-02 01:46:25,333 -                train: [    INFO] - TRAIN Iteration: [  23/226] 
Loss D Real: 0.2513 (0.2794) Acc D Real: 79.065% 
Loss D Fake: 0.4097 (0.4280) Acc D Fake: 98.261% 
Loss D: 0.661 
Loss G: 1.1018 (1.0868) Acc G: 0.580% 
LR: 2.000e-04 

2023-03-02 01:46:25,341 -                train: [    INFO] - TRAIN Iteration: [  24/226] 
Loss D Real: 0.2576 (0.2785) Acc D Real: 79.204% 
Loss D Fake: 0.4120 (0.4273) Acc D Fake: 98.333% 
Loss D: 0.670 
Loss G: 1.0986 (1.0872) Acc G: 0.556% 
LR: 2.000e-04 

2023-03-02 01:46:25,348 -                train: [    INFO] - TRAIN Iteration: [  25/226] 
Loss D Real: 0.2593 (0.2778) Acc D Real: 79.329% 
Loss D Fake: 0.4130 (0.4268) Acc D Fake: 98.400% 
Loss D: 0.672 
Loss G: 1.1000 (1.0878) Acc G: 0.533% 
LR: 2.000e-04 

2023-03-02 01:46:25,355 -                train: [    INFO] - TRAIN Iteration: [  26/226] 
Loss D Real: 0.2324 (0.2760) Acc D Real: 79.475% 
Loss D Fake: 0.4110 (0.4262) Acc D Fake: 98.462% 
Loss D: 0.643 
Loss G: 1.1066 (1.0885) Acc G: 0.513% 
LR: 2.000e-04 

2023-03-02 01:46:25,362 -                train: [    INFO] - TRAIN Iteration: [  27/226] 
Loss D Real: 0.2863 (0.2764) Acc D Real: 79.452% 
Loss D Fake: 0.4072 (0.4254) Acc D Fake: 98.519% 
Loss D: 0.693 
Loss G: 1.1146 (1.0894) Acc G: 0.494% 
LR: 2.000e-04 

2023-03-02 01:46:25,369 -                train: [    INFO] - TRAIN Iteration: [  28/226] 
Loss D Real: 0.3814 (0.2802) Acc D Real: 79.142% 
Loss D Fake: 0.4041 (0.4247) Acc D Fake: 98.571% 
Loss D: 0.785 
Loss G: 1.1186 (1.0905) Acc G: 0.476% 
LR: 2.000e-04 

2023-03-02 01:46:25,377 -                train: [    INFO] - TRAIN Iteration: [  29/226] 
Loss D Real: 0.3626 (0.2830) Acc D Real: 78.915% 
Loss D Fake: 0.4041 (0.4240) Acc D Fake: 98.621% 
Loss D: 0.767 
Loss G: 1.1139 (1.0913) Acc G: 0.460% 
LR: 2.000e-04 

2023-03-02 01:46:25,385 -                train: [    INFO] - TRAIN Iteration: [  30/226] 
Loss D Real: 0.3273 (0.2845) Acc D Real: 78.799% 
Loss D Fake: 0.4076 (0.4234) Acc D Fake: 98.667% 
Loss D: 0.735 
Loss G: 1.1081 (1.0919) Acc G: 0.444% 
LR: 2.000e-04 

2023-03-02 01:46:25,393 -                train: [    INFO] - TRAIN Iteration: [  31/226] 
Loss D Real: 0.2458 (0.2832) Acc D Real: 78.935% 
Loss D Fake: 0.4105 (0.4230) Acc D Fake: 98.710% 
Loss D: 0.656 
Loss G: 1.1029 (1.0922) Acc G: 0.430% 
LR: 2.000e-04 

2023-03-02 01:46:25,401 -                train: [    INFO] - TRAIN Iteration: [  32/226] 
Loss D Real: 0.1044 (0.2776) Acc D Real: 79.429% 
Loss D Fake: 0.4105 (0.4226) Acc D Fake: 98.750% 
Loss D: 0.515 
Loss G: 1.1129 (1.0929) Acc G: 0.417% 
LR: 2.000e-04 

2023-03-02 01:46:25,408 -                train: [    INFO] - TRAIN Iteration: [  33/226] 
Loss D Real: 0.2681 (0.2774) Acc D Real: 79.471% 
Loss D Fake: 0.4042 (0.4221) Acc D Fake: 98.788% 
Loss D: 0.672 
Loss G: 1.1208 (1.0937) Acc G: 0.404% 
LR: 2.000e-04 

2023-03-02 01:46:25,415 -                train: [    INFO] - TRAIN Iteration: [  34/226] 
Loss D Real: 0.2889 (0.2777) Acc D Real: 79.478% 
Loss D Fake: 0.4013 (0.4214) Acc D Fake: 98.824% 
Loss D: 0.690 
Loss G: 1.1271 (1.0947) Acc G: 0.392% 
LR: 2.000e-04 

2023-03-02 01:46:25,423 -                train: [    INFO] - TRAIN Iteration: [  35/226] 
Loss D Real: 0.2147 (0.2759) Acc D Real: 79.638% 
Loss D Fake: 0.4010 (0.4209) Acc D Fake: 98.857% 
Loss D: 0.616 
Loss G: 1.1073 (1.0951) Acc G: 0.381% 
LR: 2.000e-04 

2023-03-02 01:46:25,430 -                train: [    INFO] - TRAIN Iteration: [  36/226] 
Loss D Real: 0.3095 (0.2768) Acc D Real: 79.547% 
Loss D Fake: 0.6491 (0.4272) Acc D Fake: 98.056% 
Loss D: 0.959 
Loss G: 1.1704 (1.0971) Acc G: 0.370% 
LR: 2.000e-04 

2023-03-02 01:46:25,437 -                train: [    INFO] - TRAIN Iteration: [  37/226] 
Loss D Real: 0.2801 (0.2769) Acc D Real: 79.593% 
Loss D Fake: 0.3701 (0.4257) Acc D Fake: 98.108% 
Loss D: 0.650 
Loss G: 1.2135 (1.1003) Acc G: 0.360% 
LR: 2.000e-04 

2023-03-02 01:46:25,445 -                train: [    INFO] - TRAIN Iteration: [  38/226] 
Loss D Real: 0.2686 (0.2767) Acc D Real: 79.604% 
Loss D Fake: 0.3573 (0.4239) Acc D Fake: 98.158% 
Loss D: 0.626 
Loss G: 1.2387 (1.1039) Acc G: 0.351% 
LR: 2.000e-04 

2023-03-02 01:46:25,452 -                train: [    INFO] - TRAIN Iteration: [  39/226] 
Loss D Real: 0.4285 (0.2806) Acc D Real: 79.307% 
Loss D Fake: 0.3497 (0.4220) Acc D Fake: 98.205% 
Loss D: 0.778 
Loss G: 1.2536 (1.1078) Acc G: 0.342% 
LR: 2.000e-04 

2023-03-02 01:46:25,460 -                train: [    INFO] - TRAIN Iteration: [  40/226] 
Loss D Real: 0.3090 (0.2813) Acc D Real: 79.272% 
Loss D Fake: 0.3453 (0.4200) Acc D Fake: 98.250% 
Loss D: 0.654 
Loss G: 1.2627 (1.1116) Acc G: 0.333% 
LR: 2.000e-04 

2023-03-02 01:46:25,468 -                train: [    INFO] - TRAIN Iteration: [  41/226] 
Loss D Real: 0.4323 (0.2850) Acc D Real: 78.980% 
Loss D Fake: 0.3428 (0.4182) Acc D Fake: 98.293% 
Loss D: 0.775 
Loss G: 1.2669 (1.1154) Acc G: 0.325% 
LR: 2.000e-04 

2023-03-02 01:46:25,475 -                train: [    INFO] - TRAIN Iteration: [  42/226] 
Loss D Real: 0.3148 (0.2857) Acc D Real: 78.920% 
Loss D Fake: 0.3418 (0.4163) Acc D Fake: 98.333% 
Loss D: 0.657 
Loss G: 1.2687 (1.1191) Acc G: 0.317% 
LR: 2.000e-04 

2023-03-02 01:46:25,483 -                train: [    INFO] - TRAIN Iteration: [  43/226] 
Loss D Real: 0.2432 (0.2847) Acc D Real: 79.000% 
Loss D Fake: 0.3411 (0.4146) Acc D Fake: 98.372% 
Loss D: 0.584 
Loss G: 1.2710 (1.1226) Acc G: 0.310% 
LR: 2.000e-04 

2023-03-02 01:46:25,490 -                train: [    INFO] - TRAIN Iteration: [  44/226] 
Loss D Real: 0.4303 (0.2880) Acc D Real: 78.746% 
Loss D Fake: 0.3405 (0.4129) Acc D Fake: 98.409% 
Loss D: 0.771 
Loss G: 1.2710 (1.1260) Acc G: 0.303% 
LR: 2.000e-04 

2023-03-02 01:46:25,498 -                train: [    INFO] - TRAIN Iteration: [  45/226] 
Loss D Real: 0.5413 (0.2936) Acc D Real: 78.301% 
Loss D Fake: 0.3411 (0.4113) Acc D Fake: 98.444% 
Loss D: 0.882 
Loss G: 1.2669 (1.1291) Acc G: 0.296% 
LR: 2.000e-04 

2023-03-02 01:46:25,505 -                train: [    INFO] - TRAIN Iteration: [  46/226] 
Loss D Real: 0.3978 (0.2959) Acc D Real: 78.124% 
Loss D Fake: 0.3432 (0.4098) Acc D Fake: 98.478% 
Loss D: 0.741 
Loss G: 1.2607 (1.1320) Acc G: 0.290% 
LR: 2.000e-04 

2023-03-02 01:46:25,513 -                train: [    INFO] - TRAIN Iteration: [  47/226] 
Loss D Real: 0.4698 (0.2996) Acc D Real: 77.826% 
Loss D Fake: 0.3458 (0.4085) Acc D Fake: 98.511% 
Loss D: 0.816 
Loss G: 1.2527 (1.1345) Acc G: 0.284% 
LR: 2.000e-04 

2023-03-02 01:46:25,520 -                train: [    INFO] - TRAIN Iteration: [  48/226] 
Loss D Real: 0.3498 (0.3007) Acc D Real: 77.744% 
Loss D Fake: 0.3489 (0.4072) Acc D Fake: 98.542% 
Loss D: 0.699 
Loss G: 1.2446 (1.1368) Acc G: 0.278% 
LR: 2.000e-04 

2023-03-02 01:46:25,528 -                train: [    INFO] - TRAIN Iteration: [  49/226] 
Loss D Real: 0.3226 (0.3011) Acc D Real: 77.710% 
Loss D Fake: 0.3517 (0.4061) Acc D Fake: 98.571% 
Loss D: 0.674 
Loss G: 1.2383 (1.1389) Acc G: 0.272% 
LR: 2.000e-04 

2023-03-02 01:46:25,536 -                train: [    INFO] - TRAIN Iteration: [  50/226] 
Loss D Real: 0.3150 (0.3014) Acc D Real: 77.678% 
Loss D Fake: 0.3538 (0.4050) Acc D Fake: 98.600% 
Loss D: 0.669 
Loss G: 1.2333 (1.1408) Acc G: 0.267% 
LR: 2.000e-04 

2023-03-02 01:46:25,543 -                train: [    INFO] - TRAIN Iteration: [  51/226] 
Loss D Real: 0.2715 (0.3008) Acc D Real: 77.726% 
Loss D Fake: 0.3555 (0.4041) Acc D Fake: 98.627% 
Loss D: 0.627 
Loss G: 1.2301 (1.1425) Acc G: 0.261% 
LR: 2.000e-04 

2023-03-02 01:46:25,551 -                train: [    INFO] - TRAIN Iteration: [  52/226] 
Loss D Real: 0.4189 (0.3031) Acc D Real: 77.533% 
Loss D Fake: 0.3568 (0.4032) Acc D Fake: 98.654% 
Loss D: 0.776 
Loss G: 1.2258 (1.1441) Acc G: 0.256% 
LR: 2.000e-04 

2023-03-02 01:46:25,558 -                train: [    INFO] - TRAIN Iteration: [  53/226] 
Loss D Real: 0.2328 (0.3017) Acc D Real: 77.635% 
Loss D Fake: 0.3583 (0.4023) Acc D Fake: 98.679% 
Loss D: 0.591 
Loss G: 1.2232 (1.1456) Acc G: 0.252% 
LR: 2.000e-04 

2023-03-02 01:46:25,566 -                train: [    INFO] - TRAIN Iteration: [  54/226] 
Loss D Real: 0.3821 (0.3032) Acc D Real: 77.504% 
Loss D Fake: 0.3592 (0.4015) Acc D Fake: 98.704% 
Loss D: 0.741 
Loss G: 1.2207 (1.1470) Acc G: 0.247% 
LR: 2.000e-04 

2023-03-02 01:46:25,573 -                train: [    INFO] - TRAIN Iteration: [  55/226] 
Loss D Real: 0.4852 (0.3065) Acc D Real: 77.216% 
Loss D Fake: 0.3606 (0.4008) Acc D Fake: 98.727% 
Loss D: 0.846 
Loss G: 1.2158 (1.1483) Acc G: 0.242% 
LR: 2.000e-04 

2023-03-02 01:46:25,581 -                train: [    INFO] - TRAIN Iteration: [  56/226] 
Loss D Real: 0.5619 (0.3111) Acc D Real: 76.821% 
Loss D Fake: 0.3632 (0.4001) Acc D Fake: 98.750% 
Loss D: 0.925 
Loss G: 1.2072 (1.1493) Acc G: 0.238% 
LR: 2.000e-04 

2023-03-02 01:46:25,588 -                train: [    INFO] - TRAIN Iteration: [  57/226] 
Loss D Real: 0.3413 (0.3116) Acc D Real: 76.763% 
Loss D Fake: 0.3668 (0.3995) Acc D Fake: 98.772% 
Loss D: 0.708 
Loss G: 1.1992 (1.1502) Acc G: 0.234% 
LR: 2.000e-04 

2023-03-02 01:46:25,596 -                train: [    INFO] - TRAIN Iteration: [  58/226] 
Loss D Real: 0.1951 (0.3096) Acc D Real: 76.919% 
Loss D Fake: 0.3694 (0.3990) Acc D Fake: 98.793% 
Loss D: 0.564 
Loss G: 1.1954 (1.1510) Acc G: 0.230% 
LR: 2.000e-04 

2023-03-02 01:46:25,603 -                train: [    INFO] - TRAIN Iteration: [  59/226] 
Loss D Real: 0.4062 (0.3113) Acc D Real: 76.764% 
Loss D Fake: 0.3706 (0.3985) Acc D Fake: 98.814% 
Loss D: 0.777 
Loss G: 1.1917 (1.1517) Acc G: 0.226% 
LR: 2.000e-04 

2023-03-02 01:46:25,611 -                train: [    INFO] - TRAIN Iteration: [  60/226] 
Loss D Real: 0.3718 (0.3123) Acc D Real: 76.662% 
Loss D Fake: 0.3723 (0.3981) Acc D Fake: 98.833% 
Loss D: 0.744 
Loss G: 1.1876 (1.1523) Acc G: 0.222% 
LR: 2.000e-04 

2023-03-02 01:46:25,618 -                train: [    INFO] - TRAIN Iteration: [  61/226] 
Loss D Real: 0.4170 (0.3140) Acc D Real: 76.505% 
Loss D Fake: 0.3742 (0.3977) Acc D Fake: 98.852% 
Loss D: 0.791 
Loss G: 1.1824 (1.1528) Acc G: 0.219% 
LR: 2.000e-04 

2023-03-02 01:46:25,626 -                train: [    INFO] - TRAIN Iteration: [  62/226] 
Loss D Real: 0.4415 (0.3160) Acc D Real: 76.310% 
Loss D Fake: 0.3767 (0.3974) Acc D Fake: 98.871% 
Loss D: 0.818 
Loss G: 1.1756 (1.1531) Acc G: 0.215% 
LR: 2.000e-04 

2023-03-02 01:46:25,633 -                train: [    INFO] - TRAIN Iteration: [  63/226] 
Loss D Real: 0.3397 (0.3164) Acc D Real: 76.271% 
Loss D Fake: 0.3796 (0.3971) Acc D Fake: 98.889% 
Loss D: 0.719 
Loss G: 1.1697 (1.1534) Acc G: 0.212% 
LR: 2.000e-04 

2023-03-02 01:46:25,641 -                train: [    INFO] - TRAIN Iteration: [  64/226] 
Loss D Real: 0.4017 (0.3177) Acc D Real: 76.134% 
Loss D Fake: 0.3820 (0.3968) Acc D Fake: 98.906% 
Loss D: 0.784 
Loss G: 1.1636 (1.1536) Acc G: 0.208% 
LR: 2.000e-04 

2023-03-02 01:46:25,648 -                train: [    INFO] - TRAIN Iteration: [  65/226] 
Loss D Real: 0.3416 (0.3181) Acc D Real: 76.089% 
Loss D Fake: 0.3847 (0.3967) Acc D Fake: 98.923% 
Loss D: 0.726 
Loss G: 1.1580 (1.1536) Acc G: 0.205% 
LR: 2.000e-04 

2023-03-02 01:46:25,656 -                train: [    INFO] - TRAIN Iteration: [  66/226] 
Loss D Real: 0.3321 (0.3183) Acc D Real: 76.046% 
Loss D Fake: 0.3868 (0.3965) Acc D Fake: 98.939% 
Loss D: 0.719 
Loss G: 1.1540 (1.1536) Acc G: 0.202% 
LR: 2.000e-04 

2023-03-02 01:46:25,664 -                train: [    INFO] - TRAIN Iteration: [  67/226] 
Loss D Real: 0.2941 (0.3180) Acc D Real: 76.051% 
Loss D Fake: 0.3882 (0.3964) Acc D Fake: 98.955% 
Loss D: 0.682 
Loss G: 1.1514 (1.1536) Acc G: 0.199% 
LR: 2.000e-04 

2023-03-02 01:46:25,672 -                train: [    INFO] - TRAIN Iteration: [  68/226] 
Loss D Real: 0.3587 (0.3186) Acc D Real: 75.970% 
Loss D Fake: 0.3892 (0.3963) Acc D Fake: 98.971% 
Loss D: 0.748 
Loss G: 1.1493 (1.1535) Acc G: 0.196% 
LR: 2.000e-04 

2023-03-02 01:46:25,680 -                train: [    INFO] - TRAIN Iteration: [  69/226] 
Loss D Real: 0.6613 (0.3235) Acc D Real: 75.501% 
Loss D Fake: 0.3908 (0.3962) Acc D Fake: 98.986% 
Loss D: 1.052 
Loss G: 1.1423 (1.1534) Acc G: 0.193% 
LR: 2.000e-04 

2023-03-02 01:46:25,687 -                train: [    INFO] - TRAIN Iteration: [  70/226] 
Loss D Real: 0.2574 (0.3226) Acc D Real: 75.556% 
Loss D Fake: 0.3944 (0.3962) Acc D Fake: 99.000% 
Loss D: 0.652 
Loss G: 1.1355 (1.1531) Acc G: 0.190% 
LR: 2.000e-04 

2023-03-02 01:46:25,696 -                train: [    INFO] - TRAIN Iteration: [  71/226] 
Loss D Real: 0.2984 (0.3222) Acc D Real: 75.569% 
Loss D Fake: 0.3970 (0.3962) Acc D Fake: 99.014% 
Loss D: 0.695 
Loss G: 1.1304 (1.1528) Acc G: 0.188% 
LR: 2.000e-04 

2023-03-02 01:46:25,704 -                train: [    INFO] - TRAIN Iteration: [  72/226] 
Loss D Real: 0.3297 (0.3223) Acc D Real: 75.536% 
Loss D Fake: 0.3991 (0.3962) Acc D Fake: 99.028% 
Loss D: 0.729 
Loss G: 1.1263 (1.1524) Acc G: 0.185% 
LR: 2.000e-04 

2023-03-02 01:46:25,711 -                train: [    INFO] - TRAIN Iteration: [  73/226] 
Loss D Real: 0.3963 (0.3234) Acc D Real: 75.420% 
Loss D Fake: 0.4011 (0.3963) Acc D Fake: 99.041% 
Loss D: 0.797 
Loss G: 1.1214 (1.1520) Acc G: 0.183% 
LR: 2.000e-04 

2023-03-02 01:46:25,720 -                train: [    INFO] - TRAIN Iteration: [  74/226] 
Loss D Real: 0.3598 (0.3239) Acc D Real: 75.358% 
Loss D Fake: 0.4035 (0.3964) Acc D Fake: 99.054% 
Loss D: 0.763 
Loss G: 1.1162 (1.1515) Acc G: 0.180% 
LR: 2.000e-04 

2023-03-02 01:46:25,729 -                train: [    INFO] - TRAIN Iteration: [  75/226] 
Loss D Real: 0.3281 (0.3239) Acc D Real: 75.328% 
Loss D Fake: 0.4058 (0.3965) Acc D Fake: 99.067% 
Loss D: 0.734 
Loss G: 1.1112 (1.1510) Acc G: 0.178% 
LR: 2.000e-04 

2023-03-02 01:46:25,738 -                train: [    INFO] - TRAIN Iteration: [  76/226] 
Loss D Real: 0.4848 (0.3260) Acc D Real: 75.097% 
Loss D Fake: 0.4085 (0.3967) Acc D Fake: 99.079% 
Loss D: 0.893 
Loss G: 1.1044 (1.1504) Acc G: 0.175% 
LR: 2.000e-04 

2023-03-02 01:46:25,747 -                train: [    INFO] - TRAIN Iteration: [  77/226] 
Loss D Real: 0.2951 (0.3256) Acc D Real: 75.103% 
Loss D Fake: 0.4117 (0.3969) Acc D Fake: 99.091% 
Loss D: 0.707 
Loss G: 1.0989 (1.1497) Acc G: 0.173% 
LR: 2.000e-04 

2023-03-02 01:46:25,755 -                train: [    INFO] - TRAIN Iteration: [  78/226] 
Loss D Real: 0.4898 (0.3277) Acc D Real: 74.879% 
Loss D Fake: 0.4143 (0.3971) Acc D Fake: 99.103% 
Loss D: 0.904 
Loss G: 1.0923 (1.1490) Acc G: 0.171% 
LR: 2.000e-04 

2023-03-02 01:46:25,763 -                train: [    INFO] - TRAIN Iteration: [  79/226] 
Loss D Real: 0.2899 (0.3273) Acc D Real: 74.898% 
Loss D Fake: 0.4175 (0.3973) Acc D Fake: 99.114% 
Loss D: 0.707 
Loss G: 1.0866 (1.1482) Acc G: 0.169% 
LR: 2.000e-04 

2023-03-02 01:46:25,770 -                train: [    INFO] - TRAIN Iteration: [  80/226] 
Loss D Real: 0.3798 (0.3279) Acc D Real: 74.805% 
Loss D Fake: 0.4202 (0.3976) Acc D Fake: 99.125% 
Loss D: 0.800 
Loss G: 1.0805 (1.1473) Acc G: 0.167% 
LR: 2.000e-04 

2023-03-02 01:46:25,779 -                train: [    INFO] - TRAIN Iteration: [  81/226] 
Loss D Real: 0.2938 (0.3275) Acc D Real: 74.817% 
Loss D Fake: 0.4231 (0.3979) Acc D Fake: 99.136% 
Loss D: 0.717 
Loss G: 1.0761 (1.1465) Acc G: 0.165% 
LR: 2.000e-04 

2023-03-02 01:46:25,787 -                train: [    INFO] - TRAIN Iteration: [  82/226] 
Loss D Real: 0.3059 (0.3272) Acc D Real: 74.804% 
Loss D Fake: 0.4248 (0.3983) Acc D Fake: 99.146% 
Loss D: 0.731 
Loss G: 1.0735 (1.1456) Acc G: 0.163% 
LR: 2.000e-04 

2023-03-02 01:46:25,795 -                train: [    INFO] - TRAIN Iteration: [  83/226] 
Loss D Real: 0.3964 (0.3281) Acc D Real: 74.688% 
Loss D Fake: 0.4258 (0.3986) Acc D Fake: 99.157% 
Loss D: 0.822 
Loss G: 1.0718 (1.1447) Acc G: 0.161% 
LR: 2.000e-04 

2023-03-02 01:46:25,802 -                train: [    INFO] - TRAIN Iteration: [  84/226] 
Loss D Real: 0.3353 (0.3281) Acc D Real: 74.641% 
Loss D Fake: 0.4264 (0.3989) Acc D Fake: 99.167% 
Loss D: 0.762 
Loss G: 1.0710 (1.1438) Acc G: 0.159% 
LR: 2.000e-04 

2023-03-02 01:46:25,809 -                train: [    INFO] - TRAIN Iteration: [  85/226] 
Loss D Real: 0.3683 (0.3286) Acc D Real: 74.557% 
Loss D Fake: 0.4270 (0.3993) Acc D Fake: 99.176% 
Loss D: 0.795 
Loss G: 1.0691 (1.1429) Acc G: 0.157% 
LR: 2.000e-04 

2023-03-02 01:46:25,816 -                train: [    INFO] - TRAIN Iteration: [  86/226] 
Loss D Real: 0.1871 (0.3270) Acc D Real: 74.682% 
Loss D Fake: 0.4277 (0.3996) Acc D Fake: 99.186% 
Loss D: 0.615 
Loss G: 1.0688 (1.1421) Acc G: 0.155% 
LR: 2.000e-04 

2023-03-02 01:46:25,824 -                train: [    INFO] - TRAIN Iteration: [  87/226] 
Loss D Real: 0.3062 (0.3267) Acc D Real: 74.665% 
Loss D Fake: 0.4277 (0.3999) Acc D Fake: 99.195% 
Loss D: 0.734 
Loss G: 1.0686 (1.1412) Acc G: 0.153% 
LR: 2.000e-04 

2023-03-02 01:46:25,831 -                train: [    INFO] - TRAIN Iteration: [  88/226] 
Loss D Real: 0.3755 (0.3273) Acc D Real: 74.581% 
Loss D Fake: 0.4281 (0.4002) Acc D Fake: 99.205% 
Loss D: 0.804 
Loss G: 1.0671 (1.1404) Acc G: 0.152% 
LR: 2.000e-04 

2023-03-02 01:46:25,839 -                train: [    INFO] - TRAIN Iteration: [  89/226] 
Loss D Real: 0.3497 (0.3275) Acc D Real: 74.538% 
Loss D Fake: 0.4290 (0.4006) Acc D Fake: 99.213% 
Loss D: 0.779 
Loss G: 1.0649 (1.1395) Acc G: 0.150% 
LR: 2.000e-04 

2023-03-02 01:46:25,846 -                train: [    INFO] - TRAIN Iteration: [  90/226] 
Loss D Real: 0.4879 (0.3293) Acc D Real: 74.330% 
Loss D Fake: 0.4306 (0.4009) Acc D Fake: 99.222% 
Loss D: 0.918 
Loss G: 1.0604 (1.1386) Acc G: 0.148% 
LR: 2.000e-04 

2023-03-02 01:46:25,854 -                train: [    INFO] - TRAIN Iteration: [  91/226] 
Loss D Real: 0.3745 (0.3298) Acc D Real: 74.253% 
Loss D Fake: 0.4334 (0.4013) Acc D Fake: 99.231% 
Loss D: 0.808 
Loss G: 1.0543 (1.1377) Acc G: 0.147% 
LR: 2.000e-04 

2023-03-02 01:46:25,863 -                train: [    INFO] - TRAIN Iteration: [  92/226] 
Loss D Real: 0.4509 (0.3311) Acc D Real: 74.087% 
Loss D Fake: 0.4372 (0.4016) Acc D Fake: 99.239% 
Loss D: 0.888 
Loss G: 1.0454 (1.1367) Acc G: 0.145% 
LR: 2.000e-04 

2023-03-02 01:46:25,871 -                train: [    INFO] - TRAIN Iteration: [  93/226] 
Loss D Real: 0.3083 (0.3309) Acc D Real: 74.075% 
Loss D Fake: 0.4421 (0.4021) Acc D Fake: 99.247% 
Loss D: 0.750 
Loss G: 1.0374 (1.1356) Acc G: 0.143% 
LR: 2.000e-04 

2023-03-02 01:46:25,879 -                train: [    INFO] - TRAIN Iteration: [  94/226] 
Loss D Real: 0.3536 (0.3311) Acc D Real: 74.009% 
Loss D Fake: 0.4461 (0.4025) Acc D Fake: 99.255% 
Loss D: 0.800 
Loss G: 1.0294 (1.1345) Acc G: 0.142% 
LR: 2.000e-04 

2023-03-02 01:46:25,888 -                train: [    INFO] - TRAIN Iteration: [  95/226] 
Loss D Real: 0.2716 (0.3305) Acc D Real: 74.036% 
Loss D Fake: 0.4497 (0.4030) Acc D Fake: 99.263% 
Loss D: 0.721 
Loss G: 1.0263 (1.1334) Acc G: 0.140% 
LR: 2.000e-04 

2023-03-02 01:46:25,896 -                train: [    INFO] - TRAIN Iteration: [  96/226] 
Loss D Real: 0.2641 (0.3298) Acc D Real: 74.065% 
Loss D Fake: 0.4498 (0.4035) Acc D Fake: 99.271% 
Loss D: 0.714 
Loss G: 1.0280 (1.1323) Acc G: 0.139% 
LR: 2.000e-04 

2023-03-02 01:46:25,904 -                train: [    INFO] - TRAIN Iteration: [  97/226] 
Loss D Real: 0.2948 (0.3295) Acc D Real: 74.060% 
Loss D Fake: 0.4481 (0.4040) Acc D Fake: 99.278% 
Loss D: 0.743 
Loss G: 1.0314 (1.1312) Acc G: 0.137% 
LR: 2.000e-04 

2023-03-02 01:46:25,912 -                train: [    INFO] - TRAIN Iteration: [  98/226] 
Loss D Real: 0.2998 (0.3291) Acc D Real: 74.055% 
Loss D Fake: 0.4465 (0.4044) Acc D Fake: 99.286% 
Loss D: 0.746 
Loss G: 1.0332 (1.1302) Acc G: 0.136% 
LR: 2.000e-04 

2023-03-02 01:46:25,920 -                train: [    INFO] - TRAIN Iteration: [  99/226] 
Loss D Real: 0.2464 (0.3283) Acc D Real: 74.104% 
Loss D Fake: 0.4457 (0.4048) Acc D Fake: 99.293% 
Loss D: 0.692 
Loss G: 1.0350 (1.1293) Acc G: 0.135% 
LR: 2.000e-04 

2023-03-02 01:46:25,927 -                train: [    INFO] - TRAIN Iteration: [ 100/226] 
Loss D Real: 0.3022 (0.3281) Acc D Real: 74.100% 
Loss D Fake: 0.4447 (0.4052) Acc D Fake: 99.300% 
Loss D: 0.747 
Loss G: 1.0363 (1.1283) Acc G: 0.133% 
LR: 2.000e-04 

2023-03-02 01:46:25,934 -                train: [    INFO] - TRAIN Iteration: [ 101/226] 
Loss D Real: 0.3936 (0.3287) Acc D Real: 74.006% 
Loss D Fake: 0.4445 (0.4056) Acc D Fake: 99.307% 
Loss D: 0.838 
Loss G: 1.0359 (1.1274) Acc G: 0.132% 
LR: 2.000e-04 

2023-03-02 01:46:25,942 -                train: [    INFO] - TRAIN Iteration: [ 102/226] 
Loss D Real: 0.4152 (0.3295) Acc D Real: 73.882% 
Loss D Fake: 0.4453 (0.4060) Acc D Fake: 99.314% 
Loss D: 0.860 
Loss G: 1.0333 (1.1265) Acc G: 0.131% 
LR: 2.000e-04 

2023-03-02 01:46:25,950 -                train: [    INFO] - TRAIN Iteration: [ 103/226] 
Loss D Real: 0.4481 (0.3307) Acc D Real: 73.732% 
Loss D Fake: 0.4472 (0.4064) Acc D Fake: 99.320% 
Loss D: 0.895 
Loss G: 1.0287 (1.1256) Acc G: 0.129% 
LR: 2.000e-04 

2023-03-02 01:46:25,957 -                train: [    INFO] - TRAIN Iteration: [ 104/226] 
Loss D Real: 0.2854 (0.3303) Acc D Real: 73.740% 
Loss D Fake: 0.4498 (0.4068) Acc D Fake: 99.327% 
Loss D: 0.735 
Loss G: 1.0246 (1.1246) Acc G: 0.128% 
LR: 2.000e-04 

2023-03-02 01:46:25,964 -                train: [    INFO] - TRAIN Iteration: [ 105/226] 
Loss D Real: 0.3012 (0.3300) Acc D Real: 73.736% 
Loss D Fake: 0.4518 (0.4073) Acc D Fake: 99.333% 
Loss D: 0.753 
Loss G: 1.0215 (1.1236) Acc G: 0.127% 
LR: 2.000e-04 

2023-03-02 01:46:25,972 -                train: [    INFO] - TRAIN Iteration: [ 106/226] 
Loss D Real: 0.2867 (0.3296) Acc D Real: 73.738% 
Loss D Fake: 0.4532 (0.4077) Acc D Fake: 99.340% 
Loss D: 0.740 
Loss G: 1.0195 (1.1226) Acc G: 0.126% 
LR: 2.000e-04 

2023-03-02 01:46:25,979 -                train: [    INFO] - TRAIN Iteration: [ 107/226] 
Loss D Real: 0.2122 (0.3285) Acc D Real: 73.822% 
Loss D Fake: 0.4537 (0.4081) Acc D Fake: 99.346% 
Loss D: 0.666 
Loss G: 1.0199 (1.1217) Acc G: 0.125% 
LR: 2.000e-04 

2023-03-02 01:46:25,987 -                train: [    INFO] - TRAIN Iteration: [ 108/226] 
Loss D Real: 0.3477 (0.3287) Acc D Real: 73.764% 
Loss D Fake: 0.4534 (0.4085) Acc D Fake: 99.352% 
Loss D: 0.801 
Loss G: 1.0200 (1.1207) Acc G: 0.123% 
LR: 2.000e-04 

2023-03-02 01:46:25,994 -                train: [    INFO] - TRAIN Iteration: [ 109/226] 
Loss D Real: 0.3360 (0.3287) Acc D Real: 73.732% 
Loss D Fake: 0.4536 (0.4090) Acc D Fake: 99.358% 
Loss D: 0.790 
Loss G: 1.0192 (1.1198) Acc G: 0.122% 
LR: 2.000e-04 

2023-03-02 01:46:26,001 -                train: [    INFO] - TRAIN Iteration: [ 110/226] 
Loss D Real: 0.3274 (0.3287) Acc D Real: 73.704% 
Loss D Fake: 0.4543 (0.4094) Acc D Fake: 99.364% 
Loss D: 0.782 
Loss G: 1.0174 (1.1189) Acc G: 0.121% 
LR: 2.000e-04 

2023-03-02 01:46:26,009 -                train: [    INFO] - TRAIN Iteration: [ 111/226] 
Loss D Real: 0.3747 (0.3291) Acc D Real: 73.623% 
Loss D Fake: 0.4558 (0.4098) Acc D Fake: 99.369% 
Loss D: 0.831 
Loss G: 1.0136 (1.1179) Acc G: 0.120% 
LR: 2.000e-04 

2023-03-02 01:46:26,016 -                train: [    INFO] - TRAIN Iteration: [ 112/226] 
Loss D Real: 0.3479 (0.3293) Acc D Real: 73.569% 
Loss D Fake: 0.4585 (0.4102) Acc D Fake: 99.375% 
Loss D: 0.806 
Loss G: 1.0086 (1.1169) Acc G: 0.119% 
LR: 2.000e-04 

2023-03-02 01:46:26,023 -                train: [    INFO] - TRAIN Iteration: [ 113/226] 
Loss D Real: 0.4159 (0.3301) Acc D Real: 73.457% 
Loss D Fake: 0.4618 (0.4107) Acc D Fake: 99.381% 
Loss D: 0.878 
Loss G: 1.0027 (1.1159) Acc G: 0.118% 
LR: 2.000e-04 

2023-03-02 01:46:26,030 -                train: [    INFO] - TRAIN Iteration: [ 114/226] 
Loss D Real: 0.3410 (0.3302) Acc D Real: 73.410% 
Loss D Fake: 0.4643 (0.4111) Acc D Fake: 99.386% 
Loss D: 0.805 
Loss G: 1.0021 (1.1149) Acc G: 0.117% 
LR: 2.000e-04 

2023-03-02 01:46:26,037 -                train: [    INFO] - TRAIN Iteration: [ 115/226] 
Loss D Real: 0.4341 (0.3311) Acc D Real: 73.281% 
Loss D Fake: 0.4636 (0.4116) Acc D Fake: 99.391% 
Loss D: 0.898 
Loss G: 1.0019 (1.1139) Acc G: 0.116% 
LR: 2.000e-04 

2023-03-02 01:46:26,045 -                train: [    INFO] - TRAIN Iteration: [ 116/226] 
Loss D Real: 0.1776 (0.3297) Acc D Real: 73.384% 
Loss D Fake: 0.4631 (0.4120) Acc D Fake: 99.397% 
Loss D: 0.641 
Loss G: 1.0044 (1.1130) Acc G: 0.115% 
LR: 2.000e-04 

2023-03-02 01:46:26,052 -                train: [    INFO] - TRAIN Iteration: [ 117/226] 
Loss D Real: 0.2627 (0.3292) Acc D Real: 73.413% 
Loss D Fake: 0.4612 (0.4125) Acc D Fake: 99.402% 
Loss D: 0.724 
Loss G: 1.0071 (1.1121) Acc G: 0.114% 
LR: 2.000e-04 

2023-03-02 01:46:26,059 -                train: [    INFO] - TRAIN Iteration: [ 118/226] 
Loss D Real: 0.2842 (0.3288) Acc D Real: 73.417% 
Loss D Fake: 0.4600 (0.4129) Acc D Fake: 99.407% 
Loss D: 0.744 
Loss G: 1.0086 (1.1112) Acc G: 0.113% 
LR: 2.000e-04 

2023-03-02 01:46:26,067 -                train: [    INFO] - TRAIN Iteration: [ 119/226] 
Loss D Real: 0.4029 (0.3294) Acc D Real: 73.319% 
Loss D Fake: 0.4598 (0.4133) Acc D Fake: 99.412% 
Loss D: 0.863 
Loss G: 1.0075 (1.1103) Acc G: 0.112% 
LR: 2.000e-04 

2023-03-02 01:46:26,074 -                train: [    INFO] - TRAIN Iteration: [ 120/226] 
Loss D Real: 0.2832 (0.3290) Acc D Real: 73.326% 
Loss D Fake: 0.4609 (0.4137) Acc D Fake: 99.417% 
Loss D: 0.744 
Loss G: 1.0052 (1.1095) Acc G: 0.111% 
LR: 2.000e-04 

2023-03-02 01:46:26,082 -                train: [    INFO] - TRAIN Iteration: [ 121/226] 
Loss D Real: 0.2022 (0.3280) Acc D Real: 73.400% 
Loss D Fake: 0.4621 (0.4141) Acc D Fake: 99.421% 
Loss D: 0.664 
Loss G: 1.0039 (1.1086) Acc G: 0.110% 
LR: 2.000e-04 

2023-03-02 01:46:26,089 -                train: [    INFO] - TRAIN Iteration: [ 122/226] 
Loss D Real: 0.4881 (0.3293) Acc D Real: 73.230% 
Loss D Fake: 0.4636 (0.4145) Acc D Fake: 99.426% 
Loss D: 0.952 
Loss G: 0.9979 (1.1077) Acc G: 0.109% 
LR: 2.000e-04 

2023-03-02 01:46:26,096 -                train: [    INFO] - TRAIN Iteration: [ 123/226] 
Loss D Real: 0.2813 (0.3289) Acc D Real: 73.239% 
Loss D Fake: 0.4679 (0.4149) Acc D Fake: 99.431% 
Loss D: 0.749 
Loss G: 0.9946 (1.1068) Acc G: 0.108% 
LR: 2.000e-04 

2023-03-02 01:46:26,103 -                train: [    INFO] - TRAIN Iteration: [ 124/226] 
Loss D Real: 0.1662 (0.3276) Acc D Real: 73.343% 
Loss D Fake: 0.4667 (0.4153) Acc D Fake: 99.435% 
Loss D: 0.633 
Loss G: 1.0006 (1.1059) Acc G: 0.108% 
LR: 2.000e-04 

2023-03-02 01:46:26,110 -                train: [    INFO] - TRAIN Iteration: [ 125/226] 
Loss D Real: 0.3199 (0.3275) Acc D Real: 73.323% 
Loss D Fake: 0.4626 (0.4157) Acc D Fake: 99.440% 
Loss D: 0.783 
Loss G: 1.0058 (1.1051) Acc G: 0.107% 
LR: 2.000e-04 

2023-03-02 01:46:26,118 -                train: [    INFO] - TRAIN Iteration: [ 126/226] 
Loss D Real: 0.5257 (0.3291) Acc D Real: 73.130% 
Loss D Fake: 0.4610 (0.4161) Acc D Fake: 99.444% 
Loss D: 0.987 
Loss G: 1.0056 (1.1043) Acc G: 0.106% 
LR: 2.000e-04 

2023-03-02 01:46:26,125 -                train: [    INFO] - TRAIN Iteration: [ 127/226] 
Loss D Real: 0.1948 (0.3280) Acc D Real: 73.211% 
Loss D Fake: 0.4615 (0.4164) Acc D Fake: 99.449% 
Loss D: 0.656 
Loss G: 1.0058 (1.1035) Acc G: 0.105% 
LR: 2.000e-04 

2023-03-02 01:46:26,132 -                train: [    INFO] - TRAIN Iteration: [ 128/226] 
Loss D Real: 0.2610 (0.3275) Acc D Real: 73.236% 
Loss D Fake: 0.4611 (0.4168) Acc D Fake: 99.453% 
Loss D: 0.722 
Loss G: 1.0065 (1.1028) Acc G: 0.104% 
LR: 2.000e-04 

2023-03-02 01:46:26,139 -                train: [    INFO] - TRAIN Iteration: [ 129/226] 
Loss D Real: 0.3487 (0.3277) Acc D Real: 73.208% 
Loss D Fake: 0.4609 (0.4171) Acc D Fake: 99.457% 
Loss D: 0.810 
Loss G: 1.0064 (1.1020) Acc G: 0.103% 
LR: 2.000e-04 

2023-03-02 01:46:26,146 -                train: [    INFO] - TRAIN Iteration: [ 130/226] 
Loss D Real: 0.3765 (0.3281) Acc D Real: 73.139% 
Loss D Fake: 0.4615 (0.4174) Acc D Fake: 99.462% 
Loss D: 0.838 
Loss G: 1.0038 (1.1013) Acc G: 0.103% 
LR: 2.000e-04 

2023-03-02 01:46:26,154 -                train: [    INFO] - TRAIN Iteration: [ 131/226] 
Loss D Real: 0.3379 (0.3281) Acc D Real: 73.100% 
Loss D Fake: 0.4637 (0.4178) Acc D Fake: 99.466% 
Loss D: 0.802 
Loss G: 0.9995 (1.1005) Acc G: 0.102% 
LR: 2.000e-04 

2023-03-02 01:46:26,161 -                train: [    INFO] - TRAIN Iteration: [ 132/226] 
Loss D Real: 0.2079 (0.3272) Acc D Real: 73.174% 
Loss D Fake: 0.4657 (0.4182) Acc D Fake: 99.470% 
Loss D: 0.674 
Loss G: 0.9992 (1.0997) Acc G: 0.101% 
LR: 2.000e-04 

2023-03-02 01:46:26,168 -                train: [    INFO] - TRAIN Iteration: [ 133/226] 
Loss D Real: 0.3501 (0.3274) Acc D Real: 73.129% 
Loss D Fake: 0.4650 (0.4185) Acc D Fake: 99.474% 
Loss D: 0.815 
Loss G: 1.0001 (1.0990) Acc G: 0.100% 
LR: 2.000e-04 

2023-03-02 01:46:26,176 -                train: [    INFO] - TRAIN Iteration: [ 134/226] 
Loss D Real: 0.2799 (0.3270) Acc D Real: 73.135% 
Loss D Fake: 0.4642 (0.4189) Acc D Fake: 99.478% 
Loss D: 0.744 
Loss G: 1.0023 (1.0983) Acc G: 0.100% 
LR: 2.000e-04 

2023-03-02 01:46:26,183 -                train: [    INFO] - TRAIN Iteration: [ 135/226] 
Loss D Real: 0.2494 (0.3265) Acc D Real: 73.164% 
Loss D Fake: 0.4625 (0.4192) Acc D Fake: 99.481% 
Loss D: 0.712 
Loss G: 1.0059 (1.0976) Acc G: 0.099% 
LR: 2.000e-04 

2023-03-02 01:46:26,190 -                train: [    INFO] - TRAIN Iteration: [ 136/226] 
Loss D Real: 0.3409 (0.3266) Acc D Real: 73.127% 
Loss D Fake: 0.4606 (0.4195) Acc D Fake: 99.485% 
Loss D: 0.801 
Loss G: 1.0082 (1.0969) Acc G: 0.098% 
LR: 2.000e-04 

2023-03-02 01:46:26,197 -                train: [    INFO] - TRAIN Iteration: [ 137/226] 
Loss D Real: 0.1068 (0.3250) Acc D Real: 73.267% 
Loss D Fake: 0.4591 (0.4198) Acc D Fake: 99.489% 
Loss D: 0.566 
Loss G: 1.0124 (1.0963) Acc G: 0.097% 
LR: 2.000e-04 

2023-03-02 01:46:26,204 -                train: [    INFO] - TRAIN Iteration: [ 138/226] 
Loss D Real: 0.2922 (0.3247) Acc D Real: 73.271% 
Loss D Fake: 0.4567 (0.4200) Acc D Fake: 99.493% 
Loss D: 0.749 
Loss G: 1.0159 (1.0957) Acc G: 0.097% 
LR: 2.000e-04 

2023-03-02 01:46:26,212 -                train: [    INFO] - TRAIN Iteration: [ 139/226] 
Loss D Real: 0.5018 (0.3260) Acc D Real: 73.119% 
Loss D Fake: 0.4554 (0.4203) Acc D Fake: 99.496% 
Loss D: 0.957 
Loss G: 1.0164 (1.0952) Acc G: 0.096% 
LR: 2.000e-04 

2023-03-02 01:46:26,219 -                train: [    INFO] - TRAIN Iteration: [ 140/226] 
Loss D Real: 0.3089 (0.3259) Acc D Real: 73.103% 
Loss D Fake: 0.4558 (0.4205) Acc D Fake: 99.500% 
Loss D: 0.765 
Loss G: 1.0152 (1.0946) Acc G: 0.095% 
LR: 2.000e-04 

2023-03-02 01:46:26,226 -                train: [    INFO] - TRAIN Iteration: [ 141/226] 
Loss D Real: 0.3372 (0.3260) Acc D Real: 73.080% 
Loss D Fake: 0.4567 (0.4208) Acc D Fake: 99.504% 
Loss D: 0.794 
Loss G: 1.0136 (1.0940) Acc G: 0.095% 
LR: 2.000e-04 

2023-03-02 01:46:26,233 -                train: [    INFO] - TRAIN Iteration: [ 142/226] 
Loss D Real: 0.2481 (0.3254) Acc D Real: 73.112% 
Loss D Fake: 0.4574 (0.4211) Acc D Fake: 99.507% 
Loss D: 0.705 
Loss G: 1.0132 (1.0934) Acc G: 0.094% 
LR: 2.000e-04 

2023-03-02 01:46:26,241 -                train: [    INFO] - TRAIN Iteration: [ 143/226] 
Loss D Real: 0.2235 (0.3247) Acc D Real: 73.160% 
Loss D Fake: 0.4571 (0.4213) Acc D Fake: 99.510% 
Loss D: 0.681 
Loss G: 1.0144 (1.0929) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,248 -                train: [    INFO] - TRAIN Iteration: [ 144/226] 
Loss D Real: 0.1960 (0.3238) Acc D Real: 73.228% 
Loss D Fake: 0.4561 (0.4216) Acc D Fake: 99.514% 
Loss D: 0.652 
Loss G: 1.0169 (1.0924) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,255 -                train: [    INFO] - TRAIN Iteration: [ 145/226] 
Loss D Real: 0.2913 (0.3236) Acc D Real: 73.235% 
Loss D Fake: 0.4548 (0.4218) Acc D Fake: 99.517% 
Loss D: 0.746 
Loss G: 1.0186 (1.0919) Acc G: 0.092% 
LR: 2.000e-04 

2023-03-02 01:46:26,262 -                train: [    INFO] - TRAIN Iteration: [ 146/226] 
Loss D Real: 0.2583 (0.3231) Acc D Real: 73.258% 
Loss D Fake: 0.4540 (0.4220) Acc D Fake: 99.521% 
Loss D: 0.712 
Loss G: 1.0207 (1.0914) Acc G: 0.091% 
LR: 2.000e-04 

2023-03-02 01:46:26,270 -                train: [    INFO] - TRAIN Iteration: [ 147/226] 
Loss D Real: 0.2297 (0.3225) Acc D Real: 73.299% 
Loss D Fake: 0.4525 (0.4222) Acc D Fake: 99.524% 
Loss D: 0.682 
Loss G: 1.0244 (1.0909) Acc G: 0.091% 
LR: 2.000e-04 

2023-03-02 01:46:26,277 -                train: [    INFO] - TRAIN Iteration: [ 148/226] 
Loss D Real: 0.2847 (0.3222) Acc D Real: 73.304% 
Loss D Fake: 0.4502 (0.4224) Acc D Fake: 99.527% 
Loss D: 0.735 
Loss G: 1.0286 (1.0905) Acc G: 0.090% 
LR: 2.000e-04 

2023-03-02 01:46:26,284 -                train: [    INFO] - TRAIN Iteration: [ 149/226] 
Loss D Real: 0.3206 (0.3222) Acc D Real: 73.291% 
Loss D Fake: 0.4482 (0.4226) Acc D Fake: 99.530% 
Loss D: 0.769 
Loss G: 1.0316 (1.0901) Acc G: 0.089% 
LR: 2.000e-04 

2023-03-02 01:46:26,292 -                train: [    INFO] - TRAIN Iteration: [ 150/226] 
Loss D Real: 0.3456 (0.3224) Acc D Real: 73.253% 
Loss D Fake: 0.4471 (0.4227) Acc D Fake: 99.533% 
Loss D: 0.793 
Loss G: 1.0325 (1.0897) Acc G: 0.089% 
LR: 2.000e-04 

2023-03-02 01:46:26,299 -                train: [    INFO] - TRAIN Iteration: [ 151/226] 
Loss D Real: 0.2991 (0.3222) Acc D Real: 73.251% 
Loss D Fake: 0.4470 (0.4229) Acc D Fake: 99.536% 
Loss D: 0.746 
Loss G: 1.0330 (1.0893) Acc G: 0.088% 
LR: 2.000e-04 

2023-03-02 01:46:26,306 -                train: [    INFO] - TRAIN Iteration: [ 152/226] 
Loss D Real: 0.2663 (0.3219) Acc D Real: 73.271% 
Loss D Fake: 0.4467 (0.4231) Acc D Fake: 99.539% 
Loss D: 0.713 
Loss G: 1.0337 (1.0890) Acc G: 0.088% 
LR: 2.000e-04 

2023-03-02 01:46:26,313 -                train: [    INFO] - TRAIN Iteration: [ 153/226] 
Loss D Real: 0.1715 (0.3209) Acc D Real: 73.355% 
Loss D Fake: 0.4459 (0.4232) Acc D Fake: 99.542% 
Loss D: 0.617 
Loss G: 1.0365 (1.0886) Acc G: 0.087% 
LR: 2.000e-04 

2023-03-02 01:46:26,320 -                train: [    INFO] - TRAIN Iteration: [ 154/226] 
Loss D Real: 0.2946 (0.3207) Acc D Real: 73.359% 
Loss D Fake: 0.4441 (0.4233) Acc D Fake: 99.545% 
Loss D: 0.739 
Loss G: 1.0403 (1.0883) Acc G: 0.087% 
LR: 2.000e-04 

2023-03-02 01:46:26,328 -                train: [    INFO] - TRAIN Iteration: [ 155/226] 
Loss D Real: 0.2885 (0.3205) Acc D Real: 73.362% 
Loss D Fake: 0.4423 (0.4235) Acc D Fake: 99.548% 
Loss D: 0.731 
Loss G: 1.0428 (1.0880) Acc G: 0.086% 
LR: 2.000e-04 

2023-03-02 01:46:26,335 -                train: [    INFO] - TRAIN Iteration: [ 156/226] 
Loss D Real: 0.3633 (0.3208) Acc D Real: 73.321% 
Loss D Fake: 0.4414 (0.4236) Acc D Fake: 99.551% 
Loss D: 0.805 
Loss G: 1.0439 (1.0877) Acc G: 0.085% 
LR: 2.000e-04 

2023-03-02 01:46:26,342 -                train: [    INFO] - TRAIN Iteration: [ 157/226] 
Loss D Real: 0.4182 (0.3214) Acc D Real: 73.244% 
Loss D Fake: 0.4414 (0.4237) Acc D Fake: 99.554% 
Loss D: 0.860 
Loss G: 1.0422 (1.0874) Acc G: 0.085% 
LR: 2.000e-04 

2023-03-02 01:46:26,349 -                train: [    INFO] - TRAIN Iteration: [ 158/226] 
Loss D Real: 0.3224 (0.3214) Acc D Real: 73.226% 
Loss D Fake: 0.4429 (0.4238) Acc D Fake: 99.557% 
Loss D: 0.765 
Loss G: 1.0392 (1.0871) Acc G: 0.084% 
LR: 2.000e-04 

2023-03-02 01:46:26,356 -                train: [    INFO] - TRAIN Iteration: [ 159/226] 
Loss D Real: 0.2304 (0.3208) Acc D Real: 73.267% 
Loss D Fake: 0.4447 (0.4239) Acc D Fake: 99.560% 
Loss D: 0.675 
Loss G: 1.0367 (1.0868) Acc G: 0.084% 
LR: 2.000e-04 

2023-03-02 01:46:26,364 -                train: [    INFO] - TRAIN Iteration: [ 160/226] 
Loss D Real: 0.4326 (0.3215) Acc D Real: 73.179% 
Loss D Fake: 0.4462 (0.4241) Acc D Fake: 99.562% 
Loss D: 0.879 
Loss G: 1.0342 (1.0865) Acc G: 0.083% 
LR: 2.000e-04 

2023-03-02 01:46:26,371 -                train: [    INFO] - TRAIN Iteration: [ 161/226] 
Loss D Real: 0.2386 (0.3210) Acc D Real: 73.221% 
Loss D Fake: 0.4461 (0.4242) Acc D Fake: 99.565% 
Loss D: 0.685 
Loss G: 1.0375 (1.0862) Acc G: 0.083% 
LR: 2.000e-04 

2023-03-02 01:46:26,378 -                train: [    INFO] - TRAIN Iteration: [ 162/226] 
Loss D Real: 0.4383 (0.3217) Acc D Real: 73.138% 
Loss D Fake: 0.4440 (0.4243) Acc D Fake: 99.568% 
Loss D: 0.882 
Loss G: 1.0391 (1.0859) Acc G: 0.082% 
LR: 2.000e-04 

2023-03-02 01:46:26,385 -                train: [    INFO] - TRAIN Iteration: [ 163/226] 
Loss D Real: 0.1184 (0.3205) Acc D Real: 73.252% 
Loss D Fake: 0.4434 (0.4245) Acc D Fake: 99.571% 
Loss D: 0.562 
Loss G: 1.0412 (1.0856) Acc G: 0.082% 
LR: 2.000e-04 

2023-03-02 01:46:26,393 -                train: [    INFO] - TRAIN Iteration: [ 164/226] 
Loss D Real: 0.2290 (0.3199) Acc D Real: 73.294% 
Loss D Fake: 0.4420 (0.4246) Acc D Fake: 99.573% 
Loss D: 0.671 
Loss G: 1.0436 (1.0854) Acc G: 0.081% 
LR: 2.000e-04 

2023-03-02 01:46:26,400 -                train: [    INFO] - TRAIN Iteration: [ 165/226] 
Loss D Real: 0.2922 (0.3198) Acc D Real: 73.299% 
Loss D Fake: 0.4409 (0.4247) Acc D Fake: 99.576% 
Loss D: 0.733 
Loss G: 1.0453 (1.0851) Acc G: 0.081% 
LR: 2.000e-04 

2023-03-02 01:46:26,407 -                train: [    INFO] - TRAIN Iteration: [ 166/226] 
Loss D Real: 0.2531 (0.3194) Acc D Real: 73.324% 
Loss D Fake: 0.4403 (0.4248) Acc D Fake: 99.578% 
Loss D: 0.693 
Loss G: 1.0458 (1.0849) Acc G: 0.080% 
LR: 2.000e-04 

2023-03-02 01:46:26,414 -                train: [    INFO] - TRAIN Iteration: [ 167/226] 
Loss D Real: 0.3200 (0.3194) Acc D Real: 73.310% 
Loss D Fake: 0.4407 (0.4249) Acc D Fake: 99.581% 
Loss D: 0.761 
Loss G: 1.0433 (1.0846) Acc G: 0.080% 
LR: 2.000e-04 

2023-03-02 01:46:26,422 -                train: [    INFO] - TRAIN Iteration: [ 168/226] 
Loss D Real: 0.3593 (0.3196) Acc D Real: 73.275% 
Loss D Fake: 0.4439 (0.4250) Acc D Fake: 99.583% 
Loss D: 0.803 
Loss G: 1.0338 (1.0843) Acc G: 0.079% 
LR: 2.000e-04 

2023-03-02 01:46:26,429 -                train: [    INFO] - TRAIN Iteration: [ 169/226] 
Loss D Real: 0.2682 (0.3193) Acc D Real: 73.295% 
Loss D Fake: 0.4514 (0.4251) Acc D Fake: 99.586% 
Loss D: 0.720 
Loss G: 1.0348 (1.0840) Acc G: 0.079% 
LR: 2.000e-04 

2023-03-02 01:46:26,436 -                train: [    INFO] - TRAIN Iteration: [ 170/226] 
Loss D Real: 0.4015 (0.3198) Acc D Real: 73.227% 
Loss D Fake: 0.4444 (0.4252) Acc D Fake: 99.588% 
Loss D: 0.846 
Loss G: 1.0408 (1.0838) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:46:26,443 -                train: [    INFO] - TRAIN Iteration: [ 171/226] 
Loss D Real: 0.2601 (0.3194) Acc D Real: 73.248% 
Loss D Fake: 0.4422 (0.4253) Acc D Fake: 99.591% 
Loss D: 0.702 
Loss G: 1.0437 (1.0836) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:46:26,451 -                train: [    INFO] - TRAIN Iteration: [ 172/226] 
Loss D Real: 0.3704 (0.3197) Acc D Real: 73.203% 
Loss D Fake: 0.4413 (0.4254) Acc D Fake: 99.593% 
Loss D: 0.812 
Loss G: 1.0439 (1.0833) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:46:26,458 -                train: [    INFO] - TRAIN Iteration: [ 173/226] 
Loss D Real: 0.2915 (0.3196) Acc D Real: 73.206% 
Loss D Fake: 0.4416 (0.4255) Acc D Fake: 99.595% 
Loss D: 0.733 
Loss G: 1.0434 (1.0831) Acc G: 0.077% 
LR: 2.000e-04 

2023-03-02 01:46:26,465 -                train: [    INFO] - TRAIN Iteration: [ 174/226] 
Loss D Real: 0.2874 (0.3194) Acc D Real: 73.206% 
Loss D Fake: 0.4418 (0.4256) Acc D Fake: 99.598% 
Loss D: 0.729 
Loss G: 1.0427 (1.0829) Acc G: 0.077% 
LR: 2.000e-04 

2023-03-02 01:46:26,472 -                train: [    INFO] - TRAIN Iteration: [ 175/226] 
Loss D Real: 0.3476 (0.3195) Acc D Real: 73.173% 
Loss D Fake: 0.4424 (0.4257) Acc D Fake: 99.600% 
Loss D: 0.790 
Loss G: 1.0413 (1.0826) Acc G: 0.076% 
LR: 2.000e-04 

2023-03-02 01:46:26,479 -                train: [    INFO] - TRAIN Iteration: [ 176/226] 
Loss D Real: 0.3452 (0.3197) Acc D Real: 73.143% 
Loss D Fake: 0.4434 (0.4258) Acc D Fake: 99.602% 
Loss D: 0.789 
Loss G: 1.0386 (1.0824) Acc G: 0.076% 
LR: 2.000e-04 

2023-03-02 01:46:26,487 -                train: [    INFO] - TRAIN Iteration: [ 177/226] 
Loss D Real: 0.2102 (0.3191) Acc D Real: 73.197% 
Loss D Fake: 0.4450 (0.4259) Acc D Fake: 99.605% 
Loss D: 0.655 
Loss G: 1.0359 (1.0821) Acc G: 0.075% 
LR: 2.000e-04 

2023-03-02 01:46:26,494 -                train: [    INFO] - TRAIN Iteration: [ 178/226] 
Loss D Real: 0.2314 (0.3186) Acc D Real: 73.235% 
Loss D Fake: 0.4463 (0.4260) Acc D Fake: 99.607% 
Loss D: 0.678 
Loss G: 1.0350 (1.0818) Acc G: 0.075% 
LR: 2.000e-04 

2023-03-02 01:46:26,501 -                train: [    INFO] - TRAIN Iteration: [ 179/226] 
Loss D Real: 0.3182 (0.3186) Acc D Real: 73.218% 
Loss D Fake: 0.4459 (0.4261) Acc D Fake: 99.609% 
Loss D: 0.764 
Loss G: 1.0378 (1.0816) Acc G: 0.074% 
LR: 2.000e-04 

2023-03-02 01:46:26,508 -                train: [    INFO] - TRAIN Iteration: [ 180/226] 
Loss D Real: 0.2542 (0.3182) Acc D Real: 73.242% 
Loss D Fake: 0.4437 (0.4262) Acc D Fake: 99.611% 
Loss D: 0.698 
Loss G: 1.0417 (1.0814) Acc G: 0.074% 
LR: 2.000e-04 

2023-03-02 01:46:26,515 -                train: [    INFO] - TRAIN Iteration: [ 181/226] 
Loss D Real: 0.1881 (0.3175) Acc D Real: 73.299% 
Loss D Fake: 0.4416 (0.4263) Acc D Fake: 99.613% 
Loss D: 0.630 
Loss G: 1.0461 (1.0812) Acc G: 0.074% 
LR: 2.000e-04 

2023-03-02 01:46:26,523 -                train: [    INFO] - TRAIN Iteration: [ 182/226] 
Loss D Real: 0.2876 (0.3173) Acc D Real: 73.301% 
Loss D Fake: 0.4396 (0.4264) Acc D Fake: 99.615% 
Loss D: 0.727 
Loss G: 1.0484 (1.0810) Acc G: 0.073% 
LR: 2.000e-04 

2023-03-02 01:46:26,530 -                train: [    INFO] - TRAIN Iteration: [ 183/226] 
Loss D Real: 0.2474 (0.3170) Acc D Real: 73.333% 
Loss D Fake: 0.4388 (0.4265) Acc D Fake: 99.617% 
Loss D: 0.686 
Loss G: 1.0501 (1.0808) Acc G: 0.073% 
LR: 2.000e-04 

2023-03-02 01:46:26,537 -                train: [    INFO] - TRAIN Iteration: [ 184/226] 
Loss D Real: 0.2506 (0.3166) Acc D Real: 73.354% 
Loss D Fake: 0.4385 (0.4265) Acc D Fake: 99.620% 
Loss D: 0.689 
Loss G: 1.0482 (1.0807) Acc G: 0.072% 
LR: 2.000e-04 

2023-03-02 01:46:26,544 -                train: [    INFO] - TRAIN Iteration: [ 185/226] 
Loss D Real: 0.2263 (0.3161) Acc D Real: 73.389% 
Loss D Fake: 0.4409 (0.4266) Acc D Fake: 99.622% 
Loss D: 0.667 
Loss G: 1.0486 (1.0805) Acc G: 0.072% 
LR: 2.000e-04 

2023-03-02 01:46:26,551 -                train: [    INFO] - TRAIN Iteration: [ 186/226] 
Loss D Real: 0.2804 (0.3159) Acc D Real: 73.394% 
Loss D Fake: 0.4377 (0.4267) Acc D Fake: 99.624% 
Loss D: 0.718 
Loss G: 1.0567 (1.0804) Acc G: 0.072% 
LR: 2.000e-04 

2023-03-02 01:46:26,559 -                train: [    INFO] - TRAIN Iteration: [ 187/226] 
Loss D Real: 0.3594 (0.3161) Acc D Real: 73.364% 
Loss D Fake: 0.4338 (0.4267) Acc D Fake: 99.626% 
Loss D: 0.793 
Loss G: 1.0604 (1.0803) Acc G: 0.071% 
LR: 2.000e-04 

2023-03-02 01:46:26,566 -                train: [    INFO] - TRAIN Iteration: [ 188/226] 
Loss D Real: 0.3487 (0.3163) Acc D Real: 73.334% 
Loss D Fake: 0.4329 (0.4267) Acc D Fake: 99.628% 
Loss D: 0.782 
Loss G: 1.0613 (1.0802) Acc G: 0.071% 
LR: 2.000e-04 

2023-03-02 01:46:26,573 -                train: [    INFO] - TRAIN Iteration: [ 189/226] 
Loss D Real: 0.2190 (0.3158) Acc D Real: 73.378% 
Loss D Fake: 0.4324 (0.4268) Acc D Fake: 99.630% 
Loss D: 0.651 
Loss G: 1.0632 (1.0801) Acc G: 0.071% 
LR: 2.000e-04 

2023-03-02 01:46:26,580 -                train: [    INFO] - TRAIN Iteration: [ 190/226] 
Loss D Real: 0.4049 (0.3163) Acc D Real: 73.319% 
Loss D Fake: 0.4321 (0.4268) Acc D Fake: 99.632% 
Loss D: 0.837 
Loss G: 1.0600 (1.0800) Acc G: 0.070% 
LR: 2.000e-04 

2023-03-02 01:46:26,587 -                train: [    INFO] - TRAIN Iteration: [ 191/226] 
Loss D Real: 0.2931 (0.3162) Acc D Real: 73.323% 
Loss D Fake: 0.4351 (0.4268) Acc D Fake: 99.634% 
Loss D: 0.728 
Loss G: 1.0542 (1.0798) Acc G: 0.070% 
LR: 2.000e-04 

2023-03-02 01:46:26,595 -                train: [    INFO] - TRAIN Iteration: [ 192/226] 
Loss D Real: 0.2984 (0.3161) Acc D Real: 73.327% 
Loss D Fake: 0.4377 (0.4269) Acc D Fake: 99.635% 
Loss D: 0.736 
Loss G: 1.0537 (1.0797) Acc G: 0.069% 
LR: 2.000e-04 

2023-03-02 01:46:26,602 -                train: [    INFO] - TRAIN Iteration: [ 193/226] 
Loss D Real: 0.3047 (0.3160) Acc D Real: 73.324% 
Loss D Fake: 0.4360 (0.4269) Acc D Fake: 99.637% 
Loss D: 0.741 
Loss G: 1.0569 (1.0796) Acc G: 0.069% 
LR: 2.000e-04 

2023-03-02 01:46:26,609 -                train: [    INFO] - TRAIN Iteration: [ 194/226] 
Loss D Real: 0.3273 (0.3161) Acc D Real: 73.308% 
Loss D Fake: 0.4346 (0.4270) Acc D Fake: 99.639% 
Loss D: 0.762 
Loss G: 1.0583 (1.0795) Acc G: 0.069% 
LR: 2.000e-04 

2023-03-02 01:46:26,616 -                train: [    INFO] - TRAIN Iteration: [ 195/226] 
Loss D Real: 0.2464 (0.3157) Acc D Real: 73.330% 
Loss D Fake: 0.4342 (0.4270) Acc D Fake: 99.641% 
Loss D: 0.681 
Loss G: 1.0588 (1.0794) Acc G: 0.068% 
LR: 2.000e-04 

2023-03-02 01:46:26,624 -                train: [    INFO] - TRAIN Iteration: [ 196/226] 
Loss D Real: 0.2542 (0.3154) Acc D Real: 73.350% 
Loss D Fake: 0.4340 (0.4271) Acc D Fake: 99.643% 
Loss D: 0.688 
Loss G: 1.0592 (1.0792) Acc G: 0.068% 
LR: 2.000e-04 

2023-03-02 01:46:26,631 -                train: [    INFO] - TRAIN Iteration: [ 197/226] 
Loss D Real: 0.3397 (0.3155) Acc D Real: 73.322% 
Loss D Fake: 0.4339 (0.4271) Acc D Fake: 99.645% 
Loss D: 0.774 
Loss G: 1.0586 (1.0791) Acc G: 0.068% 
LR: 2.000e-04 

2023-03-02 01:46:26,639 -                train: [    INFO] - TRAIN Iteration: [ 198/226] 
Loss D Real: 0.3167 (0.3155) Acc D Real: 73.309% 
Loss D Fake: 0.4345 (0.4271) Acc D Fake: 99.646% 
Loss D: 0.751 
Loss G: 1.0571 (1.0790) Acc G: 0.067% 
LR: 2.000e-04 

2023-03-02 01:46:26,646 -                train: [    INFO] - TRAIN Iteration: [ 199/226] 
Loss D Real: 0.3690 (0.3158) Acc D Real: 73.267% 
Loss D Fake: 0.4357 (0.4272) Acc D Fake: 99.648% 
Loss D: 0.805 
Loss G: 1.0536 (1.0789) Acc G: 0.067% 
LR: 2.000e-04 

2023-03-02 01:46:26,654 -                train: [    INFO] - TRAIN Iteration: [ 200/226] 
Loss D Real: 0.3073 (0.3157) Acc D Real: 73.257% 
Loss D Fake: 0.4380 (0.4272) Acc D Fake: 99.650% 
Loss D: 0.745 
Loss G: 1.0490 (1.0788) Acc G: 0.067% 
LR: 2.000e-04 

2023-03-02 01:46:26,661 -                train: [    INFO] - TRAIN Iteration: [ 201/226] 
Loss D Real: 0.2521 (0.3154) Acc D Real: 73.280% 
Loss D Fake: 0.4407 (0.4273) Acc D Fake: 99.652% 
Loss D: 0.693 
Loss G: 1.0431 (1.0786) Acc G: 0.066% 
LR: 2.000e-04 

2023-03-02 01:46:26,668 -                train: [    INFO] - TRAIN Iteration: [ 202/226] 
Loss D Real: 0.4614 (0.3162) Acc D Real: 73.188% 
Loss D Fake: 0.4458 (0.4274) Acc D Fake: 99.653% 
Loss D: 0.907 
Loss G: 1.0309 (1.0783) Acc G: 0.066% 
LR: 2.000e-04 

2023-03-02 01:46:26,676 -                train: [    INFO] - TRAIN Iteration: [ 203/226] 
Loss D Real: 0.2696 (0.3159) Acc D Real: 73.191% 
Loss D Fake: 0.4536 (0.4275) Acc D Fake: 99.655% 
Loss D: 0.723 
Loss G: 1.0323 (1.0781) Acc G: 0.066% 
LR: 2.000e-04 

2023-03-02 01:46:26,683 -                train: [    INFO] - TRAIN Iteration: [ 204/226] 
Loss D Real: 0.2874 (0.3158) Acc D Real: 73.192% 
Loss D Fake: 0.4468 (0.4276) Acc D Fake: 99.657% 
Loss D: 0.734 
Loss G: 1.0352 (1.0779) Acc G: 0.065% 
LR: 2.000e-04 

2023-03-02 01:46:26,690 -                train: [    INFO] - TRAIN Iteration: [ 205/226] 
Loss D Real: 0.2184 (0.3153) Acc D Real: 73.226% 
Loss D Fake: 0.4466 (0.4277) Acc D Fake: 99.659% 
Loss D: 0.665 
Loss G: 1.0349 (1.0777) Acc G: 0.065% 
LR: 2.000e-04 

2023-03-02 01:46:26,697 -                train: [    INFO] - TRAIN Iteration: [ 206/226] 
Loss D Real: 0.3784 (0.3156) Acc D Real: 73.182% 
Loss D Fake: 0.4472 (0.4278) Acc D Fake: 99.660% 
Loss D: 0.826 
Loss G: 1.0327 (1.0775) Acc G: 0.065% 
LR: 2.000e-04 

2023-03-02 01:46:26,704 -                train: [    INFO] - TRAIN Iteration: [ 207/226] 
Loss D Real: 0.2701 (0.3154) Acc D Real: 73.193% 
Loss D Fake: 0.4484 (0.4279) Acc D Fake: 99.662% 
Loss D: 0.718 
Loss G: 1.0311 (1.0773) Acc G: 0.064% 
LR: 2.000e-04 

2023-03-02 01:46:26,712 -                train: [    INFO] - TRAIN Iteration: [ 208/226] 
Loss D Real: 0.2189 (0.3149) Acc D Real: 73.235% 
Loss D Fake: 0.4490 (0.4280) Acc D Fake: 99.663% 
Loss D: 0.668 
Loss G: 1.0306 (1.0770) Acc G: 0.064% 
LR: 2.000e-04 

2023-03-02 01:46:26,719 -                train: [    INFO] - TRAIN Iteration: [ 209/226] 
Loss D Real: 0.2108 (0.3144) Acc D Real: 73.278% 
Loss D Fake: 0.4489 (0.4281) Acc D Fake: 99.665% 
Loss D: 0.660 
Loss G: 1.0320 (1.0768) Acc G: 0.064% 
LR: 2.000e-04 

2023-03-02 01:46:26,727 -                train: [    INFO] - TRAIN Iteration: [ 210/226] 
Loss D Real: 0.2952 (0.3143) Acc D Real: 73.274% 
Loss D Fake: 0.4484 (0.4282) Acc D Fake: 99.659% 
Loss D: 0.744 
Loss G: 1.0320 (1.0766) Acc G: 0.067% 
LR: 2.000e-04 

2023-03-02 01:46:26,734 -                train: [    INFO] - TRAIN Iteration: [ 211/226] 
Loss D Real: 0.2489 (0.3140) Acc D Real: 73.289% 
Loss D Fake: 0.4490 (0.4283) Acc D Fake: 99.652% 
Loss D: 0.698 
Loss G: 1.0320 (1.0764) Acc G: 0.070% 
LR: 2.000e-04 

2023-03-02 01:46:26,741 -                train: [    INFO] - TRAIN Iteration: [ 212/226] 
Loss D Real: 0.3280 (0.3141) Acc D Real: 73.261% 
Loss D Fake: 0.4490 (0.4284) Acc D Fake: 99.646% 
Loss D: 0.777 
Loss G: 1.0332 (1.0762) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:46:26,748 -                train: [    INFO] - TRAIN Iteration: [ 213/226] 
Loss D Real: 0.2278 (0.3137) Acc D Real: 73.296% 
Loss D Fake: 0.4483 (0.4285) Acc D Fake: 99.640% 
Loss D: 0.676 
Loss G: 1.0336 (1.0760) Acc G: 0.085% 
LR: 2.000e-04 

2023-03-02 01:46:26,756 -                train: [    INFO] - TRAIN Iteration: [ 214/226] 
Loss D Real: 0.3013 (0.3136) Acc D Real: 73.288% 
Loss D Fake: 0.4476 (0.4286) Acc D Fake: 99.634% 
Loss D: 0.749 
Loss G: 1.0370 (1.0758) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,763 -                train: [    INFO] - TRAIN Iteration: [ 215/226] 
Loss D Real: 0.1460 (0.3129) Acc D Real: 73.355% 
Loss D Fake: 0.4454 (0.4287) Acc D Fake: 99.628% 
Loss D: 0.591 
Loss G: 1.0394 (1.0756) Acc G: 0.097% 
LR: 2.000e-04 

2023-03-02 01:46:26,770 -                train: [    INFO] - TRAIN Iteration: [ 216/226] 
Loss D Real: 0.2696 (0.3127) Acc D Real: 73.359% 
Loss D Fake: 0.4447 (0.4287) Acc D Fake: 99.622% 
Loss D: 0.714 
Loss G: 1.0396 (1.0755) Acc G: 0.096% 
LR: 2.000e-04 

2023-03-02 01:46:26,777 -                train: [    INFO] - TRAIN Iteration: [ 217/226] 
Loss D Real: 0.2192 (0.3122) Acc D Real: 73.390% 
Loss D Fake: 0.4454 (0.4288) Acc D Fake: 99.624% 
Loss D: 0.665 
Loss G: 1.0367 (1.0753) Acc G: 0.096% 
LR: 2.000e-04 

2023-03-02 01:46:26,785 -                train: [    INFO] - TRAIN Iteration: [ 218/226] 
Loss D Real: 0.2572 (0.3120) Acc D Real: 73.406% 
Loss D Fake: 0.4475 (0.4289) Acc D Fake: 99.626% 
Loss D: 0.705 
Loss G: 1.0322 (1.0751) Acc G: 0.096% 
LR: 2.000e-04 

2023-03-02 01:46:26,792 -                train: [    INFO] - TRAIN Iteration: [ 219/226] 
Loss D Real: 0.2523 (0.3117) Acc D Real: 73.428% 
Loss D Fake: 0.4501 (0.4290) Acc D Fake: 99.627% 
Loss D: 0.702 
Loss G: 1.0281 (1.0749) Acc G: 0.095% 
LR: 2.000e-04 

2023-03-02 01:46:26,799 -                train: [    INFO] - TRAIN Iteration: [ 220/226] 
Loss D Real: 0.2249 (0.3113) Acc D Real: 73.466% 
Loss D Fake: 0.4515 (0.4291) Acc D Fake: 99.629% 
Loss D: 0.676 
Loss G: 1.0293 (1.0747) Acc G: 0.095% 
LR: 2.000e-04 

2023-03-02 01:46:26,807 -                train: [    INFO] - TRAIN Iteration: [ 221/226] 
Loss D Real: 0.2597 (0.3111) Acc D Real: 73.475% 
Loss D Fake: 0.4491 (0.4292) Acc D Fake: 99.631% 
Loss D: 0.709 
Loss G: 1.0371 (1.0745) Acc G: 0.094% 
LR: 2.000e-04 

2023-03-02 01:46:26,814 -                train: [    INFO] - TRAIN Iteration: [ 222/226] 
Loss D Real: 0.2996 (0.3110) Acc D Real: 73.476% 
Loss D Fake: 0.4450 (0.4293) Acc D Fake: 99.632% 
Loss D: 0.745 
Loss G: 1.0419 (1.0744) Acc G: 0.094% 
LR: 2.000e-04 

2023-03-02 01:46:26,821 -                train: [    INFO] - TRAIN Iteration: [ 223/226] 
Loss D Real: 0.2634 (0.3108) Acc D Real: 73.490% 
Loss D Fake: 0.4429 (0.4293) Acc D Fake: 99.634% 
Loss D: 0.706 
Loss G: 1.0468 (1.0742) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,828 -                train: [    INFO] - TRAIN Iteration: [ 224/226] 
Loss D Real: 0.2373 (0.3105) Acc D Real: 73.514% 
Loss D Fake: 0.4414 (0.4294) Acc D Fake: 99.636% 
Loss D: 0.679 
Loss G: 1.0452 (1.0741) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,836 -                train: [    INFO] - TRAIN Iteration: [ 225/226] 
Loss D Real: 0.2631 (0.3103) Acc D Real: 73.536% 
Loss D Fake: 0.4456 (0.4294) Acc D Fake: 99.637% 
Loss D: 0.709 
Loss G: 1.0301 (1.0739) Acc G: 0.093% 
LR: 2.000e-04 

2023-03-02 01:46:26,843 -                train: [    INFO] - TRAIN Iteration: [ 226/226] 
Loss D Real: 0.0508 (0.3091) Acc D Real: 73.563% 
Loss D Fake: 0.4554 (0.4296) Acc D Fake: 99.638% 
Loss D: 0.506 
Loss G: 1.0457 (1.0738) Acc G: 0.092% 
LR: 2.000e-04 

2023-03-02 01:46:26,855 -                train: [    INFO] - Best Metric: At 2 Epoch Gen 1.035 Dis
2023-03-02 01:46:26,855 -                train: [    INFO] - MODEL TRAINING COMPLETED. 
 BEST RESULT SAVED
2023-03-02 01:46:27,233 -         Optimization: [    INFO] - 
 Batch: 1/121
2023-03-02 01:46:29,445 -         Optimization: [    INFO] - Batch [1/121]: Anomaly Score: 494.884 label: 0.0
2023-03-02 01:46:29,446 -         Optimization: [    INFO] - 
 Batch: 2/121
2023-03-02 01:46:31,592 -         Optimization: [    INFO] - Batch [2/121]: Anomaly Score: 555.952 label: 0.0
2023-03-02 01:46:31,594 -         Optimization: [    INFO] - 
 Batch: 3/121
2023-03-02 01:46:33,738 -         Optimization: [    INFO] - Batch [3/121]: Anomaly Score: 528.206 label: 0.0
2023-03-02 01:46:33,739 -         Optimization: [    INFO] - 
 Batch: 4/121
2023-03-02 01:46:35,912 -         Optimization: [    INFO] - Batch [4/121]: Anomaly Score: 482.159 label: 0.0
2023-03-02 01:46:35,923 -         Optimization: [    INFO] - 
 Batch: 5/121
2023-03-02 01:46:38,126 -         Optimization: [    INFO] - Batch [5/121]: Anomaly Score: 530.188 label: 0.0
2023-03-02 01:46:38,127 -         Optimization: [    INFO] - 
 Batch: 6/121
2023-03-02 01:46:40,347 -         Optimization: [    INFO] - Batch [6/121]: Anomaly Score: 432.995 label: 0.0
2023-03-02 01:46:40,348 -         Optimization: [    INFO] - 
 Batch: 7/121
2023-03-02 01:46:42,562 -         Optimization: [    INFO] - Batch [7/121]: Anomaly Score: 449.532 label: 0.0
2023-03-02 01:46:42,563 -         Optimization: [    INFO] - 
 Batch: 8/121
2023-03-02 01:46:44,781 -         Optimization: [    INFO] - Batch [8/121]: Anomaly Score: 533.661 label: 0.0
2023-03-02 01:46:44,782 -         Optimization: [    INFO] - 
 Batch: 9/121
2023-03-02 01:46:46,992 -         Optimization: [    INFO] - Batch [9/121]: Anomaly Score: 491.360 label: 0.0
2023-03-02 01:46:46,992 -         Optimization: [    INFO] - 
 Batch: 10/121
2023-03-02 01:46:49,209 -         Optimization: [    INFO] - Batch [10/121]: Anomaly Score: 401.191 label: 0.0
2023-03-02 01:46:49,210 -         Optimization: [    INFO] - 
 Batch: 11/121
2023-03-02 01:46:51,419 -         Optimization: [    INFO] - Batch [11/121]: Anomaly Score: 442.560 label: 0.0
2023-03-02 01:46:51,420 -         Optimization: [    INFO] - 
 Batch: 12/121
2023-03-02 01:46:53,629 -         Optimization: [    INFO] - Batch [12/121]: Anomaly Score: 307.250 label: 0.0
2023-03-02 01:46:53,630 -         Optimization: [    INFO] - 
 Batch: 13/121
2023-03-02 01:46:55,854 -         Optimization: [    INFO] - Batch [13/121]: Anomaly Score: 523.051 label: 0.0
2023-03-02 01:46:55,855 -         Optimization: [    INFO] - 
 Batch: 14/121
2023-03-02 01:46:58,058 -         Optimization: [    INFO] - Batch [14/121]: Anomaly Score: 463.632 label: 0.0
2023-03-02 01:46:58,059 -         Optimization: [    INFO] - 
 Batch: 15/121
2023-03-02 01:47:00,350 -         Optimization: [    INFO] - Batch [15/121]: Anomaly Score: 514.645 label: 0.0
2023-03-02 01:47:00,351 -         Optimization: [    INFO] - 
 Batch: 16/121
2023-03-02 01:47:02,517 -         Optimization: [    INFO] - Batch [16/121]: Anomaly Score: 547.532 label: 0.0
2023-03-02 01:47:02,519 -         Optimization: [    INFO] - 
 Batch: 17/121
2023-03-02 01:47:04,661 -         Optimization: [    INFO] - Batch [17/121]: Anomaly Score: 505.176 label: 0.0
2023-03-02 01:47:04,662 -         Optimization: [    INFO] - 
 Batch: 18/121
2023-03-02 01:47:06,796 -         Optimization: [    INFO] - Batch [18/121]: Anomaly Score: 494.729 label: 0.0
2023-03-02 01:47:06,797 -         Optimization: [    INFO] - 
 Batch: 19/121
2023-03-02 01:47:08,943 -         Optimization: [    INFO] - Batch [19/121]: Anomaly Score: 536.249 label: 0.0
2023-03-02 01:47:08,944 -         Optimization: [    INFO] - 
 Batch: 20/121
2023-03-02 01:47:11,092 -         Optimization: [    INFO] - Batch [20/121]: Anomaly Score: 513.245 label: 0.0
2023-03-02 01:47:11,092 -         Optimization: [    INFO] - 
 Batch: 21/121
2023-03-02 01:47:13,245 -         Optimization: [    INFO] - Batch [21/121]: Anomaly Score: 561.380 label: 0.0
2023-03-02 01:47:13,246 -         Optimization: [    INFO] - 
 Batch: 22/121
2023-03-02 01:47:15,390 -         Optimization: [    INFO] - Batch [22/121]: Anomaly Score: 526.281 label: 0.0
2023-03-02 01:47:15,390 -         Optimization: [    INFO] - 
 Batch: 23/121
2023-03-02 01:47:17,612 -         Optimization: [    INFO] - Batch [23/121]: Anomaly Score: 541.963 label: 0.0
2023-03-02 01:47:17,613 -         Optimization: [    INFO] - 
 Batch: 24/121
2023-03-02 01:47:19,764 -         Optimization: [    INFO] - Batch [24/121]: Anomaly Score: 509.156 label: 0.0
2023-03-02 01:47:19,764 -         Optimization: [    INFO] - 
 Batch: 25/121
2023-03-02 01:47:21,898 -         Optimization: [    INFO] - Batch [25/121]: Anomaly Score: 478.568 label: 0.0
2023-03-02 01:47:21,900 -         Optimization: [    INFO] - 
 Batch: 26/121
2023-03-02 01:47:24,041 -         Optimization: [    INFO] - Batch [26/121]: Anomaly Score: 484.538 label: 0.0
2023-03-02 01:47:24,042 -         Optimization: [    INFO] - 
 Batch: 27/121
2023-03-02 01:47:26,174 -         Optimization: [    INFO] - Batch [27/121]: Anomaly Score: 425.827 label: 0.0
2023-03-02 01:47:26,176 -         Optimization: [    INFO] - 
 Batch: 28/121
2023-03-02 01:47:28,400 -         Optimization: [    INFO] - Batch [28/121]: Anomaly Score: 525.287 label: 0.0
2023-03-02 01:47:28,401 -         Optimization: [    INFO] - 
 Batch: 29/121
2023-03-02 01:47:30,579 -         Optimization: [    INFO] - Batch [29/121]: Anomaly Score: 499.947 label: 0.0
2023-03-02 01:47:30,581 -         Optimization: [    INFO] - 
 Batch: 30/121
2023-03-02 01:47:32,725 -         Optimization: [    INFO] - Batch [30/121]: Anomaly Score: 448.132 label: 0.0
2023-03-02 01:47:32,727 -         Optimization: [    INFO] - 
 Batch: 31/121
2023-03-02 01:47:34,874 -         Optimization: [    INFO] - Batch [31/121]: Anomaly Score: 330.229 label: 0.0
2023-03-02 01:47:34,875 -         Optimization: [    INFO] - 
 Batch: 32/121
2023-03-02 01:47:37,006 -         Optimization: [    INFO] - Batch [32/121]: Anomaly Score: 308.200 label: 0.0
2023-03-02 01:47:37,008 -         Optimization: [    INFO] - 
 Batch: 33/121
2023-03-02 01:47:39,155 -         Optimization: [    INFO] - Batch [33/121]: Anomaly Score: 329.719 label: 0.0
2023-03-02 01:47:39,157 -         Optimization: [    INFO] - 
 Batch: 34/121
2023-03-02 01:47:41,293 -         Optimization: [    INFO] - Batch [34/121]: Anomaly Score: 414.727 label: 0.0
2023-03-02 01:47:41,295 -         Optimization: [    INFO] - 
 Batch: 35/121
2023-03-02 01:47:43,426 -         Optimization: [    INFO] - Batch [35/121]: Anomaly Score: 251.332 label: 0.0
2023-03-02 01:47:43,428 -         Optimization: [    INFO] - 
 Batch: 36/121
2023-03-02 01:47:45,571 -         Optimization: [    INFO] - Batch [36/121]: Anomaly Score: 454.235 label: 0.0
2023-03-02 01:47:45,572 -         Optimization: [    INFO] - 
 Batch: 37/121
2023-03-02 01:47:47,703 -         Optimization: [    INFO] - Batch [37/121]: Anomaly Score: 421.298 label: 0.0
2023-03-02 01:47:47,704 -         Optimization: [    INFO] - 
 Batch: 38/121
2023-03-02 01:47:49,849 -         Optimization: [    INFO] - Batch [38/121]: Anomaly Score: 443.637 label: 0.0
2023-03-02 01:47:49,851 -         Optimization: [    INFO] - 
 Batch: 39/121
2023-03-02 01:47:51,991 -         Optimization: [    INFO] - Batch [39/121]: Anomaly Score: 318.165 label: 0.0
2023-03-02 01:47:51,993 -         Optimization: [    INFO] - 
 Batch: 40/121
2023-03-02 01:47:54,138 -         Optimization: [    INFO] - Batch [40/121]: Anomaly Score: 455.642 label: 0.0
2023-03-02 01:47:54,140 -         Optimization: [    INFO] - 
 Batch: 41/121
2023-03-02 01:47:56,309 -         Optimization: [    INFO] - Batch [41/121]: Anomaly Score: 388.874 label: 0.0
2023-03-02 01:47:56,311 -         Optimization: [    INFO] - 
 Batch: 42/121
2023-03-02 01:47:58,476 -         Optimization: [    INFO] - Batch [42/121]: Anomaly Score: 296.626 label: 0.0
2023-03-02 01:47:58,477 -         Optimization: [    INFO] - 
 Batch: 43/121
2023-03-02 01:48:00,646 -         Optimization: [    INFO] - Batch [43/121]: Anomaly Score: 313.542 label: 0.0
2023-03-02 01:48:00,647 -         Optimization: [    INFO] - 
 Batch: 44/121
2023-03-02 01:48:02,819 -         Optimization: [    INFO] - Batch [44/121]: Anomaly Score: 291.422 label: 0.0
2023-03-02 01:48:02,821 -         Optimization: [    INFO] - 
 Batch: 45/121
2023-03-02 01:48:05,029 -         Optimization: [    INFO] - Batch [45/121]: Anomaly Score: 325.217 label: 0.0
2023-03-02 01:48:05,030 -         Optimization: [    INFO] - 
 Batch: 46/121
2023-03-02 01:48:07,254 -         Optimization: [    INFO] - Batch [46/121]: Anomaly Score: 419.879 label: 0.0
2023-03-02 01:48:07,255 -         Optimization: [    INFO] - 
 Batch: 47/121
2023-03-02 01:48:09,458 -         Optimization: [    INFO] - Batch [47/121]: Anomaly Score: 306.683 label: 0.0
2023-03-02 01:48:09,459 -         Optimization: [    INFO] - 
 Batch: 48/121
2023-03-02 01:48:11,630 -         Optimization: [    INFO] - Batch [48/121]: Anomaly Score: 452.744 label: 0.0
2023-03-02 01:48:11,631 -         Optimization: [    INFO] - 
 Batch: 49/121
2023-03-02 01:48:13,836 -         Optimization: [    INFO] - Batch [49/121]: Anomaly Score: 453.138 label: 0.0
2023-03-02 01:48:13,839 -         Optimization: [    INFO] - 
 Batch: 50/121
2023-03-02 01:48:16,043 -         Optimization: [    INFO] - Batch [50/121]: Anomaly Score: 22.248 label: 0.0
2023-03-02 01:48:16,044 -         Optimization: [    INFO] - 
 Batch: 51/121
2023-03-02 01:48:18,244 -         Optimization: [    INFO] - Batch [51/121]: Anomaly Score: 298.693 label: 0.0
2023-03-02 01:48:18,246 -         Optimization: [    INFO] - 
 Batch: 52/121
2023-03-02 01:48:20,452 -         Optimization: [    INFO] - Batch [52/121]: Anomaly Score: 18.405 label: 0.0
2023-03-02 01:48:20,453 -         Optimization: [    INFO] - 
 Batch: 53/121
2023-03-02 01:48:22,649 -         Optimization: [    INFO] - Batch [53/121]: Anomaly Score: 330.998 label: 0.0
2023-03-02 01:48:22,651 -         Optimization: [    INFO] - 
 Batch: 54/121
2023-03-02 01:48:24,867 -         Optimization: [    INFO] - Batch [54/121]: Anomaly Score: 329.798 label: 0.0
2023-03-02 01:48:24,869 -         Optimization: [    INFO] - 
 Batch: 55/121
2023-03-02 01:48:27,231 -         Optimization: [    INFO] - Batch [55/121]: Anomaly Score: 276.983 label: 0.0
2023-03-02 01:48:27,232 -         Optimization: [    INFO] - 
 Batch: 56/121
2023-03-02 01:48:29,448 -         Optimization: [    INFO] - Batch [56/121]: Anomaly Score: 303.370 label: 0.0
2023-03-02 01:48:29,449 -         Optimization: [    INFO] - 
 Batch: 57/121
2023-03-02 01:48:31,656 -         Optimization: [    INFO] - Batch [57/121]: Anomaly Score: 323.731 label: 0.0
2023-03-02 01:48:31,658 -         Optimization: [    INFO] - 
 Batch: 58/121
2023-03-02 01:48:33,856 -         Optimization: [    INFO] - Batch [58/121]: Anomaly Score: 417.114 label: 0.0
2023-03-02 01:48:33,857 -         Optimization: [    INFO] - 
 Batch: 59/121
2023-03-02 01:48:36,007 -         Optimization: [    INFO] - Batch [59/121]: Anomaly Score: 314.047 label: 0.0
2023-03-02 01:48:36,009 -         Optimization: [    INFO] - 
 Batch: 60/121
2023-03-02 01:48:38,151 -         Optimization: [    INFO] - Batch [60/121]: Anomaly Score: 327.726 label: 1.0
2023-03-02 01:48:38,152 -         Optimization: [    INFO] - 
 Batch: 61/121
2023-03-02 01:48:40,305 -         Optimization: [    INFO] - Batch [61/121]: Anomaly Score: 317.624 label: 1.0
2023-03-02 01:48:40,306 -         Optimization: [    INFO] - 
 Batch: 62/121
2023-03-02 01:48:42,500 -         Optimization: [    INFO] - Batch [62/121]: Anomaly Score: 353.308 label: 1.0
2023-03-02 01:48:42,502 -         Optimization: [    INFO] - 
 Batch: 63/121
2023-03-02 01:48:44,651 -         Optimization: [    INFO] - Batch [63/121]: Anomaly Score: 398.043 label: 1.0
2023-03-02 01:48:44,653 -         Optimization: [    INFO] - 
 Batch: 64/121
2023-03-02 01:48:46,792 -         Optimization: [    INFO] - Batch [64/121]: Anomaly Score: 332.994 label: 1.0
2023-03-02 01:48:46,794 -         Optimization: [    INFO] - 
 Batch: 65/121
2023-03-02 01:48:48,942 -         Optimization: [    INFO] - Batch [65/121]: Anomaly Score: 310.543 label: 1.0
2023-03-02 01:48:48,944 -         Optimization: [    INFO] - 
 Batch: 66/121
2023-03-02 01:48:51,075 -         Optimization: [    INFO] - Batch [66/121]: Anomaly Score: 316.615 label: 1.0
2023-03-02 01:48:51,077 -         Optimization: [    INFO] - 
 Batch: 67/121
2023-03-02 01:48:53,250 -         Optimization: [    INFO] - Batch [67/121]: Anomaly Score: 329.425 label: 0.0
2023-03-02 01:48:53,251 -         Optimization: [    INFO] - 
 Batch: 68/121
2023-03-02 01:48:55,417 -         Optimization: [    INFO] - Batch [68/121]: Anomaly Score: 417.265 label: 0.0
2023-03-02 01:48:55,418 -         Optimization: [    INFO] - 
 Batch: 69/121
2023-03-02 01:48:57,627 -         Optimization: [    INFO] - Batch [69/121]: Anomaly Score: 276.549 label: 0.0
2023-03-02 01:48:57,628 -         Optimization: [    INFO] - 
 Batch: 70/121
2023-03-02 01:48:59,834 -         Optimization: [    INFO] - Batch [70/121]: Anomaly Score: 29.642 label: 0.0
2023-03-02 01:48:59,836 -         Optimization: [    INFO] - 
 Batch: 71/121
2023-03-02 01:49:01,982 -         Optimization: [    INFO] - Batch [71/121]: Anomaly Score: 343.709 label: 0.0
2023-03-02 01:49:01,984 -         Optimization: [    INFO] - 
 Batch: 72/121
2023-03-02 01:49:04,240 -         Optimization: [    INFO] - Batch [72/121]: Anomaly Score: 323.553 label: 0.0
2023-03-02 01:49:04,242 -         Optimization: [    INFO] - 
 Batch: 73/121
2023-03-02 01:49:06,463 -         Optimization: [    INFO] - Batch [73/121]: Anomaly Score: 241.083 label: 0.0
2023-03-02 01:49:06,465 -         Optimization: [    INFO] - 
 Batch: 74/121
2023-03-02 01:49:08,741 -         Optimization: [    INFO] - Batch [74/121]: Anomaly Score: 416.259 label: 0.0
2023-03-02 01:49:08,743 -         Optimization: [    INFO] - 
 Batch: 75/121
2023-03-02 01:49:10,997 -         Optimization: [    INFO] - Batch [75/121]: Anomaly Score: 198.693 label: 0.0
2023-03-02 01:49:10,999 -         Optimization: [    INFO] - 
 Batch: 76/121
2023-03-02 01:49:13,258 -         Optimization: [    INFO] - Batch [76/121]: Anomaly Score: 416.539 label: 0.0
2023-03-02 01:49:13,260 -         Optimization: [    INFO] - 
 Batch: 77/121
2023-03-02 01:49:15,516 -         Optimization: [    INFO] - Batch [77/121]: Anomaly Score: 454.413 label: 0.0
2023-03-02 01:49:15,517 -         Optimization: [    INFO] - 
 Batch: 78/121
2023-03-02 01:49:17,785 -         Optimization: [    INFO] - Batch [78/121]: Anomaly Score: 491.813 label: 0.0
2023-03-02 01:49:17,787 -         Optimization: [    INFO] - 
 Batch: 79/121
2023-03-02 01:49:20,253 -         Optimization: [    INFO] - Batch [79/121]: Anomaly Score: 403.503 label: 0.0
2023-03-02 01:49:20,254 -         Optimization: [    INFO] - 
 Batch: 80/121
2023-03-02 01:49:22,465 -         Optimization: [    INFO] - Batch [80/121]: Anomaly Score: 465.682 label: 0.0
2023-03-02 01:49:22,466 -         Optimization: [    INFO] - 
 Batch: 81/121
2023-03-02 01:49:24,725 -         Optimization: [    INFO] - Batch [81/121]: Anomaly Score: 425.459 label: 0.0
2023-03-02 01:49:24,727 -         Optimization: [    INFO] - 
 Batch: 82/121
2023-03-02 01:49:26,992 -         Optimization: [    INFO] - Batch [82/121]: Anomaly Score: 458.459 label: 0.0
2023-03-02 01:49:26,994 -         Optimization: [    INFO] - 
 Batch: 83/121
2023-03-02 01:49:29,209 -         Optimization: [    INFO] - Batch [83/121]: Anomaly Score: 422.689 label: 0.0
2023-03-02 01:49:29,210 -         Optimization: [    INFO] - 
 Batch: 84/121
2023-03-02 01:49:31,429 -         Optimization: [    INFO] - Batch [84/121]: Anomaly Score: 467.501 label: 0.0
2023-03-02 01:49:31,431 -         Optimization: [    INFO] - 
 Batch: 85/121
2023-03-02 01:49:33,689 -         Optimization: [    INFO] - Batch [85/121]: Anomaly Score: 539.567 label: 0.0
2023-03-02 01:49:33,691 -         Optimization: [    INFO] - 
 Batch: 86/121
2023-03-02 01:49:35,925 -         Optimization: [    INFO] - Batch [86/121]: Anomaly Score: 509.117 label: 0.0
2023-03-02 01:49:35,927 -         Optimization: [    INFO] - 
 Batch: 87/121
2023-03-02 01:49:38,188 -         Optimization: [    INFO] - Batch [87/121]: Anomaly Score: 447.865 label: 0.0
2023-03-02 01:49:38,190 -         Optimization: [    INFO] - 
 Batch: 88/121
2023-03-02 01:49:40,480 -         Optimization: [    INFO] - Batch [88/121]: Anomaly Score: 484.809 label: 0.0
2023-03-02 01:49:40,482 -         Optimization: [    INFO] - 
 Batch: 89/121
2023-03-02 01:49:42,750 -         Optimization: [    INFO] - Batch [89/121]: Anomaly Score: 384.960 label: 0.0
2023-03-02 01:49:42,752 -         Optimization: [    INFO] - 
 Batch: 90/121
2023-03-02 01:49:45,007 -         Optimization: [    INFO] - Batch [90/121]: Anomaly Score: 506.259 label: 0.0
2023-03-02 01:49:45,008 -         Optimization: [    INFO] - 
 Batch: 91/121
2023-03-02 01:49:47,242 -         Optimization: [    INFO] - Batch [91/121]: Anomaly Score: 526.638 label: 0.0
2023-03-02 01:49:47,244 -         Optimization: [    INFO] - 
 Batch: 92/121
2023-03-02 01:49:49,504 -         Optimization: [    INFO] - Batch [92/121]: Anomaly Score: 559.858 label: 0.0
2023-03-02 01:49:49,505 -         Optimization: [    INFO] - 
 Batch: 93/121
2023-03-02 01:49:51,762 -         Optimization: [    INFO] - Batch [93/121]: Anomaly Score: 565.029 label: 0.0
2023-03-02 01:49:51,763 -         Optimization: [    INFO] - 
 Batch: 94/121
2023-03-02 01:49:53,940 -         Optimization: [    INFO] - Batch [94/121]: Anomaly Score: 488.652 label: 0.0
2023-03-02 01:49:53,942 -         Optimization: [    INFO] - 
 Batch: 95/121
2023-03-02 01:49:56,106 -         Optimization: [    INFO] - Batch [95/121]: Anomaly Score: 529.387 label: 0.0
2023-03-02 01:49:56,107 -         Optimization: [    INFO] - 
 Batch: 96/121
2023-03-02 01:49:58,275 -         Optimization: [    INFO] - Batch [96/121]: Anomaly Score: 551.660 label: 0.0
2023-03-02 01:49:58,277 -         Optimization: [    INFO] - 
 Batch: 97/121
2023-03-02 01:50:00,536 -         Optimization: [    INFO] - Batch [97/121]: Anomaly Score: 524.752 label: 0.0
2023-03-02 01:50:00,537 -         Optimization: [    INFO] - 
 Batch: 98/121
2023-03-02 01:50:02,925 -         Optimization: [    INFO] - Batch [98/121]: Anomaly Score: 536.081 label: 0.0
2023-03-02 01:50:02,927 -         Optimization: [    INFO] - 
 Batch: 99/121
2023-03-02 01:50:06,220 -         Optimization: [    INFO] - Batch [99/121]: Anomaly Score: 535.730 label: 0.0
2023-03-02 01:50:06,223 -         Optimization: [    INFO] - 
 Batch: 100/121
2023-03-02 01:50:09,408 -         Optimization: [    INFO] - Batch [100/121]: Anomaly Score: 527.910 label: 1.0
2023-03-02 01:50:09,410 -         Optimization: [    INFO] - 
 Batch: 101/121
2023-03-02 01:50:11,836 -         Optimization: [    INFO] - Batch [101/121]: Anomaly Score: 558.316 label: 1.0
2023-03-02 01:50:11,838 -         Optimization: [    INFO] - 
 Batch: 102/121
2023-03-02 01:50:14,137 -         Optimization: [    INFO] - Batch [102/121]: Anomaly Score: 533.271 label: 1.0
2023-03-02 01:50:14,139 -         Optimization: [    INFO] - 
 Batch: 103/121
2023-03-02 01:50:16,457 -         Optimization: [    INFO] - Batch [103/121]: Anomaly Score: 568.990 label: 1.0
2023-03-02 01:50:16,458 -         Optimization: [    INFO] - 
 Batch: 104/121
2023-03-02 01:50:18,785 -         Optimization: [    INFO] - Batch [104/121]: Anomaly Score: 586.278 label: 1.0
2023-03-02 01:50:18,787 -         Optimization: [    INFO] - 
 Batch: 105/121
2023-03-02 01:50:21,086 -         Optimization: [    INFO] - Batch [105/121]: Anomaly Score: 542.381 label: 1.0
2023-03-02 01:50:21,088 -         Optimization: [    INFO] - 
 Batch: 106/121
2023-03-02 01:50:23,419 -         Optimization: [    INFO] - Batch [106/121]: Anomaly Score: 566.457 label: 1.0
2023-03-02 01:50:23,421 -         Optimization: [    INFO] - 
 Batch: 107/121
2023-03-02 01:50:25,744 -         Optimization: [    INFO] - Batch [107/121]: Anomaly Score: 567.564 label: 1.0
2023-03-02 01:50:25,746 -         Optimization: [    INFO] - 
 Batch: 108/121
2023-03-02 01:50:28,045 -         Optimization: [    INFO] - Batch [108/121]: Anomaly Score: 522.567 label: 0.0
2023-03-02 01:50:28,046 -         Optimization: [    INFO] - 
 Batch: 109/121
2023-03-02 01:50:30,323 -         Optimization: [    INFO] - Batch [109/121]: Anomaly Score: 554.493 label: 0.0
2023-03-02 01:50:30,324 -         Optimization: [    INFO] - 
 Batch: 110/121
2023-03-02 01:50:32,568 -         Optimization: [    INFO] - Batch [110/121]: Anomaly Score: 562.437 label: 0.0
2023-03-02 01:50:32,570 -         Optimization: [    INFO] - 
 Batch: 111/121
2023-03-02 01:50:34,840 -         Optimization: [    INFO] - Batch [111/121]: Anomaly Score: 557.726 label: 0.0
2023-03-02 01:50:34,841 -         Optimization: [    INFO] - 
 Batch: 112/121
2023-03-02 01:50:37,112 -         Optimization: [    INFO] - Batch [112/121]: Anomaly Score: 583.230 label: 0.0
2023-03-02 01:50:37,114 -         Optimization: [    INFO] - 
 Batch: 113/121
2023-03-02 01:50:39,447 -         Optimization: [    INFO] - Batch [113/121]: Anomaly Score: 559.707 label: 0.0
2023-03-02 01:50:39,449 -         Optimization: [    INFO] - 
 Batch: 114/121
2023-03-02 01:50:41,927 -         Optimization: [    INFO] - Batch [114/121]: Anomaly Score: 521.686 label: 0.0
2023-03-02 01:50:41,928 -         Optimization: [    INFO] - 
 Batch: 115/121
2023-03-02 01:50:44,208 -         Optimization: [    INFO] - Batch [115/121]: Anomaly Score: 571.510 label: 0.0
2023-03-02 01:50:44,209 -         Optimization: [    INFO] - 
 Batch: 116/121
2023-03-02 01:50:46,516 -         Optimization: [    INFO] - Batch [116/121]: Anomaly Score: 527.773 label: 0.0
2023-03-02 01:50:46,517 -         Optimization: [    INFO] - 
 Batch: 117/121
2023-03-02 01:50:48,916 -         Optimization: [    INFO] - Batch [117/121]: Anomaly Score: 541.820 label: 0.0
2023-03-02 01:50:48,918 -         Optimization: [    INFO] - 
 Batch: 118/121
2023-03-02 01:50:51,185 -         Optimization: [    INFO] - Batch [118/121]: Anomaly Score: 583.628 label: 0.0
2023-03-02 01:50:51,186 -         Optimization: [    INFO] - 
 Batch: 119/121
2023-03-02 01:50:53,463 -         Optimization: [    INFO] - Batch [119/121]: Anomaly Score: 527.652 label: 0.0
2023-03-02 01:50:53,465 -         Optimization: [    INFO] - 
 Batch: 120/121
2023-03-02 01:50:55,741 -         Optimization: [    INFO] - Batch [120/121]: Anomaly Score: 547.657 label: 0.0
2023-03-02 01:50:55,742 -         Optimization: [    INFO] - 
 Batch: 121/121
2023-03-02 01:50:58,042 -         Optimization: [    INFO] - Batch [121/121]: Anomaly Score: 551.434 label: 0.0
2023-03-02 01:51:34,577 -                train: [    INFO] - Device: cuda:0
2023-03-02 01:51:37,127 -                train: [    INFO] - 
Epoch: 1/20
2023-03-02 01:51:37,389 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.6836 (0.6860) Acc D Real: 69.010% 
Loss D Fake: 0.7038 (0.7025) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6815 (0.6828) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,400 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.6795 (0.6839) Acc D Real: 75.035% 
Loss D Fake: 0.7064 (0.7038) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.6790 (0.6815) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,407 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.6795 (0.6828) Acc D Real: 76.953% 
Loss D Fake: 0.7089 (0.7051) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6765 (0.6803) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,415 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.6767 (0.6816) Acc D Real: 78.531% 
Loss D Fake: 0.7114 (0.7064) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6741 (0.6790) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,422 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.6726 (0.6801) Acc D Real: 81.806% 
Loss D Fake: 0.7139 (0.7076) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6717 (0.6778) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,429 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.6717 (0.6789) Acc D Real: 83.906% 
Loss D Fake: 0.7164 (0.7089) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6693 (0.6766) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,436 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.6653 (0.6772) Acc D Real: 85.794% 
Loss D Fake: 0.7189 (0.7101) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.6669 (0.6754) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,448 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.6615 (0.6754) Acc D Real: 87.367% 
Loss D Fake: 0.7215 (0.7114) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.6645 (0.6742) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,455 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.6637 (0.6743) Acc D Real: 88.620% 
Loss D Fake: 0.7241 (0.7127) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6621 (0.6730) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,465 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.6643 (0.6733) Acc D Real: 89.654% 
Loss D Fake: 0.7267 (0.7139) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6597 (0.6718) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,473 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.6627 (0.6725) Acc D Real: 90.516% 
Loss D Fake: 0.7292 (0.7152) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6573 (0.6706) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,480 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.6583 (0.6714) Acc D Real: 91.246% 
Loss D Fake: 0.7318 (0.7165) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6549 (0.6694) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,487 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.6540 (0.6701) Acc D Real: 91.871% 
Loss D Fake: 0.7343 (0.7178) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6526 (0.6682) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,495 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.6518 (0.6689) Acc D Real: 92.413% 
Loss D Fake: 0.7369 (0.7190) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.6502 (0.6670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,503 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.6489 (0.6677) Acc D Real: 92.887% 
Loss D Fake: 0.7395 (0.7203) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6478 (0.6658) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,513 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.6480 (0.6665) Acc D Real: 93.306% 
Loss D Fake: 0.7422 (0.7216) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6453 (0.6646) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,521 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.6411 (0.6651) Acc D Real: 93.678% 
Loss D Fake: 0.7449 (0.7229) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.6428 (0.6633) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,529 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.6423 (0.6639) Acc D Real: 94.010% 
Loss D Fake: 0.7478 (0.7242) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6402 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,537 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.6404 (0.6627) Acc D Real: 94.310% 
Loss D Fake: 0.7507 (0.7255) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6376 (0.6609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,545 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.6361 (0.6615) Acc D Real: 94.581% 
Loss D Fake: 0.7536 (0.7269) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6350 (0.6597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,553 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.6330 (0.6602) Acc D Real: 94.827% 
Loss D Fake: 0.7567 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6322 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,561 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.6304 (0.6589) Acc D Real: 95.052% 
Loss D Fake: 0.7599 (0.7296) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6293 (0.6572) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,569 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.6271 (0.6575) Acc D Real: 95.258% 
Loss D Fake: 0.7632 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6264 (0.6559) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,577 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.6250 (0.6562) Acc D Real: 95.448% 
Loss D Fake: 0.7666 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6235 (0.6546) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,585 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.6204 (0.6549) Acc D Real: 95.623% 
Loss D Fake: 0.7700 (0.7339) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6206 (0.6533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,594 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.6178 (0.6535) Acc D Real: 95.785% 
Loss D Fake: 0.7735 (0.7353) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6174 (0.6519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,603 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.6142 (0.6521) Acc D Real: 95.936% 
Loss D Fake: 0.7773 (0.7368) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6142 (0.6506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,611 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.6120 (0.6507) Acc D Real: 96.076% 
Loss D Fake: 0.7811 (0.7384) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6110 (0.6492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,620 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.6094 (0.6493) Acc D Real: 96.207% 
Loss D Fake: 0.7849 (0.7399) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.6078 (0.6479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,628 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.6048 (0.6479) Acc D Real: 96.329% 
Loss D Fake: 0.7889 (0.7415) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.6044 (0.6465) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,636 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5990 (0.6464) Acc D Real: 96.444% 
Loss D Fake: 0.7930 (0.7431) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6009 (0.6450) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,645 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.5943 (0.6448) Acc D Real: 96.551% 
Loss D Fake: 0.7973 (0.7447) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5974 (0.6436) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,653 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5915 (0.6432) Acc D Real: 96.653% 
Loss D Fake: 0.8017 (0.7464) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5938 (0.6421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,660 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5873 (0.6416) Acc D Real: 96.749% 
Loss D Fake: 0.8064 (0.7481) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5899 (0.6406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,668 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.5790 (0.6399) Acc D Real: 96.839% 
Loss D Fake: 0.8113 (0.7499) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.5859 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,676 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5766 (0.6382) Acc D Real: 96.924% 
Loss D Fake: 0.8166 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5816 (0.6376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,683 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5715 (0.6364) Acc D Real: 97.005% 
Loss D Fake: 0.8223 (0.7535) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5770 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,691 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5619 (0.6345) Acc D Real: 97.082% 
Loss D Fake: 0.8286 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.5720 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,699 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5587 (0.6326) Acc D Real: 97.155% 
Loss D Fake: 0.8356 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5664 (0.6326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,706 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5470 (0.6305) Acc D Real: 97.224% 
Loss D Fake: 0.8434 (0.7596) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.5602 (0.6309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,714 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.5275 (0.6281) Acc D Real: 97.290% 
Loss D Fake: 0.8524 (0.7618) Acc D Fake: 0.000% 
Loss D: 1.380 
Loss G: 0.5533 (0.6290) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,722 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.5337 (0.6259) Acc D Real: 97.353% 
Loss D Fake: 0.8628 (0.7641) Acc D Fake: 0.000% 
Loss D: 1.397 
Loss G: 0.5454 (0.6271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,729 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.5080 (0.6232) Acc D Real: 97.414% 
Loss D Fake: 0.8749 (0.7666) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.5362 (0.6250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,737 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.5145 (0.6208) Acc D Real: 97.471% 
Loss D Fake: 0.8894 (0.7694) Acc D Fake: 0.000% 
Loss D: 1.404 
Loss G: 0.5258 (0.6228) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,744 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4730 (0.6176) Acc D Real: 97.526% 
Loss D Fake: 0.9064 (0.7724) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.5135 (0.6204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,752 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4377 (0.6137) Acc D Real: 97.579% 
Loss D Fake: 0.9277 (0.7757) Acc D Fake: 0.000% 
Loss D: 1.365 
Loss G: 0.4988 (0.6178) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,760 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4545 (0.6104) Acc D Real: 97.629% 
Loss D Fake: 0.9545 (0.7794) Acc D Fake: 0.000% 
Loss D: 1.409 
Loss G: 0.4827 (0.6150) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,768 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4537 (0.6072) Acc D Real: 97.678% 
Loss D Fake: 0.9839 (0.7836) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4666 (0.6120) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,775 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4151 (0.6034) Acc D Real: 97.724% 
Loss D Fake: 1.0126 (0.7881) Acc D Fake: 0.000% 
Loss D: 1.428 
Loss G: 0.4546 (0.6088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,783 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3986 (0.5994) Acc D Real: 97.769% 
Loss D Fake: 1.0307 (0.7929) Acc D Fake: 0.000% 
Loss D: 1.429 
Loss G: 0.4478 (0.6057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,790 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3840 (0.5952) Acc D Real: 97.811% 
Loss D Fake: 1.0388 (0.7976) Acc D Fake: 0.000% 
Loss D: 1.423 
Loss G: 0.4464 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,798 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3857 (0.5913) Acc D Real: 97.853% 
Loss D Fake: 1.0347 (0.8021) Acc D Fake: 0.000% 
Loss D: 1.420 
Loss G: 0.4511 (0.5998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,806 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4319 (0.5883) Acc D Real: 97.893% 
Loss D Fake: 1.0210 (0.8062) Acc D Fake: 0.000% 
Loss D: 1.453 
Loss G: 0.4581 (0.5971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,814 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3924 (0.5848) Acc D Real: 97.931% 
Loss D Fake: 1.0056 (0.8098) Acc D Fake: 0.000% 
Loss D: 1.398 
Loss G: 0.4662 (0.5948) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,822 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3948 (0.5814) Acc D Real: 97.968% 
Loss D Fake: 0.9907 (0.8130) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.4728 (0.5926) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,830 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3971 (0.5781) Acc D Real: 98.003% 
Loss D Fake: 0.9804 (0.8159) Acc D Fake: 0.000% 
Loss D: 1.377 
Loss G: 0.4774 (0.5906) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,840 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4296 (0.5756) Acc D Real: 98.038% 
Loss D Fake: 0.9739 (0.8187) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.4801 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,849 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4266 (0.5730) Acc D Real: 98.071% 
Loss D Fake: 0.9697 (0.8212) Acc D Fake: 0.000% 
Loss D: 1.396 
Loss G: 0.4825 (0.5869) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,857 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4742 (0.5714) Acc D Real: 98.103% 
Loss D Fake: 0.9655 (0.8236) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4846 (0.5852) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,864 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3661 (0.5680) Acc D Real: 98.134% 
Loss D Fake: 0.9629 (0.8259) Acc D Fake: 0.000% 
Loss D: 1.329 
Loss G: 0.4854 (0.5835) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,872 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3856 (0.5651) Acc D Real: 98.164% 
Loss D Fake: 0.9624 (0.8281) Acc D Fake: 0.000% 
Loss D: 1.348 
Loss G: 0.4854 (0.5819) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,880 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4303 (0.5629) Acc D Real: 98.194% 
Loss D Fake: 0.9628 (0.8303) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.4852 (0.5804) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,887 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4231 (0.5608) Acc D Real: 98.222% 
Loss D Fake: 0.9639 (0.8323) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.4838 (0.5789) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,895 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3778 (0.5579) Acc D Real: 98.249% 
Loss D Fake: 0.9676 (0.8344) Acc D Fake: 0.000% 
Loss D: 1.345 
Loss G: 0.4814 (0.5774) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,903 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3998 (0.5556) Acc D Real: 98.276% 
Loss D Fake: 0.9721 (0.8365) Acc D Fake: 0.000% 
Loss D: 1.372 
Loss G: 0.4796 (0.5759) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,911 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4198 (0.5535) Acc D Real: 98.301% 
Loss D Fake: 0.9748 (0.8386) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.4787 (0.5745) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,919 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4348 (0.5518) Acc D Real: 98.326% 
Loss D Fake: 0.9753 (0.8406) Acc D Fake: 0.000% 
Loss D: 1.410 
Loss G: 0.4795 (0.5731) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,927 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3827 (0.5493) Acc D Real: 98.351% 
Loss D Fake: 0.9721 (0.8425) Acc D Fake: 0.000% 
Loss D: 1.355 
Loss G: 0.4821 (0.5717) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,934 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3489 (0.5465) Acc D Real: 98.374% 
Loss D Fake: 0.9678 (0.8443) Acc D Fake: 0.000% 
Loss D: 1.317 
Loss G: 0.4836 (0.5705) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,942 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4033 (0.5445) Acc D Real: 98.397% 
Loss D Fake: 0.9659 (0.8460) Acc D Fake: 0.000% 
Loss D: 1.369 
Loss G: 0.4850 (0.5693) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,949 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3797 (0.5422) Acc D Real: 98.419% 
Loss D Fake: 0.9646 (0.8476) Acc D Fake: 0.000% 
Loss D: 1.344 
Loss G: 0.4845 (0.5681) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,957 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4079 (0.5403) Acc D Real: 98.441% 
Loss D Fake: 0.9667 (0.8493) Acc D Fake: 0.000% 
Loss D: 1.375 
Loss G: 0.4841 (0.5670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,965 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3603 (0.5379) Acc D Real: 98.462% 
Loss D Fake: 0.9660 (0.8508) Acc D Fake: 0.000% 
Loss D: 1.326 
Loss G: 0.4859 (0.5659) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,972 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3903 (0.5359) Acc D Real: 98.483% 
Loss D Fake: 0.9612 (0.8523) Acc D Fake: 0.000% 
Loss D: 1.351 
Loss G: 0.4891 (0.5648) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,980 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3699 (0.5337) Acc D Real: 98.503% 
Loss D Fake: 0.9555 (0.8537) Acc D Fake: 0.000% 
Loss D: 1.325 
Loss G: 0.4918 (0.5639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,987 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3785 (0.5317) Acc D Real: 98.522% 
Loss D Fake: 0.9510 (0.8549) Acc D Fake: 0.000% 
Loss D: 1.330 
Loss G: 0.4947 (0.5630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:37,995 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3997 (0.5300) Acc D Real: 98.541% 
Loss D Fake: 0.9461 (0.8561) Acc D Fake: 0.000% 
Loss D: 1.346 
Loss G: 0.4974 (0.5621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,002 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4227 (0.5287) Acc D Real: 98.559% 
Loss D Fake: 0.9416 (0.8572) Acc D Fake: 0.000% 
Loss D: 1.364 
Loss G: 0.5000 (0.5613) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,010 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3914 (0.5270) Acc D Real: 98.577% 
Loss D Fake: 0.9392 (0.8582) Acc D Fake: 0.000% 
Loss D: 1.331 
Loss G: 0.4996 (0.5606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,018 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4126 (0.5255) Acc D Real: 98.595% 
Loss D Fake: 0.9415 (0.8592) Acc D Fake: 0.000% 
Loss D: 1.354 
Loss G: 0.4990 (0.5598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,025 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4222 (0.5243) Acc D Real: 98.612% 
Loss D Fake: 0.9433 (0.8603) Acc D Fake: 0.000% 
Loss D: 1.366 
Loss G: 0.4970 (0.5590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,033 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3728 (0.5225) Acc D Real: 98.629% 
Loss D Fake: 0.9475 (0.8613) Acc D Fake: 0.000% 
Loss D: 1.320 
Loss G: 0.4959 (0.5583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,040 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4051 (0.5211) Acc D Real: 98.645% 
Loss D Fake: 0.9480 (0.8624) Acc D Fake: 0.000% 
Loss D: 1.353 
Loss G: 0.4968 (0.5576) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,048 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4589 (0.5203) Acc D Real: 98.661% 
Loss D Fake: 0.9470 (0.8634) Acc D Fake: 0.000% 
Loss D: 1.406 
Loss G: 0.4964 (0.5568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,056 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3686 (0.5186) Acc D Real: 98.677% 
Loss D Fake: 0.9481 (0.8643) Acc D Fake: 0.000% 
Loss D: 1.317 
Loss G: 0.4968 (0.5561) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,063 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3712 (0.5169) Acc D Real: 98.692% 
Loss D Fake: 0.9471 (0.8653) Acc D Fake: 0.000% 
Loss D: 1.318 
Loss G: 0.4974 (0.5555) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,072 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3269 (0.5147) Acc D Real: 98.707% 
Loss D Fake: 0.9454 (0.8662) Acc D Fake: 0.000% 
Loss D: 1.272 
Loss G: 0.4990 (0.5548) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,079 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3760 (0.5132) Acc D Real: 98.721% 
Loss D Fake: 0.9421 (0.8671) Acc D Fake: 0.000% 
Loss D: 1.318 
Loss G: 0.5007 (0.5542) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,087 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3750 (0.5116) Acc D Real: 98.736% 
Loss D Fake: 0.9394 (0.8679) Acc D Fake: 0.000% 
Loss D: 1.314 
Loss G: 0.5014 (0.5536) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,094 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3853 (0.5102) Acc D Real: 98.749% 
Loss D Fake: 0.9387 (0.8686) Acc D Fake: 0.000% 
Loss D: 1.324 
Loss G: 0.5011 (0.5530) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,102 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3381 (0.5084) Acc D Real: 98.763% 
Loss D Fake: 0.9400 (0.8694) Acc D Fake: 0.000% 
Loss D: 1.278 
Loss G: 0.4992 (0.5525) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,109 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3336 (0.5065) Acc D Real: 98.776% 
Loss D Fake: 0.9437 (0.8702) Acc D Fake: 0.000% 
Loss D: 1.277 
Loss G: 0.4962 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,117 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2952 (0.5042) Acc D Real: 98.789% 
Loss D Fake: 0.9485 (0.8710) Acc D Fake: 0.000% 
Loss D: 1.244 
Loss G: 0.4923 (0.5512) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,124 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2983 (0.5021) Acc D Real: 98.802% 
Loss D Fake: 0.9545 (0.8719) Acc D Fake: 0.000% 
Loss D: 1.253 
Loss G: 0.4866 (0.5505) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,132 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2908 (0.4999) Acc D Real: 98.815% 
Loss D Fake: 0.9636 (0.8729) Acc D Fake: 0.000% 
Loss D: 1.254 
Loss G: 0.4789 (0.5498) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,139 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2683 (0.4975) Acc D Real: 98.827% 
Loss D Fake: 0.9779 (0.8740) Acc D Fake: 0.000% 
Loss D: 1.246 
Loss G: 0.4769 (0.5490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,147 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2185 (0.4946) Acc D Real: 98.839% 
Loss D Fake: 0.9711 (0.8749) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.4907 (0.5485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,154 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2246 (0.4919) Acc D Real: 98.850% 
Loss D Fake: 0.9395 (0.8756) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.5041 (0.5480) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,161 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2023 (0.4890) Acc D Real: 98.862% 
Loss D Fake: 0.9216 (0.8761) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.5130 (0.5477) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,169 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2183 (0.4863) Acc D Real: 98.873% 
Loss D Fake: 0.9107 (0.8764) Acc D Fake: 0.000% 
Loss D: 1.129 
Loss G: 0.5195 (0.5474) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,176 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2282 (0.4838) Acc D Real: 98.884% 
Loss D Fake: 0.9031 (0.8767) Acc D Fake: 0.000% 
Loss D: 1.131 
Loss G: 0.5250 (0.5472) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,183 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2061 (0.4811) Acc D Real: 98.895% 
Loss D Fake: 0.8963 (0.8769) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.5297 (0.5470) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,191 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2030 (0.4784) Acc D Real: 98.906% 
Loss D Fake: 0.8900 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.5339 (0.5469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,198 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.1701 (0.4755) Acc D Real: 98.916% 
Loss D Fake: 0.8842 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.054 
Loss G: 0.5374 (0.5468) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,206 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.1931 (0.4728) Acc D Real: 98.926% 
Loss D Fake: 0.8790 (0.8771) Acc D Fake: 0.000% 
Loss D: 1.072 
Loss G: 0.5405 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,213 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.1685 (0.4700) Acc D Real: 98.936% 
Loss D Fake: 0.8741 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.043 
Loss G: 0.5430 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,221 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.1780 (0.4673) Acc D Real: 98.946% 
Loss D Fake: 0.8698 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.048 
Loss G: 0.5450 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,228 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.1694 (0.4645) Acc D Real: 98.956% 
Loss D Fake: 0.8664 (0.8769) Acc D Fake: 0.000% 
Loss D: 1.036 
Loss G: 0.5469 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,236 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.1746 (0.4619) Acc D Real: 98.965% 
Loss D Fake: 0.8638 (0.8768) Acc D Fake: 0.000% 
Loss D: 1.038 
Loss G: 0.5486 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,244 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1877 (0.4594) Acc D Real: 98.975% 
Loss D Fake: 0.8625 (0.8766) Acc D Fake: 0.000% 
Loss D: 1.050 
Loss G: 0.5502 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,251 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.1594 (0.4568) Acc D Real: 98.984% 
Loss D Fake: 0.8604 (0.8765) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.5554 (0.5468) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,259 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1644 (0.4542) Acc D Real: 98.992% 
Loss D Fake: 0.8511 (0.8763) Acc D Fake: 0.000% 
Loss D: 1.015 
Loss G: 0.5623 (0.5469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,266 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.1610 (0.4516) Acc D Real: 99.001% 
Loss D Fake: 0.8417 (0.8760) Acc D Fake: 0.000% 
Loss D: 1.003 
Loss G: 0.5681 (0.5471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,274 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.1693 (0.4491) Acc D Real: 99.009% 
Loss D Fake: 0.8344 (0.8756) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.5730 (0.5473) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,281 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.1803 (0.4468) Acc D Real: 99.017% 
Loss D Fake: 0.8281 (0.8752) Acc D Fake: 0.000% 
Loss D: 1.008 
Loss G: 0.5772 (0.5476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,289 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.1397 (0.4442) Acc D Real: 99.025% 
Loss D Fake: 0.8226 (0.8747) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5813 (0.5479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,296 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1630 (0.4418) Acc D Real: 99.032% 
Loss D Fake: 0.8174 (0.8743) Acc D Fake: 0.000% 
Loss D: 0.980 
Loss G: 0.5851 (0.5482) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,304 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.1453 (0.4393) Acc D Real: 99.039% 
Loss D Fake: 0.8127 (0.8737) Acc D Fake: 0.000% 
Loss D: 0.958 
Loss G: 0.5886 (0.5485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,311 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.1457 (0.4369) Acc D Real: 99.045% 
Loss D Fake: 0.8086 (0.8732) Acc D Fake: 0.000% 
Loss D: 0.954 
Loss G: 0.5917 (0.5489) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,319 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.1577 (0.4346) Acc D Real: 99.053% 
Loss D Fake: 0.8047 (0.8726) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5948 (0.5493) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,326 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.1545 (0.4323) Acc D Real: 99.059% 
Loss D Fake: 0.8012 (0.8720) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.5974 (0.5497) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,333 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.1355 (0.4299) Acc D Real: 99.065% 
Loss D Fake: 0.7982 (0.8714) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6001 (0.5501) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,342 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.1437 (0.4276) Acc D Real: 99.069% 
Loss D Fake: 0.7951 (0.8708) Acc D Fake: 0.000% 
Loss D: 0.939 
Loss G: 0.6025 (0.5505) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,349 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.1522 (0.4254) Acc D Real: 99.075% 
Loss D Fake: 0.7922 (0.8702) Acc D Fake: 0.000% 
Loss D: 0.944 
Loss G: 0.6049 (0.5509) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,357 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.1367 (0.4231) Acc D Real: 99.081% 
Loss D Fake: 0.7894 (0.8696) Acc D Fake: 0.000% 
Loss D: 0.926 
Loss G: 0.6074 (0.5514) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,364 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.1475 (0.4209) Acc D Real: 99.085% 
Loss D Fake: 0.7867 (0.8689) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6094 (0.5518) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,372 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1517 (0.4188) Acc D Real: 99.087% 
Loss D Fake: 0.7848 (0.8682) Acc D Fake: 0.000% 
Loss D: 0.936 
Loss G: 0.6113 (0.5523) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,381 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.1720 (0.4169) Acc D Real: 99.088% 
Loss D Fake: 0.7830 (0.8676) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6131 (0.5528) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,390 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.1553 (0.4149) Acc D Real: 99.093% 
Loss D Fake: 0.7812 (0.8669) Acc D Fake: 0.000% 
Loss D: 0.937 
Loss G: 0.6149 (0.5533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,397 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.1254 (0.4127) Acc D Real: 99.096% 
Loss D Fake: 0.7799 (0.8663) Acc D Fake: 0.000% 
Loss D: 0.905 
Loss G: 0.6164 (0.5537) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,405 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.1386 (0.4106) Acc D Real: 99.102% 
Loss D Fake: 0.7787 (0.8656) Acc D Fake: 0.000% 
Loss D: 0.917 
Loss G: 0.6185 (0.5542) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,413 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.1272 (0.4085) Acc D Real: 99.104% 
Loss D Fake: 0.7762 (0.8649) Acc D Fake: 0.000% 
Loss D: 0.903 
Loss G: 0.6210 (0.5547) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,421 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.1424 (0.4065) Acc D Real: 99.106% 
Loss D Fake: 0.7736 (0.8642) Acc D Fake: 0.000% 
Loss D: 0.916 
Loss G: 0.6238 (0.5552) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,428 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.1318 (0.4044) Acc D Real: 99.109% 
Loss D Fake: 0.7713 (0.8636) Acc D Fake: 0.000% 
Loss D: 0.903 
Loss G: 0.6264 (0.5558) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,436 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.1414 (0.4025) Acc D Real: 99.113% 
Loss D Fake: 0.7703 (0.8629) Acc D Fake: 0.000% 
Loss D: 0.912 
Loss G: 0.6281 (0.5563) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,443 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.1791 (0.4009) Acc D Real: 99.115% 
Loss D Fake: 0.7795 (0.8623) Acc D Fake: 0.000% 
Loss D: 0.959 
Loss G: 0.6187 (0.5568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,451 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.1454 (0.3990) Acc D Real: 99.121% 
Loss D Fake: 1.5749 (0.8674) Acc D Fake: 0.000% 
Loss D: 1.720 
Loss G: 0.6206 (0.5572) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,459 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.1360 (0.3971) Acc D Real: 99.127% 
Loss D Fake: 0.7818 (0.8668) Acc D Fake: 0.000% 
Loss D: 0.918 
Loss G: 0.6307 (0.5578) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,466 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1362 (0.3953) Acc D Real: 99.127% 
Loss D Fake: 0.7695 (0.8661) Acc D Fake: 0.000% 
Loss D: 0.906 
Loss G: 0.6337 (0.5583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,474 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.1425 (0.3935) Acc D Real: 99.126% 
Loss D Fake: 0.7669 (0.8654) Acc D Fake: 0.000% 
Loss D: 0.909 
Loss G: 0.6335 (0.5588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,481 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.1240 (0.3916) Acc D Real: 99.127% 
Loss D Fake: 0.7684 (0.8647) Acc D Fake: 0.000% 
Loss D: 0.892 
Loss G: 0.6312 (0.5593) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,489 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.1867 (0.3901) Acc D Real: 99.121% 
Loss D Fake: 0.7732 (0.8641) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6269 (0.5598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,497 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.1521 (0.3885) Acc D Real: 99.120% 
Loss D Fake: 0.7819 (0.8635) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6201 (0.5602) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,504 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.1968 (0.3872) Acc D Real: 99.107% 
Loss D Fake: 0.7974 (0.8631) Acc D Fake: 0.000% 
Loss D: 0.994 
Loss G: 0.6103 (0.5606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,512 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.1459 (0.3855) Acc D Real: 99.105% 
Loss D Fake: 0.8309 (0.8628) Acc D Fake: 0.000% 
Loss D: 0.977 
Loss G: 0.5979 (0.5608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,520 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.1491 (0.3839) Acc D Real: 99.101% 
Loss D Fake: 0.9029 (0.8631) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.5969 (0.5611) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,527 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.1870 (0.3826) Acc D Real: 99.098% 
Loss D Fake: 0.8286 (0.8629) Acc D Fake: 0.000% 
Loss D: 1.016 
Loss G: 0.6047 (0.5614) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,535 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2036 (0.3814) Acc D Real: 99.087% 
Loss D Fake: 0.8058 (0.8625) Acc D Fake: 0.000% 
Loss D: 1.009 
Loss G: 0.6095 (0.5617) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,542 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2023 (0.3802) Acc D Real: 99.081% 
Loss D Fake: 0.7959 (0.8620) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6121 (0.5620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,550 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.1637 (0.3787) Acc D Real: 99.073% 
Loss D Fake: 0.7909 (0.8616) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6134 (0.5624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,558 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2269 (0.3777) Acc D Real: 99.070% 
Loss D Fake: 0.7887 (0.8611) Acc D Fake: 0.000% 
Loss D: 1.016 
Loss G: 0.6133 (0.5627) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,565 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.1669 (0.3764) Acc D Real: 99.066% 
Loss D Fake: 0.7886 (0.8606) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6124 (0.5630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,573 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.2496 (0.3755) Acc D Real: 99.056% 
Loss D Fake: 0.7896 (0.8602) Acc D Fake: 0.000% 
Loss D: 1.039 
Loss G: 0.6108 (0.5633) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,580 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.1731 (0.3742) Acc D Real: 99.052% 
Loss D Fake: 0.7918 (0.8597) Acc D Fake: 0.000% 
Loss D: 0.965 
Loss G: 0.6088 (0.5636) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,588 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2101 (0.3732) Acc D Real: 99.048% 
Loss D Fake: 0.7947 (0.8593) Acc D Fake: 0.000% 
Loss D: 1.005 
Loss G: 0.6061 (0.5639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,596 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2150 (0.3722) Acc D Real: 99.046% 
Loss D Fake: 0.7989 (0.8589) Acc D Fake: 0.000% 
Loss D: 1.014 
Loss G: 0.6032 (0.5642) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,604 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.1119 (0.3705) Acc D Real: 99.045% 
Loss D Fake: 0.8031 (0.8586) Acc D Fake: 0.000% 
Loss D: 0.915 
Loss G: 0.6005 (0.5644) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:38,823 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.005 | Generator Loss: 0.585 | Avg: 1.589 
2023-03-02 01:51:38,845 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.029 | Generator Loss: 0.585 | Avg: 1.614 
2023-03-02 01:51:38,868 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.035 | Generator Loss: 0.585 | Avg: 1.620 
2023-03-02 01:51:38,894 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.020 | Generator Loss: 0.585 | Avg: 1.605 
2023-03-02 01:51:38,920 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.028 | Generator Loss: 0.585 | Avg: 1.612 
2023-03-02 01:51:38,945 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.016 | Generator Loss: 0.585 | Avg: 1.601 
2023-03-02 01:51:38,976 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.029 | Generator Loss: 0.585 | Avg: 1.614 
2023-03-02 01:51:39,005 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.026 | Generator Loss: 0.585 | Avg: 1.611 
2023-03-02 01:51:39,031 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.025 | Generator Loss: 0.585 | Avg: 1.610 
2023-03-02 01:51:39,057 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.014 | Generator Loss: 0.585 | Avg: 1.599 
2023-03-02 01:51:39,082 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.007 | Generator Loss: 0.585 | Avg: 1.592 
2023-03-02 01:51:39,108 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.998 | Generator Loss: 0.585 | Avg: 1.582 
2023-03-02 01:51:39,134 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.992 | Generator Loss: 0.585 | Avg: 1.577 
2023-03-02 01:51:39,159 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.989 | Generator Loss: 0.585 | Avg: 1.574 
2023-03-02 01:51:39,184 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.997 | Generator Loss: 0.585 | Avg: 1.582 
2023-03-02 01:51:39,210 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.005 | Generator Loss: 0.585 | Avg: 1.589 
2023-03-02 01:51:39,235 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.008 | Generator Loss: 0.585 | Avg: 1.593 
2023-03-02 01:51:39,272 -                train: [    INFO] - Best Loss 100000000000.000 to 0.795
2023-03-02 01:51:39,272 -                train: [    INFO] - 
Epoch: 2/20
2023-03-02 01:51:39,488 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.1485 (0.1985) Acc D Real: 98.854% 
Loss D Fake: 0.8117 (0.8095) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.5951 (0.5964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,495 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2223 (0.2064) Acc D Real: 98.646% 
Loss D Fake: 0.8272 (0.8154) Acc D Fake: 0.000% 
Loss D: 1.049 
Loss G: 0.5721 (0.5883) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,502 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.1955 (0.2037) Acc D Real: 98.789% 
Loss D Fake: 1.0132 (0.8648) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.5671 (0.5830) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,519 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.1768 (0.1983) Acc D Real: 98.760% 
Loss D Fake: 0.8503 (0.8619) Acc D Fake: 0.000% 
Loss D: 1.027 
Loss G: 0.5934 (0.5851) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,526 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.1828 (0.1958) Acc D Real: 98.715% 
Loss D Fake: 0.8056 (0.8525) Acc D Fake: 0.000% 
Loss D: 0.988 
Loss G: 0.6064 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,533 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.1981 (0.1961) Acc D Real: 98.735% 
Loss D Fake: 0.7903 (0.8436) Acc D Fake: 0.000% 
Loss D: 0.988 
Loss G: 0.6134 (0.5922) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,540 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2291 (0.2002) Acc D Real: 98.763% 
Loss D Fake: 0.7822 (0.8360) Acc D Fake: 0.000% 
Loss D: 1.011 
Loss G: 0.6175 (0.5954) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,547 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.1628 (0.1961) Acc D Real: 98.796% 
Loss D Fake: 0.7774 (0.8295) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6202 (0.5981) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,554 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.1998 (0.1964) Acc D Real: 98.870% 
Loss D Fake: 0.7744 (0.8239) Acc D Fake: 0.000% 
Loss D: 0.974 
Loss G: 0.6218 (0.6005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,561 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.1837 (0.1953) Acc D Real: 98.902% 
Loss D Fake: 0.7727 (0.8193) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6224 (0.6025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,569 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2245 (0.1977) Acc D Real: 98.937% 
Loss D Fake: 0.7724 (0.8154) Acc D Fake: 0.000% 
Loss D: 0.997 
Loss G: 0.6222 (0.6041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,575 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.1680 (0.1954) Acc D Real: 98.962% 
Loss D Fake: 0.7729 (0.8121) Acc D Fake: 0.000% 
Loss D: 0.941 
Loss G: 0.6213 (0.6054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,582 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.1199 (0.1900) Acc D Real: 98.951% 
Loss D Fake: 0.7744 (0.8094) Acc D Fake: 0.000% 
Loss D: 0.894 
Loss G: 0.6201 (0.6065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,589 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.1640 (0.1883) Acc D Real: 98.986% 
Loss D Fake: 0.7761 (0.8072) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6186 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,596 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.1820 (0.1879) Acc D Real: 98.978% 
Loss D Fake: 0.7783 (0.8054) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6169 (0.6079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,604 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.1416 (0.1852) Acc D Real: 98.986% 
Loss D Fake: 0.7812 (0.8040) Acc D Fake: 0.000% 
Loss D: 0.923 
Loss G: 0.6140 (0.6083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,610 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.1739 (0.1846) Acc D Real: 99.002% 
Loss D Fake: 0.7863 (0.8030) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6103 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,617 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.1535 (0.1829) Acc D Real: 98.991% 
Loss D Fake: 0.7924 (0.8024) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6066 (0.6083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,624 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.1991 (0.1837) Acc D Real: 99.003% 
Loss D Fake: 0.7990 (0.8023) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6031 (0.6080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,631 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.1465 (0.1820) Acc D Real: 99.000% 
Loss D Fake: 0.8061 (0.8024) Acc D Fake: 0.000% 
Loss D: 0.953 
Loss G: 0.6004 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,638 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.1660 (0.1812) Acc D Real: 99.015% 
Loss D Fake: 0.8115 (0.8029) Acc D Fake: 0.000% 
Loss D: 0.977 
Loss G: 0.5996 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,645 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.1354 (0.1792) Acc D Real: 99.033% 
Loss D Fake: 0.8127 (0.8033) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.6009 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,652 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.1632 (0.1786) Acc D Real: 99.034% 
Loss D Fake: 0.8100 (0.8036) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.6034 (0.6069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,659 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.1508 (0.1775) Acc D Real: 99.029% 
Loss D Fake: 0.8054 (0.8036) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6058 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,666 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.1454 (0.1762) Acc D Real: 99.040% 
Loss D Fake: 0.8009 (0.8035) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6084 (0.6069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,672 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.1512 (0.1753) Acc D Real: 99.059% 
Loss D Fake: 0.7951 (0.8032) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6097 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,680 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.1231 (0.1734) Acc D Real: 99.062% 
Loss D Fake: 0.7898 (0.8027) Acc D Fake: 0.000% 
Loss D: 0.913 
Loss G: 0.6008 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,688 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.1232 (0.1717) Acc D Real: 99.075% 
Loss D Fake: 0.9352 (0.8073) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6031 (0.6066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,695 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.1439 (0.1708) Acc D Real: 99.085% 
Loss D Fake: 1.0233 (0.8145) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.5690 (0.6054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,702 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.1366 (0.1697) Acc D Real: 99.099% 
Loss D Fake: 0.8957 (0.8171) Acc D Fake: 0.000% 
Loss D: 1.032 
Loss G: 0.5956 (0.6051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,710 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.1528 (0.1691) Acc D Real: 99.102% 
Loss D Fake: 0.8033 (0.8167) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6108 (0.6052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,717 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.1349 (0.1681) Acc D Real: 99.096% 
Loss D Fake: 0.7865 (0.8158) Acc D Fake: 0.000% 
Loss D: 0.921 
Loss G: 0.6175 (0.6056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,726 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1751 (0.1683) Acc D Real: 99.102% 
Loss D Fake: 0.7795 (0.8147) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6209 (0.6061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,734 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.1789 (0.1686) Acc D Real: 99.101% 
Loss D Fake: 0.7763 (0.8136) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6223 (0.6065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,741 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.1455 (0.1680) Acc D Real: 99.087% 
Loss D Fake: 0.7757 (0.8126) Acc D Fake: 0.000% 
Loss D: 0.921 
Loss G: 0.6221 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,748 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.1677 (0.1680) Acc D Real: 99.088% 
Loss D Fake: 0.7776 (0.8116) Acc D Fake: 0.000% 
Loss D: 0.945 
Loss G: 0.6201 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,756 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.1955 (0.1687) Acc D Real: 99.105% 
Loss D Fake: 0.7823 (0.8109) Acc D Fake: 0.000% 
Loss D: 0.978 
Loss G: 0.6161 (0.6075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,763 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.1440 (0.1681) Acc D Real: 99.091% 
Loss D Fake: 0.7912 (0.8103) Acc D Fake: 0.000% 
Loss D: 0.935 
Loss G: 0.6097 (0.6076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,771 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.1661 (0.1680) Acc D Real: 99.085% 
Loss D Fake: 0.8067 (0.8103) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.5993 (0.6074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,778 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.1493 (0.1676) Acc D Real: 99.088% 
Loss D Fake: 0.8419 (0.8110) Acc D Fake: 0.000% 
Loss D: 0.991 
Loss G: 0.5827 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,785 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.1501 (0.1671) Acc D Real: 99.082% 
Loss D Fake: 0.9965 (0.8154) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.5763 (0.6061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,792 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2210 (0.1684) Acc D Real: 99.081% 
Loss D Fake: 0.9158 (0.8178) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.5862 (0.6056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,800 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2160 (0.1695) Acc D Real: 99.066% 
Loss D Fake: 0.8548 (0.8186) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.5918 (0.6053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,807 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2073 (0.1703) Acc D Real: 99.050% 
Loss D Fake: 0.8439 (0.8192) Acc D Fake: 0.000% 
Loss D: 1.051 
Loss G: 0.5903 (0.6050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,815 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2069 (0.1711) Acc D Real: 99.043% 
Loss D Fake: 0.8481 (0.8198) Acc D Fake: 0.000% 
Loss D: 1.055 
Loss G: 0.5852 (0.6045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,822 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2072 (0.1719) Acc D Real: 99.047% 
Loss D Fake: 0.8526 (0.8205) Acc D Fake: 0.000% 
Loss D: 1.060 
Loss G: 0.5805 (0.6040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,830 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2023 (0.1725) Acc D Real: 99.010% 
Loss D Fake: 0.8484 (0.8211) Acc D Fake: 0.000% 
Loss D: 1.051 
Loss G: 0.5781 (0.6035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,838 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3299 (0.1757) Acc D Real: 98.975% 
Loss D Fake: 0.8384 (0.8214) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.5808 (0.6030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,846 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2426 (0.1771) Acc D Real: 98.953% 
Loss D Fake: 0.8247 (0.8215) Acc D Fake: 0.000% 
Loss D: 1.067 
Loss G: 0.5878 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,854 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3100 (0.1797) Acc D Real: 98.910% 
Loss D Fake: 0.8111 (0.8213) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.5957 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,861 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2634 (0.1813) Acc D Real: 98.875% 
Loss D Fake: 0.7998 (0.8209) Acc D Fake: 0.000% 
Loss D: 1.063 
Loss G: 0.6027 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,869 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.1828 (0.1813) Acc D Real: 98.867% 
Loss D Fake: 0.7906 (0.8203) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.6094 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,876 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2495 (0.1826) Acc D Real: 98.837% 
Loss D Fake: 0.7825 (0.8196) Acc D Fake: 0.000% 
Loss D: 1.032 
Loss G: 0.6153 (0.6029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,883 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.1775 (0.1825) Acc D Real: 98.812% 
Loss D Fake: 0.7756 (0.8188) Acc D Fake: 0.000% 
Loss D: 0.953 
Loss G: 0.6208 (0.6033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,891 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2100 (0.1830) Acc D Real: 98.779% 
Loss D Fake: 0.7693 (0.8179) Acc D Fake: 0.000% 
Loss D: 0.979 
Loss G: 0.6259 (0.6037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,900 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2188 (0.1836) Acc D Real: 98.747% 
Loss D Fake: 0.7636 (0.8170) Acc D Fake: 0.000% 
Loss D: 0.982 
Loss G: 0.6305 (0.6041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,908 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2132 (0.1841) Acc D Real: 98.722% 
Loss D Fake: 0.7585 (0.8160) Acc D Fake: 0.000% 
Loss D: 0.972 
Loss G: 0.6348 (0.6047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,916 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.2241 (0.1848) Acc D Real: 98.693% 
Loss D Fake: 0.7542 (0.8149) Acc D Fake: 0.000% 
Loss D: 0.978 
Loss G: 0.6379 (0.6052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,923 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.1813 (0.1847) Acc D Real: 98.672% 
Loss D Fake: 0.7511 (0.8139) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.6407 (0.6058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,931 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2026 (0.1850) Acc D Real: 98.658% 
Loss D Fake: 0.7481 (0.8128) Acc D Fake: 0.000% 
Loss D: 0.951 
Loss G: 0.6434 (0.6064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,939 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2031 (0.1853) Acc D Real: 98.622% 
Loss D Fake: 0.7451 (0.8117) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.6461 (0.6071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,946 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2218 (0.1859) Acc D Real: 98.585% 
Loss D Fake: 0.7422 (0.8106) Acc D Fake: 0.000% 
Loss D: 0.964 
Loss G: 0.6489 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,954 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.1951 (0.1860) Acc D Real: 98.574% 
Loss D Fake: 0.7392 (0.8095) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6515 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,962 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.1608 (0.1856) Acc D Real: 98.571% 
Loss D Fake: 0.7366 (0.8084) Acc D Fake: 0.000% 
Loss D: 0.897 
Loss G: 0.6539 (0.6091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,969 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.1931 (0.1858) Acc D Real: 98.550% 
Loss D Fake: 0.7342 (0.8072) Acc D Fake: 0.000% 
Loss D: 0.927 
Loss G: 0.6558 (0.6098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:51:39,977 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.1922 (0.1859) Acc D Real: 98.532% 
Loss D Fake: 0.7326 (0.8061) Acc D Fake: 0.000% 
Loss D: 0.925 
Loss G: 0.6567 (0.6105) Acc G: 99.950% 
LR: 2.000e-04 

2023-03-02 01:51:39,984 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.1689 (0.1856) Acc D Real: 98.521% 
Loss D Fake: 0.7326 (0.8050) Acc D Fake: 0.049% 
Loss D: 0.901 
Loss G: 0.6551 (0.6112) Acc G: 99.858% 
LR: 2.000e-04 

2023-03-02 01:51:39,992 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.2177 (0.1861) Acc D Real: 98.508% 
Loss D Fake: 0.7386 (0.8041) Acc D Fake: 0.145% 
Loss D: 0.956 
Loss G: 0.6425 (0.6116) Acc G: 99.763% 
LR: 2.000e-04 

2023-03-02 01:51:40,000 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.1395 (0.1854) Acc D Real: 98.508% 
Loss D Fake: 0.7863 (0.8038) Acc D Fake: 0.262% 
Loss D: 0.926 
Loss G: 0.6589 (0.6123) Acc G: 99.624% 
LR: 2.000e-04 

2023-03-02 01:51:40,007 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.1873 (0.1854) Acc D Real: 98.482% 
Loss D Fake: 0.7269 (0.8027) Acc D Fake: 0.376% 
Loss D: 0.914 
Loss G: 0.6625 (0.6130) Acc G: 99.488% 
LR: 2.000e-04 

2023-03-02 01:51:40,015 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1855 (0.1854) Acc D Real: 98.453% 
Loss D Fake: 0.7261 (0.8017) Acc D Fake: 0.509% 
Loss D: 0.912 
Loss G: 0.6627 (0.6137) Acc G: 99.356% 
LR: 2.000e-04 

2023-03-02 01:51:40,024 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.1677 (0.1852) Acc D Real: 98.433% 
Loss D Fake: 0.7259 (0.8006) Acc D Fake: 0.639% 
Loss D: 0.894 
Loss G: 0.6629 (0.6144) Acc G: 99.205% 
LR: 2.000e-04 

2023-03-02 01:51:40,031 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2021 (0.1854) Acc D Real: 98.411% 
Loss D Fake: 0.7254 (0.7996) Acc D Fake: 0.788% 
Loss D: 0.927 
Loss G: 0.6636 (0.6151) Acc G: 99.058% 
LR: 2.000e-04 

2023-03-02 01:51:40,039 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.1717 (0.1852) Acc D Real: 98.390% 
Loss D Fake: 0.7245 (0.7986) Acc D Fake: 0.933% 
Loss D: 0.896 
Loss G: 0.6647 (0.6157) Acc G: 98.915% 
LR: 2.000e-04 

2023-03-02 01:51:40,047 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2193 (0.1857) Acc D Real: 98.357% 
Loss D Fake: 0.7230 (0.7976) Acc D Fake: 1.075% 
Loss D: 0.942 
Loss G: 0.6663 (0.6164) Acc G: 98.754% 
LR: 2.000e-04 

2023-03-02 01:51:40,054 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2061 (0.1859) Acc D Real: 98.319% 
Loss D Fake: 0.7211 (0.7966) Acc D Fake: 1.255% 
Loss D: 0.927 
Loss G: 0.6684 (0.6171) Acc G: 98.575% 
LR: 2.000e-04 

2023-03-02 01:51:40,062 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.1582 (0.1856) Acc D Real: 98.303% 
Loss D Fake: 0.7187 (0.7956) Acc D Fake: 1.432% 
Loss D: 0.877 
Loss G: 0.6707 (0.6177) Acc G: 98.380% 
LR: 2.000e-04 

2023-03-02 01:51:40,070 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2083 (0.1859) Acc D Real: 98.251% 
Loss D Fake: 0.7164 (0.7946) Acc D Fake: 1.624% 
Loss D: 0.925 
Loss G: 0.6730 (0.6184) Acc G: 98.169% 
LR: 2.000e-04 

2023-03-02 01:51:40,078 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.1723 (0.1857) Acc D Real: 98.225% 
Loss D Fake: 0.7143 (0.7936) Acc D Fake: 1.833% 
Loss D: 0.887 
Loss G: 0.6747 (0.6191) Acc G: 97.941% 
LR: 2.000e-04 

2023-03-02 01:51:40,086 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.1891 (0.1858) Acc D Real: 98.203% 
Loss D Fake: 0.7127 (0.7926) Acc D Fake: 2.058% 
Loss D: 0.902 
Loss G: 0.6764 (0.6198) Acc G: 97.720% 
LR: 2.000e-04 

2023-03-02 01:51:40,093 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.1542 (0.1854) Acc D Real: 98.194% 
Loss D Fake: 0.7109 (0.7916) Acc D Fake: 2.276% 
Loss D: 0.865 
Loss G: 0.6782 (0.6206) Acc G: 97.240% 
LR: 2.000e-04 

2023-03-02 01:51:40,101 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.1749 (0.1852) Acc D Real: 98.168% 
Loss D Fake: 0.7090 (0.7906) Acc D Fake: 2.912% 
Loss D: 0.884 
Loss G: 0.6800 (0.6213) Acc G: 96.570% 
LR: 2.000e-04 

2023-03-02 01:51:40,109 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.1669 (0.1850) Acc D Real: 98.156% 
Loss D Fake: 0.7073 (0.7896) Acc D Fake: 3.591% 
Loss D: 0.874 
Loss G: 0.6816 (0.6220) Acc G: 95.857% 
LR: 2.000e-04 

2023-03-02 01:51:40,117 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.1651 (0.1848) Acc D Real: 98.140% 
Loss D Fake: 0.7058 (0.7886) Acc D Fake: 4.314% 
Loss D: 0.871 
Loss G: 0.6829 (0.6227) Acc G: 95.121% 
LR: 2.000e-04 

2023-03-02 01:51:40,124 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.1590 (0.1845) Acc D Real: 98.128% 
Loss D Fake: 0.7047 (0.7877) Acc D Fake: 5.039% 
Loss D: 0.864 
Loss G: 0.6836 (0.6234) Acc G: 94.365% 
LR: 2.000e-04 

2023-03-02 01:51:40,133 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.1904 (0.1846) Acc D Real: 98.097% 
Loss D Fake: 0.7050 (0.7867) Acc D Fake: 5.670% 
Loss D: 0.895 
Loss G: 0.6806 (0.6241) Acc G: 93.989% 
LR: 2.000e-04 

2023-03-02 01:51:40,141 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.1721 (0.1844) Acc D Real: 98.068% 
Loss D Fake: 0.7147 (0.7859) Acc D Fake: 5.871% 
Loss D: 0.887 
Loss G: 0.6679 (0.6246) Acc G: 93.792% 
LR: 2.000e-04 

2023-03-02 01:51:40,148 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.1535 (0.1841) Acc D Real: 98.061% 
Loss D Fake: 0.7378 (0.7854) Acc D Fake: 6.067% 
Loss D: 0.891 
Loss G: 0.6847 (0.6253) Acc G: 93.188% 
LR: 2.000e-04 

2023-03-02 01:51:40,157 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.1670 (0.1839) Acc D Real: 98.035% 
Loss D Fake: 0.6983 (0.7844) Acc D Fake: 6.833% 
Loss D: 0.865 
Loss G: 0.6919 (0.6260) Acc G: 92.430% 
LR: 2.000e-04 

2023-03-02 01:51:40,165 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2157 (0.1842) Acc D Real: 97.976% 
Loss D Fake: 0.6950 (0.7834) Acc D Fake: 7.582% 
Loss D: 0.911 
Loss G: 0.6940 (0.6267) Acc G: 91.689% 
LR: 2.000e-04 

2023-03-02 01:51:40,172 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.1518 (0.1839) Acc D Real: 97.961% 
Loss D Fake: 0.6934 (0.7824) Acc D Fake: 8.315% 
Loss D: 0.845 
Loss G: 0.6955 (0.6275) Acc G: 90.964% 
LR: 2.000e-04 

2023-03-02 01:51:40,180 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.1615 (0.1836) Acc D Real: 97.938% 
Loss D Fake: 0.6919 (0.7815) Acc D Fake: 9.032% 
Loss D: 0.853 
Loss G: 0.6970 (0.6282) Acc G: 90.255% 
LR: 2.000e-04 

2023-03-02 01:51:40,187 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.1800 (0.1836) Acc D Real: 97.903% 
Loss D Fake: 0.6903 (0.7805) Acc D Fake: 9.734% 
Loss D: 0.870 
Loss G: 0.6987 (0.6290) Acc G: 89.561% 
LR: 2.000e-04 

2023-03-02 01:51:40,195 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.1690 (0.1834) Acc D Real: 97.877% 
Loss D Fake: 0.6886 (0.7795) Acc D Fake: 10.421% 
Loss D: 0.858 
Loss G: 0.7006 (0.6297) Acc G: 88.881% 
LR: 2.000e-04 

2023-03-02 01:51:40,202 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.1865 (0.1835) Acc D Real: 97.842% 
Loss D Fake: 0.6867 (0.7786) Acc D Fake: 11.094% 
Loss D: 0.873 
Loss G: 0.7026 (0.6305) Acc G: 88.198% 
LR: 2.000e-04 

2023-03-02 01:51:40,210 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.1916 (0.1836) Acc D Real: 97.801% 
Loss D Fake: 0.6847 (0.7776) Acc D Fake: 11.770% 
Loss D: 0.876 
Loss G: 0.7046 (0.6313) Acc G: 87.530% 
LR: 2.000e-04 

2023-03-02 01:51:40,217 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.1315 (0.1830) Acc D Real: 97.794% 
Loss D Fake: 0.6828 (0.7766) Acc D Fake: 12.432% 
Loss D: 0.814 
Loss G: 0.7067 (0.6320) Acc G: 86.874% 
LR: 2.000e-04 

2023-03-02 01:51:40,224 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.1386 (0.1826) Acc D Real: 97.782% 
Loss D Fake: 0.6807 (0.7757) Acc D Fake: 13.081% 
Loss D: 0.819 
Loss G: 0.7089 (0.6328) Acc G: 86.233% 
LR: 2.000e-04 

2023-03-02 01:51:40,232 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2482 (0.1832) Acc D Real: 97.673% 
Loss D Fake: 0.6786 (0.7747) Acc D Fake: 13.717% 
Loss D: 0.927 
Loss G: 0.7111 (0.6336) Acc G: 85.604% 
LR: 2.000e-04 

2023-03-02 01:51:40,239 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.1914 (0.1833) Acc D Real: 97.629% 
Loss D Fake: 0.6766 (0.7737) Acc D Fake: 14.340% 
Loss D: 0.868 
Loss G: 0.7131 (0.6344) Acc G: 84.987% 
LR: 2.000e-04 

2023-03-02 01:51:40,247 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.1777 (0.1833) Acc D Real: 97.584% 
Loss D Fake: 0.6747 (0.7727) Acc D Fake: 14.967% 
Loss D: 0.852 
Loss G: 0.7150 (0.6352) Acc G: 84.366% 
LR: 2.000e-04 

2023-03-02 01:51:40,256 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2078 (0.1835) Acc D Real: 97.520% 
Loss D Fake: 0.6732 (0.7718) Acc D Fake: 15.583% 
Loss D: 0.881 
Loss G: 0.7163 (0.6360) Acc G: 83.758% 
LR: 2.000e-04 

2023-03-02 01:51:40,265 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.1738 (0.1834) Acc D Real: 97.493% 
Loss D Fake: 0.6722 (0.7708) Acc D Fake: 16.186% 
Loss D: 0.846 
Loss G: 0.7170 (0.6367) Acc G: 83.161% 
LR: 2.000e-04 

2023-03-02 01:51:40,273 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.1576 (0.1832) Acc D Real: 97.474% 
Loss D Fake: 0.6718 (0.7699) Acc D Fake: 16.778% 
Loss D: 0.829 
Loss G: 0.7173 (0.6375) Acc G: 82.575% 
LR: 2.000e-04 

2023-03-02 01:51:40,282 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.1450 (0.1828) Acc D Real: 97.453% 
Loss D Fake: 0.6718 (0.7689) Acc D Fake: 17.358% 
Loss D: 0.817 
Loss G: 0.7173 (0.6383) Acc G: 82.000% 
LR: 2.000e-04 

2023-03-02 01:51:40,290 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.1494 (0.1825) Acc D Real: 97.450% 
Loss D Fake: 0.6718 (0.7680) Acc D Fake: 17.928% 
Loss D: 0.821 
Loss G: 0.7177 (0.6390) Acc G: 81.436% 
LR: 2.000e-04 

2023-03-02 01:51:40,298 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.1364 (0.1821) Acc D Real: 97.449% 
Loss D Fake: 0.6709 (0.7671) Acc D Fake: 18.488% 
Loss D: 0.807 
Loss G: 0.7195 (0.6397) Acc G: 80.883% 
LR: 2.000e-04 

2023-03-02 01:51:40,305 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.1683 (0.1819) Acc D Real: 97.417% 
Loss D Fake: 0.6687 (0.7662) Acc D Fake: 19.037% 
Loss D: 0.837 
Loss G: 0.7220 (0.6405) Acc G: 80.332% 
LR: 2.000e-04 

2023-03-02 01:51:40,313 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.1354 (0.1815) Acc D Real: 97.407% 
Loss D Fake: 0.6660 (0.7653) Acc D Fake: 19.591% 
Loss D: 0.801 
Loss G: 0.7253 (0.6413) Acc G: 79.783% 
LR: 2.000e-04 

2023-03-02 01:51:40,320 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1616 (0.1813) Acc D Real: 97.386% 
Loss D Fake: 0.6628 (0.7644) Acc D Fake: 20.135% 
Loss D: 0.824 
Loss G: 0.7285 (0.6421) Acc G: 79.245% 
LR: 2.000e-04 

2023-03-02 01:51:40,328 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.1914 (0.1814) Acc D Real: 97.328% 
Loss D Fake: 0.6605 (0.7635) Acc D Fake: 20.670% 
Loss D: 0.852 
Loss G: 0.7300 (0.6428) Acc G: 78.716% 
LR: 2.000e-04 

2023-03-02 01:51:40,335 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1805 (0.1814) Acc D Real: 97.293% 
Loss D Fake: 0.6603 (0.7626) Acc D Fake: 21.195% 
Loss D: 0.841 
Loss G: 0.7283 (0.6436) Acc G: 78.196% 
LR: 2.000e-04 

2023-03-02 01:51:40,343 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.1610 (0.1812) Acc D Real: 97.266% 
Loss D Fake: 0.6647 (0.7617) Acc D Fake: 21.696% 
Loss D: 0.826 
Loss G: 0.7238 (0.6443) Acc G: 77.729% 
LR: 2.000e-04 

2023-03-02 01:51:40,350 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.1429 (0.1809) Acc D Real: 97.249% 
Loss D Fake: 0.6678 (0.7609) Acc D Fake: 22.145% 
Loss D: 0.811 
Loss G: 0.7302 (0.6450) Acc G: 77.227% 
LR: 2.000e-04 

2023-03-02 01:51:40,358 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.1758 (0.1809) Acc D Real: 97.209% 
Loss D Fake: 0.6558 (0.7600) Acc D Fake: 22.644% 
Loss D: 0.832 
Loss G: 0.7384 (0.6458) Acc G: 76.734% 
LR: 2.000e-04 

2023-03-02 01:51:40,365 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.1593 (0.1807) Acc D Real: 97.191% 
Loss D Fake: 0.6502 (0.7590) Acc D Fake: 23.134% 
Loss D: 0.809 
Loss G: 0.7431 (0.6467) Acc G: 76.249% 
LR: 2.000e-04 

2023-03-02 01:51:40,372 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1691 (0.1806) Acc D Real: 97.164% 
Loss D Fake: 0.6467 (0.7581) Acc D Fake: 23.616% 
Loss D: 0.816 
Loss G: 0.7465 (0.6475) Acc G: 75.772% 
LR: 2.000e-04 

2023-03-02 01:51:40,380 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.1413 (0.1802) Acc D Real: 97.152% 
Loss D Fake: 0.6439 (0.7571) Acc D Fake: 24.090% 
Loss D: 0.785 
Loss G: 0.7495 (0.6484) Acc G: 75.304% 
LR: 2.000e-04 

2023-03-02 01:51:40,387 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.1508 (0.1800) Acc D Real: 97.143% 
Loss D Fake: 0.6414 (0.7562) Acc D Fake: 24.556% 
Loss D: 0.792 
Loss G: 0.7522 (0.6492) Acc G: 74.843% 
LR: 2.000e-04 

2023-03-02 01:51:40,395 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.1846 (0.1800) Acc D Real: 97.092% 
Loss D Fake: 0.6391 (0.7552) Acc D Fake: 25.028% 
Loss D: 0.824 
Loss G: 0.7548 (0.6501) Acc G: 74.376% 
LR: 2.000e-04 

2023-03-02 01:51:40,402 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.1561 (0.1798) Acc D Real: 97.072% 
Loss D Fake: 0.6370 (0.7542) Acc D Fake: 25.492% 
Loss D: 0.793 
Loss G: 0.7571 (0.6510) Acc G: 73.916% 
LR: 2.000e-04 

2023-03-02 01:51:40,410 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.1678 (0.1797) Acc D Real: 97.038% 
Loss D Fake: 0.6351 (0.7533) Acc D Fake: 25.949% 
Loss D: 0.803 
Loss G: 0.7592 (0.6519) Acc G: 73.465% 
LR: 2.000e-04 

2023-03-02 01:51:40,417 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.1421 (0.1794) Acc D Real: 97.020% 
Loss D Fake: 0.6333 (0.7523) Acc D Fake: 26.398% 
Loss D: 0.775 
Loss G: 0.7614 (0.6528) Acc G: 73.020% 
LR: 2.000e-04 

2023-03-02 01:51:40,424 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.1694 (0.1794) Acc D Real: 97.001% 
Loss D Fake: 0.6315 (0.7513) Acc D Fake: 26.840% 
Loss D: 0.801 
Loss G: 0.7634 (0.6536) Acc G: 72.583% 
LR: 2.000e-04 

2023-03-02 01:51:40,433 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.1641 (0.1792) Acc D Real: 96.977% 
Loss D Fake: 0.6300 (0.7504) Acc D Fake: 27.275% 
Loss D: 0.794 
Loss G: 0.7648 (0.6545) Acc G: 72.152% 
LR: 2.000e-04 

2023-03-02 01:51:40,440 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.1538 (0.1790) Acc D Real: 96.951% 
Loss D Fake: 0.6291 (0.7494) Acc D Fake: 27.703% 
Loss D: 0.783 
Loss G: 0.7657 (0.6554) Acc G: 71.728% 
LR: 2.000e-04 

2023-03-02 01:51:40,448 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1938 (0.1792) Acc D Real: 96.890% 
Loss D Fake: 0.6284 (0.7485) Acc D Fake: 28.125% 
Loss D: 0.822 
Loss G: 0.7668 (0.6563) Acc G: 71.311% 
LR: 2.000e-04 

2023-03-02 01:51:40,455 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.1825 (0.1792) Acc D Real: 96.845% 
Loss D Fake: 0.6274 (0.7475) Acc D Fake: 28.540% 
Loss D: 0.810 
Loss G: 0.7686 (0.6571) Acc G: 70.900% 
LR: 2.000e-04 

2023-03-02 01:51:40,463 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.1429 (0.1789) Acc D Real: 96.834% 
Loss D Fake: 0.6252 (0.7466) Acc D Fake: 28.949% 
Loss D: 0.768 
Loss G: 0.7723 (0.6580) Acc G: 70.488% 
LR: 2.000e-04 

2023-03-02 01:51:40,470 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2082 (0.1791) Acc D Real: 96.775% 
Loss D Fake: 0.6219 (0.7456) Acc D Fake: 29.364% 
Loss D: 0.830 
Loss G: 0.7755 (0.6589) Acc G: 70.077% 
LR: 2.000e-04 

2023-03-02 01:51:40,478 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.1555 (0.1789) Acc D Real: 96.755% 
Loss D Fake: 0.6194 (0.7447) Acc D Fake: 29.773% 
Loss D: 0.775 
Loss G: 0.7785 (0.6598) Acc G: 69.673% 
LR: 2.000e-04 

2023-03-02 01:51:40,486 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.1816 (0.1790) Acc D Real: 96.721% 
Loss D Fake: 0.6171 (0.7437) Acc D Fake: 30.175% 
Loss D: 0.799 
Loss G: 0.7808 (0.6607) Acc G: 69.274% 
LR: 2.000e-04 

2023-03-02 01:51:40,493 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.1384 (0.1787) Acc D Real: 96.715% 
Loss D Fake: 0.6153 (0.7428) Acc D Fake: 30.572% 
Loss D: 0.754 
Loss G: 0.7835 (0.6617) Acc G: 68.881% 
LR: 2.000e-04 

2023-03-02 01:51:40,501 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.1943 (0.1788) Acc D Real: 96.659% 
Loss D Fake: 0.6128 (0.7418) Acc D Fake: 30.963% 
Loss D: 0.807 
Loss G: 0.7865 (0.6626) Acc G: 68.495% 
LR: 2.000e-04 

2023-03-02 01:51:40,508 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.1542 (0.1786) Acc D Real: 96.638% 
Loss D Fake: 0.6104 (0.7408) Acc D Fake: 31.348% 
Loss D: 0.765 
Loss G: 0.7895 (0.6635) Acc G: 68.114% 
LR: 2.000e-04 

2023-03-02 01:51:40,516 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.1333 (0.1783) Acc D Real: 96.635% 
Loss D Fake: 0.6079 (0.7399) Acc D Fake: 31.727% 
Loss D: 0.741 
Loss G: 0.7928 (0.6645) Acc G: 67.738% 
LR: 2.000e-04 

2023-03-02 01:51:40,523 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.1972 (0.1784) Acc D Real: 96.578% 
Loss D Fake: 0.6052 (0.7389) Acc D Fake: 32.101% 
Loss D: 0.802 
Loss G: 0.7960 (0.6654) Acc G: 67.368% 
LR: 2.000e-04 

2023-03-02 01:51:40,531 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.1596 (0.1783) Acc D Real: 96.560% 
Loss D Fake: 0.6028 (0.7379) Acc D Fake: 32.470% 
Loss D: 0.762 
Loss G: 0.7991 (0.6664) Acc G: 67.003% 
LR: 2.000e-04 

2023-03-02 01:51:40,538 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1442 (0.1780) Acc D Real: 96.545% 
Loss D Fake: 0.6003 (0.7369) Acc D Fake: 32.833% 
Loss D: 0.744 
Loss G: 0.8022 (0.6673) Acc G: 66.644% 
LR: 2.000e-04 

2023-03-02 01:51:40,545 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.1666 (0.1779) Acc D Real: 96.517% 
Loss D Fake: 0.5979 (0.7359) Acc D Fake: 33.191% 
Loss D: 0.765 
Loss G: 0.8052 (0.6683) Acc G: 66.289% 
LR: 2.000e-04 

2023-03-02 01:51:40,553 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.1714 (0.1779) Acc D Real: 96.480% 
Loss D Fake: 0.5956 (0.7349) Acc D Fake: 33.545% 
Loss D: 0.767 
Loss G: 0.8080 (0.6693) Acc G: 65.940% 
LR: 2.000e-04 

2023-03-02 01:51:40,560 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.1781 (0.1779) Acc D Real: 96.446% 
Loss D Fake: 0.5937 (0.7340) Acc D Fake: 33.893% 
Loss D: 0.772 
Loss G: 0.8103 (0.6703) Acc G: 65.595% 
LR: 2.000e-04 

2023-03-02 01:51:40,568 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.1593 (0.1778) Acc D Real: 96.423% 
Loss D Fake: 0.5921 (0.7330) Acc D Fake: 34.236% 
Loss D: 0.751 
Loss G: 0.8124 (0.6713) Acc G: 65.244% 
LR: 2.000e-04 

2023-03-02 01:51:40,575 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.1373 (0.1775) Acc D Real: 96.414% 
Loss D Fake: 0.5917 (0.7320) Acc D Fake: 34.586% 
Loss D: 0.729 
Loss G: 0.8103 (0.6722) Acc G: 64.897% 
LR: 2.000e-04 

2023-03-02 01:51:40,582 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.1501 (0.1773) Acc D Real: 96.397% 
Loss D Fake: 0.5955 (0.7311) Acc D Fake: 34.932% 
Loss D: 0.746 
Loss G: 0.8161 (0.6732) Acc G: 64.556% 
LR: 2.000e-04 

2023-03-02 01:51:40,590 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.1954 (0.1774) Acc D Real: 96.342% 
Loss D Fake: 0.5859 (0.7301) Acc D Fake: 35.272% 
Loss D: 0.781 
Loss G: 0.8219 (0.6742) Acc G: 64.218% 
LR: 2.000e-04 

2023-03-02 01:51:40,597 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2118 (0.1777) Acc D Real: 96.280% 
Loss D Fake: 0.5829 (0.7291) Acc D Fake: 35.608% 
Loss D: 0.795 
Loss G: 0.8246 (0.6752) Acc G: 63.886% 
LR: 2.000e-04 

2023-03-02 01:51:40,604 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.1332 (0.1774) Acc D Real: 96.270% 
Loss D Fake: 0.5812 (0.7281) Acc D Fake: 35.940% 
Loss D: 0.714 
Loss G: 0.8277 (0.6763) Acc G: 63.558% 
LR: 2.000e-04 

2023-03-02 01:51:40,612 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.1718 (0.1773) Acc D Real: 96.232% 
Loss D Fake: 0.5788 (0.7271) Acc D Fake: 36.267% 
Loss D: 0.751 
Loss G: 0.8304 (0.6773) Acc G: 63.236% 
LR: 2.000e-04 

2023-03-02 01:51:40,620 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.1710 (0.1773) Acc D Real: 96.206% 
Loss D Fake: 0.5774 (0.7261) Acc D Fake: 36.578% 
Loss D: 0.748 
Loss G: 0.8320 (0.6783) Acc G: 62.928% 
LR: 2.000e-04 

2023-03-02 01:51:40,627 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.1481 (0.1771) Acc D Real: 96.195% 
Loss D Fake: 0.5766 (0.7251) Acc D Fake: 36.886% 
Loss D: 0.725 
Loss G: 0.8331 (0.6793) Acc G: 62.624% 
LR: 2.000e-04 

2023-03-02 01:51:40,635 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.1393 (0.1768) Acc D Real: 96.180% 
Loss D Fake: 0.5760 (0.7241) Acc D Fake: 37.190% 
Loss D: 0.715 
Loss G: 0.8343 (0.6804) Acc G: 62.323% 
LR: 2.000e-04 

2023-03-02 01:51:40,642 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.1781 (0.1769) Acc D Real: 96.146% 
Loss D Fake: 0.5752 (0.7232) Acc D Fake: 37.489% 
Loss D: 0.753 
Loss G: 0.8359 (0.6814) Acc G: 62.027% 
LR: 2.000e-04 

2023-03-02 01:51:40,650 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.1436 (0.1766) Acc D Real: 96.126% 
Loss D Fake: 0.5740 (0.7222) Acc D Fake: 37.785% 
Loss D: 0.718 
Loss G: 0.8381 (0.6824) Acc G: 61.734% 
LR: 2.000e-04 

2023-03-02 01:51:40,657 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.1171 (0.1763) Acc D Real: 96.127% 
Loss D Fake: 0.5722 (0.7213) Acc D Fake: 38.077% 
Loss D: 0.689 
Loss G: 0.8411 (0.6834) Acc G: 61.445% 
LR: 2.000e-04 

2023-03-02 01:51:40,664 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.1369 (0.1760) Acc D Real: 96.124% 
Loss D Fake: 0.5701 (0.7203) Acc D Fake: 38.365% 
Loss D: 0.707 
Loss G: 0.8443 (0.6844) Acc G: 61.160% 
LR: 2.000e-04 

2023-03-02 01:51:40,672 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.0638 (0.1753) Acc D Real: 96.125% 
Loss D Fake: 0.5676 (0.7193) Acc D Fake: 38.392% 
Loss D: 0.631 
Loss G: 0.8487 (0.6855) Acc G: 61.134% 
LR: 2.000e-04 

2023-03-02 01:51:40,913 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.632 | Generator Loss: 0.849 | Avg: 1.481 
2023-03-02 01:51:40,942 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.650 | Generator Loss: 0.849 | Avg: 1.499 
2023-03-02 01:51:40,967 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.647 | Generator Loss: 0.849 | Avg: 1.495 
2023-03-02 01:51:40,994 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.641 | Generator Loss: 0.849 | Avg: 1.490 
2023-03-02 01:51:41,021 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.641 | Generator Loss: 0.849 | Avg: 1.490 
2023-03-02 01:51:41,047 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.659 | Generator Loss: 0.849 | Avg: 1.508 
2023-03-02 01:51:41,074 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.663 | Generator Loss: 0.849 | Avg: 1.512 
2023-03-02 01:51:41,103 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.678 | Generator Loss: 0.849 | Avg: 1.526 
2023-03-02 01:51:41,128 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.683 | Generator Loss: 0.849 | Avg: 1.532 
2023-03-02 01:51:41,154 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.693 | Generator Loss: 0.849 | Avg: 1.542 
2023-03-02 01:51:41,179 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.703 | Generator Loss: 0.849 | Avg: 1.551 
2023-03-02 01:51:41,205 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.707 | Generator Loss: 0.849 | Avg: 1.556 
2023-03-02 01:51:41,231 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.713 | Generator Loss: 0.849 | Avg: 1.562 
2023-03-02 01:51:41,256 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.714 | Generator Loss: 0.849 | Avg: 1.562 
2023-03-02 01:51:41,282 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.710 | Generator Loss: 0.849 | Avg: 1.559 
2023-03-02 01:51:41,307 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.706 | Generator Loss: 0.849 | Avg: 1.555 
2023-03-02 01:51:41,333 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.703 | Generator Loss: 0.849 | Avg: 1.552 
2023-03-02 01:51:41,369 -                train: [    INFO] - Best Loss 0.795 to 0.774
2023-03-02 01:51:41,369 -                train: [    INFO] - 
Epoch: 3/20
2023-03-02 01:51:41,542 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.1749 (0.1572) Acc D Real: 92.943% 
Loss D Fake: 0.5605 (0.5623) Acc D Fake: 83.333% 
Loss D: 0.735 
Loss G: 0.8588 (0.8564) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,550 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.1694 (0.1613) Acc D Real: 92.448% 
Loss D Fake: 0.5580 (0.5609) Acc D Fake: 83.333% 
Loss D: 0.727 
Loss G: 0.8598 (0.8576) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,559 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.1344 (0.1545) Acc D Real: 93.477% 
Loss D Fake: 0.5624 (0.5613) Acc D Fake: 83.333% 
Loss D: 0.697 
Loss G: 0.8693 (0.8605) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,577 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.1761 (0.1589) Acc D Real: 93.125% 
Loss D Fake: 0.5491 (0.5588) Acc D Fake: 83.333% 
Loss D: 0.725 
Loss G: 0.8760 (0.8636) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,584 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.1277 (0.1537) Acc D Real: 93.420% 
Loss D Fake: 0.5456 (0.5566) Acc D Fake: 83.333% 
Loss D: 0.673 
Loss G: 0.8811 (0.8665) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,590 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.0937 (0.1451) Acc D Real: 93.981% 
Loss D Fake: 0.5420 (0.5545) Acc D Fake: 83.333% 
Loss D: 0.636 
Loss G: 0.8869 (0.8694) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,598 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.1576 (0.1467) Acc D Real: 93.750% 
Loss D Fake: 0.5380 (0.5525) Acc D Fake: 83.333% 
Loss D: 0.696 
Loss G: 0.8930 (0.8724) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,605 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.1342 (0.1453) Acc D Real: 93.848% 
Loss D Fake: 0.5339 (0.5504) Acc D Fake: 83.333% 
Loss D: 0.668 
Loss G: 0.8997 (0.8754) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 01:51:41,612 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.1715 (0.1479) Acc D Real: 93.484% 
Loss D Fake: 0.5294 (0.5483) Acc D Fake: 83.432% 
Loss D: 0.701 
Loss G: 0.9069 (0.8786) Acc G: 16.500% 
LR: 2.000e-04 

2023-03-02 01:51:41,619 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.1672 (0.1496) Acc D Real: 93.305% 
Loss D Fake: 0.5249 (0.5462) Acc D Fake: 83.575% 
Loss D: 0.692 
Loss G: 0.9141 (0.8818) Acc G: 16.364% 
LR: 2.000e-04 

2023-03-02 01:51:41,626 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.1648 (0.1509) Acc D Real: 93.134% 
Loss D Fake: 0.5202 (0.5440) Acc D Fake: 83.832% 
Loss D: 0.685 
Loss G: 0.9214 (0.8851) Acc G: 16.111% 
LR: 2.000e-04 

2023-03-02 01:51:41,633 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.1502 (0.1509) Acc D Real: 93.169% 
Loss D Fake: 0.5231 (0.5424) Acc D Fake: 84.050% 
Loss D: 0.673 
Loss G: 0.9265 (0.8883) Acc G: 15.897% 
LR: 2.000e-04 

2023-03-02 01:51:41,640 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.1291 (0.1493) Acc D Real: 93.300% 
Loss D Fake: 0.5120 (0.5402) Acc D Fake: 84.237% 
Loss D: 0.641 
Loss G: 0.9356 (0.8917) Acc G: 15.714% 
LR: 2.000e-04 

2023-03-02 01:51:41,647 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.1350 (0.1483) Acc D Real: 93.420% 
Loss D Fake: 0.5075 (0.5381) Acc D Fake: 84.399% 
Loss D: 0.642 
Loss G: 0.9431 (0.8951) Acc G: 15.556% 
LR: 2.000e-04 

2023-03-02 01:51:41,654 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.1531 (0.1486) Acc D Real: 93.398% 
Loss D Fake: 0.5030 (0.5359) Acc D Fake: 84.541% 
Loss D: 0.656 
Loss G: 0.9509 (0.8986) Acc G: 15.417% 
LR: 2.000e-04 

2023-03-02 01:51:41,662 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.0955 (0.1455) Acc D Real: 93.627% 
Loss D Fake: 0.4982 (0.5336) Acc D Fake: 84.666% 
Loss D: 0.594 
Loss G: 0.9600 (0.9022) Acc G: 15.294% 
LR: 2.000e-04 

2023-03-02 01:51:41,669 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.1867 (0.1478) Acc D Real: 93.472% 
Loss D Fake: 0.4928 (0.5314) Acc D Fake: 84.777% 
Loss D: 0.679 
Loss G: 0.9697 (0.9059) Acc G: 15.185% 
LR: 2.000e-04 

2023-03-02 01:51:41,677 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.1321 (0.1470) Acc D Real: 93.503% 
Loss D Fake: 0.4876 (0.5291) Acc D Fake: 84.877% 
Loss D: 0.620 
Loss G: 0.9768 (0.9097) Acc G: 15.088% 
LR: 2.000e-04 

2023-03-02 01:51:41,684 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.1046 (0.1449) Acc D Real: 93.682% 
Loss D Fake: 0.4915 (0.5272) Acc D Fake: 84.966% 
Loss D: 0.596 
Loss G: 0.9867 (0.9135) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:51:41,691 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.1088 (0.1431) Acc D Real: 93.812% 
Loss D Fake: 0.4808 (0.5250) Acc D Fake: 85.047% 
Loss D: 0.590 
Loss G: 0.9875 (0.9170) Acc G: 14.841% 
LR: 2.000e-04 

2023-03-02 01:51:41,698 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2042 (0.1459) Acc D Real: 93.570% 
Loss D Fake: 0.4797 (0.5229) Acc D Fake: 85.196% 
Loss D: 0.684 
Loss G: 0.9892 (0.9203) Acc G: 14.754% 
LR: 2.000e-04 

2023-03-02 01:51:41,705 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.1619 (0.1466) Acc D Real: 93.512% 
Loss D Fake: 0.4790 (0.5210) Acc D Fake: 85.260% 
Loss D: 0.641 
Loss G: 0.9922 (0.9235) Acc G: 14.692% 
LR: 2.000e-04 

2023-03-02 01:51:41,712 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.1639 (0.1473) Acc D Real: 93.468% 
Loss D Fake: 0.4773 (0.5192) Acc D Fake: 85.319% 
Loss D: 0.641 
Loss G: 0.9965 (0.9265) Acc G: 14.635% 
LR: 2.000e-04 

2023-03-02 01:51:41,719 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.1507 (0.1475) Acc D Real: 93.467% 
Loss D Fake: 0.4751 (0.5174) Acc D Fake: 85.373% 
Loss D: 0.626 
Loss G: 1.0015 (0.9295) Acc G: 14.583% 
LR: 2.000e-04 

2023-03-02 01:51:41,727 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.1821 (0.1488) Acc D Real: 93.299% 
Loss D Fake: 0.4726 (0.5157) Acc D Fake: 85.423% 
Loss D: 0.655 
Loss G: 1.0079 (0.9325) Acc G: 14.535% 
LR: 2.000e-04 

2023-03-02 01:51:41,734 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.1235 (0.1479) Acc D Real: 93.370% 
Loss D Fake: 0.4693 (0.5140) Acc D Fake: 85.469% 
Loss D: 0.593 
Loss G: 1.0164 (0.9356) Acc G: 14.491% 
LR: 2.000e-04 

2023-03-02 01:51:41,741 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.1375 (0.1475) Acc D Real: 93.441% 
Loss D Fake: 0.4650 (0.5122) Acc D Fake: 85.512% 
Loss D: 0.602 
Loss G: 1.0266 (0.9389) Acc G: 14.449% 
LR: 2.000e-04 

2023-03-02 01:51:41,749 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.1275 (0.1468) Acc D Real: 93.502% 
Loss D Fake: 0.4600 (0.5104) Acc D Fake: 85.506% 
Loss D: 0.587 
Loss G: 1.0383 (0.9423) Acc G: 14.468% 
LR: 2.000e-04 

2023-03-02 01:51:41,756 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.1227 (0.1460) Acc D Real: 93.566% 
Loss D Fake: 0.4541 (0.5086) Acc D Fake: 85.490% 
Loss D: 0.577 
Loss G: 1.0528 (0.9460) Acc G: 14.486% 
LR: 2.000e-04 

2023-03-02 01:51:41,764 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.1193 (0.1451) Acc D Real: 93.617% 
Loss D Fake: 0.4466 (0.5066) Acc D Fake: 85.474% 
Loss D: 0.566 
Loss G: 1.0717 (0.9500) Acc G: 14.503% 
LR: 2.000e-04 

2023-03-02 01:51:41,772 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.1501 (0.1453) Acc D Real: 93.626% 
Loss D Fake: 0.4367 (0.5044) Acc D Fake: 85.459% 
Loss D: 0.587 
Loss G: 1.0976 (0.9546) Acc G: 14.518% 
LR: 2.000e-04 

2023-03-02 01:51:41,780 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.1221 (0.1446) Acc D Real: 93.662% 
Loss D Fake: 1.1215 (0.5231) Acc D Fake: 84.877% 
Loss D: 1.244 
Loss G: 1.0638 (0.9580) Acc G: 14.482% 
LR: 2.000e-04 

2023-03-02 01:51:41,787 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1422 (0.1445) Acc D Real: 93.650% 
Loss D Fake: 0.4502 (0.5209) Acc D Fake: 84.979% 
Loss D: 0.592 
Loss G: 1.0373 (0.9603) Acc G: 14.350% 
LR: 2.000e-04 

2023-03-02 01:51:41,795 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.1784 (0.1455) Acc D Real: 93.551% 
Loss D Fake: 0.4567 (0.5191) Acc D Fake: 85.170% 
Loss D: 0.635 
Loss G: 1.0241 (0.9621) Acc G: 14.179% 
LR: 2.000e-04 

2023-03-02 01:51:41,803 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2343 (0.1480) Acc D Real: 93.284% 
Loss D Fake: 0.4661 (0.5176) Acc D Fake: 85.396% 
Loss D: 0.700 
Loss G: 0.9842 (0.9627) Acc G: 13.970% 
LR: 2.000e-04 

2023-03-02 01:51:41,812 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3174 (0.1525) Acc D Real: 92.815% 
Loss D Fake: 0.5065 (0.5173) Acc D Fake: 85.611% 
Loss D: 0.824 
Loss G: 0.2593 (0.9437) Acc G: 16.025% 
LR: 2.000e-04 

2023-03-02 01:51:41,820 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2674 (0.1556) Acc D Real: 92.453% 
Loss D Fake: 2.7204 (0.5753) Acc D Fake: 83.490% 
Loss D: 2.988 
Loss G: 0.1988 (0.9241) Acc G: 18.147% 
LR: 2.000e-04 

2023-03-02 01:51:41,828 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2848 (0.1589) Acc D Real: 92.053% 
Loss D Fake: 2.8294 (0.6331) Acc D Fake: 81.392% 
Loss D: 3.114 
Loss G: 0.1835 (0.9051) Acc G: 20.203% 
LR: 2.000e-04 

2023-03-02 01:51:41,836 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.1891 (0.1596) Acc D Real: 91.885% 
Loss D Fake: 2.8489 (0.6885) Acc D Fake: 79.440% 
Loss D: 3.038 
Loss G: 0.1771 (0.8869) Acc G: 22.031% 
LR: 2.000e-04 

2023-03-02 01:51:41,844 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.2040 (0.1607) Acc D Real: 91.681% 
Loss D Fake: 2.8332 (0.7408) Acc D Fake: 77.665% 
Loss D: 3.037 
Loss G: 0.1746 (0.8695) Acc G: 23.707% 
LR: 2.000e-04 

2023-03-02 01:51:41,852 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.1792 (0.1611) Acc D Real: 91.549% 
Loss D Fake: 2.7994 (0.7898) Acc D Fake: 76.084% 
Loss D: 2.979 
Loss G: 0.1741 (0.8530) Acc G: 25.246% 
LR: 2.000e-04 

2023-03-02 01:51:41,860 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2735 (0.1638) Acc D Real: 91.223% 
Loss D Fake: 2.7554 (0.8355) Acc D Fake: 74.586% 
Loss D: 3.029 
Loss G: 0.1749 (0.8372) Acc G: 26.713% 
LR: 2.000e-04 

2023-03-02 01:51:41,870 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.1976 (0.1645) Acc D Real: 91.098% 
Loss D Fake: 2.7053 (0.8780) Acc D Fake: 73.156% 
Loss D: 2.903 
Loss G: 0.1766 (0.8222) Acc G: 28.113% 
LR: 2.000e-04 

2023-03-02 01:51:41,878 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.1648 (0.1645) Acc D Real: 91.032% 
Loss D Fake: 2.6517 (0.9174) Acc D Fake: 71.789% 
Loss D: 2.816 
Loss G: 0.1790 (0.8079) Acc G: 29.451% 
LR: 2.000e-04 

2023-03-02 01:51:41,885 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.1978 (0.1653) Acc D Real: 90.879% 
Loss D Fake: 2.5961 (0.9539) Acc D Fake: 70.482% 
Loss D: 2.794 
Loss G: 0.1820 (0.7943) Acc G: 30.696% 
LR: 2.000e-04 

2023-03-02 01:51:41,893 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.1579 (0.1651) Acc D Real: 90.812% 
Loss D Fake: 2.5395 (0.9877) Acc D Fake: 69.266% 
Loss D: 2.697 
Loss G: 0.1855 (0.7814) Acc G: 31.887% 
LR: 2.000e-04 

2023-03-02 01:51:41,901 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.1932 (0.1657) Acc D Real: 90.710% 
Loss D Fake: 2.4833 (1.0188) Acc D Fake: 68.101% 
Loss D: 2.677 
Loss G: 0.1888 (0.7690) Acc G: 33.028% 
LR: 2.000e-04 

2023-03-02 01:51:41,909 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.1988 (0.1664) Acc D Real: 90.553% 
Loss D Fake: 2.4294 (1.0476) Acc D Fake: 66.983% 
Loss D: 2.628 
Loss G: 0.1924 (0.7572) Acc G: 34.123% 
LR: 2.000e-04 

2023-03-02 01:51:41,916 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2162 (0.1674) Acc D Real: 90.379% 
Loss D Fake: 2.3754 (1.0742) Acc D Fake: 65.910% 
Loss D: 2.592 
Loss G: 0.1964 (0.7460) Acc G: 35.174% 
LR: 2.000e-04 

2023-03-02 01:51:41,924 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2506 (0.1690) Acc D Real: 90.170% 
Loss D Fake: 2.3219 (1.0986) Acc D Fake: 64.879% 
Loss D: 2.572 
Loss G: 0.2007 (0.7353) Acc G: 36.184% 
LR: 2.000e-04 

2023-03-02 01:51:41,932 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2104 (0.1698) Acc D Real: 90.034% 
Loss D Fake: 2.2695 (1.1211) Acc D Fake: 63.888% 
Loss D: 2.480 
Loss G: 0.2053 (0.7251) Acc G: 37.154% 
LR: 2.000e-04 

2023-03-02 01:51:41,940 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2380 (0.1711) Acc D Real: 89.832% 
Loss D Fake: 2.2177 (1.1418) Acc D Fake: 62.934% 
Loss D: 2.456 
Loss G: 0.2102 (0.7154) Acc G: 38.057% 
LR: 2.000e-04 

2023-03-02 01:51:41,948 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.1941 (0.1715) Acc D Real: 89.747% 
Loss D Fake: 2.1670 (1.1608) Acc D Fake: 62.047% 
Loss D: 2.361 
Loss G: 0.2153 (0.7062) Acc G: 38.927% 
LR: 2.000e-04 

2023-03-02 01:51:41,955 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2356 (0.1727) Acc D Real: 89.541% 
Loss D Fake: 2.1175 (1.1782) Acc D Fake: 61.191% 
Loss D: 2.353 
Loss G: 0.2206 (0.6973) Acc G: 39.764% 
LR: 2.000e-04 

2023-03-02 01:51:41,963 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2971 (0.1749) Acc D Real: 89.247% 
Loss D Fake: 2.0691 (1.1941) Acc D Fake: 60.366% 
Loss D: 2.366 
Loss G: 0.2261 (0.6889) Acc G: 40.572% 
LR: 2.000e-04 

2023-03-02 01:51:41,971 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2451 (0.1761) Acc D Real: 89.084% 
Loss D Fake: 2.0222 (1.2086) Acc D Fake: 59.571% 
Loss D: 2.267 
Loss G: 0.2316 (0.6809) Acc G: 41.351% 
LR: 2.000e-04 

2023-03-02 01:51:41,979 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2571 (0.1775) Acc D Real: 88.892% 
Loss D Fake: 1.9769 (1.2219) Acc D Fake: 58.802% 
Loss D: 2.234 
Loss G: 0.2372 (0.6732) Acc G: 42.104% 
LR: 2.000e-04 

2023-03-02 01:51:41,986 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3182 (0.1799) Acc D Real: 88.596% 
Loss D Fake: 1.9328 (1.2339) Acc D Fake: 58.060% 
Loss D: 2.251 
Loss G: 0.2429 (0.6660) Acc G: 42.831% 
LR: 2.000e-04 

2023-03-02 01:51:41,994 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2544 (0.1811) Acc D Real: 88.438% 
Loss D Fake: 1.8899 (1.2449) Acc D Fake: 57.342% 
Loss D: 2.144 
Loss G: 0.2488 (0.6590) Acc G: 43.506% 
LR: 2.000e-04 

2023-03-02 01:51:42,002 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2638 (0.1825) Acc D Real: 88.259% 
Loss D Fake: 1.8483 (1.2548) Acc D Fake: 56.675% 
Loss D: 2.112 
Loss G: 0.2549 (0.6524) Acc G: 44.159% 
LR: 2.000e-04 

2023-03-02 01:51:42,009 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2530 (0.1836) Acc D Real: 88.129% 
Loss D Fake: 1.8072 (1.2637) Acc D Fake: 56.030% 
Loss D: 2.060 
Loss G: 0.2613 (0.6461) Acc G: 44.791% 
LR: 2.000e-04 

2023-03-02 01:51:42,017 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2110 (0.1841) Acc D Real: 88.076% 
Loss D Fake: 1.7667 (1.2717) Acc D Fake: 55.405% 
Loss D: 1.978 
Loss G: 0.2680 (0.6401) Acc G: 45.403% 
LR: 2.000e-04 

2023-03-02 01:51:42,024 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2178 (0.1846) Acc D Real: 87.987% 
Loss D Fake: 1.7275 (1.2788) Acc D Fake: 54.800% 
Loss D: 1.945 
Loss G: 0.2747 (0.6344) Acc G: 45.995% 
LR: 2.000e-04 

2023-03-02 01:51:42,032 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2318 (0.1853) Acc D Real: 87.895% 
Loss D Fake: 1.6892 (1.2851) Acc D Fake: 54.213% 
Loss D: 1.921 
Loss G: 0.2817 (0.6289) Acc G: 46.570% 
LR: 2.000e-04 

2023-03-02 01:51:42,040 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2044 (0.1856) Acc D Real: 87.857% 
Loss D Fake: 1.6516 (1.2907) Acc D Fake: 53.644% 
Loss D: 1.856 
Loss G: 0.2889 (0.6238) Acc G: 47.127% 
LR: 2.000e-04 

2023-03-02 01:51:42,047 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2739 (0.1869) Acc D Real: 87.706% 
Loss D Fake: 1.6149 (1.2955) Acc D Fake: 53.092% 
Loss D: 1.889 
Loss G: 0.2962 (0.6189) Acc G: 47.642% 
LR: 2.000e-04 

2023-03-02 01:51:42,055 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2292 (0.1876) Acc D Real: 87.619% 
Loss D Fake: 1.5792 (1.2997) Acc D Fake: 52.581% 
Loss D: 1.808 
Loss G: 0.3037 (0.6143) Acc G: 48.143% 
LR: 2.000e-04 

2023-03-02 01:51:42,062 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.2762 (0.1888) Acc D Real: 87.483% 
Loss D Fake: 1.5444 (1.3032) Acc D Fake: 52.085% 
Loss D: 1.821 
Loss G: 0.3112 (0.6099) Acc G: 48.628% 
LR: 2.000e-04 

2023-03-02 01:51:42,069 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2755 (0.1901) Acc D Real: 87.360% 
Loss D Fake: 1.5111 (1.3062) Acc D Fake: 51.603% 
Loss D: 1.787 
Loss G: 0.3187 (0.6057) Acc G: 49.100% 
LR: 2.000e-04 

2023-03-02 01:51:42,077 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.2855 (0.1914) Acc D Real: 87.227% 
Loss D Fake: 1.4787 (1.3086) Acc D Fake: 51.134% 
Loss D: 1.764 
Loss G: 0.3262 (0.6018) Acc G: 49.559% 
LR: 2.000e-04 

2023-03-02 01:51:42,084 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1952 (0.1915) Acc D Real: 87.228% 
Loss D Fake: 1.4473 (1.3105) Acc D Fake: 50.679% 
Loss D: 1.642 
Loss G: 0.3341 (0.5981) Acc G: 50.005% 
LR: 2.000e-04 

2023-03-02 01:51:42,092 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.2663 (0.1925) Acc D Real: 87.088% 
Loss D Fake: 1.4162 (1.3120) Acc D Fake: 50.235% 
Loss D: 1.683 
Loss G: 0.3422 (0.5945) Acc G: 50.439% 
LR: 2.000e-04 

2023-03-02 01:51:42,099 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3016 (0.1940) Acc D Real: 86.954% 
Loss D Fake: 1.3857 (1.3130) Acc D Fake: 49.804% 
Loss D: 1.687 
Loss G: 0.3503 (0.5912) Acc G: 50.861% 
LR: 2.000e-04 

2023-03-02 01:51:42,106 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.2776 (0.1951) Acc D Real: 86.867% 
Loss D Fake: 1.3565 (1.3136) Acc D Fake: 49.385% 
Loss D: 1.634 
Loss G: 0.3584 (0.5881) Acc G: 51.249% 
LR: 2.000e-04 

2023-03-02 01:51:42,114 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3445 (0.1971) Acc D Real: 86.673% 
Loss D Fake: 1.3281 (1.3138) Acc D Fake: 48.998% 
Loss D: 1.673 
Loss G: 0.3667 (0.5852) Acc G: 51.628% 
LR: 2.000e-04 

2023-03-02 01:51:42,121 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.1991 (0.1971) Acc D Real: 86.677% 
Loss D Fake: 1.3004 (1.3136) Acc D Fake: 48.621% 
Loss D: 1.500 
Loss G: 0.3751 (0.5825) Acc G: 51.996% 
LR: 2.000e-04 

2023-03-02 01:51:42,129 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2463 (0.1977) Acc D Real: 86.649% 
Loss D Fake: 1.2734 (1.3131) Acc D Fake: 48.255% 
Loss D: 1.520 
Loss G: 0.3835 (0.5799) Acc G: 52.355% 
LR: 2.000e-04 

2023-03-02 01:51:42,136 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3107 (0.1991) Acc D Real: 86.508% 
Loss D Fake: 1.2475 (1.3122) Acc D Fake: 47.897% 
Loss D: 1.558 
Loss G: 0.3921 (0.5776) Acc G: 52.705% 
LR: 2.000e-04 

2023-03-02 01:51:42,143 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.2751 (0.2001) Acc D Real: 86.384% 
Loss D Fake: 1.2220 (1.3111) Acc D Fake: 47.548% 
Loss D: 1.497 
Loss G: 0.4010 (0.5754) Acc G: 53.046% 
LR: 2.000e-04 

2023-03-02 01:51:42,151 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2744 (0.2010) Acc D Real: 86.320% 
Loss D Fake: 1.1967 (1.3097) Acc D Fake: 47.208% 
Loss D: 1.471 
Loss G: 0.4099 (0.5733) Acc G: 53.379% 
LR: 2.000e-04 

2023-03-02 01:51:42,158 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2372 (0.2014) Acc D Real: 86.317% 
Loss D Fake: 1.1723 (1.3080) Acc D Fake: 46.876% 
Loss D: 1.410 
Loss G: 0.4189 (0.5714) Acc G: 53.704% 
LR: 2.000e-04 

2023-03-02 01:51:42,166 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2868 (0.2025) Acc D Real: 86.195% 
Loss D Fake: 1.1488 (1.3061) Acc D Fake: 46.552% 
Loss D: 1.436 
Loss G: 0.4279 (0.5697) Acc G: 54.020% 
LR: 2.000e-04 

2023-03-02 01:51:42,173 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2744 (0.2033) Acc D Real: 86.127% 
Loss D Fake: 1.1259 (1.3040) Acc D Fake: 46.236% 
Loss D: 1.400 
Loss G: 0.4369 (0.5681) Acc G: 54.310% 
LR: 2.000e-04 

2023-03-02 01:51:42,180 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3221 (0.2047) Acc D Real: 86.010% 
Loss D Fake: 1.1028 (1.3016) Acc D Fake: 45.947% 
Loss D: 1.425 
Loss G: 0.4476 (0.5667) Acc G: 54.593% 
LR: 2.000e-04 

2023-03-02 01:51:42,188 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3100 (0.2060) Acc D Real: 85.919% 
Loss D Fake: 1.0783 (1.2990) Acc D Fake: 45.665% 
Loss D: 1.388 
Loss G: 0.4585 (0.5654) Acc G: 54.869% 
LR: 2.000e-04 

2023-03-02 01:51:42,196 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3149 (0.2072) Acc D Real: 85.817% 
Loss D Fake: 1.0554 (1.2962) Acc D Fake: 45.389% 
Loss D: 1.370 
Loss G: 0.4690 (0.5643) Acc G: 55.138% 
LR: 2.000e-04 

2023-03-02 01:51:42,203 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3297 (0.2086) Acc D Real: 85.669% 
Loss D Fake: 1.0340 (1.2932) Acc D Fake: 45.120% 
Loss D: 1.364 
Loss G: 0.4788 (0.5634) Acc G: 55.402% 
LR: 2.000e-04 

2023-03-02 01:51:42,210 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.1511 (0.2080) Acc D Real: 85.724% 
Loss D Fake: 1.0137 (1.2901) Acc D Fake: 44.856% 
Loss D: 1.165 
Loss G: 0.4888 (0.5625) Acc G: 55.641% 
LR: 2.000e-04 

2023-03-02 01:51:42,218 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2852 (0.2088) Acc D Real: 85.654% 
Loss D Fake: 0.9935 (1.2868) Acc D Fake: 44.617% 
Loss D: 1.279 
Loss G: 0.4987 (0.5618) Acc G: 55.874% 
LR: 2.000e-04 

2023-03-02 01:51:42,225 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3566 (0.2104) Acc D Real: 85.568% 
Loss D Fake: 0.9746 (1.2833) Acc D Fake: 44.383% 
Loss D: 1.331 
Loss G: 0.5082 (0.5612) Acc G: 56.103% 
LR: 2.000e-04 

2023-03-02 01:51:42,232 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3115 (0.2115) Acc D Real: 85.515% 
Loss D Fake: 0.9571 (1.2798) Acc D Fake: 44.154% 
Loss D: 1.269 
Loss G: 0.5173 (0.5608) Acc G: 56.326% 
LR: 2.000e-04 

2023-03-02 01:51:42,240 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4159 (0.2137) Acc D Real: 85.335% 
Loss D Fake: 0.9406 (1.2762) Acc D Fake: 43.930% 
Loss D: 1.357 
Loss G: 0.5261 (0.5604) Acc G: 56.545% 
LR: 2.000e-04 

2023-03-02 01:51:42,247 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2964 (0.2146) Acc D Real: 85.289% 
Loss D Fake: 0.9250 (1.2724) Acc D Fake: 43.711% 
Loss D: 1.221 
Loss G: 0.5349 (0.5601) Acc G: 56.759% 
LR: 2.000e-04 

2023-03-02 01:51:42,254 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3239 (0.2158) Acc D Real: 85.175% 
Loss D Fake: 0.9098 (1.2686) Acc D Fake: 43.497% 
Loss D: 1.234 
Loss G: 0.5438 (0.5599) Acc G: 56.969% 
LR: 2.000e-04 

2023-03-02 01:51:42,262 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2454 (0.2161) Acc D Real: 85.116% 
Loss D Fake: 0.8944 (1.2647) Acc D Fake: 43.287% 
Loss D: 1.140 
Loss G: 0.5531 (0.5599) Acc G: 57.174% 
LR: 2.000e-04 

2023-03-02 01:51:42,269 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3171 (0.2171) Acc D Real: 85.056% 
Loss D Fake: 0.8791 (1.2607) Acc D Fake: 43.081% 
Loss D: 1.196 
Loss G: 0.5625 (0.5599) Acc G: 57.375% 
LR: 2.000e-04 

2023-03-02 01:51:42,277 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3879 (0.2189) Acc D Real: 84.914% 
Loss D Fake: 0.8645 (1.2567) Acc D Fake: 42.879% 
Loss D: 1.252 
Loss G: 0.5717 (0.5600) Acc G: 57.572% 
LR: 2.000e-04 

2023-03-02 01:51:42,284 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2580 (0.2193) Acc D Real: 84.958% 
Loss D Fake: 0.8506 (1.2526) Acc D Fake: 42.682% 
Loss D: 1.109 
Loss G: 0.5806 (0.5602) Acc G: 57.748% 
LR: 2.000e-04 

2023-03-02 01:51:42,291 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3362 (0.2204) Acc D Real: 84.855% 
Loss D Fake: 0.8374 (1.2484) Acc D Fake: 42.505% 
Loss D: 1.174 
Loss G: 0.5895 (0.5605) Acc G: 57.920% 
LR: 2.000e-04 

2023-03-02 01:51:42,299 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3231 (0.2214) Acc D Real: 84.804% 
Loss D Fake: 0.8245 (1.2442) Acc D Fake: 42.332% 
Loss D: 1.148 
Loss G: 0.5983 (0.5609) Acc G: 58.089% 
LR: 2.000e-04 

2023-03-02 01:51:42,306 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3376 (0.2226) Acc D Real: 84.727% 
Loss D Fake: 0.8121 (1.2400) Acc D Fake: 42.162% 
Loss D: 1.150 
Loss G: 0.6071 (0.5613) Acc G: 58.255% 
LR: 2.000e-04 

2023-03-02 01:51:42,313 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2896 (0.2232) Acc D Real: 84.713% 
Loss D Fake: 0.8000 (1.2357) Acc D Fake: 41.995% 
Loss D: 1.090 
Loss G: 0.6159 (0.5619) Acc G: 58.418% 
LR: 2.000e-04 

2023-03-02 01:51:42,321 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3729 (0.2247) Acc D Real: 84.672% 
Loss D Fake: 0.7885 (1.2314) Acc D Fake: 41.832% 
Loss D: 1.161 
Loss G: 0.6240 (0.5625) Acc G: 58.577% 
LR: 2.000e-04 

2023-03-02 01:51:42,328 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3221 (0.2256) Acc D Real: 84.609% 
Loss D Fake: 0.7780 (1.2271) Acc D Fake: 41.672% 
Loss D: 1.100 
Loss G: 0.6321 (0.5631) Acc G: 58.734% 
LR: 2.000e-04 

2023-03-02 01:51:42,335 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3695 (0.2270) Acc D Real: 84.521% 
Loss D Fake: 0.7677 (1.2228) Acc D Fake: 41.514% 
Loss D: 1.137 
Loss G: 0.6401 (0.5639) Acc G: 58.887% 
LR: 2.000e-04 

2023-03-02 01:51:42,343 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3300 (0.2279) Acc D Real: 84.470% 
Loss D Fake: 0.7578 (1.2184) Acc D Fake: 41.360% 
Loss D: 1.088 
Loss G: 0.6479 (0.5646) Acc G: 59.038% 
LR: 2.000e-04 

2023-03-02 01:51:42,350 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4350 (0.2298) Acc D Real: 84.337% 
Loss D Fake: 0.7484 (1.2141) Acc D Fake: 41.209% 
Loss D: 1.183 
Loss G: 0.6553 (0.5655) Acc G: 59.185% 
LR: 2.000e-04 

2023-03-02 01:51:42,358 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3527 (0.2310) Acc D Real: 84.307% 
Loss D Fake: 0.7398 (1.2097) Acc D Fake: 41.060% 
Loss D: 1.092 
Loss G: 0.6622 (0.5664) Acc G: 59.331% 
LR: 2.000e-04 

2023-03-02 01:51:42,365 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3654 (0.2322) Acc D Real: 84.229% 
Loss D Fake: 0.7316 (1.2054) Acc D Fake: 40.914% 
Loss D: 1.097 
Loss G: 0.6692 (0.5673) Acc G: 59.473% 
LR: 2.000e-04 

2023-03-02 01:51:42,373 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1548 (0.2315) Acc D Real: 84.253% 
Loss D Fake: 0.7231 (1.2010) Acc D Fake: 40.770% 
Loss D: 0.878 
Loss G: 0.6771 (0.5683) Acc G: 59.613% 
LR: 2.000e-04 

2023-03-02 01:51:42,380 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2420 (0.2316) Acc D Real: 84.248% 
Loss D Fake: 0.7137 (1.1967) Acc D Fake: 40.630% 
Loss D: 0.956 
Loss G: 0.6856 (0.5693) Acc G: 59.735% 
LR: 2.000e-04 

2023-03-02 01:51:42,387 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2587 (0.2318) Acc D Real: 84.268% 
Loss D Fake: 0.7042 (1.1923) Acc D Fake: 40.506% 
Loss D: 0.963 
Loss G: 0.6945 (0.5705) Acc G: 59.856% 
LR: 2.000e-04 

2023-03-02 01:51:42,395 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3416 (0.2328) Acc D Real: 84.199% 
Loss D Fake: 0.6946 (1.1880) Acc D Fake: 40.385% 
Loss D: 1.036 
Loss G: 0.7033 (0.5716) Acc G: 59.974% 
LR: 2.000e-04 

2023-03-02 01:51:42,403 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3981 (0.2342) Acc D Real: 84.131% 
Loss D Fake: 0.6856 (1.1836) Acc D Fake: 40.265% 
Loss D: 1.084 
Loss G: 0.7114 (0.5728) Acc G: 60.090% 
LR: 2.000e-04 

2023-03-02 01:51:42,410 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2721 (0.2345) Acc D Real: 84.125% 
Loss D Fake: 0.6773 (1.1792) Acc D Fake: 40.148% 
Loss D: 0.949 
Loss G: 0.7196 (0.5741) Acc G: 60.204% 
LR: 2.000e-04 

2023-03-02 01:51:42,418 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3410 (0.2355) Acc D Real: 84.019% 
Loss D Fake: 0.6691 (1.1749) Acc D Fake: 40.432% 
Loss D: 1.010 
Loss G: 0.7276 (0.5754) Acc G: 59.875% 
LR: 2.000e-04 

2023-03-02 01:51:42,425 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3368 (0.2363) Acc D Real: 83.896% 
Loss D Fake: 0.6612 (1.1705) Acc D Fake: 40.771% 
Loss D: 0.998 
Loss G: 0.7356 (0.5768) Acc G: 59.509% 
LR: 2.000e-04 

2023-03-02 01:51:42,433 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3659 (0.2374) Acc D Real: 83.731% 
Loss D Fake: 0.6535 (1.1662) Acc D Fake: 41.129% 
Loss D: 1.019 
Loss G: 0.7435 (0.5782) Acc G: 59.121% 
LR: 2.000e-04 

2023-03-02 01:51:42,440 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2614 (0.2376) Acc D Real: 83.654% 
Loss D Fake: 0.6459 (1.1618) Acc D Fake: 41.508% 
Loss D: 0.907 
Loss G: 0.7514 (0.5796) Acc G: 58.725% 
LR: 2.000e-04 

2023-03-02 01:51:42,448 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4104 (0.2390) Acc D Real: 83.434% 
Loss D Fake: 0.6387 (1.1575) Acc D Fake: 41.895% 
Loss D: 1.049 
Loss G: 0.7589 (0.5811) Acc G: 58.323% 
LR: 2.000e-04 

2023-03-02 01:51:42,456 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2625 (0.2392) Acc D Real: 83.354% 
Loss D Fake: 0.6319 (1.1532) Acc D Fake: 42.290% 
Loss D: 0.894 
Loss G: 0.7664 (0.5826) Acc G: 57.926% 
LR: 2.000e-04 

2023-03-02 01:51:42,463 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3588 (0.2402) Acc D Real: 83.178% 
Loss D Fake: 0.6252 (1.1489) Acc D Fake: 42.677% 
Loss D: 0.984 
Loss G: 0.7735 (0.5842) Acc G: 57.523% 
LR: 2.000e-04 

2023-03-02 01:51:42,471 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3962 (0.2415) Acc D Real: 82.977% 
Loss D Fake: 0.6191 (1.1446) Acc D Fake: 43.072% 
Loss D: 1.015 
Loss G: 0.7802 (0.5857) Acc G: 57.127% 
LR: 2.000e-04 

2023-03-02 01:51:42,479 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2482 (0.2415) Acc D Real: 82.917% 
Loss D Fake: 0.6132 (1.1404) Acc D Fake: 43.462% 
Loss D: 0.861 
Loss G: 0.7871 (0.5874) Acc G: 56.710% 
LR: 2.000e-04 

2023-03-02 01:51:42,486 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4200 (0.2429) Acc D Real: 82.699% 
Loss D Fake: 0.6073 (1.1361) Acc D Fake: 43.872% 
Loss D: 1.027 
Loss G: 0.7936 (0.5890) Acc G: 56.299% 
LR: 2.000e-04 

2023-03-02 01:51:42,494 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4535 (0.2446) Acc D Real: 82.450% 
Loss D Fake: 0.6021 (1.1319) Acc D Fake: 44.274% 
Loss D: 1.056 
Loss G: 0.7995 (0.5907) Acc G: 55.882% 
LR: 2.000e-04 

2023-03-02 01:51:42,501 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4049 (0.2458) Acc D Real: 82.249% 
Loss D Fake: 0.5972 (1.1278) Acc D Fake: 44.683% 
Loss D: 1.002 
Loss G: 0.8054 (0.5923) Acc G: 55.472% 
LR: 2.000e-04 

2023-03-02 01:51:42,509 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4422 (0.2474) Acc D Real: 82.001% 
Loss D Fake: 0.5925 (1.1236) Acc D Fake: 45.099% 
Loss D: 1.035 
Loss G: 0.8106 (0.5940) Acc G: 55.042% 
LR: 2.000e-04 

2023-03-02 01:51:42,517 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3291 (0.2480) Acc D Real: 81.887% 
Loss D Fake: 0.5884 (1.1195) Acc D Fake: 45.522% 
Loss D: 0.917 
Loss G: 0.8159 (0.5957) Acc G: 54.618% 
LR: 2.000e-04 

2023-03-02 01:51:42,524 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3310 (0.2486) Acc D Real: 81.778% 
Loss D Fake: 0.5837 (1.1154) Acc D Fake: 45.938% 
Loss D: 0.915 
Loss G: 0.8223 (0.5975) Acc G: 54.201% 
LR: 2.000e-04 

2023-03-02 01:51:42,532 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2730 (0.2488) Acc D Real: 81.716% 
Loss D Fake: 0.5783 (1.1113) Acc D Fake: 46.347% 
Loss D: 0.851 
Loss G: 0.8294 (0.5992) Acc G: 53.791% 
LR: 2.000e-04 

2023-03-02 01:51:42,539 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3766 (0.2498) Acc D Real: 81.559% 
Loss D Fake: 0.5727 (1.1073) Acc D Fake: 46.750% 
Loss D: 0.949 
Loss G: 0.8366 (0.6010) Acc G: 53.386% 
LR: 2.000e-04 

2023-03-02 01:51:42,547 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2798 (0.2500) Acc D Real: 81.497% 
Loss D Fake: 0.5670 (1.1033) Acc D Fake: 47.148% 
Loss D: 0.847 
Loss G: 0.8442 (0.6028) Acc G: 52.988% 
LR: 2.000e-04 

2023-03-02 01:51:42,555 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4228 (0.2513) Acc D Real: 81.304% 
Loss D Fake: 0.5613 (1.0992) Acc D Fake: 47.539% 
Loss D: 0.984 
Loss G: 0.8516 (0.6047) Acc G: 52.595% 
LR: 2.000e-04 

2023-03-02 01:51:42,563 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3233 (0.2518) Acc D Real: 81.206% 
Loss D Fake: 0.5558 (1.0952) Acc D Fake: 47.925% 
Loss D: 0.879 
Loss G: 0.8590 (0.6065) Acc G: 52.209% 
LR: 2.000e-04 

2023-03-02 01:51:42,570 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4140 (0.2530) Acc D Real: 81.022% 
Loss D Fake: 0.5505 (1.0913) Acc D Fake: 48.305% 
Loss D: 0.965 
Loss G: 0.8658 (0.6084) Acc G: 51.827% 
LR: 2.000e-04 

2023-03-02 01:51:42,578 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3984 (0.2540) Acc D Real: 80.855% 
Loss D Fake: 0.5459 (1.0873) Acc D Fake: 48.680% 
Loss D: 0.944 
Loss G: 0.8720 (0.6103) Acc G: 51.452% 
LR: 2.000e-04 

2023-03-02 01:51:42,585 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3364 (0.2546) Acc D Real: 80.747% 
Loss D Fake: 0.5417 (1.0834) Acc D Fake: 49.049% 
Loss D: 0.878 
Loss G: 0.8777 (0.6123) Acc G: 51.082% 
LR: 2.000e-04 

2023-03-02 01:51:42,593 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.2802 (0.2548) Acc D Real: 80.689% 
Loss D Fake: 0.5376 (1.0795) Acc D Fake: 49.413% 
Loss D: 0.818 
Loss G: 0.8836 (0.6142) Acc G: 50.717% 
LR: 2.000e-04 

2023-03-02 01:51:42,600 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4281 (0.2560) Acc D Real: 80.512% 
Loss D Fake: 0.5336 (1.0756) Acc D Fake: 49.772% 
Loss D: 0.962 
Loss G: 0.8891 (0.6161) Acc G: 50.357% 
LR: 2.000e-04 

2023-03-02 01:51:42,608 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2920 (0.2563) Acc D Real: 80.442% 
Loss D Fake: 0.5300 (1.0718) Acc D Fake: 50.125% 
Loss D: 0.822 
Loss G: 0.8944 (0.6181) Acc G: 50.003% 
LR: 2.000e-04 

2023-03-02 01:51:42,615 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2860 (0.2565) Acc D Real: 80.387% 
Loss D Fake: 0.5263 (1.0680) Acc D Fake: 50.474% 
Loss D: 0.812 
Loss G: 0.9000 (0.6201) Acc G: 49.653% 
LR: 2.000e-04 

2023-03-02 01:51:42,623 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3036 (0.2568) Acc D Real: 80.315% 
Loss D Fake: 0.5224 (1.0642) Acc D Fake: 50.818% 
Loss D: 0.826 
Loss G: 0.9059 (0.6221) Acc G: 49.308% 
LR: 2.000e-04 

2023-03-02 01:51:42,631 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3947 (0.2578) Acc D Real: 80.175% 
Loss D Fake: 0.5187 (1.0604) Acc D Fake: 51.157% 
Loss D: 0.913 
Loss G: 0.9109 (0.6240) Acc G: 48.968% 
LR: 2.000e-04 

2023-03-02 01:51:42,639 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4129 (0.2588) Acc D Real: 80.021% 
Loss D Fake: 0.5157 (1.0567) Acc D Fake: 51.492% 
Loss D: 0.929 
Loss G: 0.9151 (0.6260) Acc G: 48.633% 
LR: 2.000e-04 

2023-03-02 01:51:42,647 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3468 (0.2594) Acc D Real: 79.918% 
Loss D Fake: 0.5132 (1.0530) Acc D Fake: 51.822% 
Loss D: 0.860 
Loss G: 0.9188 (0.6280) Acc G: 48.302% 
LR: 2.000e-04 

2023-03-02 01:51:42,657 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3449 (0.2600) Acc D Real: 79.819% 
Loss D Fake: 0.5109 (1.0493) Acc D Fake: 52.147% 
Loss D: 0.856 
Loss G: 0.9224 (0.6300) Acc G: 47.975% 
LR: 2.000e-04 

2023-03-02 01:51:42,664 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5126 (0.2617) Acc D Real: 79.598% 
Loss D Fake: 0.5088 (1.0457) Acc D Fake: 52.469% 
Loss D: 1.021 
Loss G: 0.9250 (0.6320) Acc G: 47.653% 
LR: 2.000e-04 

2023-03-02 01:51:42,672 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3372 (0.2622) Acc D Real: 79.518% 
Loss D Fake: 0.5072 (1.0421) Acc D Fake: 52.785% 
Loss D: 0.844 
Loss G: 0.9279 (0.6340) Acc G: 47.336% 
LR: 2.000e-04 

2023-03-02 01:51:42,679 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3631 (0.2629) Acc D Real: 79.418% 
Loss D Fake: 0.5053 (1.0386) Acc D Fake: 53.098% 
Loss D: 0.868 
Loss G: 0.9309 (0.6359) Acc G: 47.022% 
LR: 2.000e-04 

2023-03-02 01:51:42,687 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3715 (0.2636) Acc D Real: 79.311% 
Loss D Fake: 0.5034 (1.0350) Acc D Fake: 53.407% 
Loss D: 0.875 
Loss G: 0.9339 (0.6379) Acc G: 46.713% 
LR: 2.000e-04 

2023-03-02 01:51:42,694 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4474 (0.2648) Acc D Real: 79.151% 
Loss D Fake: 0.5017 (1.0315) Acc D Fake: 53.711% 
Loss D: 0.949 
Loss G: 0.9360 (0.6398) Acc G: 46.408% 
LR: 2.000e-04 

2023-03-02 01:51:42,702 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3771 (0.2655) Acc D Real: 79.044% 
Loss D Fake: 0.5006 (1.0281) Acc D Fake: 54.012% 
Loss D: 0.878 
Loss G: 0.9379 (0.6418) Acc G: 46.106% 
LR: 2.000e-04 

2023-03-02 01:51:42,709 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4584 (0.2668) Acc D Real: 78.887% 
Loss D Fake: 0.4995 (1.0247) Acc D Fake: 54.308% 
Loss D: 0.958 
Loss G: 0.9392 (0.6437) Acc G: 45.809% 
LR: 2.000e-04 

2023-03-02 01:51:42,717 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2972 (0.2670) Acc D Real: 78.840% 
Loss D Fake: 0.4988 (1.0213) Acc D Fake: 54.601% 
Loss D: 0.796 
Loss G: 0.9407 (0.6456) Acc G: 45.515% 
LR: 2.000e-04 

2023-03-02 01:51:42,724 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2013 (0.2666) Acc D Real: 78.862% 
Loss D Fake: 0.4975 (1.0180) Acc D Fake: 54.891% 
Loss D: 0.699 
Loss G: 0.9434 (0.6475) Acc G: 45.225% 
LR: 2.000e-04 

2023-03-02 01:51:42,732 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2441 (0.2664) Acc D Real: 78.862% 
Loss D Fake: 0.4954 (1.0147) Acc D Fake: 54.917% 
Loss D: 0.740 
Loss G: 0.9477 (0.6494) Acc G: 45.198% 
LR: 2.000e-04 

2023-03-02 01:51:42,956 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.550 | Generator Loss: 0.948 | Avg: 1.497 
2023-03-02 01:51:42,977 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.577 | Generator Loss: 0.948 | Avg: 1.524 
2023-03-02 01:51:43,000 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.571 | Generator Loss: 0.948 | Avg: 1.518 
2023-03-02 01:51:43,027 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.563 | Generator Loss: 0.948 | Avg: 1.511 
2023-03-02 01:51:43,054 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.562 | Generator Loss: 0.948 | Avg: 1.510 
2023-03-02 01:51:43,081 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.641 | Generator Loss: 0.948 | Avg: 1.589 
2023-03-02 01:51:43,108 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.639 | Generator Loss: 0.948 | Avg: 1.587 
2023-03-02 01:51:43,135 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.689 | Generator Loss: 0.948 | Avg: 1.637 
2023-03-02 01:51:43,162 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.700 | Generator Loss: 0.948 | Avg: 1.648 
2023-03-02 01:51:43,189 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.761 | Generator Loss: 0.948 | Avg: 1.709 
2023-03-02 01:51:43,218 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.804 | Generator Loss: 0.948 | Avg: 1.752 
2023-03-02 01:51:43,248 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.857 | Generator Loss: 0.948 | Avg: 1.805 
2023-03-02 01:51:43,275 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.884 | Generator Loss: 0.948 | Avg: 1.831 
2023-03-02 01:51:43,301 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.888 | Generator Loss: 0.948 | Avg: 1.836 
2023-03-02 01:51:43,327 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.868 | Generator Loss: 0.948 | Avg: 1.816 
2023-03-02 01:51:43,353 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.849 | Generator Loss: 0.948 | Avg: 1.797 
2023-03-02 01:51:43,379 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.833 | Generator Loss: 0.948 | Avg: 1.781 
2023-03-02 01:51:43,415 -                train: [    INFO] - 
Epoch: 4/20
2023-03-02 01:51:43,616 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3656 (0.2891) Acc D Real: 72.109% 
Loss D Fake: 0.4892 (0.4908) Acc D Fake: 100.000% 
Loss D: 0.855 
Loss G: 0.9580 (0.9555) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,625 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3455 (0.3079) Acc D Real: 70.347% 
Loss D Fake: 0.4864 (0.4894) Acc D Fake: 100.000% 
Loss D: 0.832 
Loss G: 0.9623 (0.9578) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,633 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3183 (0.3105) Acc D Real: 70.286% 
Loss D Fake: 0.4840 (0.4880) Acc D Fake: 100.000% 
Loss D: 0.802 
Loss G: 0.9663 (0.9599) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,650 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3827 (0.3249) Acc D Real: 68.781% 
Loss D Fake: 0.4817 (0.4868) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9703 (0.9620) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,657 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3079 (0.3221) Acc D Real: 69.123% 
Loss D Fake: 0.4793 (0.4855) Acc D Fake: 100.000% 
Loss D: 0.787 
Loss G: 0.9747 (0.9641) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,665 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2624 (0.3135) Acc D Real: 70.112% 
Loss D Fake: 0.4767 (0.4843) Acc D Fake: 100.000% 
Loss D: 0.739 
Loss G: 0.9794 (0.9663) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,672 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2829 (0.3097) Acc D Real: 70.612% 
Loss D Fake: 0.4740 (0.4830) Acc D Fake: 100.000% 
Loss D: 0.757 
Loss G: 0.9840 (0.9685) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,679 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4135 (0.3212) Acc D Real: 69.433% 
Loss D Fake: 0.4716 (0.4817) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 0.9878 (0.9706) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,686 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3646 (0.3256) Acc D Real: 69.052% 
Loss D Fake: 0.4697 (0.4805) Acc D Fake: 100.000% 
Loss D: 0.834 
Loss G: 0.9907 (0.9726) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,693 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3727 (0.3298) Acc D Real: 68.670% 
Loss D Fake: 0.4683 (0.4794) Acc D Fake: 100.000% 
Loss D: 0.841 
Loss G: 0.9931 (0.9745) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,700 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4063 (0.3362) Acc D Real: 68.077% 
Loss D Fake: 0.4672 (0.4784) Acc D Fake: 100.000% 
Loss D: 0.874 
Loss G: 0.9945 (0.9762) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,707 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3556 (0.3377) Acc D Real: 68.005% 
Loss D Fake: 0.4665 (0.4775) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 0.9957 (0.9777) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,715 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2830 (0.3338) Acc D Real: 68.408% 
Loss D Fake: 0.4658 (0.4766) Acc D Fake: 100.000% 
Loss D: 0.749 
Loss G: 0.9976 (0.9791) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,722 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4211 (0.3396) Acc D Real: 67.833% 
Loss D Fake: 0.4647 (0.4759) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9990 (0.9804) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,729 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4171 (0.3445) Acc D Real: 67.370% 
Loss D Fake: 0.4641 (0.4751) Acc D Fake: 100.000% 
Loss D: 0.881 
Loss G: 1.0000 (0.9816) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,736 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2798 (0.3407) Acc D Real: 67.788% 
Loss D Fake: 0.4636 (0.4744) Acc D Fake: 100.000% 
Loss D: 0.743 
Loss G: 1.0014 (0.9828) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,743 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3462 (0.3410) Acc D Real: 67.818% 
Loss D Fake: 0.4628 (0.4738) Acc D Fake: 100.000% 
Loss D: 0.809 
Loss G: 1.0026 (0.9839) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,750 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3848 (0.3433) Acc D Real: 67.632% 
Loss D Fake: 0.4623 (0.4732) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 1.0034 (0.9849) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,757 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3786 (0.3450) Acc D Real: 67.479% 
Loss D Fake: 0.4619 (0.4726) Acc D Fake: 100.000% 
Loss D: 0.840 
Loss G: 1.0040 (0.9859) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,764 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4384 (0.3495) Acc D Real: 67.029% 
Loss D Fake: 0.4617 (0.4721) Acc D Fake: 100.000% 
Loss D: 0.900 
Loss G: 1.0043 (0.9868) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,771 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2331 (0.3442) Acc D Real: 67.562% 
Loss D Fake: 0.4614 (0.4716) Acc D Fake: 100.000% 
Loss D: 0.694 
Loss G: 1.0054 (0.9876) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,778 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.2806 (0.3414) Acc D Real: 67.844% 
Loss D Fake: 0.4605 (0.4711) Acc D Fake: 100.000% 
Loss D: 0.741 
Loss G: 1.0072 (0.9885) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,786 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4786 (0.3471) Acc D Real: 67.285% 
Loss D Fake: 0.4597 (0.4707) Acc D Fake: 100.000% 
Loss D: 0.938 
Loss G: 1.0080 (0.9893) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,793 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4980 (0.3532) Acc D Real: 66.652% 
Loss D Fake: 0.4597 (0.4702) Acc D Fake: 100.000% 
Loss D: 0.958 
Loss G: 1.0076 (0.9900) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,800 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3932 (0.3547) Acc D Real: 66.522% 
Loss D Fake: 0.4600 (0.4698) Acc D Fake: 100.000% 
Loss D: 0.853 
Loss G: 1.0067 (0.9907) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,807 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4008 (0.3564) Acc D Real: 66.333% 
Loss D Fake: 0.4605 (0.4695) Acc D Fake: 100.000% 
Loss D: 0.861 
Loss G: 1.0063 (0.9912) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,814 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3970 (0.3579) Acc D Real: 66.192% 
Loss D Fake: 0.4606 (0.4692) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0060 (0.9918) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,822 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2879 (0.3555) Acc D Real: 66.467% 
Loss D Fake: 0.4607 (0.4689) Acc D Fake: 100.000% 
Loss D: 0.749 
Loss G: 1.0064 (0.9923) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,830 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3110 (0.3540) Acc D Real: 66.660% 
Loss D Fake: 0.4604 (0.4686) Acc D Fake: 100.000% 
Loss D: 0.771 
Loss G: 1.0070 (0.9928) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,838 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2885 (0.3519) Acc D Real: 66.897% 
Loss D Fake: 0.4600 (0.4683) Acc D Fake: 100.000% 
Loss D: 0.748 
Loss G: 1.0083 (0.9933) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,846 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3976 (0.3533) Acc D Real: 66.779% 
Loss D Fake: 0.4593 (0.4680) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0092 (0.9938) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,853 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2637 (0.3506) Acc D Real: 67.069% 
Loss D Fake: 0.4588 (0.4677) Acc D Fake: 100.000% 
Loss D: 0.722 
Loss G: 1.0104 (0.9943) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,861 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3633 (0.3510) Acc D Real: 67.016% 
Loss D Fake: 0.4580 (0.4675) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 1.0120 (0.9948) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,869 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3514 (0.3510) Acc D Real: 67.040% 
Loss D Fake: 0.4572 (0.4672) Acc D Fake: 100.000% 
Loss D: 0.809 
Loss G: 1.0133 (0.9953) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,877 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2504 (0.3482) Acc D Real: 67.338% 
Loss D Fake: 0.4565 (0.4669) Acc D Fake: 100.000% 
Loss D: 0.707 
Loss G: 1.0149 (0.9959) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,885 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3108 (0.3472) Acc D Real: 67.459% 
Loss D Fake: 0.4555 (0.4666) Acc D Fake: 100.000% 
Loss D: 0.766 
Loss G: 1.0168 (0.9964) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,893 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3262 (0.3466) Acc D Real: 67.537% 
Loss D Fake: 0.4546 (0.4662) Acc D Fake: 100.000% 
Loss D: 0.781 
Loss G: 1.0185 (0.9970) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,901 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3954 (0.3479) Acc D Real: 67.425% 
Loss D Fake: 0.4538 (0.4659) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 1.0196 (0.9976) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,909 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.2417 (0.3452) Acc D Real: 67.702% 
Loss D Fake: 0.4532 (0.4656) Acc D Fake: 100.000% 
Loss D: 0.695 
Loss G: 1.0211 (0.9982) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,917 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4709 (0.3483) Acc D Real: 67.384% 
Loss D Fake: 0.4525 (0.4653) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 1.0220 (0.9987) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,925 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2241 (0.3453) Acc D Real: 67.707% 
Loss D Fake: 0.4520 (0.4650) Acc D Fake: 100.000% 
Loss D: 0.676 
Loss G: 1.0234 (0.9993) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,932 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2642 (0.3434) Acc D Real: 67.902% 
Loss D Fake: 0.4511 (0.4646) Acc D Fake: 100.000% 
Loss D: 0.715 
Loss G: 1.0257 (0.9999) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,940 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3473 (0.3435) Acc D Real: 67.920% 
Loss D Fake: 0.4499 (0.4643) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 1.0275 (1.0006) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,947 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4106 (0.3450) Acc D Real: 67.786% 
Loss D Fake: 0.4492 (0.4640) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 1.0284 (1.0012) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,955 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3899 (0.3460) Acc D Real: 67.714% 
Loss D Fake: 0.4489 (0.4637) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 1.0287 (1.0018) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,962 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3195 (0.3454) Acc D Real: 67.765% 
Loss D Fake: 0.4487 (0.4633) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 1.0297 (1.0024) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,970 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3172 (0.3448) Acc D Real: 67.841% 
Loss D Fake: 0.4481 (0.4630) Acc D Fake: 100.000% 
Loss D: 0.765 
Loss G: 1.0311 (1.0030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,977 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3064 (0.3441) Acc D Real: 67.946% 
Loss D Fake: 0.4474 (0.4627) Acc D Fake: 100.000% 
Loss D: 0.754 
Loss G: 1.0323 (1.0036) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,985 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4224 (0.3456) Acc D Real: 67.775% 
Loss D Fake: 0.4468 (0.4624) Acc D Fake: 100.000% 
Loss D: 0.869 
Loss G: 1.0335 (1.0042) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:43,992 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2790 (0.3443) Acc D Real: 67.932% 
Loss D Fake: 0.4462 (0.4621) Acc D Fake: 100.000% 
Loss D: 0.725 
Loss G: 1.0348 (1.0048) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,000 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3211 (0.3439) Acc D Real: 67.988% 
Loss D Fake: 0.4455 (0.4617) Acc D Fake: 100.000% 
Loss D: 0.767 
Loss G: 1.0363 (1.0054) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,008 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4240 (0.3454) Acc D Real: 67.852% 
Loss D Fake: 0.4449 (0.4614) Acc D Fake: 100.000% 
Loss D: 0.869 
Loss G: 1.0371 (1.0060) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,015 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3211 (0.3449) Acc D Real: 67.912% 
Loss D Fake: 0.4445 (0.4611) Acc D Fake: 100.000% 
Loss D: 0.766 
Loss G: 1.0378 (1.0066) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,023 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3524 (0.3451) Acc D Real: 67.920% 
Loss D Fake: 0.4443 (0.4608) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 1.0382 (1.0071) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,030 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3133 (0.3445) Acc D Real: 67.999% 
Loss D Fake: 0.4442 (0.4605) Acc D Fake: 100.000% 
Loss D: 0.757 
Loss G: 1.0383 (1.0077) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,038 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3141 (0.3440) Acc D Real: 68.058% 
Loss D Fake: 0.4441 (0.4602) Acc D Fake: 100.000% 
Loss D: 0.758 
Loss G: 1.0387 (1.0082) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,045 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3645 (0.3443) Acc D Real: 68.038% 
Loss D Fake: 0.4438 (0.4599) Acc D Fake: 100.000% 
Loss D: 0.808 
Loss G: 1.0390 (1.0088) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,053 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3258 (0.3440) Acc D Real: 68.071% 
Loss D Fake: 0.4437 (0.4597) Acc D Fake: 100.000% 
Loss D: 0.769 
Loss G: 1.0397 (1.0093) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,060 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2196 (0.3419) Acc D Real: 68.312% 
Loss D Fake: 0.4433 (0.4594) Acc D Fake: 100.000% 
Loss D: 0.663 
Loss G: 1.0406 (1.0098) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,068 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2428 (0.3403) Acc D Real: 68.490% 
Loss D Fake: 0.4427 (0.4591) Acc D Fake: 100.000% 
Loss D: 0.685 
Loss G: 1.0419 (1.0103) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,075 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2707 (0.3392) Acc D Real: 68.625% 
Loss D Fake: 0.4421 (0.4588) Acc D Fake: 100.000% 
Loss D: 0.713 
Loss G: 1.0433 (1.0109) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,083 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3436 (0.3393) Acc D Real: 68.618% 
Loss D Fake: 0.4414 (0.4586) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0445 (1.0114) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,090 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2252 (0.3375) Acc D Real: 68.820% 
Loss D Fake: 0.4408 (0.4583) Acc D Fake: 100.000% 
Loss D: 0.666 
Loss G: 1.0459 (1.0120) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,098 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2742 (0.3365) Acc D Real: 68.922% 
Loss D Fake: 0.4401 (0.4580) Acc D Fake: 100.000% 
Loss D: 0.714 
Loss G: 1.0473 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,105 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3918 (0.3373) Acc D Real: 68.854% 
Loss D Fake: 0.4398 (0.4577) Acc D Fake: 100.000% 
Loss D: 0.832 
Loss G: 1.0466 (1.0130) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,113 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.1998 (0.3353) Acc D Real: 69.083% 
Loss D Fake: 0.4407 (0.4575) Acc D Fake: 100.000% 
Loss D: 0.641 
Loss G: 1.0444 (1.0135) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,121 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3382 (0.3353) Acc D Real: 69.107% 
Loss D Fake: 0.4420 (0.4572) Acc D Fake: 100.000% 
Loss D: 0.780 
Loss G: 1.0416 (1.0139) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,128 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3543 (0.3356) Acc D Real: 69.101% 
Loss D Fake: 0.4436 (0.4571) Acc D Fake: 100.000% 
Loss D: 0.798 
Loss G: 1.0385 (1.0143) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,136 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3306 (0.3355) Acc D Real: 69.132% 
Loss D Fake: 0.4451 (0.4569) Acc D Fake: 100.000% 
Loss D: 0.776 
Loss G: 1.0356 (1.0146) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,143 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3494 (0.3357) Acc D Real: 69.135% 
Loss D Fake: 0.4465 (0.4567) Acc D Fake: 100.000% 
Loss D: 0.796 
Loss G: 1.0331 (1.0148) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,151 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3644 (0.3361) Acc D Real: 69.105% 
Loss D Fake: 0.4479 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.812 
Loss G: 1.0304 (1.0150) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,158 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4088 (0.3371) Acc D Real: 69.032% 
Loss D Fake: 0.4493 (0.4565) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0276 (1.0152) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,166 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2593 (0.3361) Acc D Real: 69.153% 
Loss D Fake: 0.4505 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.710 
Loss G: 1.0260 (1.0154) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,173 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4338 (0.3374) Acc D Real: 69.043% 
Loss D Fake: 0.4512 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 1.0246 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,181 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2642 (0.3364) Acc D Real: 69.137% 
Loss D Fake: 0.4519 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.716 
Loss G: 1.0237 (1.0156) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,188 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2994 (0.3359) Acc D Real: 69.182% 
Loss D Fake: 0.4523 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.752 
Loss G: 1.0234 (1.0157) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,196 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3274 (0.3358) Acc D Real: 69.199% 
Loss D Fake: 0.4524 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.780 
Loss G: 1.0234 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,203 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3695 (0.3362) Acc D Real: 69.176% 
Loss D Fake: 0.4525 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 1.0231 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,211 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3739 (0.3367) Acc D Real: 69.153% 
Loss D Fake: 0.4528 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.827 
Loss G: 1.0224 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,218 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3983 (0.3375) Acc D Real: 69.095% 
Loss D Fake: 0.4534 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.852 
Loss G: 1.0210 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,226 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2854 (0.3368) Acc D Real: 69.160% 
Loss D Fake: 0.4542 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 1.0196 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,233 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3027 (0.3364) Acc D Real: 69.207% 
Loss D Fake: 0.4549 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.758 
Loss G: 1.0183 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,241 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2945 (0.3359) Acc D Real: 69.273% 
Loss D Fake: 0.4556 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 1.0170 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,248 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3216 (0.3358) Acc D Real: 69.292% 
Loss D Fake: 0.4563 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0159 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,256 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.2693 (0.3350) Acc D Real: 69.379% 
Loss D Fake: 0.4569 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.726 
Loss G: 1.0150 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,264 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3206 (0.3348) Acc D Real: 69.410% 
Loss D Fake: 0.4573 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0140 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,272 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.2638 (0.3340) Acc D Real: 69.490% 
Loss D Fake: 0.4579 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.722 
Loss G: 1.0131 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,279 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3123 (0.3338) Acc D Real: 69.522% 
Loss D Fake: 0.4584 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.771 
Loss G: 1.0123 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,287 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3195 (0.3336) Acc D Real: 69.542% 
Loss D Fake: 0.4588 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0116 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,294 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2860 (0.3331) Acc D Real: 69.599% 
Loss D Fake: 0.4592 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.745 
Loss G: 1.0109 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,302 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3253 (0.3330) Acc D Real: 69.614% 
Loss D Fake: 0.4597 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0100 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,309 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3900 (0.3336) Acc D Real: 69.572% 
Loss D Fake: 0.4603 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 1.0084 (1.0157) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,317 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.1714 (0.3319) Acc D Real: 69.753% 
Loss D Fake: 0.4612 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.633 
Loss G: 1.0073 (1.0156) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,324 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2911 (0.3315) Acc D Real: 69.794% 
Loss D Fake: 0.4617 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.753 
Loss G: 1.0063 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,332 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3097 (0.3312) Acc D Real: 69.817% 
Loss D Fake: 0.4622 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.772 
Loss G: 1.0054 (1.0154) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,340 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3718 (0.3317) Acc D Real: 69.777% 
Loss D Fake: 0.4628 (0.4565) Acc D Fake: 100.000% 
Loss D: 0.835 
Loss G: 1.0041 (1.0153) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,347 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3729 (0.3321) Acc D Real: 69.734% 
Loss D Fake: 0.4636 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 1.0025 (1.0152) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,355 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2906 (0.3317) Acc D Real: 69.777% 
Loss D Fake: 0.4645 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.755 
Loss G: 1.0007 (1.0150) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,362 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3406 (0.3317) Acc D Real: 69.773% 
Loss D Fake: 0.4655 (0.4567) Acc D Fake: 100.000% 
Loss D: 0.806 
Loss G: 0.9988 (1.0149) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,369 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3178 (0.3316) Acc D Real: 69.795% 
Loss D Fake: 0.4671 (0.4568) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 0.9937 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,377 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3202 (0.3315) Acc D Real: 69.803% 
Loss D Fake: 0.4703 (0.4570) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9877 (1.0144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,384 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2556 (0.3308) Acc D Real: 69.876% 
Loss D Fake: 0.4734 (0.4571) Acc D Fake: 100.000% 
Loss D: 0.729 
Loss G: 0.9818 (1.0141) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,392 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3667 (0.3311) Acc D Real: 69.847% 
Loss D Fake: 0.4764 (0.4573) Acc D Fake: 100.000% 
Loss D: 0.843 
Loss G: 0.9765 (1.0137) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,399 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3281 (0.3311) Acc D Real: 69.849% 
Loss D Fake: 0.4791 (0.4575) Acc D Fake: 100.000% 
Loss D: 0.807 
Loss G: 0.9711 (1.0133) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,407 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3019 (0.3308) Acc D Real: 69.890% 
Loss D Fake: 0.4819 (0.4578) Acc D Fake: 100.000% 
Loss D: 0.784 
Loss G: 0.9674 (1.0129) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,415 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3187 (0.3307) Acc D Real: 69.898% 
Loss D Fake: 0.4812 (0.4580) Acc D Fake: 100.000% 
Loss D: 0.800 
Loss G: 0.9737 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,422 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3413 (0.3308) Acc D Real: 69.888% 
Loss D Fake: 0.4770 (0.4581) Acc D Fake: 100.000% 
Loss D: 0.818 
Loss G: 0.9784 (1.0122) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,429 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2777 (0.3303) Acc D Real: 69.946% 
Loss D Fake: 0.4757 (0.4583) Acc D Fake: 100.000% 
Loss D: 0.753 
Loss G: 0.9794 (1.0119) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,437 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2639 (0.3297) Acc D Real: 70.003% 
Loss D Fake: 0.4755 (0.4585) Acc D Fake: 100.000% 
Loss D: 0.739 
Loss G: 0.9795 (1.0116) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,444 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2574 (0.3290) Acc D Real: 70.071% 
Loss D Fake: 0.4756 (0.4586) Acc D Fake: 100.000% 
Loss D: 0.733 
Loss G: 0.9793 (1.0113) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,452 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2538 (0.3284) Acc D Real: 70.143% 
Loss D Fake: 0.4757 (0.4588) Acc D Fake: 100.000% 
Loss D: 0.730 
Loss G: 0.9790 (1.0110) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,460 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1451 (0.3267) Acc D Real: 70.309% 
Loss D Fake: 0.4759 (0.4589) Acc D Fake: 100.000% 
Loss D: 0.621 
Loss G: 0.9789 (1.0108) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,468 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3450 (0.3269) Acc D Real: 70.292% 
Loss D Fake: 0.4760 (0.4591) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 0.9787 (1.0105) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,476 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3454 (0.3271) Acc D Real: 70.274% 
Loss D Fake: 0.4762 (0.4592) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 0.9784 (1.0102) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,483 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3069 (0.3269) Acc D Real: 70.280% 
Loss D Fake: 0.4765 (0.4594) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9775 (1.0099) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,491 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2416 (0.3262) Acc D Real: 70.351% 
Loss D Fake: 0.4771 (0.4595) Acc D Fake: 100.000% 
Loss D: 0.719 
Loss G: 0.9767 (1.0096) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,499 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1904 (0.3250) Acc D Real: 70.467% 
Loss D Fake: 0.4776 (0.4597) Acc D Fake: 100.000% 
Loss D: 0.668 
Loss G: 0.9761 (1.0093) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,507 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3718 (0.3254) Acc D Real: 70.418% 
Loss D Fake: 0.4780 (0.4598) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 0.9755 (1.0091) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,514 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2591 (0.3249) Acc D Real: 70.468% 
Loss D Fake: 0.4784 (0.4600) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9747 (1.0088) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,522 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2592 (0.3243) Acc D Real: 70.517% 
Loss D Fake: 0.4789 (0.4601) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9740 (1.0085) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,529 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4088 (0.3250) Acc D Real: 70.429% 
Loss D Fake: 0.4794 (0.4603) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 0.9732 (1.0082) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,537 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.2083 (0.3241) Acc D Real: 70.517% 
Loss D Fake: 0.4799 (0.4605) Acc D Fake: 100.000% 
Loss D: 0.688 
Loss G: 0.9725 (1.0079) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,544 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3026 (0.3239) Acc D Real: 70.528% 
Loss D Fake: 0.4803 (0.4606) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9718 (1.0076) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,552 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2727 (0.3235) Acc D Real: 70.560% 
Loss D Fake: 0.4809 (0.4608) Acc D Fake: 100.000% 
Loss D: 0.754 
Loss G: 0.9710 (1.0073) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,560 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3153 (0.3234) Acc D Real: 70.556% 
Loss D Fake: 0.4814 (0.4609) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 0.9700 (1.0070) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,568 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2311 (0.3227) Acc D Real: 70.620% 
Loss D Fake: 0.4821 (0.4611) Acc D Fake: 100.000% 
Loss D: 0.713 
Loss G: 0.9689 (1.0067) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,575 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1713 (0.3215) Acc D Real: 70.741% 
Loss D Fake: 0.4829 (0.4613) Acc D Fake: 100.000% 
Loss D: 0.654 
Loss G: 0.9672 (1.0064) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,583 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2178 (0.3207) Acc D Real: 70.817% 
Loss D Fake: 0.4841 (0.4615) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9655 (1.0061) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,591 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2648 (0.3203) Acc D Real: 70.850% 
Loss D Fake: 0.4851 (0.4616) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 0.9639 (1.0058) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,598 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2535 (0.3198) Acc D Real: 70.892% 
Loss D Fake: 0.4861 (0.4618) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 0.9625 (1.0054) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,606 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2151 (0.3190) Acc D Real: 70.960% 
Loss D Fake: 0.4870 (0.4620) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9612 (1.0051) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,613 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3021 (0.3188) Acc D Real: 70.958% 
Loss D Fake: 0.4879 (0.4622) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9600 (1.0048) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,621 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2721 (0.3185) Acc D Real: 70.981% 
Loss D Fake: 0.4887 (0.4624) Acc D Fake: 100.000% 
Loss D: 0.761 
Loss G: 0.9588 (1.0044) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,628 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.2150 (0.3177) Acc D Real: 71.046% 
Loss D Fake: 0.4896 (0.4626) Acc D Fake: 100.000% 
Loss D: 0.705 
Loss G: 0.9572 (1.0041) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,636 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2134 (0.3170) Acc D Real: 71.115% 
Loss D Fake: 0.4907 (0.4628) Acc D Fake: 100.000% 
Loss D: 0.704 
Loss G: 0.9556 (1.0037) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,644 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2225 (0.3163) Acc D Real: 71.180% 
Loss D Fake: 0.4919 (0.4630) Acc D Fake: 100.000% 
Loss D: 0.714 
Loss G: 0.9536 (1.0034) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,651 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2265 (0.3156) Acc D Real: 71.235% 
Loss D Fake: 0.4935 (0.4632) Acc D Fake: 100.000% 
Loss D: 0.720 
Loss G: 0.9508 (1.0030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,658 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.2464 (0.3151) Acc D Real: 71.269% 
Loss D Fake: 0.4956 (0.4635) Acc D Fake: 100.000% 
Loss D: 0.742 
Loss G: 0.9478 (1.0026) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,666 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1863 (0.3142) Acc D Real: 71.353% 
Loss D Fake: 0.4976 (0.4637) Acc D Fake: 100.000% 
Loss D: 0.684 
Loss G: 0.9450 (1.0022) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,673 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.2022 (0.3134) Acc D Real: 71.428% 
Loss D Fake: 0.4998 (0.4640) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9414 (1.0017) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,681 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2093 (0.3127) Acc D Real: 71.486% 
Loss D Fake: 0.5026 (0.4643) Acc D Fake: 100.000% 
Loss D: 0.712 
Loss G: 0.9373 (1.0013) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,688 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2614 (0.3123) Acc D Real: 71.509% 
Loss D Fake: 0.5063 (0.4645) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 0.9317 (1.0008) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,696 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2776 (0.3121) Acc D Real: 71.519% 
Loss D Fake: 0.5121 (0.4649) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9233 (1.0003) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:44,703 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2054 (0.3113) Acc D Real: 71.584% 
Loss D Fake: 0.5224 (0.4653) Acc D Fake: 99.989% 
Loss D: 0.728 
Loss G: 0.9107 (0.9996) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,711 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2627 (0.3110) Acc D Real: 71.596% 
Loss D Fake: 1.0696 (0.4694) Acc D Fake: 99.844% 
Loss D: 1.332 
Loss G: 0.9662 (0.9994) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,718 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.2443 (0.3105) Acc D Real: 71.626% 
Loss D Fake: 0.4747 (0.4695) Acc D Fake: 99.845% 
Loss D: 0.719 
Loss G: 0.9954 (0.9994) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,726 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2052 (0.3098) Acc D Real: 71.688% 
Loss D Fake: 0.4657 (0.4694) Acc D Fake: 99.846% 
Loss D: 0.671 
Loss G: 0.9861 (0.9993) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,733 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.1906 (0.3090) Acc D Real: 71.762% 
Loss D Fake: 0.4950 (0.4696) Acc D Fake: 99.847% 
Loss D: 0.686 
Loss G: 0.9325 (0.9988) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,741 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2897 (0.3089) Acc D Real: 71.763% 
Loss D Fake: 0.5154 (0.4699) Acc D Fake: 99.848% 
Loss D: 0.805 
Loss G: 0.9119 (0.9983) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,748 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3390 (0.3091) Acc D Real: 71.720% 
Loss D Fake: 0.5254 (0.4703) Acc D Fake: 99.849% 
Loss D: 0.864 
Loss G: 0.9002 (0.9976) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,755 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2372 (0.3086) Acc D Real: 71.764% 
Loss D Fake: 0.5313 (0.4707) Acc D Fake: 99.850% 
Loss D: 0.769 
Loss G: 0.8935 (0.9969) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,763 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3782 (0.3091) Acc D Real: 71.679% 
Loss D Fake: 0.5349 (0.4711) Acc D Fake: 99.851% 
Loss D: 0.913 
Loss G: 0.8891 (0.9962) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,770 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3868 (0.3096) Acc D Real: 71.599% 
Loss D Fake: 0.5374 (0.4715) Acc D Fake: 99.852% 
Loss D: 0.924 
Loss G: 0.8860 (0.9955) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,779 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2620 (0.3093) Acc D Real: 71.624% 
Loss D Fake: 0.5391 (0.4720) Acc D Fake: 99.853% 
Loss D: 0.801 
Loss G: 0.8844 (0.9948) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,786 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4110 (0.3099) Acc D Real: 71.531% 
Loss D Fake: 0.5398 (0.4724) Acc D Fake: 99.854% 
Loss D: 0.951 
Loss G: 0.8834 (0.9941) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,794 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4761 (0.3110) Acc D Real: 71.407% 
Loss D Fake: 0.5404 (0.4728) Acc D Fake: 99.855% 
Loss D: 1.016 
Loss G: 0.8828 (0.9934) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:44,801 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4679 (0.3120) Acc D Real: 71.393% 
Loss D Fake: 0.5408 (0.4733) Acc D Fake: 99.855% 
Loss D: 1.009 
Loss G: 0.8820 (0.9927) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 01:51:45,007 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.595 | Generator Loss: 0.882 | Avg: 1.477 
2023-03-02 01:51:45,030 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.653 | Generator Loss: 0.882 | Avg: 1.535 
2023-03-02 01:51:45,054 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.638 | Generator Loss: 0.882 | Avg: 1.520 
2023-03-02 01:51:45,081 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.624 | Generator Loss: 0.882 | Avg: 1.506 
2023-03-02 01:51:45,108 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.621 | Generator Loss: 0.882 | Avg: 1.503 
2023-03-02 01:51:45,135 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.691 | Generator Loss: 0.882 | Avg: 1.573 
2023-03-02 01:51:45,161 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.697 | Generator Loss: 0.882 | Avg: 1.579 
2023-03-02 01:51:45,188 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.754 | Generator Loss: 0.882 | Avg: 1.636 
2023-03-02 01:51:45,215 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.774 | Generator Loss: 0.882 | Avg: 1.656 
2023-03-02 01:51:45,242 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.839 | Generator Loss: 0.882 | Avg: 1.720 
2023-03-02 01:51:45,269 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.879 | Generator Loss: 0.882 | Avg: 1.761 
2023-03-02 01:51:45,295 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.921 | Generator Loss: 0.882 | Avg: 1.802 
2023-03-02 01:51:45,322 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.957 | Generator Loss: 0.882 | Avg: 1.838 
2023-03-02 01:51:45,349 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.966 | Generator Loss: 0.882 | Avg: 1.848 
2023-03-02 01:51:45,376 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.944 | Generator Loss: 0.882 | Avg: 1.826 
2023-03-02 01:51:45,402 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.925 | Generator Loss: 0.882 | Avg: 1.807 
2023-03-02 01:51:45,428 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.907 | Generator Loss: 0.882 | Avg: 1.789 
2023-03-02 01:51:45,462 -                train: [    INFO] - 
Epoch: 5/20
2023-03-02 01:51:45,663 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4209 (0.4733) Acc D Real: 50.417% 
Loss D Fake: 0.5422 (0.5418) Acc D Fake: 100.000% 
Loss D: 0.963 
Loss G: 0.8796 (0.8803) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,671 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5121 (0.4862) Acc D Real: 49.306% 
Loss D Fake: 0.5432 (0.5423) Acc D Fake: 100.000% 
Loss D: 1.055 
Loss G: 0.8783 (0.8796) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,679 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2773 (0.4340) Acc D Real: 55.404% 
Loss D Fake: 0.5438 (0.5427) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 0.8779 (0.8792) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,696 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3461 (0.4164) Acc D Real: 57.250% 
Loss D Fake: 0.5437 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.890 
Loss G: 0.8782 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,703 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3493 (0.4052) Acc D Real: 58.411% 
Loss D Fake: 0.5433 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.893 
Loss G: 0.8787 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,710 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3799 (0.4016) Acc D Real: 58.698% 
Loss D Fake: 0.5428 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 0.8792 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,717 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.5276 (0.4173) Acc D Real: 56.855% 
Loss D Fake: 0.5425 (0.5429) Acc D Fake: 100.000% 
Loss D: 1.070 
Loss G: 0.8793 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,725 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3850 (0.4138) Acc D Real: 57.193% 
Loss D Fake: 0.5425 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.8790 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,734 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4368 (0.4161) Acc D Real: 56.990% 
Loss D Fake: 0.5427 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.980 
Loss G: 0.8788 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,741 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3667 (0.4116) Acc D Real: 57.727% 
Loss D Fake: 0.5425 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.909 
Loss G: 0.8792 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,748 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4265 (0.4128) Acc D Real: 57.552% 
Loss D Fake: 0.5420 (0.5427) Acc D Fake: 100.000% 
Loss D: 0.969 
Loss G: 0.8800 (0.8791) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,755 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3731 (0.4098) Acc D Real: 57.953% 
Loss D Fake: 0.5413 (0.5426) Acc D Fake: 100.000% 
Loss D: 0.914 
Loss G: 0.8808 (0.8792) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,762 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4854 (0.4152) Acc D Real: 57.370% 
Loss D Fake: 0.5407 (0.5425) Acc D Fake: 100.000% 
Loss D: 1.026 
Loss G: 0.8817 (0.8794) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,769 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4336 (0.4164) Acc D Real: 57.205% 
Loss D Fake: 0.5399 (0.5423) Acc D Fake: 100.000% 
Loss D: 0.974 
Loss G: 0.8826 (0.8796) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,776 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4670 (0.4196) Acc D Real: 56.908% 
Loss D Fake: 0.5391 (0.5421) Acc D Fake: 100.000% 
Loss D: 1.006 
Loss G: 0.8837 (0.8799) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,783 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.5365 (0.4264) Acc D Real: 56.057% 
Loss D Fake: 0.5384 (0.5419) Acc D Fake: 100.000% 
Loss D: 1.075 
Loss G: 0.8842 (0.8801) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,790 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4192 (0.4260) Acc D Real: 56.134% 
Loss D Fake: 0.5380 (0.5417) Acc D Fake: 100.000% 
Loss D: 0.957 
Loss G: 0.8847 (0.8804) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,797 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3251 (0.4207) Acc D Real: 56.790% 
Loss D Fake: 0.5375 (0.5414) Acc D Fake: 100.000% 
Loss D: 0.863 
Loss G: 0.8856 (0.8807) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,804 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3798 (0.4187) Acc D Real: 57.036% 
Loss D Fake: 0.5366 (0.5412) Acc D Fake: 100.000% 
Loss D: 0.916 
Loss G: 0.8868 (0.8810) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,811 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.5011 (0.4226) Acc D Real: 56.657% 
Loss D Fake: 0.5356 (0.5409) Acc D Fake: 100.000% 
Loss D: 1.037 
Loss G: 0.8882 (0.8813) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,817 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4640 (0.4245) Acc D Real: 56.409% 
Loss D Fake: 0.5345 (0.5406) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.8894 (0.8817) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,824 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4616 (0.4261) Acc D Real: 56.205% 
Loss D Fake: 0.5338 (0.5403) Acc D Fake: 100.000% 
Loss D: 0.995 
Loss G: 0.8902 (0.8821) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,832 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4328 (0.4264) Acc D Real: 56.189% 
Loss D Fake: 0.5332 (0.5400) Acc D Fake: 100.000% 
Loss D: 0.966 
Loss G: 0.8909 (0.8824) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,840 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4440 (0.4271) Acc D Real: 56.177% 
Loss D Fake: 0.5327 (0.5398) Acc D Fake: 100.000% 
Loss D: 0.977 
Loss G: 0.8915 (0.8828) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,847 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3770 (0.4252) Acc D Real: 56.372% 
Loss D Fake: 0.5322 (0.5395) Acc D Fake: 100.000% 
Loss D: 0.909 
Loss G: 0.8921 (0.8831) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,853 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4422 (0.4258) Acc D Real: 56.283% 
Loss D Fake: 0.5317 (0.5392) Acc D Fake: 100.000% 
Loss D: 0.974 
Loss G: 0.8926 (0.8835) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,860 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.5258 (0.4294) Acc D Real: 55.956% 
Loss D Fake: 0.5313 (0.5389) Acc D Fake: 100.000% 
Loss D: 1.057 
Loss G: 0.8932 (0.8838) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,868 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3800 (0.4277) Acc D Real: 56.223% 
Loss D Fake: 0.5307 (0.5386) Acc D Fake: 100.000% 
Loss D: 0.911 
Loss G: 0.8942 (0.8842) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,875 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3973 (0.4266) Acc D Real: 56.328% 
Loss D Fake: 0.5299 (0.5383) Acc D Fake: 100.000% 
Loss D: 0.927 
Loss G: 0.8952 (0.8846) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,883 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4892 (0.4287) Acc D Real: 56.129% 
Loss D Fake: 0.5291 (0.5380) Acc D Fake: 100.000% 
Loss D: 1.018 
Loss G: 0.8963 (0.8849) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,890 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.4685 (0.4299) Acc D Real: 56.009% 
Loss D Fake: 0.5284 (0.5377) Acc D Fake: 100.000% 
Loss D: 0.997 
Loss G: 0.8972 (0.8853) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,898 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4733 (0.4312) Acc D Real: 55.870% 
Loss D Fake: 0.5278 (0.5374) Acc D Fake: 100.000% 
Loss D: 1.001 
Loss G: 0.8977 (0.8857) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,906 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3632 (0.4292) Acc D Real: 56.115% 
Loss D Fake: 0.5274 (0.5371) Acc D Fake: 100.000% 
Loss D: 0.891 
Loss G: 0.8983 (0.8861) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,915 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4200 (0.4290) Acc D Real: 56.124% 
Loss D Fake: 0.5270 (0.5368) Acc D Fake: 100.000% 
Loss D: 0.947 
Loss G: 0.8987 (0.8864) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,922 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4724 (0.4302) Acc D Real: 56.060% 
Loss D Fake: 0.5266 (0.5366) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.8995 (0.8868) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,930 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5007 (0.4321) Acc D Real: 55.892% 
Loss D Fake: 0.5259 (0.5363) Acc D Fake: 100.000% 
Loss D: 1.027 
Loss G: 0.9005 (0.8872) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,937 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3939 (0.4311) Acc D Real: 55.999% 
Loss D Fake: 0.5251 (0.5360) Acc D Fake: 100.000% 
Loss D: 0.919 
Loss G: 0.9016 (0.8875) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,944 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4614 (0.4318) Acc D Real: 55.893% 
Loss D Fake: 0.5244 (0.5357) Acc D Fake: 100.000% 
Loss D: 0.986 
Loss G: 0.9024 (0.8879) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,952 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3956 (0.4309) Acc D Real: 55.975% 
Loss D Fake: 0.5240 (0.5354) Acc D Fake: 100.000% 
Loss D: 0.920 
Loss G: 0.9029 (0.8883) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,959 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3970 (0.4301) Acc D Real: 56.104% 
Loss D Fake: 0.5236 (0.5351) Acc D Fake: 100.000% 
Loss D: 0.921 
Loss G: 0.9036 (0.8887) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,967 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2591 (0.4260) Acc D Real: 56.586% 
Loss D Fake: 0.5228 (0.5348) Acc D Fake: 100.000% 
Loss D: 0.782 
Loss G: 0.9052 (0.8891) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,974 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3798 (0.4250) Acc D Real: 56.710% 
Loss D Fake: 0.5216 (0.5345) Acc D Fake: 100.000% 
Loss D: 0.901 
Loss G: 0.9068 (0.8895) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,981 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3733 (0.4238) Acc D Real: 56.870% 
Loss D Fake: 0.5204 (0.5342) Acc D Fake: 100.000% 
Loss D: 0.894 
Loss G: 0.9086 (0.8899) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,989 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4974 (0.4254) Acc D Real: 56.716% 
Loss D Fake: 0.5192 (0.5338) Acc D Fake: 100.000% 
Loss D: 1.017 
Loss G: 0.9103 (0.8904) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:45,996 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4137 (0.4252) Acc D Real: 56.752% 
Loss D Fake: 0.5180 (0.5335) Acc D Fake: 100.000% 
Loss D: 0.932 
Loss G: 0.9118 (0.8908) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,003 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3433 (0.4234) Acc D Real: 56.989% 
Loss D Fake: 0.5170 (0.5331) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 0.9134 (0.8913) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,010 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3331 (0.4215) Acc D Real: 57.241% 
Loss D Fake: 0.5157 (0.5328) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 0.9154 (0.8918) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,018 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3607 (0.4203) Acc D Real: 57.388% 
Loss D Fake: 0.5144 (0.5324) Acc D Fake: 100.000% 
Loss D: 0.875 
Loss G: 0.9173 (0.8923) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,025 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3656 (0.4192) Acc D Real: 57.509% 
Loss D Fake: 0.5132 (0.5320) Acc D Fake: 100.000% 
Loss D: 0.879 
Loss G: 0.9188 (0.8929) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,032 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4492 (0.4198) Acc D Real: 57.453% 
Loss D Fake: 0.5123 (0.5316) Acc D Fake: 100.000% 
Loss D: 0.961 
Loss G: 0.9200 (0.8934) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,039 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4799 (0.4210) Acc D Real: 57.320% 
Loss D Fake: 0.5117 (0.5313) Acc D Fake: 100.000% 
Loss D: 0.992 
Loss G: 0.9204 (0.8939) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,047 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4701 (0.4219) Acc D Real: 57.238% 
Loss D Fake: 0.5115 (0.5309) Acc D Fake: 100.000% 
Loss D: 0.982 
Loss G: 0.9208 (0.8944) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,054 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3917 (0.4213) Acc D Real: 57.304% 
Loss D Fake: 0.5112 (0.5305) Acc D Fake: 100.000% 
Loss D: 0.903 
Loss G: 0.9212 (0.8949) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,061 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4096 (0.4211) Acc D Real: 57.356% 
Loss D Fake: 0.5108 (0.5302) Acc D Fake: 100.000% 
Loss D: 0.920 
Loss G: 0.9219 (0.8954) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,069 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3150 (0.4192) Acc D Real: 57.600% 
Loss D Fake: 0.5101 (0.5298) Acc D Fake: 100.000% 
Loss D: 0.825 
Loss G: 0.9235 (0.8959) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,076 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4882 (0.4204) Acc D Real: 57.483% 
Loss D Fake: 0.5091 (0.5294) Acc D Fake: 100.000% 
Loss D: 0.997 
Loss G: 0.9246 (0.8964) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,083 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4242 (0.4205) Acc D Real: 57.504% 
Loss D Fake: 0.5085 (0.5291) Acc D Fake: 100.000% 
Loss D: 0.933 
Loss G: 0.9254 (0.8969) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,091 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4152 (0.4204) Acc D Real: 57.542% 
Loss D Fake: 0.5080 (0.5287) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 0.9260 (0.8974) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,098 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4906 (0.4216) Acc D Real: 57.451% 
Loss D Fake: 0.5076 (0.5284) Acc D Fake: 100.000% 
Loss D: 0.998 
Loss G: 0.9266 (0.8979) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,105 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3394 (0.4202) Acc D Real: 57.628% 
Loss D Fake: 0.5071 (0.5280) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 0.9277 (0.8984) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,112 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4744 (0.4211) Acc D Real: 57.546% 
Loss D Fake: 0.5063 (0.5277) Acc D Fake: 100.000% 
Loss D: 0.981 
Loss G: 0.9288 (0.8989) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,120 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3996 (0.4208) Acc D Real: 57.608% 
Loss D Fake: 0.5054 (0.5273) Acc D Fake: 100.000% 
Loss D: 0.905 
Loss G: 0.9304 (0.8994) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,127 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4994 (0.4220) Acc D Real: 57.513% 
Loss D Fake: 0.5043 (0.5270) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9319 (0.8999) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,135 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4139 (0.4219) Acc D Real: 57.572% 
Loss D Fake: 0.5034 (0.5266) Acc D Fake: 100.000% 
Loss D: 0.917 
Loss G: 0.9333 (0.9004) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,143 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4261 (0.4219) Acc D Real: 57.570% 
Loss D Fake: 0.5026 (0.5262) Acc D Fake: 100.000% 
Loss D: 0.929 
Loss G: 0.9343 (0.9009) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,150 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3583 (0.4210) Acc D Real: 57.704% 
Loss D Fake: 0.5020 (0.5259) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 0.9354 (0.9014) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,158 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4104 (0.4208) Acc D Real: 57.755% 
Loss D Fake: 0.5012 (0.5255) Acc D Fake: 100.000% 
Loss D: 0.912 
Loss G: 0.9367 (0.9019) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,165 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3326 (0.4195) Acc D Real: 57.917% 
Loss D Fake: 0.5002 (0.5251) Acc D Fake: 100.000% 
Loss D: 0.833 
Loss G: 0.9382 (0.9025) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,173 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3718 (0.4189) Acc D Real: 57.999% 
Loss D Fake: 0.4992 (0.5248) Acc D Fake: 100.000% 
Loss D: 0.871 
Loss G: 0.9398 (0.9030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,180 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4641 (0.4195) Acc D Real: 57.939% 
Loss D Fake: 0.4983 (0.5244) Acc D Fake: 100.000% 
Loss D: 0.962 
Loss G: 0.9410 (0.9035) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,187 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3884 (0.4191) Acc D Real: 58.002% 
Loss D Fake: 0.4976 (0.5240) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9422 (0.9041) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,195 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3873 (0.4186) Acc D Real: 58.057% 
Loss D Fake: 0.4968 (0.5237) Acc D Fake: 100.000% 
Loss D: 0.884 
Loss G: 0.9432 (0.9046) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,202 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3309 (0.4174) Acc D Real: 58.207% 
Loss D Fake: 0.4961 (0.5233) Acc D Fake: 100.000% 
Loss D: 0.827 
Loss G: 0.9446 (0.9052) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,209 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3724 (0.4168) Acc D Real: 58.310% 
Loss D Fake: 0.4951 (0.5229) Acc D Fake: 100.000% 
Loss D: 0.868 
Loss G: 0.9465 (0.9057) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,217 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3684 (0.4162) Acc D Real: 58.405% 
Loss D Fake: 0.4938 (0.5225) Acc D Fake: 100.000% 
Loss D: 0.862 
Loss G: 0.9487 (0.9063) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,224 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.5599 (0.4181) Acc D Real: 58.208% 
Loss D Fake: 0.4926 (0.5221) Acc D Fake: 100.000% 
Loss D: 1.052 
Loss G: 0.9499 (0.9068) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,231 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.5646 (0.4200) Acc D Real: 58.022% 
Loss D Fake: 0.4921 (0.5217) Acc D Fake: 100.000% 
Loss D: 1.057 
Loss G: 0.9503 (0.9074) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,239 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2909 (0.4183) Acc D Real: 58.215% 
Loss D Fake: 0.4919 (0.5214) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9512 (0.9079) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,246 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3445 (0.4174) Acc D Real: 58.342% 
Loss D Fake: 0.4911 (0.5210) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 0.9526 (0.9085) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,253 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4089 (0.4173) Acc D Real: 58.376% 
Loss D Fake: 0.4902 (0.5206) Acc D Fake: 100.000% 
Loss D: 0.899 
Loss G: 0.9539 (0.9091) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,260 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4761 (0.4180) Acc D Real: 58.314% 
Loss D Fake: 0.4896 (0.5202) Acc D Fake: 100.000% 
Loss D: 0.966 
Loss G: 0.9546 (0.9096) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,268 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3777 (0.4175) Acc D Real: 58.384% 
Loss D Fake: 0.4892 (0.5199) Acc D Fake: 100.000% 
Loss D: 0.867 
Loss G: 0.9554 (0.9102) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,275 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4023 (0.4173) Acc D Real: 58.408% 
Loss D Fake: 0.4887 (0.5195) Acc D Fake: 100.000% 
Loss D: 0.891 
Loss G: 0.9561 (0.9107) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,282 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3972 (0.4171) Acc D Real: 58.456% 
Loss D Fake: 0.4882 (0.5191) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 0.9570 (0.9113) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,289 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4332 (0.4173) Acc D Real: 58.442% 
Loss D Fake: 0.4877 (0.5188) Acc D Fake: 100.000% 
Loss D: 0.921 
Loss G: 0.9574 (0.9118) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,297 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3750 (0.4168) Acc D Real: 58.505% 
Loss D Fake: 0.4876 (0.5184) Acc D Fake: 100.000% 
Loss D: 0.863 
Loss G: 0.9578 (0.9123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,304 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.5121 (0.4179) Acc D Real: 58.411% 
Loss D Fake: 0.4874 (0.5180) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.9580 (0.9128) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,312 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2872 (0.4164) Acc D Real: 58.599% 
Loss D Fake: 0.4871 (0.5177) Acc D Fake: 100.000% 
Loss D: 0.774 
Loss G: 0.9589 (0.9134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,320 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3278 (0.4154) Acc D Real: 58.725% 
Loss D Fake: 0.4864 (0.5173) Acc D Fake: 100.000% 
Loss D: 0.814 
Loss G: 0.9602 (0.9139) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,327 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3034 (0.4142) Acc D Real: 58.870% 
Loss D Fake: 0.4855 (0.5170) Acc D Fake: 100.000% 
Loss D: 0.789 
Loss G: 0.9618 (0.9144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,334 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4648 (0.4148) Acc D Real: 58.836% 
Loss D Fake: 0.4846 (0.5166) Acc D Fake: 100.000% 
Loss D: 0.949 
Loss G: 0.9632 (0.9149) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,342 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4444 (0.4151) Acc D Real: 58.823% 
Loss D Fake: 0.4839 (0.5163) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.9642 (0.9155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,349 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4022 (0.4149) Acc D Real: 58.843% 
Loss D Fake: 0.4833 (0.5159) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9652 (0.9160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,356 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4665 (0.4155) Acc D Real: 58.792% 
Loss D Fake: 0.4828 (0.5156) Acc D Fake: 100.000% 
Loss D: 0.949 
Loss G: 0.9658 (0.9165) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,364 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3566 (0.4149) Acc D Real: 58.877% 
Loss D Fake: 0.4825 (0.5152) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 0.9664 (0.9170) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,371 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4097 (0.4148) Acc D Real: 58.906% 
Loss D Fake: 0.4821 (0.5149) Acc D Fake: 100.000% 
Loss D: 0.892 
Loss G: 0.9669 (0.9176) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,378 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3722 (0.4144) Acc D Real: 58.982% 
Loss D Fake: 0.4817 (0.5146) Acc D Fake: 100.000% 
Loss D: 0.854 
Loss G: 0.9678 (0.9181) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,386 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4724 (0.4150) Acc D Real: 58.948% 
Loss D Fake: 0.4812 (0.5142) Acc D Fake: 100.000% 
Loss D: 0.954 
Loss G: 0.9685 (0.9186) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,393 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4497 (0.4153) Acc D Real: 58.923% 
Loss D Fake: 0.4809 (0.5139) Acc D Fake: 100.000% 
Loss D: 0.931 
Loss G: 0.9689 (0.9191) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,400 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.5229 (0.4164) Acc D Real: 58.818% 
Loss D Fake: 0.4808 (0.5136) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9685 (0.9196) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,407 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4578 (0.4168) Acc D Real: 58.791% 
Loss D Fake: 0.4812 (0.5133) Acc D Fake: 100.000% 
Loss D: 0.939 
Loss G: 0.9677 (0.9200) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,415 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2565 (0.4152) Acc D Real: 58.978% 
Loss D Fake: 0.4816 (0.5129) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9675 (0.9205) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,422 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.6553 (0.4175) Acc D Real: 58.749% 
Loss D Fake: 0.4818 (0.5126) Acc D Fake: 100.000% 
Loss D: 1.137 
Loss G: 0.9666 (0.9209) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,429 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3765 (0.4171) Acc D Real: 58.816% 
Loss D Fake: 0.4825 (0.5124) Acc D Fake: 100.000% 
Loss D: 0.859 
Loss G: 0.9657 (0.9214) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,437 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4811 (0.4177) Acc D Real: 58.755% 
Loss D Fake: 0.4829 (0.5121) Acc D Fake: 100.000% 
Loss D: 0.964 
Loss G: 0.9652 (0.9218) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,444 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4527 (0.4181) Acc D Real: 58.738% 
Loss D Fake: 0.4831 (0.5118) Acc D Fake: 100.000% 
Loss D: 0.936 
Loss G: 0.9647 (0.9222) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,451 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3152 (0.4171) Acc D Real: 58.859% 
Loss D Fake: 0.4833 (0.5115) Acc D Fake: 100.000% 
Loss D: 0.798 
Loss G: 0.9651 (0.9226) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,459 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.5022 (0.4179) Acc D Real: 58.794% 
Loss D Fake: 0.4827 (0.5113) Acc D Fake: 100.000% 
Loss D: 0.985 
Loss G: 0.9668 (0.9230) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,467 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4331 (0.4180) Acc D Real: 58.798% 
Loss D Fake: 0.4815 (0.5110) Acc D Fake: 100.000% 
Loss D: 0.915 
Loss G: 0.9687 (0.9234) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,475 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1962 (0.4160) Acc D Real: 59.021% 
Loss D Fake: 0.4803 (0.5107) Acc D Fake: 100.000% 
Loss D: 0.677 
Loss G: 0.9710 (0.9238) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,483 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4595 (0.4164) Acc D Real: 59.000% 
Loss D Fake: 0.4789 (0.5105) Acc D Fake: 100.000% 
Loss D: 0.938 
Loss G: 0.9731 (0.9243) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,490 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3866 (0.4162) Acc D Real: 59.047% 
Loss D Fake: 0.4778 (0.5102) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9747 (0.9247) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,498 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3926 (0.4160) Acc D Real: 59.083% 
Loss D Fake: 0.4769 (0.5099) Acc D Fake: 100.000% 
Loss D: 0.870 
Loss G: 0.9764 (0.9252) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,505 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3745 (0.4156) Acc D Real: 59.136% 
Loss D Fake: 0.4758 (0.5096) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 0.9784 (0.9256) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,513 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4642 (0.4160) Acc D Real: 59.105% 
Loss D Fake: 0.4746 (0.5093) Acc D Fake: 100.000% 
Loss D: 0.939 
Loss G: 0.9804 (0.9261) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,520 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2971 (0.4150) Acc D Real: 59.230% 
Loss D Fake: 0.4734 (0.5090) Acc D Fake: 100.000% 
Loss D: 0.770 
Loss G: 0.9829 (0.9266) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,528 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.5379 (0.4160) Acc D Real: 59.134% 
Loss D Fake: 0.4720 (0.5087) Acc D Fake: 100.000% 
Loss D: 1.010 
Loss G: 0.9848 (0.9271) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,535 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4107 (0.4160) Acc D Real: 59.157% 
Loss D Fake: 0.4711 (0.5083) Acc D Fake: 100.000% 
Loss D: 0.882 
Loss G: 0.9864 (0.9276) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,543 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4029 (0.4159) Acc D Real: 59.172% 
Loss D Fake: 0.4702 (0.5080) Acc D Fake: 100.000% 
Loss D: 0.873 
Loss G: 0.9877 (0.9281) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,550 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3385 (0.4152) Acc D Real: 59.251% 
Loss D Fake: 0.4694 (0.5077) Acc D Fake: 100.000% 
Loss D: 0.808 
Loss G: 0.9891 (0.9286) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,557 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4420 (0.4155) Acc D Real: 59.248% 
Loss D Fake: 0.4687 (0.5074) Acc D Fake: 100.000% 
Loss D: 0.911 
Loss G: 0.9904 (0.9291) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,565 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3004 (0.4145) Acc D Real: 59.363% 
Loss D Fake: 0.4680 (0.5071) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 0.9916 (0.9296) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,573 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2933 (0.4136) Acc D Real: 59.480% 
Loss D Fake: 0.4672 (0.5067) Acc D Fake: 100.000% 
Loss D: 0.760 
Loss G: 0.9931 (0.9301) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,580 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3788 (0.4133) Acc D Real: 59.521% 
Loss D Fake: 0.4664 (0.5064) Acc D Fake: 100.000% 
Loss D: 0.845 
Loss G: 0.9946 (0.9306) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,588 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.5382 (0.4143) Acc D Real: 59.426% 
Loss D Fake: 0.4656 (0.5061) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9954 (0.9312) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,595 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4414 (0.4145) Acc D Real: 59.419% 
Loss D Fake: 0.4653 (0.5058) Acc D Fake: 100.000% 
Loss D: 0.907 
Loss G: 0.9959 (0.9317) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,603 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2513 (0.4132) Acc D Real: 59.569% 
Loss D Fake: 0.4650 (0.5055) Acc D Fake: 100.000% 
Loss D: 0.716 
Loss G: 0.9971 (0.9322) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,611 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3995 (0.4131) Acc D Real: 59.601% 
Loss D Fake: 0.4642 (0.5051) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9984 (0.9327) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,618 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.5013 (0.4138) Acc D Real: 59.538% 
Loss D Fake: 0.4636 (0.5048) Acc D Fake: 100.000% 
Loss D: 0.965 
Loss G: 0.9988 (0.9332) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,626 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4647 (0.4142) Acc D Real: 59.503% 
Loss D Fake: 0.4636 (0.5045) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.9989 (0.9337) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,633 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3808 (0.4139) Acc D Real: 59.539% 
Loss D Fake: 0.4634 (0.5042) Acc D Fake: 100.000% 
Loss D: 0.844 
Loss G: 0.9995 (0.9342) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,640 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.5595 (0.4150) Acc D Real: 59.432% 
Loss D Fake: 0.4629 (0.5039) Acc D Fake: 100.000% 
Loss D: 1.022 
Loss G: 1.0006 (0.9347) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,648 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3405 (0.4145) Acc D Real: 59.515% 
Loss D Fake: 0.4623 (0.5036) Acc D Fake: 100.000% 
Loss D: 0.803 
Loss G: 1.0018 (0.9352) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,655 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4448 (0.4147) Acc D Real: 59.505% 
Loss D Fake: 0.4616 (0.5033) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0029 (0.9357) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,663 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.5326 (0.4155) Acc D Real: 59.427% 
Loss D Fake: 0.4611 (0.5029) Acc D Fake: 100.000% 
Loss D: 0.994 
Loss G: 1.0036 (0.9362) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,670 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3963 (0.4154) Acc D Real: 59.455% 
Loss D Fake: 0.4608 (0.5026) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0041 (0.9367) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,677 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2335 (0.4141) Acc D Real: 59.604% 
Loss D Fake: 0.4605 (0.5023) Acc D Fake: 100.000% 
Loss D: 0.694 
Loss G: 1.0051 (0.9372) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,686 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3716 (0.4138) Acc D Real: 59.650% 
Loss D Fake: 0.4598 (0.5020) Acc D Fake: 100.000% 
Loss D: 0.831 
Loss G: 1.0065 (0.9377) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,693 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3648 (0.4134) Acc D Real: 59.704% 
Loss D Fake: 0.4590 (0.5017) Acc D Fake: 100.000% 
Loss D: 0.824 
Loss G: 1.0080 (0.9382) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,701 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3816 (0.4132) Acc D Real: 59.746% 
Loss D Fake: 0.4583 (0.5014) Acc D Fake: 100.000% 
Loss D: 0.840 
Loss G: 1.0089 (0.9387) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,709 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4390 (0.4134) Acc D Real: 59.744% 
Loss D Fake: 0.4579 (0.5011) Acc D Fake: 100.000% 
Loss D: 0.897 
Loss G: 1.0096 (0.9392) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,717 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4692 (0.4138) Acc D Real: 59.720% 
Loss D Fake: 0.4575 (0.5008) Acc D Fake: 100.000% 
Loss D: 0.927 
Loss G: 1.0103 (0.9397) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,725 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2991 (0.4130) Acc D Real: 59.818% 
Loss D Fake: 0.4571 (0.5005) Acc D Fake: 100.000% 
Loss D: 0.756 
Loss G: 1.0111 (0.9402) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,733 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.5042 (0.4136) Acc D Real: 59.761% 
Loss D Fake: 0.4568 (0.5002) Acc D Fake: 100.000% 
Loss D: 0.961 
Loss G: 1.0113 (0.9407) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,740 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4265 (0.4137) Acc D Real: 59.773% 
Loss D Fake: 0.4568 (0.4999) Acc D Fake: 100.000% 
Loss D: 0.883 
Loss G: 1.0114 (0.9412) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,747 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4316 (0.4138) Acc D Real: 59.775% 
Loss D Fake: 0.4567 (0.4996) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 1.0115 (0.9416) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,755 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3288 (0.4132) Acc D Real: 59.847% 
Loss D Fake: 0.4566 (0.4993) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0119 (0.9421) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,762 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3537 (0.4128) Acc D Real: 59.900% 
Loss D Fake: 0.4563 (0.4990) Acc D Fake: 100.000% 
Loss D: 0.810 
Loss G: 1.0125 (0.9426) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,770 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4994 (0.4134) Acc D Real: 59.851% 
Loss D Fake: 0.4561 (0.4987) Acc D Fake: 100.000% 
Loss D: 0.956 
Loss G: 1.0127 (0.9430) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,777 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3890 (0.4133) Acc D Real: 59.882% 
Loss D Fake: 0.4561 (0.4985) Acc D Fake: 100.000% 
Loss D: 0.845 
Loss G: 1.0126 (0.9435) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,784 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4268 (0.4134) Acc D Real: 59.885% 
Loss D Fake: 0.4562 (0.4982) Acc D Fake: 100.000% 
Loss D: 0.883 
Loss G: 1.0121 (0.9440) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,791 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4215 (0.4134) Acc D Real: 59.887% 
Loss D Fake: 0.4565 (0.4979) Acc D Fake: 100.000% 
Loss D: 0.878 
Loss G: 1.0116 (0.9444) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,799 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3538 (0.4130) Acc D Real: 59.934% 
Loss D Fake: 0.4568 (0.4976) Acc D Fake: 100.000% 
Loss D: 0.811 
Loss G: 1.0113 (0.9448) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,806 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4495 (0.4133) Acc D Real: 59.923% 
Loss D Fake: 0.4570 (0.4974) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0106 (0.9453) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,813 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2873 (0.4124) Acc D Real: 60.017% 
Loss D Fake: 0.4573 (0.4971) Acc D Fake: 100.000% 
Loss D: 0.745 
Loss G: 1.0104 (0.9457) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,821 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4211 (0.4125) Acc D Real: 60.017% 
Loss D Fake: 0.4572 (0.4969) Acc D Fake: 100.000% 
Loss D: 0.878 
Loss G: 1.0109 (0.9461) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:46,828 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3642 (0.4122) Acc D Real: 60.021% 
Loss D Fake: 0.4568 (0.4966) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 1.0122 (0.9465) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,072 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.506 | Generator Loss: 1.012 | Avg: 1.518 
2023-03-02 01:51:47,094 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.556 | Generator Loss: 1.012 | Avg: 1.568 
2023-03-02 01:51:47,117 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.543 | Generator Loss: 1.012 | Avg: 1.555 
2023-03-02 01:51:47,143 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.531 | Generator Loss: 1.012 | Avg: 1.543 
2023-03-02 01:51:47,169 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.528 | Generator Loss: 1.012 | Avg: 1.541 
2023-03-02 01:51:47,196 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.613 | Generator Loss: 1.012 | Avg: 1.625 
2023-03-02 01:51:47,222 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.616 | Generator Loss: 1.012 | Avg: 1.628 
2023-03-02 01:51:47,248 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.675 | Generator Loss: 1.012 | Avg: 1.687 
2023-03-02 01:51:47,274 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.693 | Generator Loss: 1.012 | Avg: 1.706 
2023-03-02 01:51:47,302 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.765 | Generator Loss: 1.012 | Avg: 1.778 
2023-03-02 01:51:47,329 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.809 | Generator Loss: 1.012 | Avg: 1.822 
2023-03-02 01:51:47,355 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.864 | Generator Loss: 1.012 | Avg: 1.876 
2023-03-02 01:51:47,382 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.901 | Generator Loss: 1.012 | Avg: 1.913 
2023-03-02 01:51:47,408 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.909 | Generator Loss: 1.012 | Avg: 1.922 
2023-03-02 01:51:47,434 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.886 | Generator Loss: 1.012 | Avg: 1.898 
2023-03-02 01:51:47,460 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.864 | Generator Loss: 1.012 | Avg: 1.876 
2023-03-02 01:51:47,486 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.844 | Generator Loss: 1.012 | Avg: 1.856 
2023-03-02 01:51:47,521 -                train: [    INFO] - 
Epoch: 6/20
2023-03-02 01:51:47,731 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4424 (0.4424) Acc D Real: 58.490% 
Loss D Fake: 0.4553 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.898 
Loss G: 1.0148 (1.0142) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,740 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4752 (0.4533) Acc D Real: 57.153% 
Loss D Fake: 0.4547 (0.4553) Acc D Fake: 100.000% 
Loss D: 0.930 
Loss G: 1.0156 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,747 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4338 (0.4484) Acc D Real: 57.669% 
Loss D Fake: 0.4543 (0.4551) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 1.0164 (1.0151) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,765 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3237 (0.4235) Acc D Real: 60.385% 
Loss D Fake: 0.4539 (0.4548) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0172 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,772 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4305 (0.4247) Acc D Real: 60.391% 
Loss D Fake: 0.4536 (0.4546) Acc D Fake: 100.000% 
Loss D: 0.884 
Loss G: 1.0176 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,779 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.5462 (0.4420) Acc D Real: 58.862% 
Loss D Fake: 0.4535 (0.4545) Acc D Fake: 100.000% 
Loss D: 1.000 
Loss G: 1.0173 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,786 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3948 (0.4361) Acc D Real: 59.460% 
Loss D Fake: 0.4538 (0.4544) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 1.0170 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,793 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4516 (0.4378) Acc D Real: 59.363% 
Loss D Fake: 0.4540 (0.4543) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0165 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,799 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4056 (0.4346) Acc D Real: 59.740% 
Loss D Fake: 0.4543 (0.4543) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 1.0157 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,806 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4599 (0.4369) Acc D Real: 59.531% 
Loss D Fake: 0.4548 (0.4544) Acc D Fake: 100.000% 
Loss D: 0.915 
Loss G: 1.0145 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,813 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4411 (0.4373) Acc D Real: 59.457% 
Loss D Fake: 0.4556 (0.4545) Acc D Fake: 100.000% 
Loss D: 0.897 
Loss G: 1.0132 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,820 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3976 (0.4342) Acc D Real: 59.804% 
Loss D Fake: 0.4563 (0.4546) Acc D Fake: 100.000% 
Loss D: 0.854 
Loss G: 1.0120 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,827 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.5289 (0.4410) Acc D Real: 59.066% 
Loss D Fake: 0.4570 (0.4548) Acc D Fake: 100.000% 
Loss D: 0.986 
Loss G: 1.0103 (1.0151) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,834 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2628 (0.4291) Acc D Real: 60.323% 
Loss D Fake: 0.4579 (0.4550) Acc D Fake: 100.000% 
Loss D: 0.721 
Loss G: 1.0093 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,841 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4228 (0.4287) Acc D Real: 60.312% 
Loss D Fake: 0.4582 (0.4552) Acc D Fake: 100.000% 
Loss D: 0.881 
Loss G: 1.0089 (1.0144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,849 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3948 (0.4267) Acc D Real: 60.527% 
Loss D Fake: 0.4584 (0.4554) Acc D Fake: 100.000% 
Loss D: 0.853 
Loss G: 1.0086 (1.0140) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,856 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3552 (0.4227) Acc D Real: 60.859% 
Loss D Fake: 0.4586 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.814 
Loss G: 1.0082 (1.0137) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,863 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3995 (0.4215) Acc D Real: 60.962% 
Loss D Fake: 0.4588 (0.4557) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0080 (1.0134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,870 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4600 (0.4234) Acc D Real: 60.776% 
Loss D Fake: 0.4589 (0.4559) Acc D Fake: 100.000% 
Loss D: 0.919 
Loss G: 1.0077 (1.0131) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,877 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3741 (0.4211) Acc D Real: 60.977% 
Loss D Fake: 0.4590 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.833 
Loss G: 1.0078 (1.0129) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,884 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3649 (0.4185) Acc D Real: 61.219% 
Loss D Fake: 0.4588 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.824 
Loss G: 1.0084 (1.0127) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,891 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3039 (0.4135) Acc D Real: 61.825% 
Loss D Fake: 0.4584 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.762 
Loss G: 1.0095 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,898 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3839 (0.4123) Acc D Real: 61.940% 
Loss D Fake: 0.4578 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.842 
Loss G: 1.0106 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,905 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.5001 (0.4158) Acc D Real: 61.577% 
Loss D Fake: 0.4572 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.957 
Loss G: 1.0113 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,912 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3954 (0.4150) Acc D Real: 61.679% 
Loss D Fake: 0.4570 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.852 
Loss G: 1.0116 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,919 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4146 (0.4150) Acc D Real: 61.651% 
Loss D Fake: 0.4568 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.871 
Loss G: 1.0119 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,927 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4532 (0.4164) Acc D Real: 61.483% 
Loss D Fake: 0.4567 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.910 
Loss G: 1.0122 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,935 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2720 (0.4114) Acc D Real: 62.010% 
Loss D Fake: 0.4565 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.729 
Loss G: 1.0126 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,943 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3799 (0.4104) Acc D Real: 62.087% 
Loss D Fake: 0.4563 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 1.0132 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,951 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2719 (0.4059) Acc D Real: 62.544% 
Loss D Fake: 0.4559 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.728 
Loss G: 1.0141 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,958 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5178 (0.4094) Acc D Real: 62.215% 
Loss D Fake: 0.4554 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.973 
Loss G: 1.0147 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,966 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4679 (0.4112) Acc D Real: 62.025% 
Loss D Fake: 0.4552 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 1.0150 (1.0126) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,973 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1279 (0.4028) Acc D Real: 62.917% 
Loss D Fake: 0.4549 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.583 
Loss G: 1.0161 (1.0127) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,981 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3065 (0.4001) Acc D Real: 63.205% 
Loss D Fake: 0.4542 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.761 
Loss G: 1.0176 (1.0128) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,988 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.5642 (0.4046) Acc D Real: 62.713% 
Loss D Fake: 0.4535 (0.4562) Acc D Fake: 100.000% 
Loss D: 1.018 
Loss G: 1.0185 (1.0130) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:47,996 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3912 (0.4043) Acc D Real: 62.739% 
Loss D Fake: 0.4531 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.844 
Loss G: 1.0194 (1.0132) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:48,004 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3663 (0.4033) Acc D Real: 62.834% 
Loss D Fake: 0.4525 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.819 
Loss G: 1.0206 (1.0134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:48,011 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5170 (0.4062) Acc D Real: 62.519% 
Loss D Fake: 0.4520 (0.4559) Acc D Fake: 100.000% 
Loss D: 0.969 
Loss G: 1.0212 (1.0136) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:48,019 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3739 (0.4054) Acc D Real: 62.617% 
Loss D Fake: 0.4517 (0.4558) Acc D Fake: 100.000% 
Loss D: 0.826 
Loss G: 1.0218 (1.0138) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:48,027 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4581 (0.4067) Acc D Real: 62.497% 
Loss D Fake: 0.4515 (0.4557) Acc D Fake: 100.000% 
Loss D: 0.910 
Loss G: 1.0222 (1.0140) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 01:51:48,034 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4059 (0.4066) Acc D Real: 62.521% 
Loss D Fake: 0.4513 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0227 (1.0142) Acc G: 0.040% 
LR: 2.000e-04 

2023-03-02 01:51:48,042 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4479 (0.4076) Acc D Real: 62.402% 
Loss D Fake: 0.4509 (0.4555) Acc D Fake: 99.961% 
Loss D: 0.899 
Loss G: 1.0237 (1.0144) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 01:51:48,050 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3377 (0.4060) Acc D Real: 62.577% 
Loss D Fake: 0.4502 (0.4553) Acc D Fake: 99.924% 
Loss D: 0.788 
Loss G: 1.0256 (1.0146) Acc G: 0.114% 
LR: 2.000e-04 

2023-03-02 01:51:48,058 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4346 (0.4067) Acc D Real: 62.477% 
Loss D Fake: 0.4490 (0.4552) Acc D Fake: 99.889% 
Loss D: 0.884 
Loss G: 1.0287 (1.0150) Acc G: 0.148% 
LR: 2.000e-04 

2023-03-02 01:51:48,065 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4855 (0.4084) Acc D Real: 62.275% 
Loss D Fake: 0.4471 (0.4550) Acc D Fake: 99.819% 
Loss D: 0.933 
Loss G: 1.0325 (1.0153) Acc G: 0.217% 
LR: 2.000e-04 

2023-03-02 01:51:48,073 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4528 (0.4093) Acc D Real: 62.160% 
Loss D Fake: 0.4451 (0.4548) Acc D Fake: 99.752% 
Loss D: 0.898 
Loss G: 1.0362 (1.0158) Acc G: 0.284% 
LR: 2.000e-04 

2023-03-02 01:51:48,081 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4487 (0.4101) Acc D Real: 62.067% 
Loss D Fake: 0.4433 (0.4546) Acc D Fake: 99.688% 
Loss D: 0.892 
Loss G: 1.0388 (1.0163) Acc G: 0.347% 
LR: 2.000e-04 

2023-03-02 01:51:48,088 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4840 (0.4116) Acc D Real: 61.880% 
Loss D Fake: 0.4424 (0.4543) Acc D Fake: 99.626% 
Loss D: 0.926 
Loss G: 1.0398 (1.0167) Acc G: 0.408% 
LR: 2.000e-04 

2023-03-02 01:51:48,096 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3388 (0.4102) Acc D Real: 62.033% 
Loss D Fake: 0.4421 (0.4541) Acc D Fake: 99.567% 
Loss D: 0.781 
Loss G: 1.0402 (1.0172) Acc G: 0.467% 
LR: 2.000e-04 

2023-03-02 01:51:48,104 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4001 (0.4100) Acc D Real: 62.042% 
Loss D Fake: 0.4421 (0.4538) Acc D Fake: 99.510% 
Loss D: 0.842 
Loss G: 1.0402 (1.0177) Acc G: 0.523% 
LR: 2.000e-04 

2023-03-02 01:51:48,111 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4325 (0.4104) Acc D Real: 61.988% 
Loss D Fake: 0.4422 (0.4536) Acc D Fake: 99.455% 
Loss D: 0.875 
Loss G: 1.0399 (1.0181) Acc G: 0.577% 
LR: 2.000e-04 

2023-03-02 01:51:48,119 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4908 (0.4119) Acc D Real: 61.825% 
Loss D Fake: 0.4424 (0.4534) Acc D Fake: 99.403% 
Loss D: 0.933 
Loss G: 1.0395 (1.0185) Acc G: 0.629% 
LR: 2.000e-04 

2023-03-02 01:51:48,127 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4044 (0.4118) Acc D Real: 61.851% 
Loss D Fake: 0.4427 (0.4532) Acc D Fake: 99.352% 
Loss D: 0.847 
Loss G: 1.0390 (1.0189) Acc G: 0.679% 
LR: 2.000e-04 

2023-03-02 01:51:48,134 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.5489 (0.4143) Acc D Real: 61.573% 
Loss D Fake: 0.4430 (0.4530) Acc D Fake: 99.303% 
Loss D: 0.992 
Loss G: 1.0382 (1.0192) Acc G: 0.727% 
LR: 2.000e-04 

2023-03-02 01:51:48,142 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4687 (0.4153) Acc D Real: 61.485% 
Loss D Fake: 0.4434 (0.4528) Acc D Fake: 99.256% 
Loss D: 0.912 
Loss G: 1.0373 (1.0196) Acc G: 0.774% 
LR: 2.000e-04 

2023-03-02 01:51:48,150 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4751 (0.4163) Acc D Real: 61.385% 
Loss D Fake: 0.4440 (0.4527) Acc D Fake: 99.211% 
Loss D: 0.919 
Loss G: 1.0364 (1.0198) Acc G: 0.819% 
LR: 2.000e-04 

2023-03-02 01:51:48,157 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4506 (0.4169) Acc D Real: 61.302% 
Loss D Fake: 0.4445 (0.4526) Acc D Fake: 99.167% 
Loss D: 0.895 
Loss G: 1.0354 (1.0201) Acc G: 0.862% 
LR: 2.000e-04 

2023-03-02 01:51:48,166 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4161 (0.4169) Acc D Real: 61.299% 
Loss D Fake: 0.4450 (0.4524) Acc D Fake: 99.124% 
Loss D: 0.861 
Loss G: 1.0345 (1.0204) Acc G: 0.904% 
LR: 2.000e-04 

2023-03-02 01:51:48,174 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2479 (0.4141) Acc D Real: 61.573% 
Loss D Fake: 0.4455 (0.4523) Acc D Fake: 99.083% 
Loss D: 0.693 
Loss G: 1.0338 (1.0206) Acc G: 0.944% 
LR: 2.000e-04 

2023-03-02 01:51:48,181 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3707 (0.4134) Acc D Real: 61.650% 
Loss D Fake: 0.4458 (0.4522) Acc D Fake: 99.044% 
Loss D: 0.817 
Loss G: 1.0331 (1.0208) Acc G: 0.984% 
LR: 2.000e-04 

2023-03-02 01:51:48,189 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3330 (0.4121) Acc D Real: 61.773% 
Loss D Fake: 0.4462 (0.4521) Acc D Fake: 99.005% 
Loss D: 0.779 
Loss G: 1.0325 (1.0210) Acc G: 1.022% 
LR: 2.000e-04 

2023-03-02 01:51:48,197 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2218 (0.4090) Acc D Real: 62.101% 
Loss D Fake: 0.4465 (0.4520) Acc D Fake: 98.968% 
Loss D: 0.668 
Loss G: 1.0321 (1.0212) Acc G: 1.058% 
LR: 2.000e-04 

2023-03-02 01:51:48,204 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3454 (0.4080) Acc D Real: 62.198% 
Loss D Fake: 0.4467 (0.4519) Acc D Fake: 98.932% 
Loss D: 0.792 
Loss G: 1.0318 (1.0213) Acc G: 1.094% 
LR: 2.000e-04 

2023-03-02 01:51:48,212 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3845 (0.4077) Acc D Real: 62.221% 
Loss D Fake: 0.4469 (0.4519) Acc D Fake: 98.897% 
Loss D: 0.831 
Loss G: 1.0313 (1.0215) Acc G: 1.128% 
LR: 2.000e-04 

2023-03-02 01:51:48,220 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3686 (0.4071) Acc D Real: 62.277% 
Loss D Fake: 0.4472 (0.4518) Acc D Fake: 98.864% 
Loss D: 0.816 
Loss G: 1.0308 (1.0216) Acc G: 1.162% 
LR: 2.000e-04 

2023-03-02 01:51:48,228 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4810 (0.4082) Acc D Real: 62.138% 
Loss D Fake: 0.4475 (0.4517) Acc D Fake: 98.831% 
Loss D: 0.929 
Loss G: 1.0302 (1.0217) Acc G: 1.194% 
LR: 2.000e-04 

2023-03-02 01:51:48,235 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3755 (0.4077) Acc D Real: 62.174% 
Loss D Fake: 0.4479 (0.4517) Acc D Fake: 98.799% 
Loss D: 0.823 
Loss G: 1.0295 (1.0219) Acc G: 1.225% 
LR: 2.000e-04 

2023-03-02 01:51:48,243 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4693 (0.4086) Acc D Real: 62.062% 
Loss D Fake: 0.4483 (0.4516) Acc D Fake: 98.768% 
Loss D: 0.918 
Loss G: 1.0287 (1.0220) Acc G: 1.256% 
LR: 2.000e-04 

2023-03-02 01:51:48,250 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3408 (0.4076) Acc D Real: 62.150% 
Loss D Fake: 0.4488 (0.4516) Acc D Fake: 98.738% 
Loss D: 0.790 
Loss G: 1.0280 (1.0220) Acc G: 1.286% 
LR: 2.000e-04 

2023-03-02 01:51:48,258 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4139 (0.4077) Acc D Real: 62.128% 
Loss D Fake: 0.4492 (0.4515) Acc D Fake: 98.709% 
Loss D: 0.863 
Loss G: 1.0272 (1.0221) Acc G: 1.315% 
LR: 2.000e-04 

2023-03-02 01:51:48,266 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4772 (0.4087) Acc D Real: 61.976% 
Loss D Fake: 0.4497 (0.4515) Acc D Fake: 98.681% 
Loss D: 0.927 
Loss G: 1.0263 (1.0222) Acc G: 1.343% 
LR: 2.000e-04 

2023-03-02 01:51:48,273 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3508 (0.4079) Acc D Real: 62.040% 
Loss D Fake: 0.4502 (0.4515) Acc D Fake: 98.630% 
Loss D: 0.801 
Loss G: 1.0254 (1.0222) Acc G: 1.393% 
LR: 2.000e-04 

2023-03-02 01:51:48,282 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3603 (0.4073) Acc D Real: 62.088% 
Loss D Fake: 0.4507 (0.4515) Acc D Fake: 98.581% 
Loss D: 0.811 
Loss G: 1.0246 (1.0223) Acc G: 1.441% 
LR: 2.000e-04 

2023-03-02 01:51:48,289 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4021 (0.4072) Acc D Real: 62.071% 
Loss D Fake: 0.4511 (0.4515) Acc D Fake: 98.533% 
Loss D: 0.853 
Loss G: 1.0238 (1.0223) Acc G: 1.489% 
LR: 2.000e-04 

2023-03-02 01:51:48,297 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3712 (0.4067) Acc D Real: 62.111% 
Loss D Fake: 0.4516 (0.4515) Acc D Fake: 98.487% 
Loss D: 0.823 
Loss G: 1.0230 (1.0223) Acc G: 1.535% 
LR: 2.000e-04 

2023-03-02 01:51:48,304 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2694 (0.4049) Acc D Real: 62.282% 
Loss D Fake: 0.4520 (0.4515) Acc D Fake: 98.442% 
Loss D: 0.721 
Loss G: 1.0223 (1.0223) Acc G: 1.580% 
LR: 2.000e-04 

2023-03-02 01:51:48,312 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4234 (0.4052) Acc D Real: 62.230% 
Loss D Fake: 0.4524 (0.4515) Acc D Fake: 98.397% 
Loss D: 0.876 
Loss G: 1.0215 (1.0223) Acc G: 1.624% 
LR: 2.000e-04 

2023-03-02 01:51:48,319 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3620 (0.4046) Acc D Real: 62.269% 
Loss D Fake: 0.4528 (0.4515) Acc D Fake: 98.354% 
Loss D: 0.815 
Loss G: 1.0209 (1.0223) Acc G: 1.667% 
LR: 2.000e-04 

2023-03-02 01:51:48,326 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4404 (0.4051) Acc D Real: 62.164% 
Loss D Fake: 0.4532 (0.4515) Acc D Fake: 98.312% 
Loss D: 0.894 
Loss G: 1.0201 (1.0222) Acc G: 1.708% 
LR: 2.000e-04 

2023-03-02 01:51:48,334 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3435 (0.4043) Acc D Real: 62.204% 
Loss D Fake: 0.4536 (0.4516) Acc D Fake: 98.272% 
Loss D: 0.797 
Loss G: 1.0194 (1.0222) Acc G: 1.749% 
LR: 2.000e-04 

2023-03-02 01:51:48,341 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3437 (0.4036) Acc D Real: 62.276% 
Loss D Fake: 0.4540 (0.4516) Acc D Fake: 98.232% 
Loss D: 0.798 
Loss G: 1.0188 (1.0221) Acc G: 1.789% 
LR: 2.000e-04 

2023-03-02 01:51:48,349 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3448 (0.4029) Acc D Real: 62.327% 
Loss D Fake: 0.4544 (0.4516) Acc D Fake: 98.193% 
Loss D: 0.799 
Loss G: 1.0183 (1.0221) Acc G: 1.827% 
LR: 2.000e-04 

2023-03-02 01:51:48,356 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3041 (0.4017) Acc D Real: 62.414% 
Loss D Fake: 0.4546 (0.4517) Acc D Fake: 98.155% 
Loss D: 0.759 
Loss G: 1.0179 (1.0221) Acc G: 1.865% 
LR: 2.000e-04 

2023-03-02 01:51:48,364 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4044 (0.4017) Acc D Real: 62.377% 
Loss D Fake: 0.4548 (0.4517) Acc D Fake: 98.118% 
Loss D: 0.859 
Loss G: 1.0177 (1.0220) Acc G: 1.902% 
LR: 2.000e-04 

2023-03-02 01:51:48,371 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3165 (0.4007) Acc D Real: 62.452% 
Loss D Fake: 0.4550 (0.4517) Acc D Fake: 98.081% 
Loss D: 0.771 
Loss G: 1.0174 (1.0219) Acc G: 1.938% 
LR: 2.000e-04 

2023-03-02 01:51:48,379 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3497 (0.4001) Acc D Real: 62.489% 
Loss D Fake: 0.4551 (0.4518) Acc D Fake: 98.046% 
Loss D: 0.805 
Loss G: 1.0172 (1.0219) Acc G: 1.973% 
LR: 2.000e-04 

2023-03-02 01:51:48,386 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3662 (0.3998) Acc D Real: 62.507% 
Loss D Fake: 0.4553 (0.4518) Acc D Fake: 98.011% 
Loss D: 0.821 
Loss G: 1.0170 (1.0218) Acc G: 2.008% 
LR: 2.000e-04 

2023-03-02 01:51:48,394 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4186 (0.4000) Acc D Real: 62.451% 
Loss D Fake: 0.4554 (0.4519) Acc D Fake: 97.978% 
Loss D: 0.874 
Loss G: 1.0166 (1.0218) Acc G: 2.041% 
LR: 2.000e-04 

2023-03-02 01:51:48,401 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2225 (0.3980) Acc D Real: 62.643% 
Loss D Fake: 0.4557 (0.4519) Acc D Fake: 97.944% 
Loss D: 0.678 
Loss G: 1.0162 (1.0217) Acc G: 2.074% 
LR: 2.000e-04 

2023-03-02 01:51:48,409 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2580 (0.3965) Acc D Real: 62.774% 
Loss D Fake: 0.4559 (0.4519) Acc D Fake: 97.912% 
Loss D: 0.714 
Loss G: 1.0161 (1.0217) Acc G: 2.106% 
LR: 2.000e-04 

2023-03-02 01:51:48,416 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.5183 (0.3978) Acc D Real: 62.543% 
Loss D Fake: 0.4560 (0.4520) Acc D Fake: 97.880% 
Loss D: 0.974 
Loss G: 1.0155 (1.0216) Acc G: 2.138% 
LR: 2.000e-04 

2023-03-02 01:51:48,424 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4147 (0.3980) Acc D Real: 62.498% 
Loss D Fake: 0.4565 (0.4520) Acc D Fake: 97.849% 
Loss D: 0.871 
Loss G: 1.0147 (1.0215) Acc G: 2.168% 
LR: 2.000e-04 

2023-03-02 01:51:48,431 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2245 (0.3961) Acc D Real: 62.698% 
Loss D Fake: 0.4569 (0.4521) Acc D Fake: 97.819% 
Loss D: 0.681 
Loss G: 1.0143 (1.0214) Acc G: 2.199% 
LR: 2.000e-04 

2023-03-02 01:51:48,438 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3669 (0.3958) Acc D Real: 62.711% 
Loss D Fake: 0.4571 (0.4521) Acc D Fake: 97.789% 
Loss D: 0.824 
Loss G: 1.0139 (1.0214) Acc G: 2.228% 
LR: 2.000e-04 

2023-03-02 01:51:48,446 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2687 (0.3945) Acc D Real: 62.841% 
Loss D Fake: 0.4573 (0.4522) Acc D Fake: 97.760% 
Loss D: 0.726 
Loss G: 1.0138 (1.0213) Acc G: 2.257% 
LR: 2.000e-04 

2023-03-02 01:51:48,453 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.5208 (0.3958) Acc D Real: 62.649% 
Loss D Fake: 0.4574 (0.4522) Acc D Fake: 97.732% 
Loss D: 0.978 
Loss G: 1.0134 (1.0212) Acc G: 2.285% 
LR: 2.000e-04 

2023-03-02 01:51:48,461 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3530 (0.3953) Acc D Real: 62.658% 
Loss D Fake: 0.4577 (0.4523) Acc D Fake: 97.704% 
Loss D: 0.811 
Loss G: 1.0130 (1.0211) Acc G: 2.313% 
LR: 2.000e-04 

2023-03-02 01:51:48,468 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3136 (0.3945) Acc D Real: 62.721% 
Loss D Fake: 0.4579 (0.4524) Acc D Fake: 97.677% 
Loss D: 0.771 
Loss G: 1.0128 (1.0210) Acc G: 2.340% 
LR: 2.000e-04 

2023-03-02 01:51:48,476 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3554 (0.3941) Acc D Real: 62.742% 
Loss D Fake: 0.4580 (0.4524) Acc D Fake: 97.650% 
Loss D: 0.813 
Loss G: 1.0126 (1.0209) Acc G: 2.367% 
LR: 2.000e-04 

2023-03-02 01:51:48,483 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3736 (0.3939) Acc D Real: 62.748% 
Loss D Fake: 0.4582 (0.4525) Acc D Fake: 97.624% 
Loss D: 0.832 
Loss G: 1.0123 (1.0209) Acc G: 2.393% 
LR: 2.000e-04 

2023-03-02 01:51:48,491 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4293 (0.3943) Acc D Real: 62.689% 
Loss D Fake: 0.4585 (0.4525) Acc D Fake: 97.598% 
Loss D: 0.888 
Loss G: 1.0115 (1.0208) Acc G: 2.418% 
LR: 2.000e-04 

2023-03-02 01:51:48,498 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4232 (0.3946) Acc D Real: 62.645% 
Loss D Fake: 0.4590 (0.4526) Acc D Fake: 97.573% 
Loss D: 0.882 
Loss G: 1.0107 (1.0207) Acc G: 2.443% 
LR: 2.000e-04 

2023-03-02 01:51:48,505 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3843 (0.3945) Acc D Real: 62.641% 
Loss D Fake: 0.4594 (0.4527) Acc D Fake: 97.548% 
Loss D: 0.844 
Loss G: 1.0100 (1.0206) Acc G: 2.468% 
LR: 2.000e-04 

2023-03-02 01:51:48,513 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3815 (0.3943) Acc D Real: 62.628% 
Loss D Fake: 0.4598 (0.4527) Acc D Fake: 97.524% 
Loss D: 0.841 
Loss G: 1.0093 (1.0205) Acc G: 2.492% 
LR: 2.000e-04 

2023-03-02 01:51:48,520 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3947 (0.3943) Acc D Real: 62.629% 
Loss D Fake: 0.4603 (0.4528) Acc D Fake: 97.500% 
Loss D: 0.855 
Loss G: 1.0083 (1.0203) Acc G: 2.516% 
LR: 2.000e-04 

2023-03-02 01:51:48,528 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3807 (0.3942) Acc D Real: 62.742% 
Loss D Fake: 0.4610 (0.4529) Acc D Fake: 97.477% 
Loss D: 0.842 
Loss G: 1.0071 (1.0202) Acc G: 2.539% 
LR: 2.000e-04 

2023-03-02 01:51:48,535 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3202 (0.3935) Acc D Real: 62.872% 
Loss D Fake: 0.4617 (0.4530) Acc D Fake: 97.454% 
Loss D: 0.782 
Loss G: 1.0059 (1.0201) Acc G: 2.562% 
LR: 2.000e-04 

2023-03-02 01:51:48,543 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3915 (0.3935) Acc D Real: 62.970% 
Loss D Fake: 0.4624 (0.4530) Acc D Fake: 97.431% 
Loss D: 0.854 
Loss G: 1.0045 (1.0199) Acc G: 2.591% 
LR: 2.000e-04 

2023-03-02 01:51:48,550 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3721 (0.3933) Acc D Real: 63.028% 
Loss D Fake: 0.4639 (0.4531) Acc D Fake: 97.394% 
Loss D: 0.836 
Loss G: 0.9981 (1.0197) Acc G: 2.628% 
LR: 2.000e-04 

2023-03-02 01:51:48,559 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4477 (0.3938) Acc D Real: 63.063% 
Loss D Fake: 0.4707 (0.4533) Acc D Fake: 97.357% 
Loss D: 0.918 
Loss G: 0.9826 (1.0194) Acc G: 2.664% 
LR: 2.000e-04 

2023-03-02 01:51:48,567 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3957 (0.3938) Acc D Real: 63.107% 
Loss D Fake: 0.4824 (0.4536) Acc D Fake: 97.321% 
Loss D: 0.878 
Loss G: 0.9778 (1.0190) Acc G: 2.700% 
LR: 2.000e-04 

2023-03-02 01:51:48,575 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3479 (0.3934) Acc D Real: 63.253% 
Loss D Fake: 0.4754 (0.4538) Acc D Fake: 97.286% 
Loss D: 0.823 
Loss G: 0.9922 (1.0188) Acc G: 2.735% 
LR: 2.000e-04 

2023-03-02 01:51:48,584 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3796 (0.3933) Acc D Real: 63.374% 
Loss D Fake: 0.4670 (0.4539) Acc D Fake: 97.251% 
Loss D: 0.847 
Loss G: 1.0003 (1.0186) Acc G: 2.770% 
LR: 2.000e-04 

2023-03-02 01:51:48,591 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4295 (0.3936) Acc D Real: 63.393% 
Loss D Fake: 0.4643 (0.4540) Acc D Fake: 97.217% 
Loss D: 0.894 
Loss G: 1.0034 (1.0185) Acc G: 2.803% 
LR: 2.000e-04 

2023-03-02 01:51:48,599 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3144 (0.3929) Acc D Real: 63.524% 
Loss D Fake: 0.4632 (0.4540) Acc D Fake: 97.184% 
Loss D: 0.778 
Loss G: 1.0048 (1.0184) Acc G: 2.837% 
LR: 2.000e-04 

2023-03-02 01:51:48,607 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4128 (0.3931) Acc D Real: 63.555% 
Loss D Fake: 0.4627 (0.4541) Acc D Fake: 97.151% 
Loss D: 0.875 
Loss G: 1.0054 (1.0183) Acc G: 2.869% 
LR: 2.000e-04 

2023-03-02 01:51:48,615 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4745 (0.3938) Acc D Real: 63.524% 
Loss D Fake: 0.4625 (0.4542) Acc D Fake: 97.119% 
Loss D: 0.937 
Loss G: 1.0056 (1.0182) Acc G: 2.902% 
LR: 2.000e-04 

2023-03-02 01:51:48,622 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3490 (0.3934) Acc D Real: 63.609% 
Loss D Fake: 0.4625 (0.4543) Acc D Fake: 97.087% 
Loss D: 0.812 
Loss G: 1.0056 (1.0181) Acc G: 2.933% 
LR: 2.000e-04 

2023-03-02 01:51:48,630 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2247 (0.3920) Acc D Real: 63.819% 
Loss D Fake: 0.4626 (0.4543) Acc D Fake: 97.056% 
Loss D: 0.687 
Loss G: 1.0057 (1.0180) Acc G: 2.964% 
LR: 2.000e-04 

2023-03-02 01:51:48,637 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2774 (0.3911) Acc D Real: 63.954% 
Loss D Fake: 0.4626 (0.4544) Acc D Fake: 97.025% 
Loss D: 0.740 
Loss G: 1.0057 (1.0179) Acc G: 2.995% 
LR: 2.000e-04 

2023-03-02 01:51:48,645 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4049 (0.3912) Acc D Real: 63.995% 
Loss D Fake: 0.4627 (0.4545) Acc D Fake: 96.995% 
Loss D: 0.868 
Loss G: 1.0056 (1.0178) Acc G: 3.025% 
LR: 2.000e-04 

2023-03-02 01:51:48,653 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4271 (0.3915) Acc D Real: 63.983% 
Loss D Fake: 0.4628 (0.4545) Acc D Fake: 96.965% 
Loss D: 0.890 
Loss G: 1.0054 (1.0177) Acc G: 3.055% 
LR: 2.000e-04 

2023-03-02 01:51:48,661 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3056 (0.3908) Acc D Real: 64.086% 
Loss D Fake: 0.4630 (0.4546) Acc D Fake: 96.935% 
Loss D: 0.769 
Loss G: 1.0050 (1.0176) Acc G: 3.084% 
LR: 2.000e-04 

2023-03-02 01:51:48,668 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3390 (0.3904) Acc D Real: 64.183% 
Loss D Fake: 0.4632 (0.4547) Acc D Fake: 96.907% 
Loss D: 0.802 
Loss G: 1.0047 (1.0175) Acc G: 3.112% 
LR: 2.000e-04 

2023-03-02 01:51:48,676 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4288 (0.3907) Acc D Real: 64.195% 
Loss D Fake: 0.4635 (0.4547) Acc D Fake: 96.878% 
Loss D: 0.892 
Loss G: 1.0042 (1.0174) Acc G: 3.141% 
LR: 2.000e-04 

2023-03-02 01:51:48,684 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3952 (0.3907) Acc D Real: 64.220% 
Loss D Fake: 0.4638 (0.4548) Acc D Fake: 96.850% 
Loss D: 0.859 
Loss G: 1.0037 (1.0172) Acc G: 3.168% 
LR: 2.000e-04 

2023-03-02 01:51:48,694 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3459 (0.3903) Acc D Real: 64.312% 
Loss D Fake: 0.4642 (0.4549) Acc D Fake: 96.823% 
Loss D: 0.810 
Loss G: 1.0031 (1.0171) Acc G: 3.196% 
LR: 2.000e-04 

2023-03-02 01:51:48,703 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4903 (0.3911) Acc D Real: 64.276% 
Loss D Fake: 0.4646 (0.4550) Acc D Fake: 96.796% 
Loss D: 0.955 
Loss G: 1.0023 (1.0170) Acc G: 3.223% 
LR: 2.000e-04 

2023-03-02 01:51:48,713 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4500 (0.3916) Acc D Real: 64.224% 
Loss D Fake: 0.4651 (0.4550) Acc D Fake: 96.769% 
Loss D: 0.915 
Loss G: 1.0014 (1.0169) Acc G: 3.249% 
LR: 2.000e-04 

2023-03-02 01:51:48,721 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3272 (0.3911) Acc D Real: 64.319% 
Loss D Fake: 0.4656 (0.4551) Acc D Fake: 96.743% 
Loss D: 0.793 
Loss G: 1.0006 (1.0168) Acc G: 3.275% 
LR: 2.000e-04 

2023-03-02 01:51:48,729 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4261 (0.3913) Acc D Real: 64.356% 
Loss D Fake: 0.4662 (0.4552) Acc D Fake: 96.717% 
Loss D: 0.892 
Loss G: 0.9997 (1.0166) Acc G: 3.301% 
LR: 2.000e-04 

2023-03-02 01:51:48,737 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3141 (0.3908) Acc D Real: 64.451% 
Loss D Fake: 0.4667 (0.4553) Acc D Fake: 96.692% 
Loss D: 0.781 
Loss G: 0.9988 (1.0165) Acc G: 3.326% 
LR: 2.000e-04 

2023-03-02 01:51:48,745 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3663 (0.3906) Acc D Real: 64.524% 
Loss D Fake: 0.4672 (0.4554) Acc D Fake: 96.667% 
Loss D: 0.834 
Loss G: 0.9980 (1.0164) Acc G: 3.351% 
LR: 2.000e-04 

2023-03-02 01:51:48,753 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.2467 (0.3895) Acc D Real: 64.689% 
Loss D Fake: 0.4677 (0.4555) Acc D Fake: 96.642% 
Loss D: 0.714 
Loss G: 0.9974 (1.0162) Acc G: 3.376% 
LR: 2.000e-04 

2023-03-02 01:51:48,760 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3412 (0.3892) Acc D Real: 64.785% 
Loss D Fake: 0.4681 (0.4556) Acc D Fake: 96.618% 
Loss D: 0.809 
Loss G: 0.9968 (1.0161) Acc G: 3.400% 
LR: 2.000e-04 

2023-03-02 01:51:48,768 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2666 (0.3883) Acc D Real: 64.880% 
Loss D Fake: 0.4684 (0.4557) Acc D Fake: 96.594% 
Loss D: 0.735 
Loss G: 0.9963 (1.0159) Acc G: 3.424% 
LR: 2.000e-04 

2023-03-02 01:51:48,775 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3291 (0.3878) Acc D Real: 64.959% 
Loss D Fake: 0.4687 (0.4557) Acc D Fake: 96.570% 
Loss D: 0.798 
Loss G: 0.9959 (1.0158) Acc G: 3.447% 
LR: 2.000e-04 

2023-03-02 01:51:48,783 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3825 (0.3878) Acc D Real: 65.011% 
Loss D Fake: 0.4690 (0.4558) Acc D Fake: 96.547% 
Loss D: 0.852 
Loss G: 0.9954 (1.0157) Acc G: 3.470% 
LR: 2.000e-04 

2023-03-02 01:51:48,790 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3233 (0.3873) Acc D Real: 65.092% 
Loss D Fake: 0.4694 (0.4559) Acc D Fake: 96.524% 
Loss D: 0.793 
Loss G: 0.9950 (1.0155) Acc G: 3.493% 
LR: 2.000e-04 

2023-03-02 01:51:48,798 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3457 (0.3870) Acc D Real: 65.189% 
Loss D Fake: 0.4697 (0.4560) Acc D Fake: 96.489% 
Loss D: 0.815 
Loss G: 0.9945 (1.0154) Acc G: 3.528% 
LR: 2.000e-04 

2023-03-02 01:51:48,805 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2032 (0.3857) Acc D Real: 65.358% 
Loss D Fake: 0.4700 (0.4561) Acc D Fake: 96.455% 
Loss D: 0.673 
Loss G: 0.9941 (1.0152) Acc G: 3.561% 
LR: 2.000e-04 

2023-03-02 01:51:48,813 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3647 (0.3856) Acc D Real: 65.404% 
Loss D Fake: 0.4703 (0.4562) Acc D Fake: 96.422% 
Loss D: 0.835 
Loss G: 0.9937 (1.0151) Acc G: 3.595% 
LR: 2.000e-04 

2023-03-02 01:51:48,820 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3469 (0.3853) Acc D Real: 65.484% 
Loss D Fake: 0.4706 (0.4563) Acc D Fake: 96.389% 
Loss D: 0.818 
Loss G: 0.9932 (1.0149) Acc G: 3.628% 
LR: 2.000e-04 

2023-03-02 01:51:48,828 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2109 (0.3841) Acc D Real: 65.613% 
Loss D Fake: 0.4710 (0.4564) Acc D Fake: 96.356% 
Loss D: 0.682 
Loss G: 0.9927 (1.0148) Acc G: 3.660% 
LR: 2.000e-04 

2023-03-02 01:51:48,835 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3188 (0.3837) Acc D Real: 65.665% 
Loss D Fake: 0.4713 (0.4565) Acc D Fake: 96.324% 
Loss D: 0.790 
Loss G: 0.9923 (1.0146) Acc G: 3.692% 
LR: 2.000e-04 

2023-03-02 01:51:48,843 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.2926 (0.3831) Acc D Real: 65.738% 
Loss D Fake: 0.4716 (0.4566) Acc D Fake: 96.293% 
Loss D: 0.764 
Loss G: 0.9920 (1.0144) Acc G: 3.724% 
LR: 2.000e-04 

2023-03-02 01:51:48,851 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2165 (0.3819) Acc D Real: 65.872% 
Loss D Fake: 0.4718 (0.4567) Acc D Fake: 96.261% 
Loss D: 0.688 
Loss G: 0.9917 (1.0143) Acc G: 3.755% 
LR: 2.000e-04 

2023-03-02 01:51:48,858 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2811 (0.3813) Acc D Real: 65.980% 
Loss D Fake: 0.4720 (0.4568) Acc D Fake: 96.230% 
Loss D: 0.753 
Loss G: 0.9915 (1.0141) Acc G: 3.786% 
LR: 2.000e-04 

2023-03-02 01:51:48,866 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3131 (0.3808) Acc D Real: 66.070% 
Loss D Fake: 0.4723 (0.4569) Acc D Fake: 96.200% 
Loss D: 0.785 
Loss G: 0.9912 (1.0140) Acc G: 3.816% 
LR: 2.000e-04 

2023-03-02 01:51:48,873 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3487 (0.3806) Acc D Real: 66.127% 
Loss D Fake: 0.4726 (0.4571) Acc D Fake: 96.170% 
Loss D: 0.821 
Loss G: 0.9907 (1.0138) Acc G: 3.846% 
LR: 2.000e-04 

2023-03-02 01:51:48,881 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3652 (0.3805) Acc D Real: 66.172% 
Loss D Fake: 0.4730 (0.4572) Acc D Fake: 96.140% 
Loss D: 0.838 
Loss G: 0.9901 (1.0137) Acc G: 3.875% 
LR: 2.000e-04 

2023-03-02 01:51:48,889 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.2748 (0.3798) Acc D Real: 66.254% 
Loss D Fake: 0.4734 (0.4573) Acc D Fake: 96.111% 
Loss D: 0.748 
Loss G: 0.9895 (1.0135) Acc G: 3.905% 
LR: 2.000e-04 

2023-03-02 01:51:48,896 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.2890 (0.3792) Acc D Real: 66.338% 
Loss D Fake: 0.4739 (0.4574) Acc D Fake: 96.082% 
Loss D: 0.763 
Loss G: 0.9889 (1.0134) Acc G: 3.933% 
LR: 2.000e-04 

2023-03-02 01:51:48,903 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4018 (0.3794) Acc D Real: 66.343% 
Loss D Fake: 0.4743 (0.4575) Acc D Fake: 96.054% 
Loss D: 0.876 
Loss G: 0.9882 (1.0132) Acc G: 3.962% 
LR: 2.000e-04 

2023-03-02 01:51:48,911 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3253 (0.3790) Acc D Real: 66.418% 
Loss D Fake: 0.4749 (0.4576) Acc D Fake: 96.026% 
Loss D: 0.800 
Loss G: 0.9873 (1.0130) Acc G: 3.990% 
LR: 2.000e-04 

2023-03-02 01:51:48,918 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2746 (0.3783) Acc D Real: 66.501% 
Loss D Fake: 0.4756 (0.4577) Acc D Fake: 95.998% 
Loss D: 0.750 
Loss G: 0.9863 (1.0129) Acc G: 4.017% 
LR: 2.000e-04 

2023-03-02 01:51:48,925 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2599 (0.3776) Acc D Real: 66.516% 
Loss D Fake: 0.4763 (0.4578) Acc D Fake: 95.995% 
Loss D: 0.736 
Loss G: 0.9853 (1.0127) Acc G: 4.020% 
LR: 2.000e-04 

2023-03-02 01:51:49,162 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.521 | Generator Loss: 0.984 | Avg: 1.505 
2023-03-02 01:51:49,186 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.556 | Generator Loss: 0.984 | Avg: 1.540 
2023-03-02 01:51:49,208 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.548 | Generator Loss: 0.984 | Avg: 1.532 
2023-03-02 01:51:49,234 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.538 | Generator Loss: 0.984 | Avg: 1.523 
2023-03-02 01:51:49,260 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.537 | Generator Loss: 0.984 | Avg: 1.521 
2023-03-02 01:51:49,286 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.597 | Generator Loss: 0.984 | Avg: 1.581 
2023-03-02 01:51:49,312 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.600 | Generator Loss: 0.984 | Avg: 1.584 
2023-03-02 01:51:49,338 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.655 | Generator Loss: 0.984 | Avg: 1.639 
2023-03-02 01:51:49,364 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.670 | Generator Loss: 0.984 | Avg: 1.654 
2023-03-02 01:51:49,394 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.726 | Generator Loss: 0.984 | Avg: 1.710 
2023-03-02 01:51:49,420 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.764 | Generator Loss: 0.984 | Avg: 1.748 
2023-03-02 01:51:49,445 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.794 | Generator Loss: 0.984 | Avg: 1.778 
2023-03-02 01:51:49,471 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.824 | Generator Loss: 0.984 | Avg: 1.809 
2023-03-02 01:51:49,498 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.829 | Generator Loss: 0.984 | Avg: 1.814 
2023-03-02 01:51:49,525 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.811 | Generator Loss: 0.984 | Avg: 1.796 
2023-03-02 01:51:49,551 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.795 | Generator Loss: 0.984 | Avg: 1.779 
2023-03-02 01:51:49,577 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.780 | Generator Loss: 0.984 | Avg: 1.764 
2023-03-02 01:51:49,610 -                train: [    INFO] - 
Epoch: 7/20
2023-03-02 01:51:49,795 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3742 (0.3410) Acc D Real: 74.036% 
Loss D Fake: 0.4778 (0.4774) Acc D Fake: 90.990% 
Loss D: 0.852 
Loss G: 0.9831 (0.9837) Acc G: 9.141% 
LR: 2.000e-04 

2023-03-02 01:51:49,803 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2703 (0.3174) Acc D Real: 76.562% 
Loss D Fake: 0.4786 (0.4778) Acc D Fake: 90.660% 
Loss D: 0.749 
Loss G: 0.9818 (0.9831) Acc G: 9.427% 
LR: 2.000e-04 

2023-03-02 01:51:49,812 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2033 (0.2889) Acc D Real: 79.245% 
Loss D Fake: 0.4795 (0.4782) Acc D Fake: 90.495% 
Loss D: 0.683 
Loss G: 0.9806 (0.9824) Acc G: 9.570% 
LR: 2.000e-04 

2023-03-02 01:51:49,828 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3895 (0.3090) Acc D Real: 77.073% 
Loss D Fake: 0.4805 (0.4787) Acc D Fake: 90.396% 
Loss D: 0.870 
Loss G: 0.9791 (0.9818) Acc G: 9.656% 
LR: 2.000e-04 

2023-03-02 01:51:49,835 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3328 (0.3130) Acc D Real: 76.372% 
Loss D Fake: 0.4817 (0.4792) Acc D Fake: 90.330% 
Loss D: 0.814 
Loss G: 0.9768 (0.9809) Acc G: 9.714% 
LR: 2.000e-04 

2023-03-02 01:51:49,842 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2464 (0.3035) Acc D Real: 77.574% 
Loss D Fake: 0.4834 (0.4798) Acc D Fake: 90.283% 
Loss D: 0.730 
Loss G: 0.9741 (0.9800) Acc G: 9.754% 
LR: 2.000e-04 

2023-03-02 01:51:49,850 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2479 (0.2965) Acc D Real: 78.158% 
Loss D Fake: 0.4853 (0.4805) Acc D Fake: 90.247% 
Loss D: 0.733 
Loss G: 0.9713 (0.9789) Acc G: 9.785% 
LR: 2.000e-04 

2023-03-02 01:51:49,857 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3058 (0.2976) Acc D Real: 78.177% 
Loss D Fake: 0.4872 (0.4812) Acc D Fake: 90.220% 
Loss D: 0.793 
Loss G: 0.9684 (0.9777) Acc G: 9.809% 
LR: 2.000e-04 

2023-03-02 01:51:49,864 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3135 (0.2991) Acc D Real: 78.167% 
Loss D Fake: 0.4893 (0.4820) Acc D Fake: 90.198% 
Loss D: 0.803 
Loss G: 0.9651 (0.9764) Acc G: 9.828% 
LR: 2.000e-04 

2023-03-02 01:51:49,871 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3770 (0.3062) Acc D Real: 77.727% 
Loss D Fake: 0.4917 (0.4829) Acc D Fake: 90.028% 
Loss D: 0.869 
Loss G: 0.9613 (0.9751) Acc G: 9.995% 
LR: 2.000e-04 

2023-03-02 01:51:49,878 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3824 (0.3126) Acc D Real: 77.005% 
Loss D Fake: 0.4945 (0.4839) Acc D Fake: 89.887% 
Loss D: 0.877 
Loss G: 0.9570 (0.9736) Acc G: 10.135% 
LR: 2.000e-04 

2023-03-02 01:51:49,885 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4144 (0.3204) Acc D Real: 75.974% 
Loss D Fake: 0.4977 (0.4849) Acc D Fake: 89.768% 
Loss D: 0.912 
Loss G: 0.9522 (0.9719) Acc G: 10.252% 
LR: 2.000e-04 

2023-03-02 01:51:49,891 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2261 (0.3137) Acc D Real: 76.626% 
Loss D Fake: 0.5016 (0.4861) Acc D Fake: 89.665% 
Loss D: 0.728 
Loss G: 0.9465 (0.9701) Acc G: 10.353% 
LR: 2.000e-04 

2023-03-02 01:51:49,898 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3154 (0.3138) Acc D Real: 76.705% 
Loss D Fake: 0.5063 (0.4875) Acc D Fake: 89.517% 
Loss D: 0.822 
Loss G: 0.9396 (0.9681) Acc G: 10.528% 
LR: 2.000e-04 

2023-03-02 01:51:49,905 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4160 (0.3202) Acc D Real: 76.146% 
Loss D Fake: 0.5126 (0.4890) Acc D Fake: 89.339% 
Loss D: 0.929 
Loss G: 0.9299 (0.9657) Acc G: 10.703% 
LR: 2.000e-04 

2023-03-02 01:51:49,913 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2582 (0.3165) Acc D Real: 76.688% 
Loss D Fake: 0.5217 (0.4910) Acc D Fake: 89.182% 
Loss D: 0.780 
Loss G: 0.9163 (0.9628) Acc G: 10.861% 
LR: 2.000e-04 

2023-03-02 01:51:49,920 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3180 (0.3166) Acc D Real: 76.704% 
Loss D Fake: 0.5361 (0.4935) Acc D Fake: 88.950% 
Loss D: 0.854 
Loss G: 0.8950 (0.9590) Acc G: 11.091% 
LR: 2.000e-04 

2023-03-02 01:51:49,927 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3352 (0.3176) Acc D Real: 76.414% 
Loss D Fake: 0.5613 (0.4970) Acc D Fake: 88.566% 
Loss D: 0.896 
Loss G: 0.8589 (0.9537) Acc G: 11.472% 
LR: 2.000e-04 

2023-03-02 01:51:49,933 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2217 (0.3128) Acc D Real: 76.852% 
Loss D Fake: 0.6103 (0.5027) Acc D Fake: 87.549% 
Loss D: 0.832 
Loss G: 0.7948 (0.9458) Acc G: 12.482% 
LR: 2.000e-04 

2023-03-02 01:51:49,940 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2679 (0.3107) Acc D Real: 77.054% 
Loss D Fake: 4.8658 (0.7105) Acc D Fake: 83.380% 
Loss D: 5.134 
Loss G: 0.2296 (0.9117) Acc G: 16.649% 
LR: 2.000e-04 

2023-03-02 01:51:49,947 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3178 (0.3110) Acc D Real: 77.043% 
Loss D Fake: 5.5871 (0.9321) Acc D Fake: 79.590% 
Loss D: 5.905 
Loss G: 0.1703 (0.8780) Acc G: 20.438% 
LR: 2.000e-04 

2023-03-02 01:51:49,955 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.2611 (0.3088) Acc D Real: 77.409% 
Loss D Fake: 5.8057 (1.1440) Acc D Fake: 76.130% 
Loss D: 6.067 
Loss G: 0.1500 (0.8463) Acc G: 23.897% 
LR: 2.000e-04 

2023-03-02 01:51:49,963 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3664 (0.3112) Acc D Real: 77.101% 
Loss D Fake: 5.8920 (1.3419) Acc D Fake: 72.958% 
Loss D: 6.258 
Loss G: 0.1389 (0.8169) Acc G: 27.068% 
LR: 2.000e-04 

2023-03-02 01:51:49,971 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3215 (0.3116) Acc D Real: 76.975% 
Loss D Fake: 5.9159 (1.5248) Acc D Fake: 70.040% 
Loss D: 6.237 
Loss G: 0.1318 (0.7895) Acc G: 29.985% 
LR: 2.000e-04 

2023-03-02 01:51:49,978 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.2411 (0.3089) Acc D Real: 77.192% 
Loss D Fake: 5.9042 (1.6933) Acc D Fake: 67.346% 
Loss D: 6.145 
Loss G: 0.1270 (0.7640) Acc G: 32.678% 
LR: 2.000e-04 

2023-03-02 01:51:49,985 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4254 (0.3132) Acc D Real: 76.757% 
Loss D Fake: 5.8703 (1.8480) Acc D Fake: 64.851% 
Loss D: 6.296 
Loss G: 0.1235 (0.7403) Acc G: 35.172% 
LR: 2.000e-04 

2023-03-02 01:51:49,992 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3232 (0.3136) Acc D Real: 76.745% 
Loss D Fake: 5.8221 (1.9899) Acc D Fake: 62.535% 
Loss D: 6.145 
Loss G: 0.1208 (0.7181) Acc G: 37.487% 
LR: 2.000e-04 

2023-03-02 01:51:49,999 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3408 (0.3145) Acc D Real: 76.613% 
Loss D Fake: 5.7642 (2.1200) Acc D Fake: 60.379% 
Loss D: 6.105 
Loss G: 0.1188 (0.6975) Acc G: 39.643% 
LR: 2.000e-04 

2023-03-02 01:51:50,007 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.2313 (0.3117) Acc D Real: 76.788% 
Loss D Fake: 5.6995 (2.2394) Acc D Fake: 58.366% 
Loss D: 5.931 
Loss G: 0.1173 (0.6781) Acc G: 41.655% 
LR: 2.000e-04 

2023-03-02 01:51:50,014 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2352 (0.3093) Acc D Real: 77.088% 
Loss D Fake: 5.6300 (2.3487) Acc D Fake: 56.484% 
Loss D: 5.865 
Loss G: 0.1161 (0.6600) Acc G: 43.537% 
LR: 2.000e-04 

2023-03-02 01:51:50,021 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3239 (0.3097) Acc D Real: 77.030% 
Loss D Fake: 5.5571 (2.4490) Acc D Fake: 54.718% 
Loss D: 5.881 
Loss G: 0.1151 (0.6430) Acc G: 45.301% 
LR: 2.000e-04 

2023-03-02 01:51:50,029 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2869 (0.3090) Acc D Real: 77.208% 
Loss D Fake: 5.4818 (2.5409) Acc D Fake: 53.060% 
Loss D: 5.769 
Loss G: 0.1144 (0.6270) Acc G: 46.959% 
LR: 2.000e-04 

2023-03-02 01:51:50,036 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3634 (0.3106) Acc D Real: 77.132% 
Loss D Fake: 5.4049 (2.6251) Acc D Fake: 51.500% 
Loss D: 5.768 
Loss G: 0.1138 (0.6119) Acc G: 48.519% 
LR: 2.000e-04 

2023-03-02 01:51:50,043 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2262 (0.3082) Acc D Real: 77.388% 
Loss D Fake: 5.3269 (2.7023) Acc D Fake: 50.028% 
Loss D: 5.553 
Loss G: 0.1134 (0.5976) Acc G: 49.990% 
LR: 2.000e-04 

2023-03-02 01:51:50,051 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3294 (0.3088) Acc D Real: 77.367% 
Loss D Fake: 5.2480 (2.7730) Acc D Fake: 48.639% 
Loss D: 5.577 
Loss G: 0.1131 (0.5842) Acc G: 51.379% 
LR: 2.000e-04 

2023-03-02 01:51:50,058 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3527 (0.3100) Acc D Real: 77.242% 
Loss D Fake: 5.1686 (2.8378) Acc D Fake: 47.324% 
Loss D: 5.521 
Loss G: 0.1129 (0.5714) Acc G: 52.693% 
LR: 2.000e-04 

2023-03-02 01:51:50,066 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3260 (0.3104) Acc D Real: 77.233% 
Loss D Fake: 5.0889 (2.8970) Acc D Fake: 46.079% 
Loss D: 5.415 
Loss G: 0.1128 (0.5594) Acc G: 53.938% 
LR: 2.000e-04 

2023-03-02 01:51:50,073 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2560 (0.3090) Acc D Real: 77.408% 
Loss D Fake: 5.0093 (2.9512) Acc D Fake: 44.897% 
Loss D: 5.265 
Loss G: 0.1127 (0.5479) Acc G: 55.119% 
LR: 2.000e-04 

2023-03-02 01:51:50,080 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3462 (0.3100) Acc D Real: 77.279% 
Loss D Fake: 4.9297 (3.0007) Acc D Fake: 43.775% 
Loss D: 5.276 
Loss G: 0.1127 (0.5370) Acc G: 56.241% 
LR: 2.000e-04 

2023-03-02 01:51:50,088 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3151 (0.3101) Acc D Real: 77.353% 
Loss D Fake: 4.8505 (3.0458) Acc D Fake: 42.707% 
Loss D: 5.166 
Loss G: 0.1128 (0.5267) Acc G: 57.308% 
LR: 2.000e-04 

2023-03-02 01:51:50,095 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3922 (0.3120) Acc D Real: 77.176% 
Loss D Fake: 4.7720 (3.0869) Acc D Fake: 41.690% 
Loss D: 5.164 
Loss G: 0.1129 (0.5168) Acc G: 58.325% 
LR: 2.000e-04 

2023-03-02 01:51:50,103 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2676 (0.3110) Acc D Real: 77.350% 
Loss D Fake: 4.6943 (3.1243) Acc D Fake: 40.721% 
Loss D: 4.962 
Loss G: 0.1131 (0.5074) Acc G: 59.294% 
LR: 2.000e-04 

2023-03-02 01:51:50,110 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3237 (0.3113) Acc D Real: 77.430% 
Loss D Fake: 4.6176 (3.1582) Acc D Fake: 39.795% 
Loss D: 4.941 
Loss G: 0.1133 (0.4985) Acc G: 60.219% 
LR: 2.000e-04 

2023-03-02 01:51:50,117 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2178 (0.3092) Acc D Real: 77.625% 
Loss D Fake: 4.5421 (3.1889) Acc D Fake: 38.911% 
Loss D: 4.760 
Loss G: 0.1136 (0.4899) Acc G: 61.103% 
LR: 2.000e-04 

2023-03-02 01:51:50,124 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2461 (0.3078) Acc D Real: 77.741% 
Loss D Fake: 4.4675 (3.2167) Acc D Fake: 38.065% 
Loss D: 4.714 
Loss G: 0.1139 (0.4817) Acc G: 61.949% 
LR: 2.000e-04 

2023-03-02 01:51:50,132 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2735 (0.3071) Acc D Real: 77.930% 
Loss D Fake: 4.3942 (3.2418) Acc D Fake: 37.255% 
Loss D: 4.668 
Loss G: 0.1142 (0.4739) Acc G: 62.758% 
LR: 2.000e-04 

2023-03-02 01:51:50,141 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3072 (0.3071) Acc D Real: 77.859% 
Loss D Fake: 4.3223 (3.2643) Acc D Fake: 36.479% 
Loss D: 4.629 
Loss G: 0.1146 (0.4664) Acc G: 63.534% 
LR: 2.000e-04 

2023-03-02 01:51:50,148 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3278 (0.3075) Acc D Real: 77.883% 
Loss D Fake: 4.2515 (3.2844) Acc D Fake: 35.734% 
Loss D: 4.579 
Loss G: 0.1150 (0.4593) Acc G: 64.278% 
LR: 2.000e-04 

2023-03-02 01:51:50,156 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4068 (0.3095) Acc D Real: 77.802% 
Loss D Fake: 4.1821 (3.3024) Acc D Fake: 35.020% 
Loss D: 4.589 
Loss G: 0.1154 (0.4524) Acc G: 64.993% 
LR: 2.000e-04 

2023-03-02 01:51:50,163 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2734 (0.3088) Acc D Real: 77.925% 
Loss D Fake: 4.1140 (3.3183) Acc D Fake: 34.333% 
Loss D: 4.387 
Loss G: 0.1159 (0.4458) Acc G: 65.679% 
LR: 2.000e-04 

2023-03-02 01:51:50,170 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3471 (0.3095) Acc D Real: 77.887% 
Loss D Fake: 4.0470 (3.3323) Acc D Fake: 33.673% 
Loss D: 4.394 
Loss G: 0.1165 (0.4395) Acc G: 66.339% 
LR: 2.000e-04 

2023-03-02 01:51:50,179 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2773 (0.3089) Acc D Real: 77.975% 
Loss D Fake: 3.9812 (3.3446) Acc D Fake: 33.038% 
Loss D: 4.258 
Loss G: 0.1170 (0.4334) Acc G: 66.974% 
LR: 2.000e-04 

2023-03-02 01:51:50,186 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2568 (0.3080) Acc D Real: 78.143% 
Loss D Fake: 3.9164 (3.3552) Acc D Fake: 32.426% 
Loss D: 4.173 
Loss G: 0.1176 (0.4275) Acc G: 67.586% 
LR: 2.000e-04 

2023-03-02 01:51:50,194 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3190 (0.3082) Acc D Real: 78.095% 
Loss D Fake: 3.8527 (3.3642) Acc D Fake: 31.836% 
Loss D: 4.172 
Loss G: 0.1183 (0.4219) Acc G: 68.175% 
LR: 2.000e-04 

2023-03-02 01:51:50,202 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3791 (0.3094) Acc D Real: 78.078% 
Loss D Fake: 3.7899 (3.3718) Acc D Fake: 31.268% 
Loss D: 4.169 
Loss G: 0.1190 (0.4165) Acc G: 68.743% 
LR: 2.000e-04 

2023-03-02 01:51:50,210 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2850 (0.3090) Acc D Real: 78.166% 
Loss D Fake: 3.7283 (3.3781) Acc D Fake: 30.719% 
Loss D: 4.013 
Loss G: 0.1198 (0.4113) Acc G: 69.292% 
LR: 2.000e-04 

2023-03-02 01:51:50,218 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3303 (0.3094) Acc D Real: 78.240% 
Loss D Fake: 3.6676 (3.3831) Acc D Fake: 30.189% 
Loss D: 3.998 
Loss G: 0.1205 (0.4063) Acc G: 69.821% 
LR: 2.000e-04 

2023-03-02 01:51:50,226 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.2467 (0.3083) Acc D Real: 78.358% 
Loss D Fake: 3.6080 (3.3869) Acc D Fake: 29.678% 
Loss D: 3.855 
Loss G: 0.1214 (0.4015) Acc G: 70.305% 
LR: 2.000e-04 

2023-03-02 01:51:50,234 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3430 (0.3089) Acc D Real: 78.436% 
Loss D Fake: 3.5492 (3.3896) Acc D Fake: 29.211% 
Loss D: 3.892 
Loss G: 0.1222 (0.3968) Acc G: 70.772% 
LR: 2.000e-04 

2023-03-02 01:51:50,241 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3245 (0.3092) Acc D Real: 78.356% 
Loss D Fake: 3.4912 (3.3912) Acc D Fake: 28.759% 
Loss D: 3.816 
Loss G: 0.1232 (0.3923) Acc G: 71.224% 
LR: 2.000e-04 

2023-03-02 01:51:50,248 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2633 (0.3084) Acc D Real: 78.495% 
Loss D Fake: 3.4340 (3.3919) Acc D Fake: 28.322% 
Loss D: 3.697 
Loss G: 0.1242 (0.3880) Acc G: 71.661% 
LR: 2.000e-04 

2023-03-02 01:51:50,256 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2896 (0.3081) Acc D Real: 78.589% 
Loss D Fake: 3.3777 (3.3917) Acc D Fake: 27.899% 
Loss D: 3.667 
Loss G: 0.1252 (0.3838) Acc G: 72.084% 
LR: 2.000e-04 

2023-03-02 01:51:50,263 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2524 (0.3072) Acc D Real: 78.709% 
Loss D Fake: 3.3221 (3.3906) Acc D Fake: 27.489% 
Loss D: 3.575 
Loss G: 0.1263 (0.3798) Acc G: 72.494% 
LR: 2.000e-04 

2023-03-02 01:51:50,271 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3798 (0.3084) Acc D Real: 78.639% 
Loss D Fake: 3.2673 (3.3887) Acc D Fake: 27.092% 
Loss D: 3.647 
Loss G: 0.1275 (0.3759) Acc G: 72.892% 
LR: 2.000e-04 

2023-03-02 01:51:50,278 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2480 (0.3074) Acc D Real: 78.716% 
Loss D Fake: 3.2132 (3.3861) Acc D Fake: 26.707% 
Loss D: 3.461 
Loss G: 0.1287 (0.3722) Acc G: 73.277% 
LR: 2.000e-04 

2023-03-02 01:51:50,285 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2342 (0.3064) Acc D Real: 78.812% 
Loss D Fake: 3.1598 (3.3827) Acc D Fake: 26.333% 
Loss D: 3.394 
Loss G: 0.1301 (0.3686) Acc G: 73.651% 
LR: 2.000e-04 

2023-03-02 01:51:50,293 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3295 (0.3067) Acc D Real: 78.858% 
Loss D Fake: 3.1070 (3.3786) Acc D Fake: 25.970% 
Loss D: 3.436 
Loss G: 0.1315 (0.3651) Acc G: 74.014% 
LR: 2.000e-04 

2023-03-02 01:51:50,300 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3032 (0.3066) Acc D Real: 78.817% 
Loss D Fake: 3.0551 (3.3739) Acc D Fake: 25.618% 
Loss D: 3.358 
Loss G: 0.1329 (0.3617) Acc G: 74.367% 
LR: 2.000e-04 

2023-03-02 01:51:50,308 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3426 (0.3072) Acc D Real: 78.826% 
Loss D Fake: 3.0039 (3.3687) Acc D Fake: 25.276% 
Loss D: 3.346 
Loss G: 0.1345 (0.3585) Acc G: 74.709% 
LR: 2.000e-04 

2023-03-02 01:51:50,316 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3392 (0.3076) Acc D Real: 78.869% 
Loss D Fake: 2.9535 (3.3628) Acc D Fake: 24.944% 
Loss D: 3.293 
Loss G: 0.1361 (0.3553) Acc G: 75.042% 
LR: 2.000e-04 

2023-03-02 01:51:50,323 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3241 (0.3078) Acc D Real: 78.877% 
Loss D Fake: 2.9039 (3.3564) Acc D Fake: 24.620% 
Loss D: 3.228 
Loss G: 0.1378 (0.3523) Acc G: 75.365% 
LR: 2.000e-04 

2023-03-02 01:51:50,330 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.2917 (0.3076) Acc D Real: 78.901% 
Loss D Fake: 2.8550 (3.3496) Acc D Fake: 24.306% 
Loss D: 3.147 
Loss G: 0.1396 (0.3494) Acc G: 75.657% 
LR: 2.000e-04 

2023-03-02 01:51:50,338 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3691 (0.3084) Acc D Real: 78.765% 
Loss D Fake: 2.8069 (3.3422) Acc D Fake: 24.022% 
Loss D: 3.176 
Loss G: 0.1414 (0.3466) Acc G: 75.941% 
LR: 2.000e-04 

2023-03-02 01:51:50,345 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3941 (0.3096) Acc D Real: 78.689% 
Loss D Fake: 2.7593 (3.3345) Acc D Fake: 23.747% 
Loss D: 3.153 
Loss G: 0.1434 (0.3439) Acc G: 76.217% 
LR: 2.000e-04 

2023-03-02 01:51:50,352 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2889 (0.3093) Acc D Real: 78.732% 
Loss D Fake: 2.7126 (3.3263) Acc D Fake: 23.478% 
Loss D: 3.002 
Loss G: 0.1455 (0.3413) Acc G: 76.486% 
LR: 2.000e-04 

2023-03-02 01:51:50,360 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2935 (0.3091) Acc D Real: 78.825% 
Loss D Fake: 2.6666 (3.3177) Acc D Fake: 23.216% 
Loss D: 2.960 
Loss G: 0.1476 (0.3387) Acc G: 76.749% 
LR: 2.000e-04 

2023-03-02 01:51:50,368 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3111 (0.3091) Acc D Real: 78.858% 
Loss D Fake: 2.6214 (3.3088) Acc D Fake: 22.961% 
Loss D: 2.932 
Loss G: 0.1499 (0.3363) Acc G: 77.004% 
LR: 2.000e-04 

2023-03-02 01:51:50,376 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3057 (0.3091) Acc D Real: 78.905% 
Loss D Fake: 2.5770 (3.2995) Acc D Fake: 22.713% 
Loss D: 2.883 
Loss G: 0.1523 (0.3340) Acc G: 77.253% 
LR: 2.000e-04 

2023-03-02 01:51:50,383 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3213 (0.3092) Acc D Real: 78.954% 
Loss D Fake: 2.5333 (3.2899) Acc D Fake: 22.471% 
Loss D: 2.855 
Loss G: 0.1547 (0.3318) Acc G: 77.495% 
LR: 2.000e-04 

2023-03-02 01:51:50,391 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3468 (0.3097) Acc D Real: 78.946% 
Loss D Fake: 2.4903 (3.2801) Acc D Fake: 22.234% 
Loss D: 2.837 
Loss G: 0.1573 (0.3296) Acc G: 77.732% 
LR: 2.000e-04 

2023-03-02 01:51:50,398 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2754 (0.3093) Acc D Real: 79.025% 
Loss D Fake: 2.4480 (3.2699) Acc D Fake: 22.004% 
Loss D: 2.723 
Loss G: 0.1599 (0.3275) Acc G: 77.963% 
LR: 2.000e-04 

2023-03-02 01:51:50,405 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3017 (0.3092) Acc D Real: 79.110% 
Loss D Fake: 2.4065 (3.2595) Acc D Fake: 21.779% 
Loss D: 2.708 
Loss G: 0.1627 (0.3255) Acc G: 78.188% 
LR: 2.000e-04 

2023-03-02 01:51:50,413 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3530 (0.3097) Acc D Real: 79.084% 
Loss D Fake: 2.3657 (3.2489) Acc D Fake: 21.579% 
Loss D: 2.719 
Loss G: 0.1656 (0.3236) Acc G: 78.389% 
LR: 2.000e-04 

2023-03-02 01:51:50,420 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3317 (0.3100) Acc D Real: 79.133% 
Loss D Fake: 2.3255 (3.2380) Acc D Fake: 21.384% 
Loss D: 2.657 
Loss G: 0.1686 (0.3218) Acc G: 78.584% 
LR: 2.000e-04 

2023-03-02 01:51:50,428 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3640 (0.3106) Acc D Real: 79.139% 
Loss D Fake: 2.2860 (3.2269) Acc D Fake: 21.194% 
Loss D: 2.650 
Loss G: 0.1717 (0.3201) Acc G: 78.775% 
LR: 2.000e-04 

2023-03-02 01:51:50,437 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3520 (0.3111) Acc D Real: 79.125% 
Loss D Fake: 2.2471 (3.2157) Acc D Fake: 21.008% 
Loss D: 2.599 
Loss G: 0.1749 (0.3184) Acc G: 78.961% 
LR: 2.000e-04 

2023-03-02 01:51:50,444 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4386 (0.3125) Acc D Real: 78.986% 
Loss D Fake: 2.2088 (3.2042) Acc D Fake: 20.826% 
Loss D: 2.647 
Loss G: 0.1783 (0.3168) Acc G: 79.144% 
LR: 2.000e-04 

2023-03-02 01:51:50,452 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3226 (0.3126) Acc D Real: 79.029% 
Loss D Fake: 2.1710 (3.1926) Acc D Fake: 20.648% 
Loss D: 2.494 
Loss G: 0.1817 (0.3153) Acc G: 79.322% 
LR: 2.000e-04 

2023-03-02 01:51:50,460 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3601 (0.3132) Acc D Real: 79.040% 
Loss D Fake: 2.1339 (3.1809) Acc D Fake: 20.474% 
Loss D: 2.494 
Loss G: 0.1853 (0.3138) Acc G: 79.496% 
LR: 2.000e-04 

2023-03-02 01:51:50,468 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4093 (0.3142) Acc D Real: 78.977% 
Loss D Fake: 2.0975 (3.1690) Acc D Fake: 20.304% 
Loss D: 2.507 
Loss G: 0.1890 (0.3125) Acc G: 79.666% 
LR: 2.000e-04 

2023-03-02 01:51:50,475 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3098 (0.3142) Acc D Real: 79.057% 
Loss D Fake: 2.0617 (3.1569) Acc D Fake: 20.138% 
Loss D: 2.371 
Loss G: 0.1928 (0.3112) Acc G: 79.833% 
LR: 2.000e-04 

2023-03-02 01:51:50,483 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3177 (0.3142) Acc D Real: 79.075% 
Loss D Fake: 2.0267 (3.1448) Acc D Fake: 19.975% 
Loss D: 2.344 
Loss G: 0.1967 (0.3099) Acc G: 79.996% 
LR: 2.000e-04 

2023-03-02 01:51:50,490 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3515 (0.3146) Acc D Real: 79.091% 
Loss D Fake: 1.9924 (3.1325) Acc D Fake: 19.815% 
Loss D: 2.344 
Loss G: 0.2008 (0.3088) Acc G: 80.156% 
LR: 2.000e-04 

2023-03-02 01:51:50,497 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3579 (0.3151) Acc D Real: 79.131% 
Loss D Fake: 1.9587 (3.1202) Acc D Fake: 19.660% 
Loss D: 2.317 
Loss G: 0.2049 (0.3077) Acc G: 80.312% 
LR: 2.000e-04 

2023-03-02 01:51:50,504 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3327 (0.3153) Acc D Real: 79.158% 
Loss D Fake: 1.9257 (3.1077) Acc D Fake: 19.507% 
Loss D: 2.258 
Loss G: 0.2092 (0.3067) Acc G: 80.465% 
LR: 2.000e-04 

2023-03-02 01:51:50,512 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4079 (0.3162) Acc D Real: 79.115% 
Loss D Fake: 1.8934 (3.0952) Acc D Fake: 19.370% 
Loss D: 2.301 
Loss G: 0.2135 (0.3057) Acc G: 80.598% 
LR: 2.000e-04 

2023-03-02 01:51:50,520 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3637 (0.3167) Acc D Real: 79.144% 
Loss D Fake: 1.8617 (3.0826) Acc D Fake: 19.240% 
Loss D: 2.225 
Loss G: 0.2180 (0.3048) Acc G: 80.728% 
LR: 2.000e-04 

2023-03-02 01:51:50,528 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3413 (0.3169) Acc D Real: 79.192% 
Loss D Fake: 1.8307 (3.0700) Acc D Fake: 19.113% 
Loss D: 2.172 
Loss G: 0.2226 (0.3040) Acc G: 80.855% 
LR: 2.000e-04 

2023-03-02 01:51:50,536 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3734 (0.3175) Acc D Real: 79.236% 
Loss D Fake: 1.8004 (3.0573) Acc D Fake: 18.989% 
Loss D: 2.174 
Loss G: 0.2273 (0.3032) Acc G: 80.980% 
LR: 2.000e-04 

2023-03-02 01:51:50,544 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4040 (0.3184) Acc D Real: 79.169% 
Loss D Fake: 1.7706 (3.0445) Acc D Fake: 18.867% 
Loss D: 2.175 
Loss G: 0.2321 (0.3025) Acc G: 81.102% 
LR: 2.000e-04 

2023-03-02 01:51:50,552 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3835 (0.3190) Acc D Real: 79.160% 
Loss D Fake: 1.7415 (3.0318) Acc D Fake: 18.747% 
Loss D: 2.125 
Loss G: 0.2370 (0.3019) Acc G: 81.222% 
LR: 2.000e-04 

2023-03-02 01:51:50,560 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4537 (0.3203) Acc D Real: 79.144% 
Loss D Fake: 1.7128 (3.0189) Acc D Fake: 18.630% 
Loss D: 2.167 
Loss G: 0.2420 (0.3013) Acc G: 81.340% 
LR: 2.000e-04 

2023-03-02 01:51:50,567 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3362 (0.3205) Acc D Real: 79.204% 
Loss D Fake: 1.6846 (3.0061) Acc D Fake: 18.515% 
Loss D: 2.021 
Loss G: 0.2471 (0.3008) Acc G: 81.455% 
LR: 2.000e-04 

2023-03-02 01:51:50,575 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3857 (0.3211) Acc D Real: 79.250% 
Loss D Fake: 1.6572 (2.9933) Acc D Fake: 18.402% 
Loss D: 2.043 
Loss G: 0.2523 (0.3003) Acc G: 81.568% 
LR: 2.000e-04 

2023-03-02 01:51:50,583 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3739 (0.3216) Acc D Real: 79.303% 
Loss D Fake: 1.6305 (2.9804) Acc D Fake: 18.291% 
Loss D: 2.004 
Loss G: 0.2576 (0.2999) Acc G: 81.679% 
LR: 2.000e-04 

2023-03-02 01:51:50,590 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3600 (0.3219) Acc D Real: 79.354% 
Loss D Fake: 1.6045 (2.9676) Acc D Fake: 18.182% 
Loss D: 1.964 
Loss G: 0.2630 (0.2996) Acc G: 81.788% 
LR: 2.000e-04 

2023-03-02 01:51:50,598 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4027 (0.3227) Acc D Real: 79.338% 
Loss D Fake: 1.5791 (2.9547) Acc D Fake: 18.076% 
Loss D: 1.982 
Loss G: 0.2684 (0.2993) Acc G: 81.895% 
LR: 2.000e-04 

2023-03-02 01:51:50,606 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4564 (0.3239) Acc D Real: 79.232% 
Loss D Fake: 1.5544 (2.9419) Acc D Fake: 17.971% 
Loss D: 2.011 
Loss G: 0.2739 (0.2990) Acc G: 82.000% 
LR: 2.000e-04 

2023-03-02 01:51:50,613 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3897 (0.3245) Acc D Real: 79.253% 
Loss D Fake: 1.5301 (2.9290) Acc D Fake: 17.868% 
Loss D: 1.920 
Loss G: 0.2795 (0.2989) Acc G: 82.103% 
LR: 2.000e-04 

2023-03-02 01:51:50,621 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3986 (0.3252) Acc D Real: 79.280% 
Loss D Fake: 1.5065 (2.9162) Acc D Fake: 17.767% 
Loss D: 1.905 
Loss G: 0.2851 (0.2987) Acc G: 82.204% 
LR: 2.000e-04 

2023-03-02 01:51:50,629 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3995 (0.3258) Acc D Real: 79.286% 
Loss D Fake: 1.4836 (2.9034) Acc D Fake: 17.668% 
Loss D: 1.883 
Loss G: 0.2907 (0.2987) Acc G: 82.303% 
LR: 2.000e-04 

2023-03-02 01:51:50,637 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4131 (0.3266) Acc D Real: 79.306% 
Loss D Fake: 1.4614 (2.8907) Acc D Fake: 17.571% 
Loss D: 1.874 
Loss G: 0.2964 (0.2986) Acc G: 82.401% 
LR: 2.000e-04 

2023-03-02 01:51:50,644 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4137 (0.3274) Acc D Real: 79.305% 
Loss D Fake: 1.4397 (2.8779) Acc D Fake: 17.475% 
Loss D: 1.853 
Loss G: 0.3021 (0.2987) Acc G: 82.497% 
LR: 2.000e-04 

2023-03-02 01:51:50,652 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4169 (0.3282) Acc D Real: 79.300% 
Loss D Fake: 1.4186 (2.8652) Acc D Fake: 17.381% 
Loss D: 1.836 
Loss G: 0.3079 (0.2987) Acc G: 82.591% 
LR: 2.000e-04 

2023-03-02 01:51:50,659 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4283 (0.3290) Acc D Real: 79.282% 
Loss D Fake: 1.3982 (2.8526) Acc D Fake: 17.289% 
Loss D: 1.826 
Loss G: 0.3137 (0.2989) Acc G: 82.684% 
LR: 2.000e-04 

2023-03-02 01:51:50,666 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4320 (0.3299) Acc D Real: 79.271% 
Loss D Fake: 1.3783 (2.8400) Acc D Fake: 17.198% 
Loss D: 1.810 
Loss G: 0.3194 (0.2991) Acc G: 82.775% 
LR: 2.000e-04 

2023-03-02 01:51:50,674 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4138 (0.3306) Acc D Real: 79.297% 
Loss D Fake: 1.3590 (2.8274) Acc D Fake: 17.109% 
Loss D: 1.773 
Loss G: 0.3252 (0.2993) Acc G: 82.864% 
LR: 2.000e-04 

2023-03-02 01:51:50,681 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3793 (0.3310) Acc D Real: 79.368% 
Loss D Fake: 1.3404 (2.8149) Acc D Fake: 17.021% 
Loss D: 1.720 
Loss G: 0.3309 (0.2995) Acc G: 82.952% 
LR: 2.000e-04 

2023-03-02 01:51:50,688 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4306 (0.3318) Acc D Real: 79.405% 
Loss D Fake: 1.3226 (2.8025) Acc D Fake: 16.935% 
Loss D: 1.753 
Loss G: 0.3366 (0.2998) Acc G: 83.039% 
LR: 2.000e-04 

2023-03-02 01:51:50,696 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4419 (0.3328) Acc D Real: 79.401% 
Loss D Fake: 1.3050 (2.7901) Acc D Fake: 16.850% 
Loss D: 1.747 
Loss G: 0.3424 (0.3002) Acc G: 83.124% 
LR: 2.000e-04 

2023-03-02 01:51:50,703 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4154 (0.3334) Acc D Real: 79.409% 
Loss D Fake: 1.2880 (2.7778) Acc D Fake: 16.767% 
Loss D: 1.703 
Loss G: 0.3480 (0.3006) Acc G: 83.207% 
LR: 2.000e-04 

2023-03-02 01:51:50,712 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4554 (0.3344) Acc D Real: 79.387% 
Loss D Fake: 1.2718 (2.7656) Acc D Fake: 16.684% 
Loss D: 1.727 
Loss G: 0.3536 (0.3010) Acc G: 83.290% 
LR: 2.000e-04 

2023-03-02 01:51:50,719 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4138 (0.3351) Acc D Real: 79.444% 
Loss D Fake: 1.2560 (2.7534) Acc D Fake: 16.604% 
Loss D: 1.670 
Loss G: 0.3592 (0.3015) Acc G: 83.371% 
LR: 2.000e-04 

2023-03-02 01:51:50,727 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4384 (0.3359) Acc D Real: 79.457% 
Loss D Fake: 1.2407 (2.7413) Acc D Fake: 16.524% 
Loss D: 1.679 
Loss G: 0.3648 (0.3020) Acc G: 83.450% 
LR: 2.000e-04 

2023-03-02 01:51:50,735 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4343 (0.3367) Acc D Real: 79.496% 
Loss D Fake: 1.2258 (2.7293) Acc D Fake: 16.446% 
Loss D: 1.660 
Loss G: 0.3704 (0.3025) Acc G: 83.529% 
LR: 2.000e-04 

2023-03-02 01:51:50,742 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4164 (0.3373) Acc D Real: 79.548% 
Loss D Fake: 1.2113 (2.7173) Acc D Fake: 16.369% 
Loss D: 1.628 
Loss G: 0.3758 (0.3031) Acc G: 83.606% 
LR: 2.000e-04 

2023-03-02 01:51:50,750 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4531 (0.3382) Acc D Real: 79.567% 
Loss D Fake: 1.1974 (2.7054) Acc D Fake: 16.293% 
Loss D: 1.651 
Loss G: 0.3813 (0.3037) Acc G: 83.682% 
LR: 2.000e-04 

2023-03-02 01:51:50,759 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4834 (0.3393) Acc D Real: 79.523% 
Loss D Fake: 1.1836 (2.6936) Acc D Fake: 16.219% 
Loss D: 1.667 
Loss G: 0.3869 (0.3044) Acc G: 83.757% 
LR: 2.000e-04 

2023-03-02 01:51:50,767 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4827 (0.3404) Acc D Real: 79.485% 
Loss D Fake: 1.1701 (2.6819) Acc D Fake: 16.145% 
Loss D: 1.653 
Loss G: 0.3925 (0.3051) Acc G: 83.831% 
LR: 2.000e-04 

2023-03-02 01:51:50,774 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4469 (0.3412) Acc D Real: 79.513% 
Loss D Fake: 1.1568 (2.6703) Acc D Fake: 16.073% 
Loss D: 1.604 
Loss G: 0.3981 (0.3058) Acc G: 83.903% 
LR: 2.000e-04 

2023-03-02 01:51:50,782 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4707 (0.3422) Acc D Real: 79.512% 
Loss D Fake: 1.1438 (2.6587) Acc D Fake: 16.001% 
Loss D: 1.615 
Loss G: 0.4038 (0.3065) Acc G: 83.975% 
LR: 2.000e-04 

2023-03-02 01:51:50,789 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4433 (0.3430) Acc D Real: 79.547% 
Loss D Fake: 1.1311 (2.6472) Acc D Fake: 15.931% 
Loss D: 1.574 
Loss G: 0.4091 (0.3073) Acc G: 84.045% 
LR: 2.000e-04 

2023-03-02 01:51:50,796 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4844 (0.3440) Acc D Real: 79.522% 
Loss D Fake: 1.1195 (2.6358) Acc D Fake: 15.862% 
Loss D: 1.604 
Loss G: 0.4143 (0.3081) Acc G: 84.114% 
LR: 2.000e-04 

2023-03-02 01:51:50,804 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4761 (0.3450) Acc D Real: 79.519% 
Loss D Fake: 1.1083 (2.6245) Acc D Fake: 15.794% 
Loss D: 1.584 
Loss G: 0.4194 (0.3089) Acc G: 84.182% 
LR: 2.000e-04 

2023-03-02 01:51:50,811 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4806 (0.3460) Acc D Real: 79.512% 
Loss D Fake: 1.0976 (2.6133) Acc D Fake: 15.727% 
Loss D: 1.578 
Loss G: 0.4244 (0.3097) Acc G: 84.250% 
LR: 2.000e-04 

2023-03-02 01:51:50,818 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4693 (0.3469) Acc D Real: 79.527% 
Loss D Fake: 1.0873 (2.6022) Acc D Fake: 15.661% 
Loss D: 1.557 
Loss G: 0.4292 (0.3106) Acc G: 84.316% 
LR: 2.000e-04 

2023-03-02 01:51:50,826 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4836 (0.3479) Acc D Real: 79.523% 
Loss D Fake: 1.0773 (2.5911) Acc D Fake: 15.596% 
Loss D: 1.561 
Loss G: 0.4341 (0.3115) Acc G: 84.381% 
LR: 2.000e-04 

2023-03-02 01:51:50,833 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4885 (0.3489) Acc D Real: 79.515% 
Loss D Fake: 1.0674 (2.5801) Acc D Fake: 15.531% 
Loss D: 1.556 
Loss G: 0.4390 (0.3124) Acc G: 84.446% 
LR: 2.000e-04 

2023-03-02 01:51:50,840 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4884 (0.3499) Acc D Real: 79.514% 
Loss D Fake: 1.0578 (2.5693) Acc D Fake: 15.468% 
Loss D: 1.546 
Loss G: 0.4438 (0.3134) Acc G: 84.509% 
LR: 2.000e-04 

2023-03-02 01:51:50,848 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4827 (0.3509) Acc D Real: 79.526% 
Loss D Fake: 1.0486 (2.5585) Acc D Fake: 15.406% 
Loss D: 1.531 
Loss G: 0.4483 (0.3143) Acc G: 84.572% 
LR: 2.000e-04 

2023-03-02 01:51:50,855 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4974 (0.3519) Acc D Real: 79.514% 
Loss D Fake: 1.0399 (2.5478) Acc D Fake: 15.344% 
Loss D: 1.537 
Loss G: 0.4529 (0.3153) Acc G: 84.634% 
LR: 2.000e-04 

2023-03-02 01:51:50,862 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4660 (0.3527) Acc D Real: 79.552% 
Loss D Fake: 1.0313 (2.5372) Acc D Fake: 15.283% 
Loss D: 1.497 
Loss G: 0.4574 (0.3163) Acc G: 84.694% 
LR: 2.000e-04 

2023-03-02 01:51:50,870 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4749 (0.3535) Acc D Real: 79.580% 
Loss D Fake: 1.0229 (2.5267) Acc D Fake: 15.224% 
Loss D: 1.498 
Loss G: 0.4620 (0.3173) Acc G: 84.754% 
LR: 2.000e-04 

2023-03-02 01:51:50,877 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.5107 (0.3546) Acc D Real: 79.564% 
Loss D Fake: 1.0147 (2.5162) Acc D Fake: 15.165% 
Loss D: 1.525 
Loss G: 0.4664 (0.3183) Acc G: 84.814% 
LR: 2.000e-04 

2023-03-02 01:51:50,884 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.5111 (0.3557) Acc D Real: 79.541% 
Loss D Fake: 1.0067 (2.5059) Acc D Fake: 15.106% 
Loss D: 1.518 
Loss G: 0.4708 (0.3194) Acc G: 84.872% 
LR: 2.000e-04 

2023-03-02 01:51:50,892 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.5048 (0.3567) Acc D Real: 79.537% 
Loss D Fake: 0.9989 (2.4956) Acc D Fake: 15.049% 
Loss D: 1.504 
Loss G: 0.4752 (0.3204) Acc G: 84.929% 
LR: 2.000e-04 

2023-03-02 01:51:50,899 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4945 (0.3576) Acc D Real: 79.552% 
Loss D Fake: 0.9914 (2.4855) Acc D Fake: 14.992% 
Loss D: 1.486 
Loss G: 0.4794 (0.3215) Acc G: 84.986% 
LR: 2.000e-04 

2023-03-02 01:51:50,906 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4880 (0.3585) Acc D Real: 79.572% 
Loss D Fake: 0.9842 (2.4754) Acc D Fake: 14.936% 
Loss D: 1.472 
Loss G: 0.4835 (0.3226) Acc G: 85.042% 
LR: 2.000e-04 

2023-03-02 01:51:50,914 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4676 (0.3592) Acc D Real: 79.612% 
Loss D Fake: 0.9771 (2.4654) Acc D Fake: 14.881% 
Loss D: 1.445 
Loss G: 0.4877 (0.3237) Acc G: 85.098% 
LR: 2.000e-04 

2023-03-02 01:51:50,921 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.5250 (0.3603) Acc D Real: 79.581% 
Loss D Fake: 0.9700 (2.4555) Acc D Fake: 14.827% 
Loss D: 1.495 
Loss G: 0.4920 (0.3248) Acc G: 85.152% 
LR: 2.000e-04 

2023-03-02 01:51:50,928 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.5367 (0.3615) Acc D Real: 79.516% 
Loss D Fake: 0.9627 (2.4457) Acc D Fake: 14.773% 
Loss D: 1.499 
Loss G: 0.4965 (0.3259) Acc G: 85.206% 
LR: 2.000e-04 

2023-03-02 01:51:50,935 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4857 (0.3623) Acc D Real: 79.541% 
Loss D Fake: 0.9552 (2.4360) Acc D Fake: 14.720% 
Loss D: 1.441 
Loss G: 0.5011 (0.3271) Acc G: 85.259% 
LR: 2.000e-04 

2023-03-02 01:51:50,943 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4825 (0.3631) Acc D Real: 79.573% 
Loss D Fake: 0.9478 (2.4263) Acc D Fake: 14.668% 
Loss D: 1.430 
Loss G: 0.5056 (0.3283) Acc G: 85.311% 
LR: 2.000e-04 

2023-03-02 01:51:50,950 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4949 (0.3639) Acc D Real: 79.623% 
Loss D Fake: 0.9409 (2.4167) Acc D Fake: 14.616% 
Loss D: 1.436 
Loss G: 0.5097 (0.3294) Acc G: 85.363% 
LR: 2.000e-04 

2023-03-02 01:51:50,957 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4693 (0.3646) Acc D Real: 79.674% 
Loss D Fake: 0.9347 (2.4072) Acc D Fake: 14.565% 
Loss D: 1.404 
Loss G: 0.5135 (0.3306) Acc G: 85.414% 
LR: 2.000e-04 

2023-03-02 01:51:50,965 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4987 (0.3655) Acc D Real: 79.740% 
Loss D Fake: 0.9290 (2.3978) Acc D Fake: 14.515% 
Loss D: 1.428 
Loss G: 0.5169 (0.3318) Acc G: 85.465% 
LR: 2.000e-04 

2023-03-02 01:51:50,972 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5354 (0.3665) Acc D Real: 79.743% 
Loss D Fake: 0.9246 (2.3885) Acc D Fake: 14.510% 
Loss D: 1.460 
Loss G: 0.5192 (0.3330) Acc G: 85.469% 
LR: 2.000e-04 

2023-03-02 01:51:51,211 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.430 | Generator Loss: 0.519 | Avg: 1.949 
2023-03-02 01:51:51,236 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.457 | Generator Loss: 0.519 | Avg: 1.976 
2023-03-02 01:51:51,261 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.450 | Generator Loss: 0.519 | Avg: 1.969 
2023-03-02 01:51:51,291 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.442 | Generator Loss: 0.519 | Avg: 1.961 
2023-03-02 01:51:51,318 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.442 | Generator Loss: 0.519 | Avg: 1.961 
2023-03-02 01:51:51,344 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.431 | Generator Loss: 0.519 | Avg: 1.950 
2023-03-02 01:51:51,370 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.440 | Generator Loss: 0.519 | Avg: 1.959 
2023-03-02 01:51:51,396 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.438 | Generator Loss: 0.519 | Avg: 1.957 
2023-03-02 01:51:51,422 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.444 | Generator Loss: 0.519 | Avg: 1.963 
2023-03-02 01:51:51,448 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.441 | Generator Loss: 0.519 | Avg: 1.960 
2023-03-02 01:51:51,474 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.437 | Generator Loss: 0.519 | Avg: 1.956 
2023-03-02 01:51:51,500 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.425 | Generator Loss: 0.519 | Avg: 1.944 
2023-03-02 01:51:51,526 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.426 | Generator Loss: 0.519 | Avg: 1.945 
2023-03-02 01:51:51,552 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.430 | Generator Loss: 0.519 | Avg: 1.949 
2023-03-02 01:51:51,577 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.433 | Generator Loss: 0.519 | Avg: 1.952 
2023-03-02 01:51:51,605 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.434 | Generator Loss: 0.519 | Avg: 1.953 
2023-03-02 01:51:51,631 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.434 | Generator Loss: 0.519 | Avg: 1.953 
2023-03-02 01:51:51,666 -                train: [    INFO] - 
Epoch: 8/20
2023-03-02 01:51:51,852 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4958 (0.5029) Acc D Real: 82.682% 
Loss D Fake: 0.9178 (0.9196) Acc D Fake: 6.667% 
Loss D: 1.414 
Loss G: 0.5238 (0.5226) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,860 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5056 (0.5038) Acc D Real: 83.663% 
Loss D Fake: 0.9141 (0.9178) Acc D Fake: 6.667% 
Loss D: 1.420 
Loss G: 0.5262 (0.5238) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,868 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.5144 (0.5064) Acc D Real: 84.505% 
Loss D Fake: 0.9107 (0.9160) Acc D Fake: 6.667% 
Loss D: 1.425 
Loss G: 0.5282 (0.5249) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,885 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.5174 (0.5086) Acc D Real: 84.562% 
Loss D Fake: 0.9080 (0.9144) Acc D Fake: 6.667% 
Loss D: 1.425 
Loss G: 0.5299 (0.5259) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,892 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.5340 (0.5128) Acc D Real: 83.438% 
Loss D Fake: 0.9054 (0.9129) Acc D Fake: 6.667% 
Loss D: 1.439 
Loss G: 0.5317 (0.5269) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,899 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4994 (0.5109) Acc D Real: 83.824% 
Loss D Fake: 0.9027 (0.9114) Acc D Fake: 6.667% 
Loss D: 1.402 
Loss G: 0.5335 (0.5278) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,906 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.5159 (0.5115) Acc D Real: 83.405% 
Loss D Fake: 0.9000 (0.9100) Acc D Fake: 6.667% 
Loss D: 1.416 
Loss G: 0.5355 (0.5288) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,913 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.5371 (0.5144) Acc D Real: 82.888% 
Loss D Fake: 0.8971 (0.9086) Acc D Fake: 6.667% 
Loss D: 1.434 
Loss G: 0.5374 (0.5297) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,920 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.5261 (0.5156) Acc D Real: 82.807% 
Loss D Fake: 0.8944 (0.9072) Acc D Fake: 6.667% 
Loss D: 1.420 
Loss G: 0.5392 (0.5307) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,927 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.5243 (0.5163) Acc D Real: 82.566% 
Loss D Fake: 0.8918 (0.9058) Acc D Fake: 6.667% 
Loss D: 1.416 
Loss G: 0.5410 (0.5316) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,934 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.5055 (0.5154) Acc D Real: 82.326% 
Loss D Fake: 0.8889 (0.9044) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5433 (0.5326) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,942 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.5070 (0.5148) Acc D Real: 82.007% 
Loss D Fake: 0.8853 (0.9029) Acc D Fake: 6.667% 
Loss D: 1.392 
Loss G: 0.5460 (0.5336) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,949 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4886 (0.5129) Acc D Real: 82.165% 
Loss D Fake: 0.8812 (0.9014) Acc D Fake: 6.667% 
Loss D: 1.370 
Loss G: 0.5490 (0.5347) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,956 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.5095 (0.5127) Acc D Real: 82.257% 
Loss D Fake: 0.8770 (0.8997) Acc D Fake: 6.667% 
Loss D: 1.387 
Loss G: 0.5518 (0.5359) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,963 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.5083 (0.5124) Acc D Real: 82.116% 
Loss D Fake: 0.8730 (0.8981) Acc D Fake: 6.667% 
Loss D: 1.381 
Loss G: 0.5549 (0.5371) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,971 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.5248 (0.5131) Acc D Real: 82.037% 
Loss D Fake: 0.8689 (0.8963) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5577 (0.5383) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,978 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.5038 (0.5126) Acc D Real: 82.219% 
Loss D Fake: 0.8650 (0.8946) Acc D Fake: 6.667% 
Loss D: 1.369 
Loss G: 0.5605 (0.5395) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,986 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.5328 (0.5137) Acc D Real: 81.804% 
Loss D Fake: 0.8612 (0.8928) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5634 (0.5408) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:51,994 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.5347 (0.5147) Acc D Real: 81.714% 
Loss D Fake: 0.8573 (0.8911) Acc D Fake: 6.667% 
Loss D: 1.392 
Loss G: 0.5661 (0.5420) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,001 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.5343 (0.5157) Acc D Real: 81.845% 
Loss D Fake: 0.8539 (0.8893) Acc D Fake: 6.667% 
Loss D: 1.388 
Loss G: 0.5683 (0.5433) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,008 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.5086 (0.5153) Acc D Real: 81.856% 
Loss D Fake: 0.8511 (0.8876) Acc D Fake: 6.667% 
Loss D: 1.360 
Loss G: 0.5705 (0.5445) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,015 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.5339 (0.5162) Acc D Real: 81.927% 
Loss D Fake: 0.8483 (0.8859) Acc D Fake: 6.667% 
Loss D: 1.382 
Loss G: 0.5724 (0.5457) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,022 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.5209 (0.5163) Acc D Real: 81.858% 
Loss D Fake: 0.8458 (0.8842) Acc D Fake: 6.667% 
Loss D: 1.367 
Loss G: 0.5744 (0.5469) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,029 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.5234 (0.5166) Acc D Real: 82.096% 
Loss D Fake: 0.8434 (0.8826) Acc D Fake: 6.667% 
Loss D: 1.367 
Loss G: 0.5759 (0.5481) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,036 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.5264 (0.5170) Acc D Real: 82.488% 
Loss D Fake: 0.8418 (0.8810) Acc D Fake: 6.667% 
Loss D: 1.368 
Loss G: 0.5767 (0.5492) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,043 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.5118 (0.5168) Acc D Real: 82.296% 
Loss D Fake: 0.8408 (0.8795) Acc D Fake: 6.667% 
Loss D: 1.353 
Loss G: 0.5779 (0.5503) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,050 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.5690 (0.5187) Acc D Real: 81.955% 
Loss D Fake: 0.8390 (0.8780) Acc D Fake: 6.667% 
Loss D: 1.408 
Loss G: 0.5793 (0.5513) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,058 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.5572 (0.5200) Acc D Real: 81.909% 
Loss D Fake: 0.8372 (0.8766) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5804 (0.5523) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,065 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.5184 (0.5200) Acc D Real: 81.892% 
Loss D Fake: 0.8358 (0.8753) Acc D Fake: 6.667% 
Loss D: 1.354 
Loss G: 0.5816 (0.5533) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,072 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.5283 (0.5202) Acc D Real: 81.976% 
Loss D Fake: 0.8343 (0.8740) Acc D Fake: 6.667% 
Loss D: 1.363 
Loss G: 0.5826 (0.5542) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,080 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5238 (0.5203) Acc D Real: 82.077% 
Loss D Fake: 0.8330 (0.8727) Acc D Fake: 6.667% 
Loss D: 1.357 
Loss G: 0.5835 (0.5551) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,088 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4693 (0.5188) Acc D Real: 82.303% 
Loss D Fake: 0.8319 (0.8714) Acc D Fake: 6.667% 
Loss D: 1.301 
Loss G: 0.5845 (0.5560) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,095 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5326 (0.5192) Acc D Real: 82.587% 
Loss D Fake: 0.8307 (0.8702) Acc D Fake: 6.667% 
Loss D: 1.363 
Loss G: 0.5849 (0.5569) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,102 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5440 (0.5199) Acc D Real: 82.506% 
Loss D Fake: 0.8304 (0.8691) Acc D Fake: 6.667% 
Loss D: 1.374 
Loss G: 0.5852 (0.5577) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,110 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4593 (0.5182) Acc D Real: 82.577% 
Loss D Fake: 0.8297 (0.8680) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.5861 (0.5585) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,117 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4690 (0.5169) Acc D Real: 82.672% 
Loss D Fake: 0.8281 (0.8669) Acc D Fake: 6.667% 
Loss D: 1.297 
Loss G: 0.5877 (0.5593) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,124 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5020 (0.5165) Acc D Real: 82.797% 
Loss D Fake: 0.8260 (0.8659) Acc D Fake: 6.667% 
Loss D: 1.328 
Loss G: 0.5892 (0.5601) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,132 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4961 (0.5160) Acc D Real: 82.985% 
Loss D Fake: 0.8242 (0.8648) Acc D Fake: 6.667% 
Loss D: 1.320 
Loss G: 0.5904 (0.5608) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,139 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5503 (0.5168) Acc D Real: 82.999% 
Loss D Fake: 0.8229 (0.8637) Acc D Fake: 6.667% 
Loss D: 1.373 
Loss G: 0.5912 (0.5616) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,147 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5102 (0.5167) Acc D Real: 82.942% 
Loss D Fake: 0.8219 (0.8627) Acc D Fake: 6.667% 
Loss D: 1.332 
Loss G: 0.5922 (0.5623) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,155 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4951 (0.5162) Acc D Real: 83.047% 
Loss D Fake: 0.8205 (0.8617) Acc D Fake: 6.667% 
Loss D: 1.316 
Loss G: 0.5933 (0.5631) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,162 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.5174 (0.5162) Acc D Real: 83.161% 
Loss D Fake: 0.8193 (0.8607) Acc D Fake: 6.667% 
Loss D: 1.337 
Loss G: 0.5941 (0.5638) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,170 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.5368 (0.5167) Acc D Real: 83.252% 
Loss D Fake: 0.8185 (0.8598) Acc D Fake: 6.667% 
Loss D: 1.355 
Loss G: 0.5944 (0.5645) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,178 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.5028 (0.5163) Acc D Real: 83.229% 
Loss D Fake: 0.8180 (0.8588) Acc D Fake: 6.667% 
Loss D: 1.321 
Loss G: 0.5950 (0.5652) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,185 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4950 (0.5159) Acc D Real: 83.293% 
Loss D Fake: 0.8171 (0.8579) Acc D Fake: 6.667% 
Loss D: 1.312 
Loss G: 0.5958 (0.5658) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,193 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.5235 (0.5160) Acc D Real: 83.317% 
Loss D Fake: 0.8161 (0.8570) Acc D Fake: 6.667% 
Loss D: 1.340 
Loss G: 0.5965 (0.5665) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,200 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.5445 (0.5166) Acc D Real: 83.403% 
Loss D Fake: 0.8154 (0.8562) Acc D Fake: 6.667% 
Loss D: 1.360 
Loss G: 0.5967 (0.5671) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,208 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.5269 (0.5168) Acc D Real: 83.440% 
Loss D Fake: 0.8154 (0.8553) Acc D Fake: 6.667% 
Loss D: 1.342 
Loss G: 0.5966 (0.5677) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,216 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4793 (0.5161) Acc D Real: 83.477% 
Loss D Fake: 0.8153 (0.8545) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5969 (0.5683) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,224 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.5041 (0.5159) Acc D Real: 83.599% 
Loss D Fake: 0.8148 (0.8538) Acc D Fake: 6.667% 
Loss D: 1.319 
Loss G: 0.5972 (0.5689) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,232 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.5050 (0.5157) Acc D Real: 83.662% 
Loss D Fake: 0.8145 (0.8530) Acc D Fake: 6.667% 
Loss D: 1.320 
Loss G: 0.5974 (0.5694) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,239 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.5153 (0.5156) Acc D Real: 83.648% 
Loss D Fake: 0.8141 (0.8523) Acc D Fake: 6.667% 
Loss D: 1.329 
Loss G: 0.5978 (0.5700) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,246 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.5489 (0.5163) Acc D Real: 83.597% 
Loss D Fake: 0.8136 (0.8516) Acc D Fake: 6.667% 
Loss D: 1.362 
Loss G: 0.5981 (0.5705) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,254 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4893 (0.5158) Acc D Real: 83.680% 
Loss D Fake: 0.8132 (0.8509) Acc D Fake: 6.667% 
Loss D: 1.303 
Loss G: 0.5984 (0.5710) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,261 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4667 (0.5149) Acc D Real: 83.787% 
Loss D Fake: 0.8127 (0.8502) Acc D Fake: 6.667% 
Loss D: 1.279 
Loss G: 0.5989 (0.5715) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,268 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.5330 (0.5152) Acc D Real: 83.912% 
Loss D Fake: 0.8122 (0.8495) Acc D Fake: 6.667% 
Loss D: 1.345 
Loss G: 0.5989 (0.5720) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,276 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.5350 (0.5156) Acc D Real: 83.878% 
Loss D Fake: 0.8124 (0.8489) Acc D Fake: 6.667% 
Loss D: 1.347 
Loss G: 0.5987 (0.5724) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,283 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.5008 (0.5153) Acc D Real: 83.955% 
Loss D Fake: 0.8126 (0.8483) Acc D Fake: 6.667% 
Loss D: 1.313 
Loss G: 0.5985 (0.5729) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,290 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4575 (0.5143) Acc D Real: 84.022% 
Loss D Fake: 0.8126 (0.8477) Acc D Fake: 6.667% 
Loss D: 1.270 
Loss G: 0.5988 (0.5733) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,297 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4951 (0.5140) Acc D Real: 84.097% 
Loss D Fake: 0.8121 (0.8471) Acc D Fake: 6.667% 
Loss D: 1.307 
Loss G: 0.5991 (0.5737) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,305 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.5327 (0.5143) Acc D Real: 84.127% 
Loss D Fake: 0.8118 (0.8465) Acc D Fake: 6.667% 
Loss D: 1.344 
Loss G: 0.5991 (0.5741) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,312 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4841 (0.5138) Acc D Real: 84.238% 
Loss D Fake: 0.8119 (0.8460) Acc D Fake: 6.667% 
Loss D: 1.296 
Loss G: 0.5990 (0.5745) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,320 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4613 (0.5130) Acc D Real: 84.351% 
Loss D Fake: 0.8119 (0.8454) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.5990 (0.5749) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,327 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4800 (0.5125) Acc D Real: 84.405% 
Loss D Fake: 0.8117 (0.8449) Acc D Fake: 6.667% 
Loss D: 1.292 
Loss G: 0.5993 (0.5753) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,335 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.5025 (0.5124) Acc D Real: 84.408% 
Loss D Fake: 0.8112 (0.8444) Acc D Fake: 6.667% 
Loss D: 1.314 
Loss G: 0.5997 (0.5756) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,342 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4839 (0.5119) Acc D Real: 84.537% 
Loss D Fake: 0.8108 (0.8439) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5999 (0.5760) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,350 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4740 (0.5114) Acc D Real: 84.611% 
Loss D Fake: 0.8105 (0.8434) Acc D Fake: 6.667% 
Loss D: 1.284 
Loss G: 0.6002 (0.5764) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,357 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4482 (0.5105) Acc D Real: 84.682% 
Loss D Fake: 0.8099 (0.8429) Acc D Fake: 6.667% 
Loss D: 1.258 
Loss G: 0.6008 (0.5767) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,364 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4844 (0.5101) Acc D Real: 84.648% 
Loss D Fake: 0.8088 (0.8424) Acc D Fake: 6.667% 
Loss D: 1.293 
Loss G: 0.6019 (0.5771) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,372 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.5279 (0.5103) Acc D Real: 84.711% 
Loss D Fake: 0.8076 (0.8419) Acc D Fake: 6.667% 
Loss D: 1.336 
Loss G: 0.6025 (0.5774) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,379 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4841 (0.5100) Acc D Real: 84.795% 
Loss D Fake: 0.8071 (0.8415) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6029 (0.5778) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,386 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4906 (0.5097) Acc D Real: 84.871% 
Loss D Fake: 0.8067 (0.8410) Acc D Fake: 6.667% 
Loss D: 1.297 
Loss G: 0.6030 (0.5781) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,393 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4816 (0.5093) Acc D Real: 84.988% 
Loss D Fake: 0.8067 (0.8405) Acc D Fake: 6.667% 
Loss D: 1.288 
Loss G: 0.6029 (0.5785) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,401 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4660 (0.5088) Acc D Real: 85.060% 
Loss D Fake: 0.8068 (0.8401) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.6029 (0.5788) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,408 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4843 (0.5084) Acc D Real: 85.036% 
Loss D Fake: 0.8065 (0.8396) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6034 (0.5791) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,416 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.5580 (0.5091) Acc D Real: 84.948% 
Loss D Fake: 0.8058 (0.8392) Acc D Fake: 6.667% 
Loss D: 1.364 
Loss G: 0.6038 (0.5794) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,423 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4852 (0.5088) Acc D Real: 85.015% 
Loss D Fake: 0.8055 (0.8388) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6040 (0.5798) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,432 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4833 (0.5084) Acc D Real: 85.067% 
Loss D Fake: 0.8052 (0.8383) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.6042 (0.5801) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,439 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4808 (0.5081) Acc D Real: 85.124% 
Loss D Fake: 0.8049 (0.8379) Acc D Fake: 6.667% 
Loss D: 1.286 
Loss G: 0.6044 (0.5804) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,447 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4303 (0.5071) Acc D Real: 85.199% 
Loss D Fake: 0.8044 (0.8375) Acc D Fake: 6.667% 
Loss D: 1.235 
Loss G: 0.6050 (0.5807) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,454 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4913 (0.5070) Acc D Real: 85.248% 
Loss D Fake: 0.8036 (0.8371) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.6056 (0.5810) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,461 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4666 (0.5065) Acc D Real: 85.297% 
Loss D Fake: 0.8028 (0.8367) Acc D Fake: 6.667% 
Loss D: 1.269 
Loss G: 0.6063 (0.5813) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,468 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4367 (0.5056) Acc D Real: 85.368% 
Loss D Fake: 0.8019 (0.8363) Acc D Fake: 6.667% 
Loss D: 1.239 
Loss G: 0.6072 (0.5816) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,476 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4789 (0.5053) Acc D Real: 85.394% 
Loss D Fake: 0.8006 (0.8358) Acc D Fake: 6.667% 
Loss D: 1.279 
Loss G: 0.6082 (0.5819) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,483 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4658 (0.5049) Acc D Real: 85.399% 
Loss D Fake: 0.7992 (0.8354) Acc D Fake: 6.667% 
Loss D: 1.265 
Loss G: 0.6096 (0.5822) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,490 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4658 (0.5044) Acc D Real: 85.457% 
Loss D Fake: 0.7975 (0.8350) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6110 (0.5826) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,498 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4851 (0.5042) Acc D Real: 85.509% 
Loss D Fake: 0.7959 (0.8345) Acc D Fake: 6.667% 
Loss D: 1.281 
Loss G: 0.6122 (0.5829) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,505 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4928 (0.5041) Acc D Real: 85.546% 
Loss D Fake: 0.7947 (0.8341) Acc D Fake: 6.667% 
Loss D: 1.287 
Loss G: 0.6130 (0.5832) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,512 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4603 (0.5036) Acc D Real: 85.580% 
Loss D Fake: 0.7937 (0.8336) Acc D Fake: 6.667% 
Loss D: 1.254 
Loss G: 0.6139 (0.5836) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,520 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4635 (0.5031) Acc D Real: 85.610% 
Loss D Fake: 0.7924 (0.8332) Acc D Fake: 6.667% 
Loss D: 1.256 
Loss G: 0.6151 (0.5839) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,527 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4856 (0.5029) Acc D Real: 85.688% 
Loss D Fake: 0.7912 (0.8327) Acc D Fake: 6.667% 
Loss D: 1.277 
Loss G: 0.6158 (0.5843) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,535 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.5073 (0.5030) Acc D Real: 85.720% 
Loss D Fake: 0.7905 (0.8323) Acc D Fake: 6.667% 
Loss D: 1.298 
Loss G: 0.6161 (0.5846) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,542 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4738 (0.5027) Acc D Real: 85.750% 
Loss D Fake: 0.7902 (0.8318) Acc D Fake: 6.667% 
Loss D: 1.264 
Loss G: 0.6164 (0.5849) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,549 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4349 (0.5020) Acc D Real: 85.747% 
Loss D Fake: 0.7896 (0.8314) Acc D Fake: 6.667% 
Loss D: 1.224 
Loss G: 0.6174 (0.5853) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,556 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4840 (0.5018) Acc D Real: 85.782% 
Loss D Fake: 0.7882 (0.8309) Acc D Fake: 6.667% 
Loss D: 1.272 
Loss G: 0.6184 (0.5856) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,564 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4950 (0.5017) Acc D Real: 85.827% 
Loss D Fake: 0.7873 (0.8305) Acc D Fake: 6.667% 
Loss D: 1.282 
Loss G: 0.6189 (0.5860) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,571 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4589 (0.5013) Acc D Real: 85.891% 
Loss D Fake: 0.7868 (0.8300) Acc D Fake: 6.667% 
Loss D: 1.246 
Loss G: 0.6194 (0.5863) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,578 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.5362 (0.5016) Acc D Real: 85.897% 
Loss D Fake: 0.7864 (0.8296) Acc D Fake: 6.667% 
Loss D: 1.323 
Loss G: 0.6193 (0.5867) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,586 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4113 (0.5007) Acc D Real: 85.954% 
Loss D Fake: 0.7864 (0.8292) Acc D Fake: 6.667% 
Loss D: 1.198 
Loss G: 0.6197 (0.5870) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,593 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4491 (0.5002) Acc D Real: 86.029% 
Loss D Fake: 0.7857 (0.8287) Acc D Fake: 6.667% 
Loss D: 1.235 
Loss G: 0.6203 (0.5873) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,601 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4707 (0.4999) Acc D Real: 86.071% 
Loss D Fake: 0.7850 (0.8283) Acc D Fake: 6.667% 
Loss D: 1.256 
Loss G: 0.6208 (0.5876) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,608 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4868 (0.4998) Acc D Real: 86.088% 
Loss D Fake: 0.7845 (0.8279) Acc D Fake: 6.667% 
Loss D: 1.271 
Loss G: 0.6212 (0.5880) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,616 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4528 (0.4993) Acc D Real: 86.127% 
Loss D Fake: 0.7839 (0.8275) Acc D Fake: 6.667% 
Loss D: 1.237 
Loss G: 0.6218 (0.5883) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,624 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4490 (0.4989) Acc D Real: 86.195% 
Loss D Fake: 0.7831 (0.8270) Acc D Fake: 6.667% 
Loss D: 1.232 
Loss G: 0.6225 (0.5886) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,631 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4906 (0.4988) Acc D Real: 86.222% 
Loss D Fake: 0.7824 (0.8266) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.6229 (0.5889) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,639 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.5259 (0.4990) Acc D Real: 86.249% 
Loss D Fake: 0.7822 (0.8262) Acc D Fake: 6.667% 
Loss D: 1.308 
Loss G: 0.6227 (0.5893) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,647 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4870 (0.4989) Acc D Real: 86.305% 
Loss D Fake: 0.7827 (0.8258) Acc D Fake: 6.667% 
Loss D: 1.270 
Loss G: 0.6221 (0.5896) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,654 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4044 (0.4981) Acc D Real: 86.347% 
Loss D Fake: 0.7831 (0.8254) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6221 (0.5899) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,661 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4633 (0.4977) Acc D Real: 86.405% 
Loss D Fake: 0.7829 (0.8250) Acc D Fake: 6.667% 
Loss D: 1.246 
Loss G: 0.6222 (0.5902) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,669 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4328 (0.4972) Acc D Real: 86.397% 
Loss D Fake: 0.7825 (0.8246) Acc D Fake: 6.667% 
Loss D: 1.215 
Loss G: 0.6229 (0.5904) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,676 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4712 (0.4969) Acc D Real: 86.435% 
Loss D Fake: 0.7815 (0.8243) Acc D Fake: 6.667% 
Loss D: 1.253 
Loss G: 0.6237 (0.5907) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,684 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4058 (0.4961) Acc D Real: 86.488% 
Loss D Fake: 0.7804 (0.8239) Acc D Fake: 6.667% 
Loss D: 1.186 
Loss G: 0.6249 (0.5910) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,692 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4633 (0.4958) Acc D Real: 86.512% 
Loss D Fake: 0.7789 (0.8235) Acc D Fake: 6.667% 
Loss D: 1.242 
Loss G: 0.6261 (0.5914) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,699 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3753 (0.4948) Acc D Real: 86.529% 
Loss D Fake: 0.7772 (0.8231) Acc D Fake: 6.667% 
Loss D: 1.152 
Loss G: 0.6281 (0.5917) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,707 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4846 (0.4947) Acc D Real: 86.541% 
Loss D Fake: 0.7747 (0.8226) Acc D Fake: 6.667% 
Loss D: 1.259 
Loss G: 0.6300 (0.5920) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,714 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4375 (0.4942) Acc D Real: 86.583% 
Loss D Fake: 0.7726 (0.8222) Acc D Fake: 6.667% 
Loss D: 1.210 
Loss G: 0.6318 (0.5923) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,722 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4568 (0.4939) Acc D Real: 86.599% 
Loss D Fake: 0.7706 (0.8218) Acc D Fake: 6.667% 
Loss D: 1.227 
Loss G: 0.6335 (0.5927) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,729 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4497 (0.4935) Acc D Real: 86.643% 
Loss D Fake: 0.7687 (0.8213) Acc D Fake: 6.667% 
Loss D: 1.218 
Loss G: 0.6351 (0.5930) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,736 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4613 (0.4933) Acc D Real: 86.673% 
Loss D Fake: 0.7670 (0.8209) Acc D Fake: 6.667% 
Loss D: 1.228 
Loss G: 0.6364 (0.5934) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,744 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4475 (0.4929) Acc D Real: 86.730% 
Loss D Fake: 0.7657 (0.8204) Acc D Fake: 6.667% 
Loss D: 1.213 
Loss G: 0.6374 (0.5938) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,752 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4399 (0.4924) Acc D Real: 86.765% 
Loss D Fake: 0.7645 (0.8200) Acc D Fake: 6.667% 
Loss D: 1.204 
Loss G: 0.6386 (0.5941) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,759 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4625 (0.4922) Acc D Real: 86.821% 
Loss D Fake: 0.7633 (0.8195) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6394 (0.5945) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,766 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4493 (0.4918) Acc D Real: 86.876% 
Loss D Fake: 0.7625 (0.8190) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.6399 (0.5949) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,774 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3911 (0.4910) Acc D Real: 86.912% 
Loss D Fake: 0.7617 (0.8186) Acc D Fake: 6.667% 
Loss D: 1.153 
Loss G: 0.6410 (0.5952) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,783 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4838 (0.4910) Acc D Real: 86.951% 
Loss D Fake: 0.7604 (0.8181) Acc D Fake: 6.667% 
Loss D: 1.244 
Loss G: 0.6418 (0.5956) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,792 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4971 (0.4910) Acc D Real: 86.975% 
Loss D Fake: 0.7598 (0.8177) Acc D Fake: 6.667% 
Loss D: 1.257 
Loss G: 0.6421 (0.5960) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,801 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4646 (0.4908) Acc D Real: 86.992% 
Loss D Fake: 0.7596 (0.8172) Acc D Fake: 6.667% 
Loss D: 1.224 
Loss G: 0.6424 (0.5963) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,810 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4614 (0.4906) Acc D Real: 87.034% 
Loss D Fake: 0.7593 (0.8168) Acc D Fake: 6.667% 
Loss D: 1.221 
Loss G: 0.6425 (0.5967) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,818 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4815 (0.4905) Acc D Real: 87.080% 
Loss D Fake: 0.7594 (0.8163) Acc D Fake: 6.667% 
Loss D: 1.241 
Loss G: 0.6420 (0.5970) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,826 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4540 (0.4903) Acc D Real: 87.094% 
Loss D Fake: 0.7599 (0.8159) Acc D Fake: 6.667% 
Loss D: 1.214 
Loss G: 0.6418 (0.5974) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,834 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.5036 (0.4904) Acc D Real: 87.138% 
Loss D Fake: 0.7603 (0.8155) Acc D Fake: 6.667% 
Loss D: 1.264 
Loss G: 0.6410 (0.5977) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,841 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.5013 (0.4904) Acc D Real: 87.150% 
Loss D Fake: 0.7613 (0.8151) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6399 (0.5980) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,849 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4166 (0.4899) Acc D Real: 87.207% 
Loss D Fake: 0.7625 (0.8147) Acc D Fake: 6.667% 
Loss D: 1.179 
Loss G: 0.6391 (0.5983) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,856 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4706 (0.4897) Acc D Real: 87.231% 
Loss D Fake: 0.7633 (0.8143) Acc D Fake: 6.667% 
Loss D: 1.234 
Loss G: 0.6383 (0.5986) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,864 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4616 (0.4895) Acc D Real: 87.235% 
Loss D Fake: 0.7640 (0.8139) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6378 (0.5989) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,871 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4611 (0.4893) Acc D Real: 87.255% 
Loss D Fake: 0.7644 (0.8136) Acc D Fake: 6.667% 
Loss D: 1.225 
Loss G: 0.6375 (0.5992) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,878 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4237 (0.4888) Acc D Real: 87.295% 
Loss D Fake: 0.7645 (0.8132) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6376 (0.5995) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,885 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4364 (0.4885) Acc D Real: 87.315% 
Loss D Fake: 0.7642 (0.8129) Acc D Fake: 6.667% 
Loss D: 1.201 
Loss G: 0.6380 (0.5998) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,894 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.5076 (0.4886) Acc D Real: 87.337% 
Loss D Fake: 0.7639 (0.8125) Acc D Fake: 6.667% 
Loss D: 1.271 
Loss G: 0.6378 (0.6000) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,902 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4452 (0.4883) Acc D Real: 87.359% 
Loss D Fake: 0.7642 (0.8122) Acc D Fake: 6.667% 
Loss D: 1.209 
Loss G: 0.6376 (0.6003) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,910 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.5307 (0.4886) Acc D Real: 87.393% 
Loss D Fake: 0.7647 (0.8118) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.6365 (0.6006) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,917 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4836 (0.4886) Acc D Real: 87.410% 
Loss D Fake: 0.7663 (0.8115) Acc D Fake: 6.667% 
Loss D: 1.250 
Loss G: 0.6350 (0.6008) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,925 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4845 (0.4885) Acc D Real: 87.438% 
Loss D Fake: 0.7680 (0.8112) Acc D Fake: 6.667% 
Loss D: 1.252 
Loss G: 0.6333 (0.6010) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,933 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4379 (0.4882) Acc D Real: 87.461% 
Loss D Fake: 0.7698 (0.8109) Acc D Fake: 6.667% 
Loss D: 1.208 
Loss G: 0.6320 (0.6012) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,940 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4079 (0.4876) Acc D Real: 87.503% 
Loss D Fake: 0.7709 (0.8106) Acc D Fake: 6.667% 
Loss D: 1.179 
Loss G: 0.6313 (0.6014) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,948 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4624 (0.4875) Acc D Real: 87.538% 
Loss D Fake: 0.7715 (0.8104) Acc D Fake: 6.667% 
Loss D: 1.234 
Loss G: 0.6306 (0.6016) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,956 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.5170 (0.4877) Acc D Real: 87.564% 
Loss D Fake: 0.7726 (0.8101) Acc D Fake: 6.667% 
Loss D: 1.290 
Loss G: 0.6292 (0.6018) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,964 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5060 (0.4878) Acc D Real: 87.574% 
Loss D Fake: 0.7745 (0.8099) Acc D Fake: 6.667% 
Loss D: 1.280 
Loss G: 0.6273 (0.6020) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,972 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4358 (0.4874) Acc D Real: 87.597% 
Loss D Fake: 0.7766 (0.8097) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.6257 (0.6022) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,980 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4367 (0.4871) Acc D Real: 87.630% 
Loss D Fake: 0.7781 (0.8095) Acc D Fake: 6.667% 
Loss D: 1.215 
Loss G: 0.6244 (0.6023) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,988 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4133 (0.4866) Acc D Real: 87.639% 
Loss D Fake: 0.7792 (0.8093) Acc D Fake: 6.667% 
Loss D: 1.192 
Loss G: 0.6239 (0.6024) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:52,996 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4949 (0.4867) Acc D Real: 87.669% 
Loss D Fake: 0.7797 (0.8091) Acc D Fake: 6.667% 
Loss D: 1.275 
Loss G: 0.6230 (0.6026) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,004 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4040 (0.4861) Acc D Real: 87.692% 
Loss D Fake: 0.7807 (0.8089) Acc D Fake: 6.667% 
Loss D: 1.185 
Loss G: 0.6226 (0.6027) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,012 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4519 (0.4859) Acc D Real: 87.705% 
Loss D Fake: 0.7809 (0.8087) Acc D Fake: 6.667% 
Loss D: 1.233 
Loss G: 0.6225 (0.6028) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,020 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4346 (0.4856) Acc D Real: 87.745% 
Loss D Fake: 0.7809 (0.8085) Acc D Fake: 6.667% 
Loss D: 1.216 
Loss G: 0.6224 (0.6030) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,028 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4878 (0.4856) Acc D Real: 87.777% 
Loss D Fake: 0.7813 (0.8083) Acc D Fake: 6.667% 
Loss D: 1.269 
Loss G: 0.6215 (0.6031) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,035 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4614 (0.4854) Acc D Real: 87.782% 
Loss D Fake: 0.7827 (0.8082) Acc D Fake: 6.667% 
Loss D: 1.244 
Loss G: 0.6199 (0.6032) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,268 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.374 | Generator Loss: 0.620 | Avg: 1.994 
2023-03-02 01:51:53,291 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.356 | Generator Loss: 0.620 | Avg: 1.976 
2023-03-02 01:51:53,315 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.361 | Generator Loss: 0.620 | Avg: 1.980 
2023-03-02 01:51:53,342 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.364 | Generator Loss: 0.620 | Avg: 1.984 
2023-03-02 01:51:53,373 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.369 | Generator Loss: 0.620 | Avg: 1.989 
2023-03-02 01:51:53,401 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.331 | Generator Loss: 0.620 | Avg: 1.950 
2023-03-02 01:51:53,428 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.333 | Generator Loss: 0.620 | Avg: 1.952 
2023-03-02 01:51:53,455 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.308 | Generator Loss: 0.620 | Avg: 1.928 
2023-03-02 01:51:53,482 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.300 | Generator Loss: 0.620 | Avg: 1.920 
2023-03-02 01:51:53,511 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.268 | Generator Loss: 0.620 | Avg: 1.888 
2023-03-02 01:51:53,538 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.248 | Generator Loss: 0.620 | Avg: 1.868 
2023-03-02 01:51:53,565 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.223 | Generator Loss: 0.620 | Avg: 1.842 
2023-03-02 01:51:53,593 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.206 | Generator Loss: 0.620 | Avg: 1.825 
2023-03-02 01:51:53,620 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.200 | Generator Loss: 0.620 | Avg: 1.820 
2023-03-02 01:51:53,647 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.212 | Generator Loss: 0.620 | Avg: 1.832 
2023-03-02 01:51:53,673 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.223 | Generator Loss: 0.620 | Avg: 1.843 
2023-03-02 01:51:53,700 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.232 | Generator Loss: 0.620 | Avg: 1.852 
2023-03-02 01:51:53,734 -                train: [    INFO] - 
Epoch: 9/20
2023-03-02 01:51:53,935 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4031 (0.4085) Acc D Real: 90.260% 
Loss D Fake: 0.7851 (0.7848) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6186 (0.6187) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,945 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4381 (0.4183) Acc D Real: 91.076% 
Loss D Fake: 0.7852 (0.7849) Acc D Fake: 6.667% 
Loss D: 1.223 
Loss G: 0.6184 (0.6186) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,953 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4191 (0.4185) Acc D Real: 91.016% 
Loss D Fake: 0.7852 (0.7850) Acc D Fake: 6.667% 
Loss D: 1.204 
Loss G: 0.6186 (0.6186) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,969 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4022 (0.4153) Acc D Real: 91.510% 
Loss D Fake: 0.7846 (0.7849) Acc D Fake: 6.667% 
Loss D: 1.187 
Loss G: 0.6192 (0.6187) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,976 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4186 (0.4158) Acc D Real: 91.918% 
Loss D Fake: 0.7838 (0.7847) Acc D Fake: 6.667% 
Loss D: 1.202 
Loss G: 0.6199 (0.6189) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,983 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4688 (0.4234) Acc D Real: 91.153% 
Loss D Fake: 0.7830 (0.7845) Acc D Fake: 6.667% 
Loss D: 1.252 
Loss G: 0.6206 (0.6192) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,990 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3392 (0.4129) Acc D Real: 91.823% 
Loss D Fake: 0.7819 (0.7841) Acc D Fake: 6.667% 
Loss D: 1.121 
Loss G: 0.6221 (0.6195) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:53,998 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3386 (0.4046) Acc D Real: 92.193% 
Loss D Fake: 0.7795 (0.7836) Acc D Fake: 6.667% 
Loss D: 1.118 
Loss G: 0.6247 (0.6201) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,006 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4533 (0.4095) Acc D Real: 92.198% 
Loss D Fake: 0.7764 (0.7829) Acc D Fake: 6.667% 
Loss D: 1.230 
Loss G: 0.6269 (0.6208) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,014 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4152 (0.4100) Acc D Real: 92.230% 
Loss D Fake: 0.7742 (0.7821) Acc D Fake: 6.667% 
Loss D: 1.189 
Loss G: 0.6287 (0.6215) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,021 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4300 (0.4117) Acc D Real: 92.287% 
Loss D Fake: 0.7724 (0.7813) Acc D Fake: 6.667% 
Loss D: 1.202 
Loss G: 0.6300 (0.6222) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,028 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3993 (0.4107) Acc D Real: 92.464% 
Loss D Fake: 0.7712 (0.7805) Acc D Fake: 6.667% 
Loss D: 1.171 
Loss G: 0.6309 (0.6229) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,035 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4369 (0.4126) Acc D Real: 92.452% 
Loss D Fake: 0.7704 (0.7798) Acc D Fake: 6.667% 
Loss D: 1.207 
Loss G: 0.6314 (0.6235) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,042 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4135 (0.4126) Acc D Real: 92.451% 
Loss D Fake: 0.7698 (0.7791) Acc D Fake: 6.667% 
Loss D: 1.183 
Loss G: 0.6319 (0.6240) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,049 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3983 (0.4117) Acc D Real: 92.500% 
Loss D Fake: 0.7690 (0.7785) Acc D Fake: 6.667% 
Loss D: 1.167 
Loss G: 0.6332 (0.6246) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,057 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4378 (0.4133) Acc D Real: 92.546% 
Loss D Fake: 0.7672 (0.7778) Acc D Fake: 6.667% 
Loss D: 1.205 
Loss G: 0.6349 (0.6252) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,064 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4316 (0.4143) Acc D Real: 92.474% 
Loss D Fake: 0.7653 (0.7771) Acc D Fake: 6.667% 
Loss D: 1.197 
Loss G: 0.6369 (0.6259) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,071 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.4047 (0.4138) Acc D Real: 92.519% 
Loss D Fake: 0.7626 (0.7764) Acc D Fake: 6.667% 
Loss D: 1.167 
Loss G: 0.6400 (0.6266) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,078 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4247 (0.4143) Acc D Real: 92.240% 
Loss D Fake: 0.7585 (0.7755) Acc D Fake: 6.667% 
Loss D: 1.183 
Loss G: 0.6447 (0.6275) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,086 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4624 (0.4166) Acc D Real: 92.205% 
Loss D Fake: 0.7536 (0.7744) Acc D Fake: 6.667% 
Loss D: 1.216 
Loss G: 0.6483 (0.6285) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,093 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4231 (0.4169) Acc D Real: 92.299% 
Loss D Fake: 0.7502 (0.7733) Acc D Fake: 6.667% 
Loss D: 1.173 
Loss G: 0.6516 (0.6296) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 01:51:54,100 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4278 (0.4174) Acc D Real: 92.235% 
Loss D Fake: 0.7466 (0.7722) Acc D Fake: 6.667% 
Loss D: 1.174 
Loss G: 0.6553 (0.6307) Acc G: 91.667% 
LR: 2.000e-04 

2023-03-02 01:51:54,107 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4516 (0.4188) Acc D Real: 92.159% 
Loss D Fake: 0.7430 (0.7710) Acc D Fake: 8.403% 
Loss D: 1.195 
Loss G: 0.6579 (0.6318) Acc G: 89.653% 
LR: 2.000e-04 

2023-03-02 01:51:54,115 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4465 (0.4199) Acc D Real: 92.079% 
Loss D Fake: 0.7414 (0.7698) Acc D Fake: 10.267% 
Loss D: 1.188 
Loss G: 0.6583 (0.6329) Acc G: 87.733% 
LR: 2.000e-04 

2023-03-02 01:51:54,122 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.4084 (0.4195) Acc D Real: 91.905% 
Loss D Fake: 0.7414 (0.7687) Acc D Fake: 12.051% 
Loss D: 1.150 
Loss G: 0.6604 (0.6339) Acc G: 85.769% 
LR: 2.000e-04 

2023-03-02 01:51:54,129 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4448 (0.4204) Acc D Real: 91.821% 
Loss D Fake: 0.7377 (0.7675) Acc D Fake: 14.012% 
Loss D: 1.183 
Loss G: 0.6643 (0.6351) Acc G: 83.765% 
LR: 2.000e-04 

2023-03-02 01:51:54,137 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3895 (0.4193) Acc D Real: 91.743% 
Loss D Fake: 0.7336 (0.7663) Acc D Fake: 15.952% 
Loss D: 1.123 
Loss G: 0.6680 (0.6362) Acc G: 81.845% 
LR: 2.000e-04 

2023-03-02 01:51:54,147 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3765 (0.4178) Acc D Real: 91.722% 
Loss D Fake: 0.7300 (0.7651) Acc D Fake: 17.816% 
Loss D: 1.107 
Loss G: 0.6712 (0.6374) Acc G: 80.000% 
LR: 2.000e-04 

2023-03-02 01:51:54,157 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3934 (0.4170) Acc D Real: 91.774% 
Loss D Fake: 0.7267 (0.7638) Acc D Fake: 19.611% 
Loss D: 1.120 
Loss G: 0.6744 (0.6387) Acc G: 78.222% 
LR: 2.000e-04 

2023-03-02 01:51:54,167 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3753 (0.4157) Acc D Real: 91.725% 
Loss D Fake: 0.7235 (0.7625) Acc D Fake: 21.344% 
Loss D: 1.099 
Loss G: 0.6773 (0.6399) Acc G: 76.559% 
LR: 2.000e-04 

2023-03-02 01:51:54,177 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3834 (0.4147) Acc D Real: 91.662% 
Loss D Fake: 0.7206 (0.7612) Acc D Fake: 22.969% 
Loss D: 1.104 
Loss G: 0.6805 (0.6412) Acc G: 74.948% 
LR: 2.000e-04 

2023-03-02 01:51:54,188 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4192 (0.4148) Acc D Real: 91.512% 
Loss D Fake: 0.7176 (0.7599) Acc D Fake: 24.545% 
Loss D: 1.137 
Loss G: 0.6828 (0.6424) Acc G: 73.434% 
LR: 2.000e-04 

2023-03-02 01:51:54,198 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3826 (0.4139) Acc D Real: 91.441% 
Loss D Fake: 0.7158 (0.7586) Acc D Fake: 26.029% 
Loss D: 1.098 
Loss G: 0.6848 (0.6437) Acc G: 72.010% 
LR: 2.000e-04 

2023-03-02 01:51:54,208 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4232 (0.4141) Acc D Real: 91.321% 
Loss D Fake: 0.7147 (0.7573) Acc D Fake: 27.429% 
Loss D: 1.138 
Loss G: 0.6841 (0.6448) Acc G: 70.667% 
LR: 2.000e-04 

2023-03-02 01:51:54,217 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3478 (0.4123) Acc D Real: 91.412% 
Loss D Fake: 0.7163 (0.7562) Acc D Fake: 28.750% 
Loss D: 1.064 
Loss G: 0.6860 (0.6460) Acc G: 69.398% 
LR: 2.000e-04 

2023-03-02 01:51:54,224 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3709 (0.4112) Acc D Real: 91.341% 
Loss D Fake: 0.7111 (0.7550) Acc D Fake: 30.000% 
Loss D: 1.082 
Loss G: 0.6921 (0.6472) Acc G: 68.153% 
LR: 2.000e-04 

2023-03-02 01:51:54,232 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3650 (0.4099) Acc D Real: 91.327% 
Loss D Fake: 0.7047 (0.7536) Acc D Fake: 31.228% 
Loss D: 1.070 
Loss G: 0.6976 (0.6486) Acc G: 66.974% 
LR: 2.000e-04 

2023-03-02 01:51:54,239 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4003 (0.4097) Acc D Real: 91.178% 
Loss D Fake: 0.7008 (0.7523) Acc D Fake: 32.436% 
Loss D: 1.101 
Loss G: 0.7002 (0.6499) Acc G: 65.812% 
LR: 2.000e-04 

2023-03-02 01:51:54,247 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4042 (0.4096) Acc D Real: 91.128% 
Loss D Fake: 0.6992 (0.7510) Acc D Fake: 33.583% 
Loss D: 1.103 
Loss G: 0.7014 (0.6512) Acc G: 64.708% 
LR: 2.000e-04 

2023-03-02 01:51:54,254 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4093 (0.4095) Acc D Real: 91.014% 
Loss D Fake: 0.6987 (0.7497) Acc D Fake: 34.675% 
Loss D: 1.108 
Loss G: 0.7019 (0.6524) Acc G: 63.659% 
LR: 2.000e-04 

2023-03-02 01:51:54,262 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3360 (0.4078) Acc D Real: 91.026% 
Loss D Fake: 0.6976 (0.7484) Acc D Fake: 35.714% 
Loss D: 1.034 
Loss G: 0.7052 (0.6537) Acc G: 62.659% 
LR: 2.000e-04 

2023-03-02 01:51:54,269 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4696 (0.4092) Acc D Real: 90.742% 
Loss D Fake: 0.6938 (0.7472) Acc D Fake: 36.705% 
Loss D: 1.163 
Loss G: 0.7071 (0.6549) Acc G: 61.705% 
LR: 2.000e-04 

2023-03-02 01:51:54,277 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3455 (0.4078) Acc D Real: 90.627% 
Loss D Fake: 0.6931 (0.7459) Acc D Fake: 37.652% 
Loss D: 1.039 
Loss G: 0.7080 (0.6561) Acc G: 60.795% 
LR: 2.000e-04 

2023-03-02 01:51:54,285 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3567 (0.4067) Acc D Real: 90.499% 
Loss D Fake: 0.6940 (0.7448) Acc D Fake: 38.556% 
Loss D: 1.051 
Loss G: 0.7005 (0.6571) Acc G: 59.926% 
LR: 2.000e-04 

2023-03-02 01:51:54,292 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3691 (0.4058) Acc D Real: 90.437% 
Loss D Fake: 0.8413 (0.7469) Acc D Fake: 38.188% 
Loss D: 1.210 
Loss G: 0.7070 (0.6582) Acc G: 59.058% 
LR: 2.000e-04 

2023-03-02 01:51:54,300 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3778 (0.4052) Acc D Real: 90.376% 
Loss D Fake: 0.7184 (0.7463) Acc D Fake: 38.617% 
Loss D: 1.096 
Loss G: 0.6852 (0.6588) Acc G: 58.582% 
LR: 2.000e-04 

2023-03-02 01:51:54,308 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3543 (0.4042) Acc D Real: 90.237% 
Loss D Fake: 0.7056 (0.7454) Acc D Fake: 39.236% 
Loss D: 1.060 
Loss G: 0.7301 (0.6603) Acc G: 57.743% 
LR: 2.000e-04 

2023-03-02 01:51:54,315 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3858 (0.4038) Acc D Real: 90.134% 
Loss D Fake: 0.6571 (0.7436) Acc D Fake: 40.102% 
Loss D: 1.043 
Loss G: 0.7548 (0.6622) Acc G: 56.905% 
LR: 2.000e-04 

2023-03-02 01:51:54,323 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3812 (0.4034) Acc D Real: 89.857% 
Loss D Fake: 0.6464 (0.7417) Acc D Fake: 40.969% 
Loss D: 1.028 
Loss G: 0.7626 (0.6642) Acc G: 56.070% 
LR: 2.000e-04 

2023-03-02 01:51:54,330 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4985 (0.4052) Acc D Real: 89.304% 
Loss D Fake: 0.6420 (0.7397) Acc D Fake: 41.832% 
Loss D: 1.140 
Loss G: 0.7653 (0.6662) Acc G: 55.265% 
LR: 2.000e-04 

2023-03-02 01:51:54,338 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4918 (0.4069) Acc D Real: 88.736% 
Loss D Fake: 0.6411 (0.7378) Acc D Fake: 42.662% 
Loss D: 1.133 
Loss G: 0.7650 (0.6681) Acc G: 54.490% 
LR: 2.000e-04 

2023-03-02 01:51:54,345 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3621 (0.4060) Acc D Real: 88.520% 
Loss D Fake: 0.6417 (0.7360) Acc D Fake: 43.461% 
Loss D: 1.004 
Loss G: 0.7642 (0.6699) Acc G: 53.745% 
LR: 2.000e-04 

2023-03-02 01:51:54,353 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.5193 (0.4081) Acc D Real: 87.914% 
Loss D Fake: 0.6431 (0.7343) Acc D Fake: 44.230% 
Loss D: 1.162 
Loss G: 0.7603 (0.6716) Acc G: 53.028% 
LR: 2.000e-04 

2023-03-02 01:51:54,361 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3693 (0.4074) Acc D Real: 87.719% 
Loss D Fake: 0.6471 (0.7327) Acc D Fake: 44.972% 
Loss D: 1.016 
Loss G: 0.7553 (0.6731) Acc G: 52.336% 
LR: 2.000e-04 

2023-03-02 01:51:54,368 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3967 (0.4072) Acc D Real: 87.504% 
Loss D Fake: 0.6516 (0.7313) Acc D Fake: 45.686% 
Loss D: 1.048 
Loss G: 0.7487 (0.6744) Acc G: 51.669% 
LR: 2.000e-04 

2023-03-02 01:51:54,376 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3088 (0.4055) Acc D Real: 87.475% 
Loss D Fake: 0.6584 (0.7300) Acc D Fake: 46.376% 
Loss D: 0.967 
Loss G: 0.7384 (0.6756) Acc G: 51.026% 
LR: 2.000e-04 

2023-03-02 01:51:54,384 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3678 (0.4049) Acc D Real: 87.322% 
Loss D Fake: 0.6687 (0.7289) Acc D Fake: 47.042% 
Loss D: 1.036 
Loss G: 0.7261 (0.6764) Acc G: 50.405% 
LR: 2.000e-04 

2023-03-02 01:51:54,393 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3858 (0.4045) Acc D Real: 87.180% 
Loss D Fake: 0.6806 (0.7281) Acc D Fake: 47.657% 
Loss D: 1.066 
Loss G: 0.7117 (0.6770) Acc G: 49.833% 
LR: 2.000e-04 

2023-03-02 01:51:54,401 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3750 (0.4040) Acc D Real: 87.072% 
Loss D Fake: 0.6951 (0.7276) Acc D Fake: 48.252% 
Loss D: 1.070 
Loss G: 0.6970 (0.6774) Acc G: 49.280% 
LR: 2.000e-04 

2023-03-02 01:51:54,409 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3207 (0.4027) Acc D Real: 87.007% 
Loss D Fake: 0.7081 (0.7272) Acc D Fake: 48.827% 
Loss D: 1.029 
Loss G: 0.6991 (0.6777) Acc G: 48.746% 
LR: 2.000e-04 

2023-03-02 01:51:54,416 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3586 (0.4020) Acc D Real: 86.936% 
Loss D Fake: 0.6952 (0.7267) Acc D Fake: 49.383% 
Loss D: 1.054 
Loss G: 0.7165 (0.6783) Acc G: 48.228% 
LR: 2.000e-04 

2023-03-02 01:51:54,424 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3053 (0.4004) Acc D Real: 87.030% 
Loss D Fake: 0.6755 (0.7259) Acc D Fake: 49.922% 
Loss D: 0.981 
Loss G: 0.7342 (0.6792) Acc G: 47.701% 
LR: 2.000e-04 

2023-03-02 01:51:54,432 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3151 (0.3991) Acc D Real: 87.060% 
Loss D Fake: 0.6603 (0.7249) Acc D Fake: 50.470% 
Loss D: 0.975 
Loss G: 0.7506 (0.6803) Acc G: 47.190% 
LR: 2.000e-04 

2023-03-02 01:51:54,439 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3948 (0.3990) Acc D Real: 86.935% 
Loss D Fake: 0.6480 (0.7237) Acc D Fake: 51.002% 
Loss D: 1.043 
Loss G: 0.7618 (0.6816) Acc G: 46.695% 
LR: 2.000e-04 

2023-03-02 01:51:54,447 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4419 (0.3997) Acc D Real: 86.701% 
Loss D Fake: 0.6407 (0.7224) Acc D Fake: 51.517% 
Loss D: 1.083 
Loss G: 0.7693 (0.6829) Acc G: 46.214% 
LR: 2.000e-04 

2023-03-02 01:51:54,454 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3123 (0.3984) Acc D Real: 86.699% 
Loss D Fake: 0.6347 (0.7211) Acc D Fake: 52.016% 
Loss D: 0.947 
Loss G: 0.7778 (0.6843) Acc G: 45.749% 
LR: 2.000e-04 

2023-03-02 01:51:54,462 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3889 (0.3982) Acc D Real: 86.620% 
Loss D Fake: 0.6471 (0.7200) Acc D Fake: 52.502% 
Loss D: 1.036 
Loss G: 0.4741 (0.6813) Acc G: 46.252% 
LR: 2.000e-04 

2023-03-02 01:51:54,470 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3913 (0.3981) Acc D Real: 86.470% 
Loss D Fake: 1.1128 (0.7257) Acc D Fake: 51.861% 
Loss D: 1.504 
Loss G: 0.4189 (0.6774) Acc G: 46.910% 
LR: 2.000e-04 

2023-03-02 01:51:54,477 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2888 (0.3966) Acc D Real: 86.516% 
Loss D Fake: 1.1566 (0.7319) Acc D Fake: 51.240% 
Loss D: 1.445 
Loss G: 0.4052 (0.6736) Acc G: 47.550% 
LR: 2.000e-04 

2023-03-02 01:51:54,485 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3330 (0.3957) Acc D Real: 86.545% 
Loss D Fake: 1.1668 (0.7380) Acc D Fake: 50.635% 
Loss D: 1.500 
Loss G: 0.4026 (0.6697) Acc G: 48.171% 
LR: 2.000e-04 

2023-03-02 01:51:54,493 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3182 (0.3946) Acc D Real: 86.611% 
Loss D Fake: 1.1588 (0.7439) Acc D Fake: 50.048% 
Loss D: 1.477 
Loss G: 0.4073 (0.6661) Acc G: 48.775% 
LR: 2.000e-04 

2023-03-02 01:51:54,500 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3384 (0.3938) Acc D Real: 86.654% 
Loss D Fake: 1.1365 (0.7492) Acc D Fake: 49.476% 
Loss D: 1.475 
Loss G: 0.4181 (0.6627) Acc G: 49.363% 
LR: 2.000e-04 

2023-03-02 01:51:54,508 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2861 (0.3924) Acc D Real: 86.765% 
Loss D Fake: 1.1030 (0.7540) Acc D Fake: 48.920% 
Loss D: 1.389 
Loss G: 0.4340 (0.6596) Acc G: 49.935% 
LR: 2.000e-04 

2023-03-02 01:51:54,516 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3123 (0.3913) Acc D Real: 86.838% 
Loss D Fake: 1.0621 (0.7581) Acc D Fake: 48.379% 
Loss D: 1.374 
Loss G: 0.4537 (0.6569) Acc G: 50.491% 
LR: 2.000e-04 

2023-03-02 01:51:54,524 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3218 (0.3904) Acc D Real: 86.913% 
Loss D Fake: 1.0183 (0.7615) Acc D Fake: 47.852% 
Loss D: 1.340 
Loss G: 0.4759 (0.6545) Acc G: 51.033% 
LR: 2.000e-04 

2023-03-02 01:51:54,531 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3485 (0.3899) Acc D Real: 86.974% 
Loss D Fake: 0.9748 (0.7643) Acc D Fake: 47.339% 
Loss D: 1.323 
Loss G: 0.4995 (0.6525) Acc G: 51.560% 
LR: 2.000e-04 

2023-03-02 01:51:54,538 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3304 (0.3891) Acc D Real: 87.055% 
Loss D Fake: 0.9320 (0.7665) Acc D Fake: 46.839% 
Loss D: 1.262 
Loss G: 0.5264 (0.6509) Acc G: 52.075% 
LR: 2.000e-04 

2023-03-02 01:51:54,546 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4053 (0.3893) Acc D Real: 87.154% 
Loss D Fake: 0.8863 (0.7680) Acc D Fake: 46.352% 
Loss D: 1.292 
Loss G: 0.5585 (0.6497) Acc G: 52.576% 
LR: 2.000e-04 

2023-03-02 01:51:54,553 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3957 (0.3894) Acc D Real: 87.204% 
Loss D Fake: 0.8358 (0.7688) Acc D Fake: 45.876% 
Loss D: 1.231 
Loss G: 0.5905 (0.6489) Acc G: 53.064% 
LR: 2.000e-04 

2023-03-02 01:51:54,560 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3864 (0.3893) Acc D Real: 87.274% 
Loss D Fake: 0.8009 (0.7692) Acc D Fake: 45.413% 
Loss D: 1.187 
Loss G: 0.6097 (0.6485) Acc G: 53.500% 
LR: 2.000e-04 

2023-03-02 01:51:54,568 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4006 (0.3895) Acc D Real: 87.271% 
Loss D Fake: 0.7821 (0.7694) Acc D Fake: 45.042% 
Loss D: 1.183 
Loss G: 0.6228 (0.6481) Acc G: 53.864% 
LR: 2.000e-04 

2023-03-02 01:51:54,576 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4296 (0.3900) Acc D Real: 87.208% 
Loss D Fake: 0.7686 (0.7694) Acc D Fake: 44.700% 
Loss D: 1.198 
Loss G: 0.6331 (0.6480) Acc G: 54.199% 
LR: 2.000e-04 

2023-03-02 01:51:54,583 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3537 (0.3895) Acc D Real: 87.212% 
Loss D Fake: 0.7577 (0.7692) Acc D Fake: 44.406% 
Loss D: 1.111 
Loss G: 0.6425 (0.6479) Acc G: 54.486% 
LR: 2.000e-04 

2023-03-02 01:51:54,591 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3376 (0.3889) Acc D Real: 87.240% 
Loss D Fake: 0.7475 (0.7690) Acc D Fake: 44.156% 
Loss D: 1.085 
Loss G: 0.6514 (0.6479) Acc G: 54.727% 
LR: 2.000e-04 

2023-03-02 01:51:54,599 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3526 (0.3885) Acc D Real: 87.266% 
Loss D Fake: 0.7382 (0.7686) Acc D Fake: 43.953% 
Loss D: 1.091 
Loss G: 0.6598 (0.6481) Acc G: 54.924% 
LR: 2.000e-04 

2023-03-02 01:51:54,606 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4109 (0.3888) Acc D Real: 87.228% 
Loss D Fake: 0.7297 (0.7682) Acc D Fake: 43.793% 
Loss D: 1.141 
Loss G: 0.6673 (0.6483) Acc G: 55.078% 
LR: 2.000e-04 

2023-03-02 01:51:54,613 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4275 (0.3892) Acc D Real: 87.156% 
Loss D Fake: 0.7225 (0.7677) Acc D Fake: 43.674% 
Loss D: 1.150 
Loss G: 0.6735 (0.6486) Acc G: 55.172% 
LR: 2.000e-04 

2023-03-02 01:51:54,620 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4556 (0.3899) Acc D Real: 87.096% 
Loss D Fake: 0.7168 (0.7671) Acc D Fake: 43.595% 
Loss D: 1.172 
Loss G: 0.6785 (0.6489) Acc G: 55.226% 
LR: 2.000e-04 

2023-03-02 01:51:54,628 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4886 (0.3910) Acc D Real: 86.930% 
Loss D Fake: 0.7125 (0.7665) Acc D Fake: 43.573% 
Loss D: 1.201 
Loss G: 0.6820 (0.6493) Acc G: 55.242% 
LR: 2.000e-04 

2023-03-02 01:51:54,635 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4457 (0.3916) Acc D Real: 86.846% 
Loss D Fake: 0.7098 (0.7659) Acc D Fake: 43.589% 
Loss D: 1.156 
Loss G: 0.6839 (0.6497) Acc G: 55.221% 
LR: 2.000e-04 

2023-03-02 01:51:54,642 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4502 (0.3923) Acc D Real: 86.785% 
Loss D Fake: 0.7087 (0.7652) Acc D Fake: 43.623% 
Loss D: 1.159 
Loss G: 0.6845 (0.6501) Acc G: 55.183% 
LR: 2.000e-04 

2023-03-02 01:51:54,649 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3951 (0.3923) Acc D Real: 86.750% 
Loss D Fake: 0.7084 (0.7646) Acc D Fake: 43.655% 
Loss D: 1.104 
Loss G: 0.6848 (0.6504) Acc G: 55.145% 
LR: 2.000e-04 

2023-03-02 01:51:54,656 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3858 (0.3922) Acc D Real: 86.735% 
Loss D Fake: 0.7085 (0.7640) Acc D Fake: 43.697% 
Loss D: 1.094 
Loss G: 0.6843 (0.6508) Acc G: 55.108% 
LR: 2.000e-04 

2023-03-02 01:51:54,664 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4391 (0.3927) Acc D Real: 86.637% 
Loss D Fake: 0.7096 (0.7635) Acc D Fake: 43.729% 
Loss D: 1.149 
Loss G: 0.6832 (0.6511) Acc G: 55.072% 
LR: 2.000e-04 

2023-03-02 01:51:54,671 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4679 (0.3935) Acc D Real: 86.540% 
Loss D Fake: 0.7112 (0.7629) Acc D Fake: 43.759% 
Loss D: 1.179 
Loss G: 0.6825 (0.6515) Acc G: 55.019% 
LR: 2.000e-04 

2023-03-02 01:51:54,678 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3616 (0.3932) Acc D Real: 86.512% 
Loss D Fake: 0.7108 (0.7624) Acc D Fake: 43.824% 
Loss D: 1.072 
Loss G: 0.6855 (0.6518) Acc G: 54.933% 
LR: 2.000e-04 

2023-03-02 01:51:54,685 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4291 (0.3935) Acc D Real: 86.482% 
Loss D Fake: 0.7059 (0.7618) Acc D Fake: 43.938% 
Loss D: 1.135 
Loss G: 0.6917 (0.6522) Acc G: 54.763% 
LR: 2.000e-04 

2023-03-02 01:51:54,692 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3736 (0.3933) Acc D Real: 86.388% 
Loss D Fake: 0.6978 (0.7611) Acc D Fake: 44.167% 
Loss D: 1.071 
Loss G: 0.7020 (0.6527) Acc G: 54.412% 
LR: 2.000e-04 

2023-03-02 01:51:54,700 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4697 (0.3941) Acc D Real: 86.330% 
Loss D Fake: 0.6875 (0.7604) Acc D Fake: 44.592% 
Loss D: 1.157 
Loss G: 0.7110 (0.6533) Acc G: 54.002% 
LR: 2.000e-04 

2023-03-02 01:51:54,707 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3627 (0.3938) Acc D Real: 86.280% 
Loss D Fake: 0.6791 (0.7596) Acc D Fake: 45.009% 
Loss D: 1.042 
Loss G: 0.7205 (0.6540) Acc G: 53.599% 
LR: 2.000e-04 

2023-03-02 01:51:54,714 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4173 (0.3940) Acc D Real: 86.262% 
Loss D Fake: 0.6707 (0.7587) Acc D Fake: 45.417% 
Loss D: 1.088 
Loss G: 0.7271 (0.6547) Acc G: 53.204% 
LR: 2.000e-04 

2023-03-02 01:51:54,721 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3327 (0.3934) Acc D Real: 86.289% 
Loss D Fake: 0.6699 (0.7579) Acc D Fake: 45.818% 
Loss D: 1.003 
Loss G: 0.7304 (0.6554) Acc G: 52.817% 
LR: 2.000e-04 

2023-03-02 01:51:54,729 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3949 (0.3934) Acc D Real: 86.287% 
Loss D Fake: 0.6630 (0.7570) Acc D Fake: 46.210% 
Loss D: 1.058 
Loss G: 0.7347 (0.6562) Acc G: 52.437% 
LR: 2.000e-04 

2023-03-02 01:51:54,736 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3472 (0.3930) Acc D Real: 86.309% 
Loss D Fake: 0.6608 (0.7560) Acc D Fake: 46.596% 
Loss D: 1.008 
Loss G: 0.7372 (0.6569) Acc G: 52.065% 
LR: 2.000e-04 

2023-03-02 01:51:54,743 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4047 (0.3931) Acc D Real: 86.261% 
Loss D Fake: 0.6590 (0.7551) Acc D Fake: 46.974% 
Loss D: 1.064 
Loss G: 0.7385 (0.6577) Acc G: 51.700% 
LR: 2.000e-04 

2023-03-02 01:51:54,750 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4225 (0.3934) Acc D Real: 86.173% 
Loss D Fake: 0.6582 (0.7542) Acc D Fake: 47.345% 
Loss D: 1.081 
Loss G: 0.7408 (0.6585) Acc G: 51.341% 
LR: 2.000e-04 

2023-03-02 01:51:54,757 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3465 (0.3930) Acc D Real: 86.208% 
Loss D Fake: 0.6557 (0.7533) Acc D Fake: 47.709% 
Loss D: 1.002 
Loss G: 0.7439 (0.6593) Acc G: 50.989% 
LR: 2.000e-04 

2023-03-02 01:51:54,764 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4135 (0.3931) Acc D Real: 86.139% 
Loss D Fake: 0.6526 (0.7524) Acc D Fake: 48.066% 
Loss D: 1.066 
Loss G: 0.7486 (0.6601) Acc G: 50.644% 
LR: 2.000e-04 

2023-03-02 01:51:54,772 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4169 (0.3934) Acc D Real: 86.057% 
Loss D Fake: 0.6481 (0.7514) Acc D Fake: 48.417% 
Loss D: 1.065 
Loss G: 0.7525 (0.6609) Acc G: 50.304% 
LR: 2.000e-04 

2023-03-02 01:51:54,779 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3656 (0.3931) Acc D Real: 86.065% 
Loss D Fake: 0.6458 (0.7505) Acc D Fake: 48.762% 
Loss D: 1.011 
Loss G: 0.7543 (0.6618) Acc G: 49.971% 
LR: 2.000e-04 

2023-03-02 01:51:54,786 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3857 (0.3930) Acc D Real: 86.019% 
Loss D Fake: 0.6446 (0.7495) Acc D Fake: 49.100% 
Loss D: 1.030 
Loss G: 0.7545 (0.6626) Acc G: 49.644% 
LR: 2.000e-04 

2023-03-02 01:51:54,793 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3595 (0.3928) Acc D Real: 85.973% 
Loss D Fake: 0.6464 (0.7486) Acc D Fake: 49.433% 
Loss D: 1.006 
Loss G: 0.7559 (0.6634) Acc G: 49.323% 
LR: 2.000e-04 

2023-03-02 01:51:54,801 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3684 (0.3925) Acc D Real: 85.937% 
Loss D Fake: 0.6422 (0.7477) Acc D Fake: 49.759% 
Loss D: 1.011 
Loss G: 0.7587 (0.6643) Acc G: 49.007% 
LR: 2.000e-04 

2023-03-02 01:51:54,809 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3508 (0.3922) Acc D Real: 85.901% 
Loss D Fake: 0.6425 (0.7468) Acc D Fake: 50.080% 
Loss D: 0.993 
Loss G: 0.7480 (0.6650) Acc G: 48.697% 
LR: 2.000e-04 

2023-03-02 01:51:54,817 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3068 (0.3914) Acc D Real: 85.904% 
Loss D Fake: 0.7061 (0.7464) Acc D Fake: 50.194% 
Loss D: 1.013 
Loss G: 0.7698 (0.6659) Acc G: 48.392% 
LR: 2.000e-04 

2023-03-02 01:51:54,825 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3676 (0.3912) Acc D Real: 85.822% 
Loss D Fake: 0.6421 (0.7455) Acc D Fake: 50.506% 
Loss D: 1.010 
Loss G: 0.7871 (0.6669) Acc G: 48.093% 
LR: 2.000e-04 

2023-03-02 01:51:54,833 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3520 (0.3909) Acc D Real: 85.757% 
Loss D Fake: 0.6129 (0.7444) Acc D Fake: 50.813% 
Loss D: 0.965 
Loss G: 0.7967 (0.6680) Acc G: 47.784% 
LR: 2.000e-04 

2023-03-02 01:51:54,840 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3349 (0.3904) Acc D Real: 85.709% 
Loss D Fake: 0.6093 (0.7433) Acc D Fake: 51.128% 
Loss D: 0.944 
Loss G: 0.8015 (0.6692) Acc G: 47.480% 
LR: 2.000e-04 

2023-03-02 01:51:54,847 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3405 (0.3900) Acc D Real: 85.647% 
Loss D Fake: 0.6051 (0.7421) Acc D Fake: 51.438% 
Loss D: 0.946 
Loss G: 0.8077 (0.6703) Acc G: 47.182% 
LR: 2.000e-04 

2023-03-02 01:51:54,854 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2938 (0.3892) Acc D Real: 85.648% 
Loss D Fake: 0.5996 (0.7409) Acc D Fake: 51.743% 
Loss D: 0.893 
Loss G: 0.8157 (0.6715) Acc G: 46.888% 
LR: 2.000e-04 

2023-03-02 01:51:54,862 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3593 (0.3890) Acc D Real: 85.587% 
Loss D Fake: 0.5938 (0.7397) Acc D Fake: 52.043% 
Loss D: 0.953 
Loss G: 0.8223 (0.6728) Acc G: 46.600% 
LR: 2.000e-04 

2023-03-02 01:51:54,869 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3449 (0.3886) Acc D Real: 85.528% 
Loss D Fake: 0.5897 (0.7385) Acc D Fake: 52.338% 
Loss D: 0.935 
Loss G: 0.8268 (0.6740) Acc G: 46.316% 
LR: 2.000e-04 

2023-03-02 01:51:54,876 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3388 (0.3882) Acc D Real: 85.465% 
Loss D Fake: 0.5863 (0.7373) Acc D Fake: 52.628% 
Loss D: 0.925 
Loss G: 0.8330 (0.6753) Acc G: 46.036% 
LR: 2.000e-04 

2023-03-02 01:51:54,884 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2995 (0.3875) Acc D Real: 85.435% 
Loss D Fake: 0.5817 (0.7361) Acc D Fake: 52.914% 
Loss D: 0.881 
Loss G: 0.8387 (0.6766) Acc G: 45.761% 
LR: 2.000e-04 

2023-03-02 01:51:54,891 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3638 (0.3873) Acc D Real: 85.381% 
Loss D Fake: 0.5786 (0.7348) Acc D Fake: 53.195% 
Loss D: 0.942 
Loss G: 0.8411 (0.6779) Acc G: 45.491% 
LR: 2.000e-04 

2023-03-02 01:51:54,898 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2794 (0.3865) Acc D Real: 85.374% 
Loss D Fake: 0.5770 (0.7336) Acc D Fake: 53.472% 
Loss D: 0.856 
Loss G: 0.8455 (0.6792) Acc G: 45.224% 
LR: 2.000e-04 

2023-03-02 01:51:54,906 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3750 (0.3864) Acc D Real: 85.298% 
Loss D Fake: 0.5737 (0.7323) Acc D Fake: 53.744% 
Loss D: 0.949 
Loss G: 0.8492 (0.6806) Acc G: 44.962% 
LR: 2.000e-04 

2023-03-02 01:51:54,913 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3794 (0.3863) Acc D Real: 85.211% 
Loss D Fake: 0.5728 (0.7311) Acc D Fake: 54.012% 
Loss D: 0.952 
Loss G: 0.8476 (0.6818) Acc G: 44.704% 
LR: 2.000e-04 

2023-03-02 01:51:54,920 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3716 (0.3862) Acc D Real: 85.151% 
Loss D Fake: 0.5769 (0.7299) Acc D Fake: 54.276% 
Loss D: 0.948 
Loss G: 0.8417 (0.6831) Acc G: 44.450% 
LR: 2.000e-04 

2023-03-02 01:51:54,929 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2621 (0.3853) Acc D Real: 85.145% 
Loss D Fake: 0.5775 (0.7287) Acc D Fake: 54.536% 
Loss D: 0.840 
Loss G: 0.8555 (0.6844) Acc G: 44.200% 
LR: 2.000e-04 

2023-03-02 01:51:54,939 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3208 (0.3848) Acc D Real: 85.107% 
Loss D Fake: 0.5617 (0.7275) Acc D Fake: 54.792% 
Loss D: 0.883 
Loss G: 0.8738 (0.6858) Acc G: 43.953% 
LR: 2.000e-04 

2023-03-02 01:51:54,949 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.2919 (0.3841) Acc D Real: 85.099% 
Loss D Fake: 0.5511 (0.7261) Acc D Fake: 55.044% 
Loss D: 0.843 
Loss G: 0.8873 (0.6873) Acc G: 43.710% 
LR: 2.000e-04 

2023-03-02 01:51:54,959 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3534 (0.3839) Acc D Real: 85.010% 
Loss D Fake: 0.5434 (0.7248) Acc D Fake: 55.293% 
Loss D: 0.897 
Loss G: 0.8978 (0.6889) Acc G: 43.471% 
LR: 2.000e-04 

2023-03-02 01:51:54,969 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4057 (0.3840) Acc D Real: 84.900% 
Loss D Fake: 0.5380 (0.7234) Acc D Fake: 55.537% 
Loss D: 0.944 
Loss G: 0.9038 (0.6905) Acc G: 43.236% 
LR: 2.000e-04 

2023-03-02 01:51:54,978 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3494 (0.3838) Acc D Real: 84.829% 
Loss D Fake: 0.5351 (0.7220) Acc D Fake: 55.779% 
Loss D: 0.885 
Loss G: 0.9083 (0.6921) Acc G: 43.004% 
LR: 2.000e-04 

2023-03-02 01:51:54,988 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2670 (0.3829) Acc D Real: 84.824% 
Loss D Fake: 0.5326 (0.7206) Acc D Fake: 56.016% 
Loss D: 0.800 
Loss G: 0.9121 (0.6937) Acc G: 42.775% 
LR: 2.000e-04 

2023-03-02 01:51:54,996 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3781 (0.3829) Acc D Real: 84.735% 
Loss D Fake: 0.5306 (0.7192) Acc D Fake: 56.250% 
Loss D: 0.909 
Loss G: 0.9145 (0.6953) Acc G: 42.549% 
LR: 2.000e-04 

2023-03-02 01:51:55,004 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3463 (0.3826) Acc D Real: 84.674% 
Loss D Fake: 0.5293 (0.7179) Acc D Fake: 56.481% 
Loss D: 0.876 
Loss G: 0.9171 (0.6969) Acc G: 42.327% 
LR: 2.000e-04 

2023-03-02 01:51:55,012 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3396 (0.3823) Acc D Real: 84.615% 
Loss D Fake: 0.5277 (0.7165) Acc D Fake: 56.709% 
Loss D: 0.867 
Loss G: 0.9193 (0.6985) Acc G: 42.108% 
LR: 2.000e-04 

2023-03-02 01:51:55,020 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3063 (0.3818) Acc D Real: 84.580% 
Loss D Fake: 0.5266 (0.7152) Acc D Fake: 56.933% 
Loss D: 0.833 
Loss G: 0.9212 (0.7001) Acc G: 41.892% 
LR: 2.000e-04 

2023-03-02 01:51:55,027 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2520 (0.3808) Acc D Real: 84.595% 
Loss D Fake: 0.5255 (0.7138) Acc D Fake: 57.154% 
Loss D: 0.777 
Loss G: 0.9232 (0.7016) Acc G: 41.680% 
LR: 2.000e-04 

2023-03-02 01:51:55,035 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3453 (0.3806) Acc D Real: 84.540% 
Loss D Fake: 0.5243 (0.7125) Acc D Fake: 57.372% 
Loss D: 0.870 
Loss G: 0.9255 (0.7032) Acc G: 41.470% 
LR: 2.000e-04 

2023-03-02 01:51:55,042 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4166 (0.3808) Acc D Real: 84.421% 
Loss D Fake: 0.5243 (0.7112) Acc D Fake: 57.587% 
Loss D: 0.941 
Loss G: 0.9206 (0.7047) Acc G: 41.263% 
LR: 2.000e-04 

2023-03-02 01:51:55,050 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3695 (0.3808) Acc D Real: 84.349% 
Loss D Fake: 0.5312 (0.7100) Acc D Fake: 57.799% 
Loss D: 0.901 
Loss G: 0.9088 (0.7061) Acc G: 41.059% 
LR: 2.000e-04 

2023-03-02 01:51:55,057 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3611 (0.3806) Acc D Real: 84.281% 
Loss D Fake: 0.5447 (0.7088) Acc D Fake: 58.008% 
Loss D: 0.906 
Loss G: 0.9118 (0.7075) Acc G: 40.857% 
LR: 2.000e-04 

2023-03-02 01:51:55,065 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4042 (0.3808) Acc D Real: 84.172% 
Loss D Fake: 0.5286 (0.7076) Acc D Fake: 58.215% 
Loss D: 0.933 
Loss G: 0.9230 (0.7090) Acc G: 40.659% 
LR: 2.000e-04 

2023-03-02 01:51:55,072 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4076 (0.3810) Acc D Real: 84.068% 
Loss D Fake: 0.5249 (0.7064) Acc D Fake: 58.418% 
Loss D: 0.932 
Loss G: 0.9228 (0.7104) Acc G: 40.463% 
LR: 2.000e-04 

2023-03-02 01:51:55,080 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4233 (0.3813) Acc D Real: 83.945% 
Loss D Fake: 0.5263 (0.7052) Acc D Fake: 58.619% 
Loss D: 0.950 
Loss G: 0.9201 (0.7119) Acc G: 40.270% 
LR: 2.000e-04 

2023-03-02 01:51:55,087 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2947 (0.3807) Acc D Real: 83.936% 
Loss D Fake: 0.5272 (0.7040) Acc D Fake: 58.817% 
Loss D: 0.822 
Loss G: 0.9207 (0.7132) Acc G: 40.079% 
LR: 2.000e-04 

2023-03-02 01:51:55,094 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2943 (0.3801) Acc D Real: 83.916% 
Loss D Fake: 0.5264 (0.7028) Acc D Fake: 59.012% 
Loss D: 0.821 
Loss G: 0.9244 (0.7146) Acc G: 39.891% 
LR: 2.000e-04 

2023-03-02 01:51:55,102 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2909 (0.3795) Acc D Real: 83.899% 
Loss D Fake: 0.5233 (0.7016) Acc D Fake: 59.216% 
Loss D: 0.814 
Loss G: 0.9307 (0.7161) Acc G: 39.694% 
LR: 2.000e-04 

2023-03-02 01:51:55,109 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3306 (0.3792) Acc D Real: 83.857% 
Loss D Fake: 0.5199 (0.7004) Acc D Fake: 59.418% 
Loss D: 0.850 
Loss G: 0.9359 (0.7175) Acc G: 39.500% 
LR: 2.000e-04 

2023-03-02 01:51:55,117 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3517 (0.3790) Acc D Real: 83.789% 
Loss D Fake: 0.5177 (0.6992) Acc D Fake: 59.616% 
Loss D: 0.869 
Loss G: 0.9415 (0.7190) Acc G: 39.308% 
LR: 2.000e-04 

2023-03-02 01:51:55,124 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2808 (0.3784) Acc D Real: 83.772% 
Loss D Fake: 0.5128 (0.6980) Acc D Fake: 59.812% 
Loss D: 0.794 
Loss G: 0.9530 (0.7205) Acc G: 39.119% 
LR: 2.000e-04 

2023-03-02 01:51:55,131 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3481 (0.3782) Acc D Real: 83.704% 
Loss D Fake: 0.5059 (0.6968) Acc D Fake: 60.006% 
Loss D: 0.854 
Loss G: 0.9623 (0.7220) Acc G: 38.933% 
LR: 2.000e-04 

2023-03-02 01:51:55,139 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3414 (0.3780) Acc D Real: 83.655% 
Loss D Fake: 0.5018 (0.6956) Acc D Fake: 60.197% 
Loss D: 0.843 
Loss G: 0.9677 (0.7236) Acc G: 38.748% 
LR: 2.000e-04 

2023-03-02 01:51:55,146 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5217 (0.3789) Acc D Real: 83.639% 
Loss D Fake: 0.5153 (0.6944) Acc D Fake: 60.215% 
Loss D: 1.037 
Loss G: 0.9333 (0.7249) Acc G: 38.731% 
LR: 2.000e-04 

2023-03-02 01:51:55,400 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.144 | Generator Loss: 0.931 | Avg: 2.075 
2023-03-02 01:51:55,423 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.047 | Generator Loss: 0.931 | Avg: 1.978 
2023-03-02 01:51:55,446 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.066 | Generator Loss: 0.931 | Avg: 1.997 
2023-03-02 01:51:55,473 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.123 | Generator Loss: 0.931 | Avg: 2.054 
2023-03-02 01:51:55,500 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.118 | Generator Loss: 0.931 | Avg: 2.049 
2023-03-02 01:51:55,526 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.071 | Generator Loss: 0.931 | Avg: 2.002 
2023-03-02 01:51:55,553 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.034 | Generator Loss: 0.931 | Avg: 1.965 
2023-03-02 01:51:55,579 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.996 | Generator Loss: 0.931 | Avg: 1.926 
2023-03-02 01:51:55,608 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.971 | Generator Loss: 0.931 | Avg: 1.902 
2023-03-02 01:51:55,634 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.938 | Generator Loss: 0.931 | Avg: 1.868 
2023-03-02 01:51:55,660 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.914 | Generator Loss: 0.931 | Avg: 1.845 
2023-03-02 01:51:55,686 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.889 | Generator Loss: 0.931 | Avg: 1.820 
2023-03-02 01:51:55,712 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.870 | Generator Loss: 0.931 | Avg: 1.801 
2023-03-02 01:51:55,738 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.859 | Generator Loss: 0.931 | Avg: 1.789 
2023-03-02 01:51:55,763 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.860 | Generator Loss: 0.931 | Avg: 1.791 
2023-03-02 01:51:55,789 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.870 | Generator Loss: 0.931 | Avg: 1.800 
2023-03-02 01:51:55,815 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.882 | Generator Loss: 0.931 | Avg: 1.812 
2023-03-02 01:51:55,848 -                train: [    INFO] - 
Epoch: 10/20
2023-03-02 01:51:56,050 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4066 (0.3858) Acc D Real: 69.505% 
Loss D Fake: 0.5019 (0.5122) Acc D Fake: 90.000% 
Loss D: 0.909 
Loss G: 0.9563 (0.9650) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,057 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3511 (0.3743) Acc D Real: 70.955% 
Loss D Fake: 0.5153 (0.5132) Acc D Fake: 90.000% 
Loss D: 0.866 
Loss G: 0.9540 (0.9613) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,070 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2639 (0.3467) Acc D Real: 74.479% 
Loss D Fake: 0.5050 (0.5112) Acc D Fake: 90.000% 
Loss D: 0.769 
Loss G: 0.9738 (0.9644) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,091 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4445 (0.3662) Acc D Real: 72.750% 
Loss D Fake: 0.4961 (0.5082) Acc D Fake: 90.000% 
Loss D: 0.941 
Loss G: 0.9790 (0.9674) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,099 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2741 (0.3509) Acc D Real: 74.731% 
Loss D Fake: 0.4951 (0.5060) Acc D Fake: 90.000% 
Loss D: 0.769 
Loss G: 0.9839 (0.9701) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,106 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3709 (0.3537) Acc D Real: 74.598% 
Loss D Fake: 0.4921 (0.5040) Acc D Fake: 90.000% 
Loss D: 0.863 
Loss G: 0.9888 (0.9728) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,113 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3171 (0.3492) Acc D Real: 75.228% 
Loss D Fake: 0.4898 (0.5022) Acc D Fake: 90.000% 
Loss D: 0.807 
Loss G: 0.9940 (0.9754) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,120 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3671 (0.3512) Acc D Real: 74.925% 
Loss D Fake: 0.4871 (0.5005) Acc D Fake: 90.000% 
Loss D: 0.854 
Loss G: 0.9989 (0.9780) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,127 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3338 (0.3494) Acc D Real: 75.031% 
Loss D Fake: 0.4871 (0.4992) Acc D Fake: 90.000% 
Loss D: 0.821 
Loss G: 0.9907 (0.9793) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,134 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3030 (0.3452) Acc D Real: 75.559% 
Loss D Fake: 0.4949 (0.4988) Acc D Fake: 90.000% 
Loss D: 0.798 
Loss G: 0.9797 (0.9793) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,141 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2862 (0.3403) Acc D Real: 76.029% 
Loss D Fake: 0.4999 (0.4989) Acc D Fake: 90.000% 
Loss D: 0.786 
Loss G: 0.9835 (0.9797) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,149 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.2988 (0.3371) Acc D Real: 76.462% 
Loss D Fake: 0.4922 (0.4984) Acc D Fake: 89.872% 
Loss D: 0.791 
Loss G: 0.9994 (0.9812) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 01:51:56,159 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3546 (0.3383) Acc D Real: 76.261% 
Loss D Fake: 0.4846 (0.4974) Acc D Fake: 89.762% 
Loss D: 0.839 
Loss G: 1.0055 (0.9829) Acc G: 10.119% 
LR: 2.000e-04 

2023-03-02 01:51:56,169 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3016 (0.3359) Acc D Real: 76.479% 
Loss D Fake: 0.4857 (0.4966) Acc D Fake: 89.667% 
Loss D: 0.787 
Loss G: 0.9920 (0.9835) Acc G: 10.222% 
LR: 2.000e-04 

2023-03-02 01:51:56,178 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3707 (0.3381) Acc D Real: 76.201% 
Loss D Fake: 0.5165 (0.4979) Acc D Fake: 89.583% 
Loss D: 0.887 
Loss G: 1.0134 (0.9854) Acc G: 10.312% 
LR: 2.000e-04 

2023-03-02 01:51:56,186 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3345 (0.3379) Acc D Real: 76.241% 
Loss D Fake: 0.4987 (0.4979) Acc D Fake: 89.510% 
Loss D: 0.833 
Loss G: 0.9741 (0.9847) Acc G: 10.392% 
LR: 2.000e-04 

2023-03-02 01:51:56,194 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4154 (0.3422) Acc D Real: 75.660% 
Loss D Fake: 0.4917 (0.4976) Acc D Fake: 89.444% 
Loss D: 0.907 
Loss G: 1.0445 (0.9881) Acc G: 10.463% 
LR: 2.000e-04 

2023-03-02 01:51:56,201 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2770 (0.3387) Acc D Real: 76.028% 
Loss D Fake: 0.4595 (0.4956) Acc D Fake: 89.298% 
Loss D: 0.736 
Loss G: 1.0542 (0.9916) Acc G: 10.614% 
LR: 2.000e-04 

2023-03-02 01:51:56,208 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4116 (0.3424) Acc D Real: 75.573% 
Loss D Fake: 0.4606 (0.4938) Acc D Fake: 89.167% 
Loss D: 0.872 
Loss G: 1.0495 (0.9944) Acc G: 10.750% 
LR: 2.000e-04 

2023-03-02 01:51:56,217 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4019 (0.3452) Acc D Real: 75.312% 
Loss D Fake: 0.4647 (0.4924) Acc D Fake: 89.048% 
Loss D: 0.867 
Loss G: 1.0365 (0.9964) Acc G: 10.873% 
LR: 2.000e-04 

2023-03-02 01:51:56,227 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4180 (0.3485) Acc D Real: 74.934% 
Loss D Fake: 0.4741 (0.4916) Acc D Fake: 88.939% 
Loss D: 0.892 
Loss G: 1.0169 (0.9974) Acc G: 10.985% 
LR: 2.000e-04 

2023-03-02 01:51:56,236 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3515 (0.3487) Acc D Real: 74.880% 
Loss D Fake: 0.4893 (0.4915) Acc D Fake: 88.841% 
Loss D: 0.841 
Loss G: 0.9990 (0.9975) Acc G: 11.087% 
LR: 2.000e-04 

2023-03-02 01:51:56,245 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.2168 (0.3432) Acc D Real: 75.510% 
Loss D Fake: 0.4937 (0.4916) Acc D Fake: 88.750% 
Loss D: 0.711 
Loss G: 1.0206 (0.9984) Acc G: 11.181% 
LR: 2.000e-04 

2023-03-02 01:51:56,253 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.2833 (0.3408) Acc D Real: 75.744% 
Loss D Fake: 0.4752 (0.4909) Acc D Fake: 88.667% 
Loss D: 0.758 
Loss G: 1.0322 (0.9998) Acc G: 11.267% 
LR: 2.000e-04 

2023-03-02 01:51:56,260 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3157 (0.3398) Acc D Real: 75.921% 
Loss D Fake: 0.4724 (0.4902) Acc D Fake: 88.590% 
Loss D: 0.788 
Loss G: 1.0382 (1.0012) Acc G: 11.346% 
LR: 2.000e-04 

2023-03-02 01:51:56,267 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4523 (0.3440) Acc D Real: 75.459% 
Loss D Fake: 0.4684 (0.4894) Acc D Fake: 88.519% 
Loss D: 0.921 
Loss G: 1.0477 (1.0030) Acc G: 11.420% 
LR: 2.000e-04 

2023-03-02 01:51:56,277 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3227 (0.3432) Acc D Real: 75.502% 
Loss D Fake: 0.4636 (0.4885) Acc D Fake: 88.512% 
Loss D: 0.786 
Loss G: 1.0497 (1.0046) Acc G: 11.429% 
LR: 2.000e-04 

2023-03-02 01:51:56,286 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3477 (0.3434) Acc D Real: 75.424% 
Loss D Fake: 0.4695 (0.4878) Acc D Fake: 88.506% 
Loss D: 0.817 
Loss G: 0.9951 (1.0043) Acc G: 11.437% 
LR: 2.000e-04 

2023-03-02 01:51:56,295 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3701 (0.3443) Acc D Real: 75.292% 
Loss D Fake: 1.5329 (0.5227) Acc D Fake: 86.444% 
Loss D: 1.903 
Loss G: 0.4612 (0.9862) Acc G: 13.167% 
LR: 2.000e-04 

2023-03-02 01:51:56,304 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3360 (0.3440) Acc D Real: 75.247% 
Loss D Fake: 1.3235 (0.5485) Acc D Fake: 84.839% 
Loss D: 1.659 
Loss G: 0.8414 (0.9815) Acc G: 13.118% 
LR: 2.000e-04 

2023-03-02 01:51:56,314 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3819 (0.3452) Acc D Real: 75.003% 
Loss D Fake: 0.5797 (0.5495) Acc D Fake: 84.948% 
Loss D: 0.962 
Loss G: 0.8374 (0.9770) Acc G: 13.073% 
LR: 2.000e-04 

2023-03-02 01:51:56,324 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.5612 (0.3517) Acc D Real: 74.145% 
Loss D Fake: 0.5741 (0.5502) Acc D Fake: 85.051% 
Loss D: 1.135 
Loss G: 0.8447 (0.9730) Acc G: 13.030% 
LR: 2.000e-04 

2023-03-02 01:51:56,333 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5163 (0.3566) Acc D Real: 73.511% 
Loss D Fake: 0.5693 (0.5508) Acc D Fake: 85.147% 
Loss D: 1.086 
Loss G: 0.8495 (0.9694) Acc G: 12.990% 
LR: 2.000e-04 

2023-03-02 01:51:56,342 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5403 (0.3618) Acc D Real: 72.817% 
Loss D Fake: 0.5669 (0.5512) Acc D Fake: 85.284% 
Loss D: 1.107 
Loss G: 0.8513 (0.9660) Acc G: 12.905% 
LR: 2.000e-04 

2023-03-02 01:51:56,351 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4761 (0.3650) Acc D Real: 72.352% 
Loss D Fake: 0.5665 (0.5517) Acc D Fake: 85.415% 
Loss D: 1.043 
Loss G: 0.8509 (0.9628) Acc G: 12.824% 
LR: 2.000e-04 

2023-03-02 01:51:56,360 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5092 (0.3689) Acc D Real: 71.857% 
Loss D Fake: 0.5673 (0.5521) Acc D Fake: 85.539% 
Loss D: 1.076 
Loss G: 0.8488 (0.9597) Acc G: 12.748% 
LR: 2.000e-04 

2023-03-02 01:51:56,369 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5921 (0.3748) Acc D Real: 71.073% 
Loss D Fake: 0.5695 (0.5525) Acc D Fake: 85.657% 
Loss D: 1.162 
Loss G: 0.8442 (0.9567) Acc G: 12.763% 
LR: 2.000e-04 

2023-03-02 01:51:56,378 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5186 (0.3784) Acc D Real: 70.576% 
Loss D Fake: 0.5734 (0.5531) Acc D Fake: 85.682% 
Loss D: 1.092 
Loss G: 0.8381 (0.9537) Acc G: 12.778% 
LR: 2.000e-04 

2023-03-02 01:51:56,387 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5125 (0.3818) Acc D Real: 70.064% 
Loss D Fake: 0.5782 (0.5537) Acc D Fake: 85.707% 
Loss D: 1.091 
Loss G: 0.8310 (0.9506) Acc G: 12.792% 
LR: 2.000e-04 

2023-03-02 01:51:56,396 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5083 (0.3849) Acc D Real: 69.605% 
Loss D Fake: 0.5835 (0.5544) Acc D Fake: 85.730% 
Loss D: 1.092 
Loss G: 0.8234 (0.9475) Acc G: 12.805% 
LR: 2.000e-04 

2023-03-02 01:51:56,406 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3914 (0.3850) Acc D Real: 69.475% 
Loss D Fake: 0.5890 (0.5553) Acc D Fake: 85.753% 
Loss D: 0.980 
Loss G: 0.8166 (0.9444) Acc G: 12.817% 
LR: 2.000e-04 

2023-03-02 01:51:56,416 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4759 (0.3872) Acc D Real: 69.110% 
Loss D Fake: 0.5940 (0.5562) Acc D Fake: 85.774% 
Loss D: 1.070 
Loss G: 0.8096 (0.9412) Acc G: 12.829% 
LR: 2.000e-04 

2023-03-02 01:51:56,425 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4764 (0.3892) Acc D Real: 68.780% 
Loss D Fake: 0.5993 (0.5571) Acc D Fake: 85.794% 
Loss D: 1.076 
Loss G: 0.8026 (0.9381) Acc G: 12.841% 
LR: 2.000e-04 

2023-03-02 01:51:56,436 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4739 (0.3911) Acc D Real: 68.443% 
Loss D Fake: 0.6047 (0.5582) Acc D Fake: 85.814% 
Loss D: 1.079 
Loss G: 0.7956 (0.9349) Acc G: 12.852% 
LR: 2.000e-04 

2023-03-02 01:51:56,445 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.5212 (0.3939) Acc D Real: 68.014% 
Loss D Fake: 0.6102 (0.5593) Acc D Fake: 85.832% 
Loss D: 1.131 
Loss G: 0.7885 (0.9317) Acc G: 12.862% 
LR: 2.000e-04 

2023-03-02 01:51:56,454 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.5027 (0.3962) Acc D Real: 67.588% 
Loss D Fake: 0.6161 (0.5605) Acc D Fake: 85.814% 
Loss D: 1.119 
Loss G: 0.7807 (0.9285) Acc G: 12.908% 
LR: 2.000e-04 

2023-03-02 01:51:56,463 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4998 (0.3984) Acc D Real: 67.207% 
Loss D Fake: 0.6224 (0.5618) Acc D Fake: 85.798% 
Loss D: 1.122 
Loss G: 0.7728 (0.9253) Acc G: 12.986% 
LR: 2.000e-04 

2023-03-02 01:51:56,472 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3704 (0.3978) Acc D Real: 67.190% 
Loss D Fake: 0.6286 (0.5632) Acc D Fake: 85.747% 
Loss D: 0.999 
Loss G: 0.7663 (0.9220) Acc G: 13.061% 
LR: 2.000e-04 

2023-03-02 01:51:56,481 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.5127 (0.4001) Acc D Real: 66.771% 
Loss D Fake: 0.6338 (0.5646) Acc D Fake: 85.699% 
Loss D: 1.146 
Loss G: 0.7596 (0.9188) Acc G: 13.133% 
LR: 2.000e-04 

2023-03-02 01:51:56,489 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4763 (0.4016) Acc D Real: 66.458% 
Loss D Fake: 0.6395 (0.5661) Acc D Fake: 85.653% 
Loss D: 1.116 
Loss G: 0.7528 (0.9155) Acc G: 13.203% 
LR: 2.000e-04 

2023-03-02 01:51:56,499 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4237 (0.4020) Acc D Real: 66.320% 
Loss D Fake: 0.6452 (0.5676) Acc D Fake: 85.608% 
Loss D: 1.069 
Loss G: 0.7467 (0.9123) Acc G: 13.269% 
LR: 2.000e-04 

2023-03-02 01:51:56,508 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4032 (0.4020) Acc D Real: 66.207% 
Loss D Fake: 0.6502 (0.5691) Acc D Fake: 85.565% 
Loss D: 1.053 
Loss G: 0.7413 (0.9091) Acc G: 13.333% 
LR: 2.000e-04 

2023-03-02 01:51:56,517 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3755 (0.4015) Acc D Real: 66.160% 
Loss D Fake: 0.6545 (0.5707) Acc D Fake: 85.524% 
Loss D: 1.030 
Loss G: 0.7369 (0.9059) Acc G: 13.395% 
LR: 2.000e-04 

2023-03-02 01:51:56,525 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4148 (0.4018) Acc D Real: 66.012% 
Loss D Fake: 0.6581 (0.5723) Acc D Fake: 85.484% 
Loss D: 1.073 
Loss G: 0.7330 (0.9027) Acc G: 13.455% 
LR: 2.000e-04 

2023-03-02 01:51:56,534 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3990 (0.4017) Acc D Real: 65.907% 
Loss D Fake: 0.6615 (0.5739) Acc D Fake: 85.445% 
Loss D: 1.060 
Loss G: 0.7294 (0.8996) Acc G: 13.512% 
LR: 2.000e-04 

2023-03-02 01:51:56,544 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3920 (0.4016) Acc D Real: 65.809% 
Loss D Fake: 0.6644 (0.5755) Acc D Fake: 85.408% 
Loss D: 1.056 
Loss G: 0.7264 (0.8966) Acc G: 13.567% 
LR: 2.000e-04 

2023-03-02 01:51:56,553 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4372 (0.4022) Acc D Real: 65.611% 
Loss D Fake: 0.6672 (0.5771) Acc D Fake: 85.373% 
Loss D: 1.104 
Loss G: 0.7233 (0.8936) Acc G: 13.621% 
LR: 2.000e-04 

2023-03-02 01:51:56,562 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4285 (0.4026) Acc D Real: 65.443% 
Loss D Fake: 0.6701 (0.5786) Acc D Fake: 85.338% 
Loss D: 1.099 
Loss G: 0.7201 (0.8907) Acc G: 13.672% 
LR: 2.000e-04 

2023-03-02 01:51:56,572 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3854 (0.4023) Acc D Real: 65.379% 
Loss D Fake: 0.6729 (0.5802) Acc D Fake: 85.305% 
Loss D: 1.058 
Loss G: 0.7172 (0.8878) Acc G: 13.722% 
LR: 2.000e-04 

2023-03-02 01:51:56,582 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3056 (0.4008) Acc D Real: 65.518% 
Loss D Fake: 0.6751 (0.5818) Acc D Fake: 85.272% 
Loss D: 0.981 
Loss G: 0.7159 (0.8850) Acc G: 13.770% 
LR: 2.000e-04 

2023-03-02 01:51:56,591 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4467 (0.4015) Acc D Real: 65.386% 
Loss D Fake: 0.6761 (0.5833) Acc D Fake: 85.241% 
Loss D: 1.123 
Loss G: 0.7145 (0.8822) Acc G: 13.817% 
LR: 2.000e-04 

2023-03-02 01:51:56,602 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4570 (0.4024) Acc D Real: 65.198% 
Loss D Fake: 0.6777 (0.5848) Acc D Fake: 85.211% 
Loss D: 1.135 
Loss G: 0.7122 (0.8795) Acc G: 13.862% 
LR: 2.000e-04 

2023-03-02 01:51:56,611 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3263 (0.4012) Acc D Real: 65.306% 
Loss D Fake: 0.6797 (0.5863) Acc D Fake: 85.181% 
Loss D: 1.006 
Loss G: 0.7107 (0.8769) Acc G: 13.906% 
LR: 2.000e-04 

2023-03-02 01:51:56,620 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4251 (0.4016) Acc D Real: 65.225% 
Loss D Fake: 0.6810 (0.5877) Acc D Fake: 85.127% 
Loss D: 1.106 
Loss G: 0.7092 (0.8743) Acc G: 13.974% 
LR: 2.000e-04 

2023-03-02 01:51:56,628 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3242 (0.4004) Acc D Real: 65.321% 
Loss D Fake: 0.6823 (0.5892) Acc D Fake: 85.075% 
Loss D: 1.006 
Loss G: 0.7084 (0.8718) Acc G: 14.040% 
LR: 2.000e-04 

2023-03-02 01:51:56,637 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3410 (0.3995) Acc D Real: 65.483% 
Loss D Fake: 0.6826 (0.5906) Acc D Fake: 85.024% 
Loss D: 1.024 
Loss G: 0.7084 (0.8693) Acc G: 14.104% 
LR: 2.000e-04 

2023-03-02 01:51:56,646 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4357 (0.4000) Acc D Real: 65.427% 
Loss D Fake: 0.6826 (0.5919) Acc D Fake: 84.975% 
Loss D: 1.118 
Loss G: 0.7080 (0.8670) Acc G: 14.167% 
LR: 2.000e-04 

2023-03-02 01:51:56,655 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3413 (0.3992) Acc D Real: 65.515% 
Loss D Fake: 0.6830 (0.5932) Acc D Fake: 84.927% 
Loss D: 1.024 
Loss G: 0.7080 (0.8647) Acc G: 14.227% 
LR: 2.000e-04 

2023-03-02 01:51:56,663 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4323 (0.3996) Acc D Real: 65.478% 
Loss D Fake: 0.6831 (0.5945) Acc D Fake: 84.880% 
Loss D: 1.115 
Loss G: 0.7075 (0.8624) Acc G: 14.286% 
LR: 2.000e-04 

2023-03-02 01:51:56,673 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3903 (0.3995) Acc D Real: 65.484% 
Loss D Fake: 0.6838 (0.5958) Acc D Fake: 84.835% 
Loss D: 1.074 
Loss G: 0.7067 (0.8602) Acc G: 14.343% 
LR: 2.000e-04 

2023-03-02 01:51:56,682 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3608 (0.3990) Acc D Real: 65.549% 
Loss D Fake: 0.6845 (0.5970) Acc D Fake: 84.791% 
Loss D: 1.045 
Loss G: 0.7061 (0.8581) Acc G: 14.398% 
LR: 2.000e-04 

2023-03-02 01:51:56,691 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3337 (0.3981) Acc D Real: 65.659% 
Loss D Fake: 0.6847 (0.5982) Acc D Fake: 84.748% 
Loss D: 1.018 
Loss G: 0.7064 (0.8560) Acc G: 14.452% 
LR: 2.000e-04 

2023-03-02 01:51:56,700 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4480 (0.3988) Acc D Real: 65.612% 
Loss D Fake: 0.6846 (0.5994) Acc D Fake: 84.707% 
Loss D: 1.133 
Loss G: 0.7059 (0.8540) Acc G: 14.505% 
LR: 2.000e-04 

2023-03-02 01:51:56,708 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3742 (0.3984) Acc D Real: 65.688% 
Loss D Fake: 0.6852 (0.6005) Acc D Fake: 84.666% 
Loss D: 1.059 
Loss G: 0.7054 (0.8520) Acc G: 14.556% 
LR: 2.000e-04 

2023-03-02 01:51:56,718 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3916 (0.3983) Acc D Real: 65.656% 
Loss D Fake: 0.6857 (0.6016) Acc D Fake: 84.627% 
Loss D: 1.077 
Loss G: 0.7048 (0.8501) Acc G: 14.605% 
LR: 2.000e-04 

2023-03-02 01:51:56,726 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3571 (0.3978) Acc D Real: 65.724% 
Loss D Fake: 0.6862 (0.6027) Acc D Fake: 84.588% 
Loss D: 1.043 
Loss G: 0.7045 (0.8482) Acc G: 14.654% 
LR: 2.000e-04 

2023-03-02 01:51:56,736 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3593 (0.3973) Acc D Real: 65.783% 
Loss D Fake: 0.6863 (0.6038) Acc D Fake: 84.551% 
Loss D: 1.046 
Loss G: 0.7046 (0.8463) Acc G: 14.701% 
LR: 2.000e-04 

2023-03-02 01:51:56,744 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3647 (0.3969) Acc D Real: 65.906% 
Loss D Fake: 0.6861 (0.6049) Acc D Fake: 84.514% 
Loss D: 1.051 
Loss G: 0.7049 (0.8445) Acc G: 14.747% 
LR: 2.000e-04 

2023-03-02 01:51:56,754 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3200 (0.3959) Acc D Real: 66.050% 
Loss D Fake: 0.6856 (0.6059) Acc D Fake: 84.479% 
Loss D: 1.006 
Loss G: 0.7057 (0.8428) Acc G: 14.792% 
LR: 2.000e-04 

2023-03-02 01:51:56,762 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4395 (0.3965) Acc D Real: 65.990% 
Loss D Fake: 0.6848 (0.6068) Acc D Fake: 84.444% 
Loss D: 1.124 
Loss G: 0.7061 (0.8411) Acc G: 14.835% 
LR: 2.000e-04 

2023-03-02 01:51:56,771 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4822 (0.3975) Acc D Real: 65.966% 
Loss D Fake: 0.6851 (0.6078) Acc D Fake: 84.410% 
Loss D: 1.167 
Loss G: 0.7051 (0.8395) Acc G: 14.878% 
LR: 2.000e-04 

2023-03-02 01:51:56,780 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4120 (0.3977) Acc D Real: 65.997% 
Loss D Fake: 0.6864 (0.6087) Acc D Fake: 84.377% 
Loss D: 1.098 
Loss G: 0.7036 (0.8378) Acc G: 14.920% 
LR: 2.000e-04 

2023-03-02 01:51:56,789 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3502 (0.3971) Acc D Real: 66.096% 
Loss D Fake: 0.6878 (0.6097) Acc D Fake: 84.345% 
Loss D: 1.038 
Loss G: 0.7024 (0.8362) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 01:51:56,797 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3629 (0.3967) Acc D Real: 66.176% 
Loss D Fake: 0.6887 (0.6106) Acc D Fake: 84.294% 
Loss D: 1.052 
Loss G: 0.7017 (0.8346) Acc G: 15.036% 
LR: 2.000e-04 

2023-03-02 01:51:56,806 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4690 (0.3976) Acc D Real: 66.126% 
Loss D Fake: 0.6895 (0.6115) Acc D Fake: 84.244% 
Loss D: 1.159 
Loss G: 0.7003 (0.8331) Acc G: 15.094% 
LR: 2.000e-04 

2023-03-02 01:51:56,815 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3993 (0.3976) Acc D Real: 66.162% 
Loss D Fake: 0.6911 (0.6124) Acc D Fake: 84.195% 
Loss D: 1.090 
Loss G: 0.6987 (0.8315) Acc G: 15.150% 
LR: 2.000e-04 

2023-03-02 01:51:56,824 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4700 (0.3984) Acc D Real: 66.161% 
Loss D Fake: 0.6929 (0.6134) Acc D Fake: 84.147% 
Loss D: 1.163 
Loss G: 0.6964 (0.8300) Acc G: 15.205% 
LR: 2.000e-04 

2023-03-02 01:51:56,833 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4107 (0.3986) Acc D Real: 66.222% 
Loss D Fake: 0.6954 (0.6143) Acc D Fake: 84.101% 
Loss D: 1.106 
Loss G: 0.6939 (0.8285) Acc G: 15.259% 
LR: 2.000e-04 

2023-03-02 01:51:56,843 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3361 (0.3979) Acc D Real: 66.339% 
Loss D Fake: 0.6975 (0.6152) Acc D Fake: 84.055% 
Loss D: 1.034 
Loss G: 0.6924 (0.8269) Acc G: 15.312% 
LR: 2.000e-04 

2023-03-02 01:51:56,852 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4095 (0.3980) Acc D Real: 66.304% 
Loss D Fake: 0.6988 (0.6161) Acc D Fake: 84.010% 
Loss D: 1.108 
Loss G: 0.6910 (0.8254) Acc G: 15.363% 
LR: 2.000e-04 

2023-03-02 01:51:56,862 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3838 (0.3978) Acc D Real: 66.402% 
Loss D Fake: 0.7001 (0.6170) Acc D Fake: 83.967% 
Loss D: 1.084 
Loss G: 0.6897 (0.8240) Acc G: 15.414% 
LR: 2.000e-04 

2023-03-02 01:51:56,871 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4224 (0.3981) Acc D Real: 66.495% 
Loss D Fake: 0.7014 (0.6179) Acc D Fake: 83.924% 
Loss D: 1.124 
Loss G: 0.6882 (0.8225) Acc G: 15.463% 
LR: 2.000e-04 

2023-03-02 01:51:56,880 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4242 (0.3984) Acc D Real: 66.565% 
Loss D Fake: 0.7032 (0.6189) Acc D Fake: 83.882% 
Loss D: 1.127 
Loss G: 0.6863 (0.8211) Acc G: 15.511% 
LR: 2.000e-04 

2023-03-02 01:51:56,889 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3953 (0.3983) Acc D Real: 66.692% 
Loss D Fake: 0.7051 (0.6198) Acc D Fake: 83.842% 
Loss D: 1.100 
Loss G: 0.6846 (0.8196) Acc G: 15.559% 
LR: 2.000e-04 

2023-03-02 01:51:56,898 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4033 (0.3984) Acc D Real: 66.763% 
Loss D Fake: 0.7066 (0.6207) Acc D Fake: 83.792% 
Loss D: 1.110 
Loss G: 0.6831 (0.8182) Acc G: 15.619% 
LR: 2.000e-04 

2023-03-02 01:51:56,906 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3491 (0.3979) Acc D Real: 66.909% 
Loss D Fake: 0.7080 (0.6216) Acc D Fake: 83.736% 
Loss D: 1.057 
Loss G: 0.6822 (0.8168) Acc G: 15.681% 
LR: 2.000e-04 

2023-03-02 01:51:56,916 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4761 (0.3987) Acc D Real: 66.951% 
Loss D Fake: 0.7090 (0.6225) Acc D Fake: 83.681% 
Loss D: 1.185 
Loss G: 0.6805 (0.8154) Acc G: 15.742% 
LR: 2.000e-04 

2023-03-02 01:51:56,925 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3876 (0.3986) Acc D Real: 67.049% 
Loss D Fake: 0.7109 (0.6233) Acc D Fake: 83.627% 
Loss D: 1.098 
Loss G: 0.6789 (0.8140) Acc G: 15.802% 
LR: 2.000e-04 

2023-03-02 01:51:56,934 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3878 (0.3985) Acc D Real: 67.193% 
Loss D Fake: 0.7122 (0.6242) Acc D Fake: 83.191% 
Loss D: 1.100 
Loss G: 0.6778 (0.8127) Acc G: 16.170% 
LR: 2.000e-04 

2023-03-02 01:51:56,943 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4209 (0.3987) Acc D Real: 67.223% 
Loss D Fake: 0.7133 (0.6251) Acc D Fake: 82.631% 
Loss D: 1.134 
Loss G: 0.6766 (0.8113) Acc G: 16.736% 
LR: 2.000e-04 

2023-03-02 01:51:56,952 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2983 (0.3977) Acc D Real: 67.399% 
Loss D Fake: 0.7142 (0.6260) Acc D Fake: 82.034% 
Loss D: 1.013 
Loss G: 0.6765 (0.8100) Acc G: 17.291% 
LR: 2.000e-04 

2023-03-02 01:51:56,962 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3169 (0.3969) Acc D Real: 67.566% 
Loss D Fake: 0.7137 (0.6268) Acc D Fake: 81.496% 
Loss D: 1.031 
Loss G: 0.6776 (0.8087) Acc G: 17.703% 
LR: 2.000e-04 

2023-03-02 01:51:56,971 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4124 (0.3971) Acc D Real: 67.683% 
Loss D Fake: 0.7125 (0.6277) Acc D Fake: 81.065% 
Loss D: 1.125 
Loss G: 0.6784 (0.8075) Acc G: 17.741% 
LR: 2.000e-04 

2023-03-02 01:51:56,981 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3940 (0.3970) Acc D Real: 67.818% 
Loss D Fake: 0.7119 (0.6285) Acc D Fake: 81.039% 
Loss D: 1.106 
Loss G: 0.6788 (0.8062) Acc G: 17.778% 
LR: 2.000e-04 

2023-03-02 01:51:56,990 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3635 (0.3967) Acc D Real: 67.954% 
Loss D Fake: 0.7115 (0.6293) Acc D Fake: 81.014% 
Loss D: 1.075 
Loss G: 0.6794 (0.8050) Acc G: 17.815% 
LR: 2.000e-04 

2023-03-02 01:51:56,998 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4025 (0.3968) Acc D Real: 68.033% 
Loss D Fake: 0.7110 (0.6300) Acc D Fake: 80.989% 
Loss D: 1.113 
Loss G: 0.6797 (0.8039) Acc G: 17.851% 
LR: 2.000e-04 

2023-03-02 01:51:57,006 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3430 (0.3963) Acc D Real: 68.150% 
Loss D Fake: 0.7106 (0.6308) Acc D Fake: 80.964% 
Loss D: 1.054 
Loss G: 0.6803 (0.8027) Acc G: 17.886% 
LR: 2.000e-04 

2023-03-02 01:51:57,014 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4168 (0.3965) Acc D Real: 68.242% 
Loss D Fake: 0.7100 (0.6315) Acc D Fake: 80.940% 
Loss D: 1.127 
Loss G: 0.6807 (0.8016) Acc G: 17.921% 
LR: 2.000e-04 

2023-03-02 01:51:57,021 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3413 (0.3960) Acc D Real: 68.335% 
Loss D Fake: 0.7096 (0.6322) Acc D Fake: 80.916% 
Loss D: 1.051 
Loss G: 0.6813 (0.8005) Acc G: 17.955% 
LR: 2.000e-04 

2023-03-02 01:51:57,029 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3869 (0.3959) Acc D Real: 68.387% 
Loss D Fake: 0.7088 (0.6329) Acc D Fake: 80.893% 
Loss D: 1.096 
Loss G: 0.6821 (0.7994) Acc G: 17.988% 
LR: 2.000e-04 

2023-03-02 01:51:57,037 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3253 (0.3953) Acc D Real: 68.535% 
Loss D Fake: 0.7079 (0.6336) Acc D Fake: 80.870% 
Loss D: 1.033 
Loss G: 0.6834 (0.7984) Acc G: 18.021% 
LR: 2.000e-04 

2023-03-02 01:51:57,044 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3984 (0.3953) Acc D Real: 68.591% 
Loss D Fake: 0.7065 (0.6342) Acc D Fake: 80.848% 
Loss D: 1.105 
Loss G: 0.6846 (0.7974) Acc G: 18.054% 
LR: 2.000e-04 

2023-03-02 01:51:57,052 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2597 (0.3941) Acc D Real: 68.750% 
Loss D Fake: 0.7049 (0.6348) Acc D Fake: 80.826% 
Loss D: 0.965 
Loss G: 0.6871 (0.7964) Acc G: 18.085% 
LR: 2.000e-04 

2023-03-02 01:51:57,060 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.2620 (0.3929) Acc D Real: 68.875% 
Loss D Fake: 0.7017 (0.6354) Acc D Fake: 80.804% 
Loss D: 0.964 
Loss G: 0.6910 (0.7955) Acc G: 18.116% 
LR: 2.000e-04 

2023-03-02 01:51:57,068 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4029 (0.3930) Acc D Real: 68.883% 
Loss D Fake: 0.6979 (0.6359) Acc D Fake: 80.783% 
Loss D: 1.101 
Loss G: 0.6943 (0.7946) Acc G: 18.147% 
LR: 2.000e-04 

2023-03-02 01:51:57,075 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3470 (0.3926) Acc D Real: 68.974% 
Loss D Fake: 0.6949 (0.6364) Acc D Fake: 80.762% 
Loss D: 1.042 
Loss G: 0.6973 (0.7938) Acc G: 18.163% 
LR: 2.000e-04 

2023-03-02 01:51:57,083 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3841 (0.3926) Acc D Real: 69.031% 
Loss D Fake: 0.6922 (0.6369) Acc D Fake: 80.755% 
Loss D: 1.076 
Loss G: 0.6997 (0.7930) Acc G: 18.178% 
LR: 2.000e-04 

2023-03-02 01:51:57,090 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4338 (0.3929) Acc D Real: 69.003% 
Loss D Fake: 0.6903 (0.6374) Acc D Fake: 80.749% 
Loss D: 1.124 
Loss G: 0.7011 (0.7922) Acc G: 18.194% 
LR: 2.000e-04 

2023-03-02 01:51:57,099 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4005 (0.3930) Acc D Real: 69.005% 
Loss D Fake: 0.6893 (0.6378) Acc D Fake: 80.743% 
Loss D: 1.090 
Loss G: 0.7020 (0.7915) Acc G: 18.209% 
LR: 2.000e-04 

2023-03-02 01:51:57,107 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3984 (0.3930) Acc D Real: 68.982% 
Loss D Fake: 0.6887 (0.6382) Acc D Fake: 80.736% 
Loss D: 1.087 
Loss G: 0.7024 (0.7908) Acc G: 18.224% 
LR: 2.000e-04 

2023-03-02 01:51:57,114 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4217 (0.3933) Acc D Real: 68.965% 
Loss D Fake: 0.6886 (0.6386) Acc D Fake: 80.730% 
Loss D: 1.110 
Loss G: 0.7021 (0.7900) Acc G: 18.238% 
LR: 2.000e-04 

2023-03-02 01:51:57,122 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4767 (0.3939) Acc D Real: 68.877% 
Loss D Fake: 0.6894 (0.6390) Acc D Fake: 80.725% 
Loss D: 1.166 
Loss G: 0.7006 (0.7893) Acc G: 18.252% 
LR: 2.000e-04 

2023-03-02 01:51:57,130 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3716 (0.3938) Acc D Real: 68.881% 
Loss D Fake: 0.6910 (0.6395) Acc D Fake: 80.719% 
Loss D: 1.063 
Loss G: 0.6993 (0.7886) Acc G: 18.267% 
LR: 2.000e-04 

2023-03-02 01:51:57,137 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4532 (0.3942) Acc D Real: 68.820% 
Loss D Fake: 0.6924 (0.6399) Acc D Fake: 80.713% 
Loss D: 1.146 
Loss G: 0.6974 (0.7878) Acc G: 18.280% 
LR: 2.000e-04 

2023-03-02 01:51:57,145 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3692 (0.3940) Acc D Real: 68.922% 
Loss D Fake: 0.6942 (0.6403) Acc D Fake: 80.707% 
Loss D: 1.063 
Loss G: 0.6958 (0.7871) Acc G: 18.294% 
LR: 2.000e-04 

2023-03-02 01:51:57,153 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4306 (0.3943) Acc D Real: 68.905% 
Loss D Fake: 0.6957 (0.6408) Acc D Fake: 80.702% 
Loss D: 1.126 
Loss G: 0.6941 (0.7864) Acc G: 18.317% 
LR: 2.000e-04 

2023-03-02 01:51:57,161 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3304 (0.3938) Acc D Real: 68.968% 
Loss D Fake: 0.6972 (0.6412) Acc D Fake: 80.683% 
Loss D: 1.028 
Loss G: 0.6931 (0.7857) Acc G: 18.343% 
LR: 2.000e-04 

2023-03-02 01:51:57,169 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3860 (0.3938) Acc D Real: 69.034% 
Loss D Fake: 0.6979 (0.6416) Acc D Fake: 80.665% 
Loss D: 1.084 
Loss G: 0.6924 (0.7849) Acc G: 18.368% 
LR: 2.000e-04 

2023-03-02 01:51:57,176 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4505 (0.3942) Acc D Real: 69.038% 
Loss D Fake: 0.6988 (0.6421) Acc D Fake: 80.647% 
Loss D: 1.149 
Loss G: 0.6912 (0.7842) Acc G: 18.394% 
LR: 2.000e-04 

2023-03-02 01:51:57,184 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4501 (0.3946) Acc D Real: 69.044% 
Loss D Fake: 0.7003 (0.6425) Acc D Fake: 80.629% 
Loss D: 1.150 
Loss G: 0.6894 (0.7835) Acc G: 18.419% 
LR: 2.000e-04 

2023-03-02 01:51:57,192 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3636 (0.3944) Acc D Real: 69.137% 
Loss D Fake: 0.7021 (0.6430) Acc D Fake: 80.612% 
Loss D: 1.066 
Loss G: 0.6879 (0.7828) Acc G: 18.443% 
LR: 2.000e-04 

2023-03-02 01:51:57,199 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3861 (0.3943) Acc D Real: 69.178% 
Loss D Fake: 0.7034 (0.6434) Acc D Fake: 80.595% 
Loss D: 1.089 
Loss G: 0.6867 (0.7820) Acc G: 18.468% 
LR: 2.000e-04 

2023-03-02 01:51:57,207 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3849 (0.3942) Acc D Real: 69.230% 
Loss D Fake: 0.7044 (0.6439) Acc D Fake: 80.578% 
Loss D: 1.089 
Loss G: 0.6858 (0.7813) Acc G: 18.492% 
LR: 2.000e-04 

2023-03-02 01:51:57,215 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3346 (0.3938) Acc D Real: 69.311% 
Loss D Fake: 0.7051 (0.6443) Acc D Fake: 80.561% 
Loss D: 1.040 
Loss G: 0.6856 (0.7806) Acc G: 18.515% 
LR: 2.000e-04 

2023-03-02 01:51:57,222 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3728 (0.3937) Acc D Real: 69.334% 
Loss D Fake: 0.7050 (0.6448) Acc D Fake: 80.545% 
Loss D: 1.078 
Loss G: 0.6858 (0.7799) Acc G: 18.538% 
LR: 2.000e-04 

2023-03-02 01:51:57,230 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3658 (0.3935) Acc D Real: 69.418% 
Loss D Fake: 0.7047 (0.6452) Acc D Fake: 80.529% 
Loss D: 1.071 
Loss G: 0.6862 (0.7792) Acc G: 18.561% 
LR: 2.000e-04 

2023-03-02 01:51:57,237 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4432 (0.3938) Acc D Real: 69.426% 
Loss D Fake: 0.7046 (0.6457) Acc D Fake: 80.513% 
Loss D: 1.148 
Loss G: 0.6859 (0.7786) Acc G: 18.584% 
LR: 2.000e-04 

2023-03-02 01:51:57,245 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.2908 (0.3931) Acc D Real: 69.522% 
Loss D Fake: 0.7046 (0.6461) Acc D Fake: 80.497% 
Loss D: 0.995 
Loss G: 0.6866 (0.7779) Acc G: 18.606% 
LR: 2.000e-04 

2023-03-02 01:51:57,252 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3316 (0.3926) Acc D Real: 69.556% 
Loss D Fake: 0.7034 (0.6465) Acc D Fake: 80.482% 
Loss D: 1.035 
Loss G: 0.6882 (0.7773) Acc G: 18.628% 
LR: 2.000e-04 

2023-03-02 01:51:57,260 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4230 (0.3928) Acc D Real: 69.562% 
Loss D Fake: 0.7020 (0.6469) Acc D Fake: 80.467% 
Loss D: 1.125 
Loss G: 0.6892 (0.7766) Acc G: 18.649% 
LR: 2.000e-04 

2023-03-02 01:51:57,268 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3822 (0.3928) Acc D Real: 69.580% 
Loss D Fake: 0.7013 (0.6473) Acc D Fake: 80.452% 
Loss D: 1.084 
Loss G: 0.6898 (0.7760) Acc G: 18.670% 
LR: 2.000e-04 

2023-03-02 01:51:57,276 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3664 (0.3926) Acc D Real: 69.628% 
Loss D Fake: 0.7007 (0.6476) Acc D Fake: 80.437% 
Loss D: 1.067 
Loss G: 0.6904 (0.7754) Acc G: 18.691% 
LR: 2.000e-04 

2023-03-02 01:51:57,284 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3048 (0.3920) Acc D Real: 69.718% 
Loss D Fake: 0.6999 (0.6480) Acc D Fake: 80.422% 
Loss D: 1.005 
Loss G: 0.6917 (0.7748) Acc G: 18.700% 
LR: 2.000e-04 

2023-03-02 01:51:57,292 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4160 (0.3921) Acc D Real: 69.704% 
Loss D Fake: 0.6985 (0.6483) Acc D Fake: 80.419% 
Loss D: 1.115 
Loss G: 0.6927 (0.7743) Acc G: 18.709% 
LR: 2.000e-04 

2023-03-02 01:51:57,300 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4356 (0.3924) Acc D Real: 69.714% 
Loss D Fake: 0.6981 (0.6487) Acc D Fake: 80.416% 
Loss D: 1.134 
Loss G: 0.6925 (0.7737) Acc G: 18.718% 
LR: 2.000e-04 

2023-03-02 01:51:57,308 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3107 (0.3919) Acc D Real: 69.802% 
Loss D Fake: 0.6982 (0.6490) Acc D Fake: 80.413% 
Loss D: 1.009 
Loss G: 0.6930 (0.7732) Acc G: 18.727% 
LR: 2.000e-04 

2023-03-02 01:51:57,316 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3636 (0.3917) Acc D Real: 69.885% 
Loss D Fake: 0.6975 (0.6494) Acc D Fake: 80.411% 
Loss D: 1.061 
Loss G: 0.6937 (0.7726) Acc G: 18.736% 
LR: 2.000e-04 

2023-03-02 01:51:57,323 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2806 (0.3909) Acc D Real: 69.960% 
Loss D Fake: 0.6964 (0.6497) Acc D Fake: 80.408% 
Loss D: 0.977 
Loss G: 0.6955 (0.7721) Acc G: 18.744% 
LR: 2.000e-04 

2023-03-02 01:51:57,331 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4776 (0.3915) Acc D Real: 69.887% 
Loss D Fake: 0.6949 (0.6500) Acc D Fake: 80.405% 
Loss D: 1.172 
Loss G: 0.6960 (0.7716) Acc G: 18.752% 
LR: 2.000e-04 

2023-03-02 01:51:57,339 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3739 (0.3914) Acc D Real: 69.934% 
Loss D Fake: 0.6949 (0.6503) Acc D Fake: 80.403% 
Loss D: 1.069 
Loss G: 0.6961 (0.7711) Acc G: 18.761% 
LR: 2.000e-04 

2023-03-02 01:51:57,347 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3648 (0.3912) Acc D Real: 69.990% 
Loss D Fake: 0.6947 (0.6506) Acc D Fake: 80.400% 
Loss D: 1.060 
Loss G: 0.6964 (0.7706) Acc G: 18.769% 
LR: 2.000e-04 

2023-03-02 01:51:57,354 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3537 (0.3910) Acc D Real: 70.012% 
Loss D Fake: 0.6943 (0.6508) Acc D Fake: 80.397% 
Loss D: 1.048 
Loss G: 0.6970 (0.7701) Acc G: 18.777% 
LR: 2.000e-04 

2023-03-02 01:51:57,362 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4557 (0.3914) Acc D Real: 69.953% 
Loss D Fake: 0.6939 (0.6511) Acc D Fake: 80.395% 
Loss D: 1.150 
Loss G: 0.6969 (0.7697) Acc G: 18.785% 
LR: 2.000e-04 

2023-03-02 01:51:57,370 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3135 (0.3909) Acc D Real: 69.999% 
Loss D Fake: 0.6941 (0.6514) Acc D Fake: 80.392% 
Loss D: 1.008 
Loss G: 0.6972 (0.7692) Acc G: 18.793% 
LR: 2.000e-04 

2023-03-02 01:51:57,377 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4310 (0.3912) Acc D Real: 69.947% 
Loss D Fake: 0.6938 (0.6517) Acc D Fake: 80.390% 
Loss D: 1.125 
Loss G: 0.6971 (0.7687) Acc G: 18.800% 
LR: 2.000e-04 

2023-03-02 01:51:57,385 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4192 (0.3913) Acc D Real: 69.935% 
Loss D Fake: 0.6942 (0.6519) Acc D Fake: 80.387% 
Loss D: 1.113 
Loss G: 0.6963 (0.7683) Acc G: 18.808% 
LR: 2.000e-04 

2023-03-02 01:51:57,392 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4938 (0.3920) Acc D Real: 69.927% 
Loss D Fake: 0.6955 (0.6522) Acc D Fake: 80.387% 
Loss D: 1.189 
Loss G: 0.6941 (0.7678) Acc G: 18.809% 
LR: 2.000e-04 

2023-03-02 01:51:57,615 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.346 | Generator Loss: 0.694 | Avg: 2.040 
2023-03-02 01:51:57,638 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.267 | Generator Loss: 0.694 | Avg: 1.961 
2023-03-02 01:51:57,662 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.290 | Generator Loss: 0.694 | Avg: 1.984 
2023-03-02 01:51:57,689 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.304 | Generator Loss: 0.694 | Avg: 1.998 
2023-03-02 01:51:57,716 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.308 | Generator Loss: 0.694 | Avg: 2.002 
2023-03-02 01:51:57,743 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.263 | Generator Loss: 0.694 | Avg: 1.957 
2023-03-02 01:51:57,770 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.244 | Generator Loss: 0.694 | Avg: 1.938 
2023-03-02 01:51:57,796 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.206 | Generator Loss: 0.694 | Avg: 1.900 
2023-03-02 01:51:57,823 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.185 | Generator Loss: 0.694 | Avg: 1.879 
2023-03-02 01:51:57,850 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.145 | Generator Loss: 0.694 | Avg: 1.839 
2023-03-02 01:51:57,877 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.116 | Generator Loss: 0.694 | Avg: 1.810 
2023-03-02 01:51:57,904 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.087 | Generator Loss: 0.694 | Avg: 1.781 
2023-03-02 01:51:57,930 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.064 | Generator Loss: 0.694 | Avg: 1.758 
2023-03-02 01:51:57,957 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.047 | Generator Loss: 0.694 | Avg: 1.741 
2023-03-02 01:51:57,985 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.063 | Generator Loss: 0.694 | Avg: 1.757 
2023-03-02 01:51:58,011 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.079 | Generator Loss: 0.694 | Avg: 1.773 
2023-03-02 01:51:58,038 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.092 | Generator Loss: 0.694 | Avg: 1.786 
2023-03-02 01:51:58,073 -                train: [    INFO] - 
Epoch: 11/20
2023-03-02 01:51:58,261 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3544 (0.3931) Acc D Real: 71.771% 
Loss D Fake: 0.7008 (0.6995) Acc D Fake: 79.167% 
Loss D: 1.055 
Loss G: 0.6892 (0.6902) Acc G: 20.833% 
LR: 2.000e-04 

2023-03-02 01:51:58,268 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2935 (0.3599) Acc D Real: 74.705% 
Loss D Fake: 0.7021 (0.7004) Acc D Fake: 78.889% 
Loss D: 0.996 
Loss G: 0.6887 (0.6897) Acc G: 21.111% 
LR: 2.000e-04 

2023-03-02 01:51:58,276 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3881 (0.3669) Acc D Real: 73.958% 
Loss D Fake: 0.7021 (0.7008) Acc D Fake: 78.750% 
Loss D: 1.090 
Loss G: 0.6887 (0.6894) Acc G: 21.250% 
LR: 2.000e-04 

2023-03-02 01:51:58,295 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3713 (0.3678) Acc D Real: 73.708% 
Loss D Fake: 0.7022 (0.7011) Acc D Fake: 78.667% 
Loss D: 1.073 
Loss G: 0.6887 (0.6893) Acc G: 21.333% 
LR: 2.000e-04 

2023-03-02 01:51:58,302 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4085 (0.3746) Acc D Real: 73.550% 
Loss D Fake: 0.7022 (0.7013) Acc D Fake: 78.333% 
Loss D: 1.111 
Loss G: 0.6885 (0.6892) Acc G: 21.667% 
LR: 2.000e-04 

2023-03-02 01:51:58,309 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3820 (0.3757) Acc D Real: 73.021% 
Loss D Fake: 0.7024 (0.7014) Acc D Fake: 78.095% 
Loss D: 1.084 
Loss G: 0.6885 (0.6891) Acc G: 21.905% 
LR: 2.000e-04 

2023-03-02 01:51:58,316 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3597 (0.3737) Acc D Real: 73.939% 
Loss D Fake: 0.7023 (0.7015) Acc D Fake: 77.917% 
Loss D: 1.062 
Loss G: 0.6886 (0.6890) Acc G: 22.083% 
LR: 2.000e-04 

2023-03-02 01:51:58,323 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3624 (0.3724) Acc D Real: 74.057% 
Loss D Fake: 0.7021 (0.7016) Acc D Fake: 77.778% 
Loss D: 1.065 
Loss G: 0.6890 (0.6890) Acc G: 22.222% 
LR: 2.000e-04 

2023-03-02 01:51:58,331 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4050 (0.3757) Acc D Real: 73.932% 
Loss D Fake: 0.7018 (0.7016) Acc D Fake: 77.667% 
Loss D: 1.107 
Loss G: 0.6891 (0.6890) Acc G: 22.333% 
LR: 2.000e-04 

2023-03-02 01:51:58,338 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3820 (0.3762) Acc D Real: 73.887% 
Loss D Fake: 0.7017 (0.7016) Acc D Fake: 77.576% 
Loss D: 1.084 
Loss G: 0.6892 (0.6890) Acc G: 22.287% 
LR: 2.000e-04 

2023-03-02 01:51:58,345 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4086 (0.3789) Acc D Real: 73.303% 
Loss D Fake: 0.7018 (0.7016) Acc D Fake: 77.500% 
Loss D: 1.110 
Loss G: 0.6889 (0.6890) Acc G: 22.374% 
LR: 2.000e-04 

2023-03-02 01:51:58,352 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4490 (0.3843) Acc D Real: 72.696% 
Loss D Fake: 0.7024 (0.7017) Acc D Fake: 77.436% 
Loss D: 1.151 
Loss G: 0.6878 (0.6889) Acc G: 22.448% 
LR: 2.000e-04 

2023-03-02 01:51:58,359 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3446 (0.3815) Acc D Real: 72.857% 
Loss D Fake: 0.7035 (0.7018) Acc D Fake: 77.381% 
Loss D: 1.048 
Loss G: 0.6871 (0.6888) Acc G: 22.511% 
LR: 2.000e-04 

2023-03-02 01:51:58,367 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3920 (0.3822) Acc D Real: 72.688% 
Loss D Fake: 0.7041 (0.7020) Acc D Fake: 77.333% 
Loss D: 1.096 
Loss G: 0.6864 (0.6886) Acc G: 22.566% 
LR: 2.000e-04 

2023-03-02 01:51:58,374 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4389 (0.3857) Acc D Real: 72.490% 
Loss D Fake: 0.7051 (0.7022) Acc D Fake: 77.292% 
Loss D: 1.144 
Loss G: 0.6850 (0.6884) Acc G: 22.614% 
LR: 2.000e-04 

2023-03-02 01:51:58,381 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2769 (0.3793) Acc D Real: 73.560% 
Loss D Fake: 0.7061 (0.7024) Acc D Fake: 77.255% 
Loss D: 0.983 
Loss G: 0.6850 (0.6882) Acc G: 22.656% 
LR: 2.000e-04 

2023-03-02 01:51:58,388 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3622 (0.3784) Acc D Real: 73.507% 
Loss D Fake: 0.7057 (0.7026) Acc D Fake: 77.222% 
Loss D: 1.068 
Loss G: 0.6855 (0.6881) Acc G: 22.694% 
LR: 2.000e-04 

2023-03-02 01:51:58,395 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3424 (0.3765) Acc D Real: 73.799% 
Loss D Fake: 0.7050 (0.7027) Acc D Fake: 77.193% 
Loss D: 1.047 
Loss G: 0.6865 (0.6880) Acc G: 22.728% 
LR: 2.000e-04 

2023-03-02 01:51:58,402 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3682 (0.3761) Acc D Real: 73.833% 
Loss D Fake: 0.7039 (0.7028) Acc D Fake: 77.167% 
Loss D: 1.072 
Loss G: 0.6875 (0.6880) Acc G: 22.758% 
LR: 2.000e-04 

2023-03-02 01:51:58,409 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3532 (0.3750) Acc D Real: 73.874% 
Loss D Fake: 0.7027 (0.7028) Acc D Fake: 77.143% 
Loss D: 1.056 
Loss G: 0.6889 (0.6880) Acc G: 22.706% 
LR: 2.000e-04 

2023-03-02 01:51:58,416 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3704 (0.3748) Acc D Real: 73.726% 
Loss D Fake: 0.7013 (0.7027) Acc D Fake: 77.197% 
Loss D: 1.072 
Loss G: 0.6903 (0.6881) Acc G: 22.659% 
LR: 2.000e-04 

2023-03-02 01:51:58,423 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4535 (0.3782) Acc D Real: 73.727% 
Loss D Fake: 0.7003 (0.7026) Acc D Fake: 77.246% 
Loss D: 1.154 
Loss G: 0.6907 (0.6882) Acc G: 22.615% 
LR: 2.000e-04 

2023-03-02 01:51:58,429 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3128 (0.3755) Acc D Real: 73.976% 
Loss D Fake: 0.7000 (0.7025) Acc D Fake: 77.292% 
Loss D: 1.013 
Loss G: 0.6915 (0.6884) Acc G: 22.576% 
LR: 2.000e-04 

2023-03-02 01:51:58,436 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3756 (0.3755) Acc D Real: 74.069% 
Loss D Fake: 0.6990 (0.7023) Acc D Fake: 77.333% 
Loss D: 1.075 
Loss G: 0.6926 (0.6885) Acc G: 22.540% 
LR: 2.000e-04 

2023-03-02 01:51:58,444 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3247 (0.3735) Acc D Real: 74.259% 
Loss D Fake: 0.6976 (0.7022) Acc D Fake: 77.372% 
Loss D: 1.022 
Loss G: 0.6943 (0.6887) Acc G: 22.506% 
LR: 2.000e-04 

2023-03-02 01:51:58,452 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3409 (0.3723) Acc D Real: 74.419% 
Loss D Fake: 0.6958 (0.7019) Acc D Fake: 77.407% 
Loss D: 1.037 
Loss G: 0.6962 (0.6890) Acc G: 22.475% 
LR: 2.000e-04 

2023-03-02 01:51:58,460 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3570 (0.3718) Acc D Real: 74.423% 
Loss D Fake: 0.6940 (0.7016) Acc D Fake: 77.440% 
Loss D: 1.051 
Loss G: 0.6982 (0.6894) Acc G: 22.446% 
LR: 2.000e-04 

2023-03-02 01:51:58,468 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3486 (0.3710) Acc D Real: 74.551% 
Loss D Fake: 0.6920 (0.7013) Acc D Fake: 77.471% 
Loss D: 1.041 
Loss G: 0.7002 (0.6897) Acc G: 22.419% 
LR: 2.000e-04 

2023-03-02 01:51:58,476 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3846 (0.3714) Acc D Real: 74.460% 
Loss D Fake: 0.6902 (0.7009) Acc D Fake: 77.500% 
Loss D: 1.075 
Loss G: 0.7018 (0.6901) Acc G: 22.394% 
LR: 2.000e-04 

2023-03-02 01:51:58,484 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4077 (0.3726) Acc D Real: 74.309% 
Loss D Fake: 0.6891 (0.7006) Acc D Fake: 77.527% 
Loss D: 1.097 
Loss G: 0.7025 (0.6905) Acc G: 22.371% 
LR: 2.000e-04 

2023-03-02 01:51:58,491 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3546 (0.3720) Acc D Real: 74.251% 
Loss D Fake: 0.6886 (0.7002) Acc D Fake: 77.552% 
Loss D: 1.043 
Loss G: 0.7032 (0.6909) Acc G: 22.349% 
LR: 2.000e-04 

2023-03-02 01:51:58,499 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4519 (0.3745) Acc D Real: 73.857% 
Loss D Fake: 0.6881 (0.6998) Acc D Fake: 77.576% 
Loss D: 1.140 
Loss G: 0.7029 (0.6913) Acc G: 22.328% 
LR: 2.000e-04 

2023-03-02 01:51:58,506 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3643 (0.3742) Acc D Real: 73.822% 
Loss D Fake: 0.6887 (0.6995) Acc D Fake: 77.598% 
Loss D: 1.053 
Loss G: 0.7025 (0.6916) Acc G: 22.309% 
LR: 2.000e-04 

2023-03-02 01:51:58,514 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3774 (0.3743) Acc D Real: 73.677% 
Loss D Fake: 0.6890 (0.6992) Acc D Fake: 77.619% 
Loss D: 1.066 
Loss G: 0.7023 (0.6919) Acc G: 22.290% 
LR: 2.000e-04 

2023-03-02 01:51:58,521 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3835 (0.3745) Acc D Real: 73.488% 
Loss D Fake: 0.6892 (0.6989) Acc D Fake: 77.639% 
Loss D: 1.073 
Loss G: 0.7021 (0.6922) Acc G: 22.273% 
LR: 2.000e-04 

2023-03-02 01:51:58,529 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4223 (0.3758) Acc D Real: 73.177% 
Loss D Fake: 0.6894 (0.6987) Acc D Fake: 77.658% 
Loss D: 1.112 
Loss G: 0.7016 (0.6925) Acc G: 22.256% 
LR: 2.000e-04 

2023-03-02 01:51:58,537 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3651 (0.3755) Acc D Real: 73.209% 
Loss D Fake: 0.6900 (0.6984) Acc D Fake: 77.675% 
Loss D: 1.055 
Loss G: 0.7011 (0.6927) Acc G: 22.241% 
LR: 2.000e-04 

2023-03-02 01:51:58,544 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3861 (0.3758) Acc D Real: 73.094% 
Loss D Fake: 0.6904 (0.6982) Acc D Fake: 77.692% 
Loss D: 1.076 
Loss G: 0.7008 (0.6929) Acc G: 22.226% 
LR: 2.000e-04 

2023-03-02 01:51:58,552 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3326 (0.3747) Acc D Real: 73.185% 
Loss D Fake: 0.6905 (0.6980) Acc D Fake: 77.708% 
Loss D: 1.023 
Loss G: 0.7010 (0.6931) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 01:51:58,560 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3579 (0.3743) Acc D Real: 73.121% 
Loss D Fake: 0.6900 (0.6978) Acc D Fake: 77.724% 
Loss D: 1.048 
Loss G: 0.7017 (0.6933) Acc G: 22.199% 
LR: 2.000e-04 

2023-03-02 01:51:58,568 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3473 (0.3737) Acc D Real: 73.065% 
Loss D Fake: 0.6892 (0.6976) Acc D Fake: 77.738% 
Loss D: 1.037 
Loss G: 0.7027 (0.6935) Acc G: 22.186% 
LR: 2.000e-04 

2023-03-02 01:51:58,577 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2688 (0.3712) Acc D Real: 73.289% 
Loss D Fake: 0.6877 (0.6974) Acc D Fake: 77.752% 
Loss D: 0.956 
Loss G: 0.7052 (0.6938) Acc G: 22.174% 
LR: 2.000e-04 

2023-03-02 01:51:58,586 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3633 (0.3710) Acc D Real: 73.258% 
Loss D Fake: 0.6849 (0.6971) Acc D Fake: 77.765% 
Loss D: 1.048 
Loss G: 0.7080 (0.6941) Acc G: 22.163% 
LR: 2.000e-04 

2023-03-02 01:51:58,595 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4473 (0.3727) Acc D Real: 73.080% 
Loss D Fake: 0.6828 (0.6968) Acc D Fake: 77.778% 
Loss D: 1.130 
Loss G: 0.7095 (0.6945) Acc G: 22.152% 
LR: 2.000e-04 

2023-03-02 01:51:58,602 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4384 (0.3742) Acc D Real: 72.806% 
Loss D Fake: 0.6820 (0.6965) Acc D Fake: 77.790% 
Loss D: 1.120 
Loss G: 0.7096 (0.6948) Acc G: 22.141% 
LR: 2.000e-04 

2023-03-02 01:51:58,610 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3725 (0.3741) Acc D Real: 72.708% 
Loss D Fake: 0.6822 (0.6962) Acc D Fake: 77.801% 
Loss D: 1.055 
Loss G: 0.7094 (0.6951) Acc G: 22.131% 
LR: 2.000e-04 

2023-03-02 01:51:58,617 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3451 (0.3735) Acc D Real: 72.696% 
Loss D Fake: 0.6823 (0.6959) Acc D Fake: 77.812% 
Loss D: 1.027 
Loss G: 0.7097 (0.6954) Acc G: 22.121% 
LR: 2.000e-04 

2023-03-02 01:51:58,625 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3367 (0.3728) Acc D Real: 72.705% 
Loss D Fake: 0.6818 (0.6956) Acc D Fake: 77.823% 
Loss D: 1.018 
Loss G: 0.7105 (0.6957) Acc G: 22.078% 
LR: 2.000e-04 

2023-03-02 01:51:58,632 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4007 (0.3733) Acc D Real: 72.539% 
Loss D Fake: 0.6810 (0.6953) Acc D Fake: 77.867% 
Loss D: 1.082 
Loss G: 0.7111 (0.6960) Acc G: 22.036% 
LR: 2.000e-04 

2023-03-02 01:51:58,640 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3963 (0.3738) Acc D Real: 72.379% 
Loss D Fake: 0.6808 (0.6950) Acc D Fake: 77.908% 
Loss D: 1.077 
Loss G: 0.7110 (0.6963) Acc G: 21.997% 
LR: 2.000e-04 

2023-03-02 01:51:58,648 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3432 (0.3732) Acc D Real: 72.397% 
Loss D Fake: 0.6808 (0.6947) Acc D Fake: 77.949% 
Loss D: 1.024 
Loss G: 0.7112 (0.6966) Acc G: 21.958% 
LR: 2.000e-04 

2023-03-02 01:51:58,655 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3802 (0.3733) Acc D Real: 72.191% 
Loss D Fake: 0.6805 (0.6945) Acc D Fake: 77.987% 
Loss D: 1.061 
Loss G: 0.7114 (0.6969) Acc G: 21.921% 
LR: 2.000e-04 

2023-03-02 01:51:58,662 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3970 (0.3738) Acc D Real: 72.121% 
Loss D Fake: 0.6805 (0.6942) Acc D Fake: 78.025% 
Loss D: 1.078 
Loss G: 0.7112 (0.6971) Acc G: 21.886% 
LR: 2.000e-04 

2023-03-02 01:51:58,670 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4326 (0.3748) Acc D Real: 72.010% 
Loss D Fake: 0.6811 (0.6940) Acc D Fake: 78.061% 
Loss D: 1.114 
Loss G: 0.7101 (0.6974) Acc G: 21.851% 
LR: 2.000e-04 

2023-03-02 01:51:58,677 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3166 (0.3738) Acc D Real: 72.064% 
Loss D Fake: 0.6819 (0.6938) Acc D Fake: 78.095% 
Loss D: 0.999 
Loss G: 0.7099 (0.6976) Acc G: 21.818% 
LR: 2.000e-04 

2023-03-02 01:51:58,684 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4003 (0.3743) Acc D Real: 71.974% 
Loss D Fake: 0.6820 (0.6936) Acc D Fake: 78.129% 
Loss D: 1.082 
Loss G: 0.7096 (0.6978) Acc G: 21.786% 
LR: 2.000e-04 

2023-03-02 01:51:58,692 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4472 (0.3755) Acc D Real: 71.787% 
Loss D Fake: 0.6826 (0.6934) Acc D Fake: 78.132% 
Loss D: 1.130 
Loss G: 0.7084 (0.6980) Acc G: 21.784% 
LR: 2.000e-04 

2023-03-02 01:51:58,700 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3466 (0.3750) Acc D Real: 71.763% 
Loss D Fake: 0.6839 (0.6932) Acc D Fake: 78.136% 
Loss D: 1.030 
Loss G: 0.7075 (0.6982) Acc G: 21.782% 
LR: 2.000e-04 

2023-03-02 01:51:58,708 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3407 (0.3745) Acc D Real: 71.805% 
Loss D Fake: 0.6843 (0.6931) Acc D Fake: 78.139% 
Loss D: 1.025 
Loss G: 0.7074 (0.6983) Acc G: 21.780% 
LR: 2.000e-04 

2023-03-02 01:51:58,716 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3108 (0.3734) Acc D Real: 71.849% 
Loss D Fake: 0.6840 (0.6929) Acc D Fake: 78.142% 
Loss D: 0.995 
Loss G: 0.7082 (0.6985) Acc G: 21.779% 
LR: 2.000e-04 

2023-03-02 01:51:58,724 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3742 (0.3734) Acc D Real: 71.794% 
Loss D Fake: 0.6831 (0.6928) Acc D Fake: 78.145% 
Loss D: 1.057 
Loss G: 0.7090 (0.6986) Acc G: 21.750% 
LR: 2.000e-04 

2023-03-02 01:51:58,731 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3658 (0.3733) Acc D Real: 71.724% 
Loss D Fake: 0.6824 (0.6926) Acc D Fake: 78.175% 
Loss D: 1.048 
Loss G: 0.7098 (0.6988) Acc G: 21.722% 
LR: 2.000e-04 

2023-03-02 01:51:58,739 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3578 (0.3731) Acc D Real: 71.737% 
Loss D Fake: 0.6817 (0.6924) Acc D Fake: 78.203% 
Loss D: 1.040 
Loss G: 0.7102 (0.6990) Acc G: 21.695% 
LR: 2.000e-04 

2023-03-02 01:51:58,747 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4114 (0.3736) Acc D Real: 71.640% 
Loss D Fake: 0.6817 (0.6923) Acc D Fake: 78.231% 
Loss D: 1.093 
Loss G: 0.7099 (0.6992) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 01:51:58,755 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2748 (0.3722) Acc D Real: 71.766% 
Loss D Fake: 0.6817 (0.6921) Acc D Fake: 78.232% 
Loss D: 0.957 
Loss G: 0.7108 (0.6993) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 01:51:58,762 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3604 (0.3720) Acc D Real: 71.810% 
Loss D Fake: 0.6806 (0.6919) Acc D Fake: 78.234% 
Loss D: 1.041 
Loss G: 0.7120 (0.6995) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 01:51:58,770 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3103 (0.3711) Acc D Real: 71.902% 
Loss D Fake: 0.6792 (0.6917) Acc D Fake: 78.235% 
Loss D: 0.990 
Loss G: 0.7138 (0.6997) Acc G: 21.697% 
LR: 2.000e-04 

2023-03-02 01:51:58,778 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3527 (0.3708) Acc D Real: 71.880% 
Loss D Fake: 0.6775 (0.6915) Acc D Fake: 78.213% 
Loss D: 1.030 
Loss G: 0.7155 (0.7000) Acc G: 21.721% 
LR: 2.000e-04 

2023-03-02 01:51:58,785 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2995 (0.3698) Acc D Real: 71.971% 
Loss D Fake: 0.6759 (0.6913) Acc D Fake: 78.190% 
Loss D: 0.975 
Loss G: 0.7177 (0.7002) Acc G: 21.744% 
LR: 2.000e-04 

2023-03-02 01:51:58,793 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3658 (0.3697) Acc D Real: 71.923% 
Loss D Fake: 0.6737 (0.6911) Acc D Fake: 78.169% 
Loss D: 1.040 
Loss G: 0.7199 (0.7005) Acc G: 21.766% 
LR: 2.000e-04 

2023-03-02 01:51:58,800 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4097 (0.3703) Acc D Real: 71.759% 
Loss D Fake: 0.6722 (0.6908) Acc D Fake: 78.148% 
Loss D: 1.082 
Loss G: 0.7210 (0.7008) Acc G: 21.788% 
LR: 2.000e-04 

2023-03-02 01:51:58,808 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3150 (0.3695) Acc D Real: 71.790% 
Loss D Fake: 0.6712 (0.6905) Acc D Fake: 78.128% 
Loss D: 0.986 
Loss G: 0.7224 (0.7011) Acc G: 21.809% 
LR: 2.000e-04 

2023-03-02 01:51:58,815 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3432 (0.3692) Acc D Real: 71.772% 
Loss D Fake: 0.6698 (0.6902) Acc D Fake: 78.108% 
Loss D: 1.013 
Loss G: 0.7241 (0.7014) Acc G: 21.830% 
LR: 2.000e-04 

2023-03-02 01:51:58,823 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3821 (0.3693) Acc D Real: 71.713% 
Loss D Fake: 0.6690 (0.6900) Acc D Fake: 78.089% 
Loss D: 1.051 
Loss G: 0.7233 (0.7017) Acc G: 21.850% 
LR: 2.000e-04 

2023-03-02 01:51:58,831 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4192 (0.3700) Acc D Real: 71.580% 
Loss D Fake: 0.6710 (0.6897) Acc D Fake: 78.070% 
Loss D: 1.090 
Loss G: 0.7209 (0.7019) Acc G: 21.870% 
LR: 2.000e-04 

2023-03-02 01:51:58,839 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3337 (0.3695) Acc D Real: 71.581% 
Loss D Fake: 0.6733 (0.6895) Acc D Fake: 78.052% 
Loss D: 1.007 
Loss G: 0.7188 (0.7022) Acc G: 21.889% 
LR: 2.000e-04 

2023-03-02 01:51:58,846 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3185 (0.3689) Acc D Real: 71.579% 
Loss D Fake: 0.6749 (0.6893) Acc D Fake: 78.034% 
Loss D: 0.993 
Loss G: 0.7177 (0.7024) Acc G: 21.907% 
LR: 2.000e-04 

2023-03-02 01:51:58,854 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2742 (0.3677) Acc D Real: 71.644% 
Loss D Fake: 0.6755 (0.6891) Acc D Fake: 78.017% 
Loss D: 0.950 
Loss G: 0.7180 (0.7026) Acc G: 21.925% 
LR: 2.000e-04 

2023-03-02 01:51:58,861 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4196 (0.3683) Acc D Real: 71.521% 
Loss D Fake: 0.6753 (0.6890) Acc D Fake: 78.000% 
Loss D: 1.095 
Loss G: 0.7178 (0.7027) Acc G: 21.943% 
LR: 2.000e-04 

2023-03-02 01:51:58,869 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2870 (0.3673) Acc D Real: 71.584% 
Loss D Fake: 0.6754 (0.6888) Acc D Fake: 77.984% 
Loss D: 0.962 
Loss G: 0.7184 (0.7029) Acc G: 21.960% 
LR: 2.000e-04 

2023-03-02 01:51:58,877 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3009 (0.3665) Acc D Real: 71.638% 
Loss D Fake: 0.6744 (0.6886) Acc D Fake: 77.967% 
Loss D: 0.975 
Loss G: 0.7200 (0.7031) Acc G: 21.977% 
LR: 2.000e-04 

2023-03-02 01:51:58,885 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2588 (0.3652) Acc D Real: 71.724% 
Loss D Fake: 0.6724 (0.6884) Acc D Fake: 77.952% 
Loss D: 0.931 
Loss G: 0.7230 (0.7034) Acc G: 21.993% 
LR: 2.000e-04 

2023-03-02 01:51:58,894 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2997 (0.3644) Acc D Real: 71.752% 
Loss D Fake: 0.6693 (0.6882) Acc D Fake: 77.937% 
Loss D: 0.969 
Loss G: 0.7267 (0.7037) Acc G: 22.009% 
LR: 2.000e-04 

2023-03-02 01:51:58,904 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.2676 (0.3633) Acc D Real: 71.829% 
Loss D Fake: 0.6658 (0.6879) Acc D Fake: 77.922% 
Loss D: 0.933 
Loss G: 0.7311 (0.7040) Acc G: 22.025% 
LR: 2.000e-04 

2023-03-02 01:51:58,913 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.2806 (0.3623) Acc D Real: 71.870% 
Loss D Fake: 0.6616 (0.6876) Acc D Fake: 77.907% 
Loss D: 0.942 
Loss G: 0.7361 (0.7044) Acc G: 22.040% 
LR: 2.000e-04 

2023-03-02 01:51:58,920 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2652 (0.3612) Acc D Real: 71.955% 
Loss D Fake: 0.6572 (0.6873) Acc D Fake: 77.893% 
Loss D: 0.922 
Loss G: 0.7412 (0.7048) Acc G: 22.055% 
LR: 2.000e-04 

2023-03-02 01:51:58,928 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3724 (0.3613) Acc D Real: 71.867% 
Loss D Fake: 0.6530 (0.6869) Acc D Fake: 77.879% 
Loss D: 1.025 
Loss G: 0.7455 (0.7052) Acc G: 22.069% 
LR: 2.000e-04 

2023-03-02 01:51:58,936 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3149 (0.3608) Acc D Real: 71.883% 
Loss D Fake: 0.6496 (0.6865) Acc D Fake: 77.865% 
Loss D: 0.965 
Loss G: 0.7495 (0.7057) Acc G: 22.083% 
LR: 2.000e-04 

2023-03-02 01:51:58,943 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3602 (0.3608) Acc D Real: 71.833% 
Loss D Fake: 0.6465 (0.6860) Acc D Fake: 77.852% 
Loss D: 1.007 
Loss G: 0.7527 (0.7063) Acc G: 22.097% 
LR: 2.000e-04 

2023-03-02 01:51:58,951 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3433 (0.3606) Acc D Real: 71.802% 
Loss D Fake: 0.6440 (0.6856) Acc D Fake: 77.839% 
Loss D: 0.987 
Loss G: 0.7554 (0.7068) Acc G: 22.111% 
LR: 2.000e-04 

2023-03-02 01:51:58,959 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.2694 (0.3596) Acc D Real: 71.851% 
Loss D Fake: 0.6417 (0.6851) Acc D Fake: 77.826% 
Loss D: 0.911 
Loss G: 0.7587 (0.7074) Acc G: 22.124% 
LR: 2.000e-04 

2023-03-02 01:51:58,966 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.2970 (0.3590) Acc D Real: 71.876% 
Loss D Fake: 0.6387 (0.6846) Acc D Fake: 77.814% 
Loss D: 0.936 
Loss G: 0.7624 (0.7080) Acc G: 22.137% 
LR: 2.000e-04 

2023-03-02 01:51:58,974 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3104 (0.3584) Acc D Real: 71.869% 
Loss D Fake: 0.6356 (0.6841) Acc D Fake: 77.801% 
Loss D: 0.946 
Loss G: 0.7661 (0.7086) Acc G: 22.150% 
LR: 2.000e-04 

2023-03-02 01:51:58,981 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3273 (0.3581) Acc D Real: 71.865% 
Loss D Fake: 0.6327 (0.6835) Acc D Fake: 77.789% 
Loss D: 0.960 
Loss G: 0.7695 (0.7092) Acc G: 22.162% 
LR: 2.000e-04 

2023-03-02 01:51:58,989 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4643 (0.3592) Acc D Real: 71.673% 
Loss D Fake: 0.6305 (0.6830) Acc D Fake: 77.778% 
Loss D: 1.095 
Loss G: 0.7708 (0.7099) Acc G: 22.174% 
LR: 2.000e-04 

2023-03-02 01:51:58,997 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2920 (0.3585) Acc D Real: 71.691% 
Loss D Fake: 0.6299 (0.6824) Acc D Fake: 77.766% 
Loss D: 0.922 
Loss G: 0.7721 (0.7105) Acc G: 22.186% 
LR: 2.000e-04 

2023-03-02 01:51:59,004 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2495 (0.3574) Acc D Real: 71.801% 
Loss D Fake: 0.6283 (0.6819) Acc D Fake: 77.755% 
Loss D: 0.878 
Loss G: 0.7748 (0.7112) Acc G: 22.198% 
LR: 2.000e-04 

2023-03-02 01:51:59,012 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3244 (0.3571) Acc D Real: 71.802% 
Loss D Fake: 0.6259 (0.6813) Acc D Fake: 77.744% 
Loss D: 0.950 
Loss G: 0.7776 (0.7118) Acc G: 22.210% 
LR: 2.000e-04 

2023-03-02 01:51:59,020 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3542 (0.3570) Acc D Real: 71.770% 
Loss D Fake: 0.6239 (0.6807) Acc D Fake: 77.733% 
Loss D: 0.978 
Loss G: 0.7798 (0.7125) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 01:51:59,027 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2983 (0.3565) Acc D Real: 71.796% 
Loss D Fake: 0.6223 (0.6802) Acc D Fake: 77.723% 
Loss D: 0.921 
Loss G: 0.7821 (0.7132) Acc G: 22.232% 
LR: 2.000e-04 

2023-03-02 01:51:59,035 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2789 (0.3557) Acc D Real: 71.835% 
Loss D Fake: 0.6203 (0.6796) Acc D Fake: 77.712% 
Loss D: 0.899 
Loss G: 0.7850 (0.7139) Acc G: 22.243% 
LR: 2.000e-04 

2023-03-02 01:51:59,042 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3219 (0.3554) Acc D Real: 71.831% 
Loss D Fake: 0.6178 (0.6790) Acc D Fake: 77.702% 
Loss D: 0.940 
Loss G: 0.7881 (0.7146) Acc G: 22.237% 
LR: 2.000e-04 

2023-03-02 01:51:59,050 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2185 (0.3541) Acc D Real: 71.961% 
Loss D Fake: 0.6154 (0.6784) Acc D Fake: 77.708% 
Loss D: 0.834 
Loss G: 0.7912 (0.7154) Acc G: 22.232% 
LR: 2.000e-04 

2023-03-02 01:51:59,058 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2861 (0.3534) Acc D Real: 72.004% 
Loss D Fake: 0.6130 (0.6777) Acc D Fake: 77.714% 
Loss D: 0.899 
Loss G: 0.7947 (0.7161) Acc G: 22.226% 
LR: 2.000e-04 

2023-03-02 01:51:59,065 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2775 (0.3527) Acc D Real: 72.036% 
Loss D Fake: 0.6101 (0.6771) Acc D Fake: 77.720% 
Loss D: 0.888 
Loss G: 0.7987 (0.7169) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 01:51:59,073 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3063 (0.3523) Acc D Real: 72.065% 
Loss D Fake: 0.6071 (0.6764) Acc D Fake: 77.726% 
Loss D: 0.913 
Loss G: 0.8024 (0.7177) Acc G: 22.216% 
LR: 2.000e-04 

2023-03-02 01:51:59,081 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3706 (0.3524) Acc D Real: 72.003% 
Loss D Fake: 0.6047 (0.6758) Acc D Fake: 77.731% 
Loss D: 0.975 
Loss G: 0.8048 (0.7185) Acc G: 22.211% 
LR: 2.000e-04 

2023-03-02 01:51:59,089 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2026 (0.3511) Acc D Real: 72.136% 
Loss D Fake: 0.6029 (0.6751) Acc D Fake: 77.737% 
Loss D: 0.805 
Loss G: 0.8084 (0.7193) Acc G: 22.206% 
LR: 2.000e-04 

2023-03-02 01:51:59,097 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2499 (0.3501) Acc D Real: 72.216% 
Loss D Fake: 0.5998 (0.6744) Acc D Fake: 77.742% 
Loss D: 0.850 
Loss G: 0.8131 (0.7202) Acc G: 22.201% 
LR: 2.000e-04 

2023-03-02 01:51:59,104 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1987 (0.3488) Acc D Real: 72.345% 
Loss D Fake: 0.5957 (0.6737) Acc D Fake: 77.748% 
Loss D: 0.794 
Loss G: 0.8195 (0.7211) Acc G: 22.196% 
LR: 2.000e-04 

2023-03-02 01:51:59,111 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3066 (0.3484) Acc D Real: 72.350% 
Loss D Fake: 0.5910 (0.6730) Acc D Fake: 77.753% 
Loss D: 0.898 
Loss G: 0.8255 (0.7220) Acc G: 22.191% 
LR: 2.000e-04 

2023-03-02 01:51:59,119 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2349 (0.3474) Acc D Real: 72.457% 
Loss D Fake: 0.5867 (0.6722) Acc D Fake: 77.758% 
Loss D: 0.822 
Loss G: 0.8317 (0.7230) Acc G: 22.187% 
LR: 2.000e-04 

2023-03-02 01:51:59,127 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2524 (0.3466) Acc D Real: 72.535% 
Loss D Fake: 0.5821 (0.6714) Acc D Fake: 77.763% 
Loss D: 0.835 
Loss G: 0.8384 (0.7240) Acc G: 22.182% 
LR: 2.000e-04 

2023-03-02 01:51:59,135 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3239 (0.3464) Acc D Real: 72.550% 
Loss D Fake: 0.5778 (0.6706) Acc D Fake: 77.768% 
Loss D: 0.902 
Loss G: 0.8438 (0.7250) Acc G: 22.178% 
LR: 2.000e-04 

2023-03-02 01:51:59,142 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2865 (0.3458) Acc D Real: 72.607% 
Loss D Fake: 0.5761 (0.6698) Acc D Fake: 77.773% 
Loss D: 0.863 
Loss G: 0.8415 (0.7260) Acc G: 22.173% 
LR: 2.000e-04 

2023-03-02 01:51:59,150 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2228 (0.3448) Acc D Real: 72.725% 
Loss D Fake: 0.5804 (0.6690) Acc D Fake: 77.764% 
Loss D: 0.803 
Loss G: 0.8352 (0.7270) Acc G: 22.183% 
LR: 2.000e-04 

2023-03-02 01:51:59,158 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.2017 (0.3436) Acc D Real: 72.835% 
Loss D Fake: 0.5868 (0.6683) Acc D Fake: 77.754% 
Loss D: 0.788 
Loss G: 0.8270 (0.7278) Acc G: 22.193% 
LR: 2.000e-04 

2023-03-02 01:51:59,166 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2817 (0.3431) Acc D Real: 72.897% 
Loss D Fake: 0.5968 (0.6677) Acc D Fake: 77.745% 
Loss D: 0.879 
Loss G: 0.8212 (0.7286) Acc G: 22.202% 
LR: 2.000e-04 

2023-03-02 01:51:59,173 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2551 (0.3423) Acc D Real: 72.962% 
Loss D Fake: 0.5977 (0.6672) Acc D Fake: 77.736% 
Loss D: 0.853 
Loss G: 0.8405 (0.7295) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 01:51:59,180 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3274 (0.3422) Acc D Real: 72.955% 
Loss D Fake: 0.5742 (0.6664) Acc D Fake: 77.727% 
Loss D: 0.902 
Loss G: 0.8618 (0.7306) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 01:51:59,188 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3296 (0.3421) Acc D Real: 72.947% 
Loss D Fake: 0.5607 (0.6655) Acc D Fake: 77.719% 
Loss D: 0.890 
Loss G: 0.8778 (0.7318) Acc G: 22.217% 
LR: 2.000e-04 

2023-03-02 01:51:59,195 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3067 (0.3418) Acc D Real: 72.967% 
Loss D Fake: 0.5509 (0.6646) Acc D Fake: 77.724% 
Loss D: 0.858 
Loss G: 0.8899 (0.7331) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 01:51:59,203 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2732 (0.3413) Acc D Real: 73.025% 
Loss D Fake: 0.5436 (0.6636) Acc D Fake: 77.728% 
Loss D: 0.817 
Loss G: 0.9001 (0.7345) Acc G: 22.208% 
LR: 2.000e-04 

2023-03-02 01:51:59,211 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2804 (0.3408) Acc D Real: 73.081% 
Loss D Fake: 0.5375 (0.6626) Acc D Fake: 77.733% 
Loss D: 0.818 
Loss G: 0.9089 (0.7359) Acc G: 22.203% 
LR: 2.000e-04 

2023-03-02 01:51:59,218 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3904 (0.3412) Acc D Real: 73.037% 
Loss D Fake: 0.5332 (0.6616) Acc D Fake: 77.738% 
Loss D: 0.924 
Loss G: 0.9129 (0.7373) Acc G: 22.199% 
LR: 2.000e-04 

2023-03-02 01:51:59,226 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2339 (0.3403) Acc D Real: 73.130% 
Loss D Fake: 0.5320 (0.6606) Acc D Fake: 77.743% 
Loss D: 0.766 
Loss G: 0.9148 (0.7387) Acc G: 22.195% 
LR: 2.000e-04 

2023-03-02 01:51:59,235 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3385 (0.3403) Acc D Real: 73.138% 
Loss D Fake: 0.5318 (0.6595) Acc D Fake: 77.747% 
Loss D: 0.870 
Loss G: 0.9147 (0.7400) Acc G: 22.191% 
LR: 2.000e-04 

2023-03-02 01:51:59,243 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2255 (0.3394) Acc D Real: 73.234% 
Loss D Fake: 0.5325 (0.6586) Acc D Fake: 77.739% 
Loss D: 0.758 
Loss G: 0.9154 (0.7414) Acc G: 22.200% 
LR: 2.000e-04 

2023-03-02 01:51:59,252 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3162 (0.3392) Acc D Real: 73.255% 
Loss D Fake: 0.5349 (0.6576) Acc D Fake: 77.731% 
Loss D: 0.851 
Loss G: 0.9043 (0.7427) Acc G: 22.208% 
LR: 2.000e-04 

2023-03-02 01:51:59,261 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3109 (0.3390) Acc D Real: 73.290% 
Loss D Fake: 0.8571 (0.6591) Acc D Fake: 77.432% 
Loss D: 1.168 
Loss G: 0.9510 (0.7442) Acc G: 22.204% 
LR: 2.000e-04 

2023-03-02 01:51:59,269 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4288 (0.3397) Acc D Real: 73.224% 
Loss D Fake: 0.4966 (0.6579) Acc D Fake: 77.439% 
Loss D: 0.925 
Loss G: 0.9777 (0.7460) Acc G: 22.188% 
LR: 2.000e-04 

2023-03-02 01:51:59,277 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3182 (0.3395) Acc D Real: 73.258% 
Loss D Fake: 0.4854 (0.6566) Acc D Fake: 77.484% 
Loss D: 0.804 
Loss G: 0.9887 (0.7478) Acc G: 22.133% 
LR: 2.000e-04 

2023-03-02 01:51:59,285 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3601 (0.3397) Acc D Real: 73.258% 
Loss D Fake: 0.4797 (0.6553) Acc D Fake: 77.540% 
Loss D: 0.840 
Loss G: 0.9952 (0.7497) Acc G: 22.068% 
LR: 2.000e-04 

2023-03-02 01:51:59,293 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.5023 (0.3409) Acc D Real: 73.136% 
Loss D Fake: 0.4759 (0.6540) Acc D Fake: 77.607% 
Loss D: 0.978 
Loss G: 0.9994 (0.7515) Acc G: 21.991% 
LR: 2.000e-04 

2023-03-02 01:51:59,301 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4477 (0.3417) Acc D Real: 73.059% 
Loss D Fake: 0.4735 (0.6526) Acc D Fake: 77.686% 
Loss D: 0.921 
Loss G: 1.0025 (0.7534) Acc G: 21.915% 
LR: 2.000e-04 

2023-03-02 01:51:59,309 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3438 (0.3417) Acc D Real: 73.065% 
Loss D Fake: 0.4715 (0.6513) Acc D Fake: 77.764% 
Loss D: 0.815 
Loss G: 1.0056 (0.7552) Acc G: 21.828% 
LR: 2.000e-04 

2023-03-02 01:51:59,316 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4271 (0.3423) Acc D Real: 73.012% 
Loss D Fake: 0.4698 (0.6500) Acc D Fake: 77.853% 
Loss D: 0.897 
Loss G: 1.0074 (0.7570) Acc G: 21.742% 
LR: 2.000e-04 

2023-03-02 01:51:59,324 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.5115 (0.3435) Acc D Real: 72.892% 
Loss D Fake: 0.4693 (0.6487) Acc D Fake: 77.940% 
Loss D: 0.981 
Loss G: 1.0064 (0.7588) Acc G: 21.658% 
LR: 2.000e-04 

2023-03-02 01:51:59,332 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.5403 (0.3449) Acc D Real: 72.756% 
Loss D Fake: 0.4705 (0.6474) Acc D Fake: 78.026% 
Loss D: 1.011 
Loss G: 1.0025 (0.7606) Acc G: 21.574% 
LR: 2.000e-04 

2023-03-02 01:51:59,341 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4024 (0.3454) Acc D Real: 72.727% 
Loss D Fake: 0.4728 (0.6462) Acc D Fake: 78.111% 
Loss D: 0.875 
Loss G: 0.9976 (0.7623) Acc G: 21.492% 
LR: 2.000e-04 

2023-03-02 01:51:59,348 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.5580 (0.3468) Acc D Real: 72.576% 
Loss D Fake: 0.4756 (0.6450) Acc D Fake: 78.195% 
Loss D: 1.034 
Loss G: 0.9911 (0.7639) Acc G: 21.411% 
LR: 2.000e-04 

2023-03-02 01:51:59,356 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3920 (0.3472) Acc D Real: 72.546% 
Loss D Fake: 0.4793 (0.6438) Acc D Fake: 78.277% 
Loss D: 0.871 
Loss G: 0.9845 (0.7654) Acc G: 21.332% 
LR: 2.000e-04 

2023-03-02 01:51:59,363 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.5498 (0.3486) Acc D Real: 72.397% 
Loss D Fake: 0.4830 (0.6427) Acc D Fake: 78.359% 
Loss D: 1.033 
Loss G: 0.9768 (0.7669) Acc G: 21.253% 
LR: 2.000e-04 

2023-03-02 01:51:59,371 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4627 (0.3494) Acc D Real: 72.306% 
Loss D Fake: 0.4875 (0.6416) Acc D Fake: 78.439% 
Loss D: 0.950 
Loss G: 0.9684 (0.7683) Acc G: 21.175% 
LR: 2.000e-04 

2023-03-02 01:51:59,378 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.5712 (0.3509) Acc D Real: 72.136% 
Loss D Fake: 0.4924 (0.6406) Acc D Fake: 78.518% 
Loss D: 1.064 
Loss G: 0.9586 (0.7696) Acc G: 21.099% 
LR: 2.000e-04 

2023-03-02 01:51:59,387 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.5915 (0.3525) Acc D Real: 71.961% 
Loss D Fake: 0.4984 (0.6396) Acc D Fake: 78.596% 
Loss D: 1.090 
Loss G: 0.9474 (0.7708) Acc G: 21.023% 
LR: 2.000e-04 

2023-03-02 01:51:59,396 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4009 (0.3528) Acc D Real: 71.915% 
Loss D Fake: 0.5049 (0.6387) Acc D Fake: 78.673% 
Loss D: 0.906 
Loss G: 0.9370 (0.7719) Acc G: 20.949% 
LR: 2.000e-04 

2023-03-02 01:51:59,404 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5088 (0.3539) Acc D Real: 71.796% 
Loss D Fake: 0.5110 (0.6379) Acc D Fake: 78.749% 
Loss D: 1.020 
Loss G: 0.9267 (0.7729) Acc G: 20.875% 
LR: 2.000e-04 

2023-03-02 01:51:59,411 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4425 (0.3545) Acc D Real: 71.719% 
Loss D Fake: 0.5172 (0.6371) Acc D Fake: 78.824% 
Loss D: 0.960 
Loss G: 0.9166 (0.7739) Acc G: 20.803% 
LR: 2.000e-04 

2023-03-02 01:51:59,422 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4144 (0.3549) Acc D Real: 71.656% 
Loss D Fake: 0.5233 (0.6363) Acc D Fake: 78.898% 
Loss D: 0.938 
Loss G: 0.9072 (0.7748) Acc G: 20.731% 
LR: 2.000e-04 

2023-03-02 01:51:59,430 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4381 (0.3554) Acc D Real: 71.582% 
Loss D Fake: 0.5290 (0.6356) Acc D Fake: 78.971% 
Loss D: 0.967 
Loss G: 0.8984 (0.7756) Acc G: 20.661% 
LR: 2.000e-04 

2023-03-02 01:51:59,437 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.5291 (0.3566) Acc D Real: 71.435% 
Loss D Fake: 0.5348 (0.6350) Acc D Fake: 79.043% 
Loss D: 1.064 
Loss G: 0.8890 (0.7763) Acc G: 20.591% 
LR: 2.000e-04 

2023-03-02 01:51:59,445 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.5269 (0.3577) Acc D Real: 71.285% 
Loss D Fake: 0.5411 (0.6343) Acc D Fake: 79.115% 
Loss D: 1.068 
Loss G: 0.8788 (0.7770) Acc G: 20.533% 
LR: 2.000e-04 

2023-03-02 01:51:59,452 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4486 (0.3583) Acc D Real: 71.200% 
Loss D Fake: 0.5480 (0.6338) Acc D Fake: 79.174% 
Loss D: 0.997 
Loss G: 0.8689 (0.7776) Acc G: 20.476% 
LR: 2.000e-04 

2023-03-02 01:51:59,459 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4195 (0.3586) Acc D Real: 71.136% 
Loss D Fake: 0.5544 (0.6333) Acc D Fake: 79.233% 
Loss D: 0.974 
Loss G: 0.8599 (0.7781) Acc G: 20.419% 
LR: 2.000e-04 

2023-03-02 01:51:59,467 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3093 (0.3583) Acc D Real: 71.142% 
Loss D Fake: 0.5601 (0.6328) Acc D Fake: 79.291% 
Loss D: 0.869 
Loss G: 0.8527 (0.7786) Acc G: 20.364% 
LR: 2.000e-04 

2023-03-02 01:51:59,474 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5689 (0.3597) Acc D Real: 71.126% 
Loss D Fake: 0.5651 (0.6324) Acc D Fake: 79.296% 
Loss D: 1.134 
Loss G: 0.8448 (0.7790) Acc G: 20.358% 
LR: 2.000e-04 

2023-03-02 01:51:59,679 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.360 | Generator Loss: 0.845 | Avg: 2.205 
2023-03-02 01:51:59,701 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.257 | Generator Loss: 0.845 | Avg: 2.102 
2023-03-02 01:51:59,724 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.289 | Generator Loss: 0.845 | Avg: 2.133 
2023-03-02 01:51:59,750 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.308 | Generator Loss: 0.845 | Avg: 2.152 
2023-03-02 01:51:59,776 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.313 | Generator Loss: 0.845 | Avg: 2.158 
2023-03-02 01:51:59,802 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.256 | Generator Loss: 0.845 | Avg: 2.100 
2023-03-02 01:51:59,829 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.228 | Generator Loss: 0.845 | Avg: 2.073 
2023-03-02 01:51:59,855 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.177 | Generator Loss: 0.845 | Avg: 2.022 
2023-03-02 01:51:59,881 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.149 | Generator Loss: 0.845 | Avg: 1.994 
2023-03-02 01:51:59,908 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.100 | Generator Loss: 0.845 | Avg: 1.944 
2023-03-02 01:51:59,934 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.062 | Generator Loss: 0.845 | Avg: 1.907 
2023-03-02 01:51:59,961 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.026 | Generator Loss: 0.845 | Avg: 1.870 
2023-03-02 01:51:59,987 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.997 | Generator Loss: 0.845 | Avg: 1.842 
2023-03-02 01:52:00,012 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.976 | Generator Loss: 0.845 | Avg: 1.820 
2023-03-02 01:52:00,044 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.995 | Generator Loss: 0.845 | Avg: 1.839 
2023-03-02 01:52:00,069 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.014 | Generator Loss: 0.845 | Avg: 1.859 
2023-03-02 01:52:00,095 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.031 | Generator Loss: 0.845 | Avg: 1.876 
2023-03-02 01:52:00,128 -                train: [    INFO] - 
Epoch: 12/20
2023-03-02 01:52:00,315 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4105 (0.4463) Acc D Real: 56.536% 
Loss D Fake: 0.5770 (0.5740) Acc D Fake: 88.333% 
Loss D: 0.987 
Loss G: 0.8286 (0.8325) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,322 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4534 (0.4486) Acc D Real: 56.458% 
Loss D Fake: 0.5824 (0.5768) Acc D Fake: 88.333% 
Loss D: 1.036 
Loss G: 0.8214 (0.8288) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,331 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4921 (0.4595) Acc D Real: 54.622% 
Loss D Fake: 0.5879 (0.5796) Acc D Fake: 88.333% 
Loss D: 1.080 
Loss G: 0.8139 (0.8251) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,346 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4468 (0.4570) Acc D Real: 54.781% 
Loss D Fake: 0.5936 (0.5824) Acc D Fake: 88.333% 
Loss D: 1.040 
Loss G: 0.8065 (0.8214) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,353 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3283 (0.4355) Acc D Real: 57.274% 
Loss D Fake: 0.5988 (0.5851) Acc D Fake: 88.333% 
Loss D: 0.927 
Loss G: 0.8007 (0.8179) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,360 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4122 (0.4322) Acc D Real: 57.307% 
Loss D Fake: 0.6030 (0.5877) Acc D Fake: 88.333% 
Loss D: 1.015 
Loss G: 0.7955 (0.8147) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,367 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3583 (0.4230) Acc D Real: 58.118% 
Loss D Fake: 0.6069 (0.5901) Acc D Fake: 88.333% 
Loss D: 0.965 
Loss G: 0.7911 (0.8118) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,374 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4300 (0.4237) Acc D Real: 57.946% 
Loss D Fake: 0.6103 (0.5923) Acc D Fake: 88.333% 
Loss D: 1.040 
Loss G: 0.7868 (0.8090) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,381 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4077 (0.4221) Acc D Real: 58.099% 
Loss D Fake: 0.6137 (0.5945) Acc D Fake: 88.333% 
Loss D: 1.021 
Loss G: 0.7826 (0.8064) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,388 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3814 (0.4184) Acc D Real: 58.362% 
Loss D Fake: 0.6169 (0.5965) Acc D Fake: 88.333% 
Loss D: 0.998 
Loss G: 0.7789 (0.8039) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,395 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4690 (0.4226) Acc D Real: 57.643% 
Loss D Fake: 0.6201 (0.5985) Acc D Fake: 88.333% 
Loss D: 1.089 
Loss G: 0.7747 (0.8014) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 01:52:00,402 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4085 (0.4216) Acc D Real: 57.736% 
Loss D Fake: 0.6236 (0.6004) Acc D Fake: 88.217% 
Loss D: 1.032 
Loss G: 0.7706 (0.7990) Acc G: 11.795% 
LR: 2.000e-04 

2023-03-02 01:52:00,409 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4828 (0.4259) Acc D Real: 57.083% 
Loss D Fake: 0.6271 (0.6023) Acc D Fake: 88.103% 
Loss D: 1.110 
Loss G: 0.7659 (0.7967) Acc G: 12.020% 
LR: 2.000e-04 

2023-03-02 01:52:00,416 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3737 (0.4225) Acc D Real: 57.479% 
Loss D Fake: 0.6311 (0.6042) Acc D Fake: 87.896% 
Loss D: 1.005 
Loss G: 0.7615 (0.7943) Acc G: 12.219% 
LR: 2.000e-04 

2023-03-02 01:52:00,423 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3318 (0.4168) Acc D Real: 58.242% 
Loss D Fake: 0.6344 (0.6061) Acc D Fake: 87.715% 
Loss D: 0.966 
Loss G: 0.7582 (0.7921) Acc G: 12.393% 
LR: 2.000e-04 

2023-03-02 01:52:00,430 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3319 (0.4118) Acc D Real: 58.802% 
Loss D Fake: 0.6368 (0.6079) Acc D Fake: 87.555% 
Loss D: 0.969 
Loss G: 0.7560 (0.7900) Acc G: 12.546% 
LR: 2.000e-04 

2023-03-02 01:52:00,437 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4747 (0.4153) Acc D Real: 58.267% 
Loss D Fake: 0.6387 (0.6096) Acc D Fake: 87.413% 
Loss D: 1.113 
Loss G: 0.7532 (0.7879) Acc G: 12.682% 
LR: 2.000e-04 

2023-03-02 01:52:00,444 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3895 (0.4139) Acc D Real: 58.391% 
Loss D Fake: 0.6413 (0.6113) Acc D Fake: 87.286% 
Loss D: 1.031 
Loss G: 0.7503 (0.7859) Acc G: 12.892% 
LR: 2.000e-04 

2023-03-02 01:52:00,451 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3481 (0.4106) Acc D Real: 58.685% 
Loss D Fake: 0.6436 (0.6129) Acc D Fake: 87.089% 
Loss D: 0.992 
Loss G: 0.7479 (0.7840) Acc G: 13.081% 
LR: 2.000e-04 

2023-03-02 01:52:00,457 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3263 (0.4066) Acc D Real: 59.149% 
Loss D Fake: 0.6454 (0.6145) Acc D Fake: 86.910% 
Loss D: 0.972 
Loss G: 0.7465 (0.7822) Acc G: 13.251% 
LR: 2.000e-04 

2023-03-02 01:52:00,464 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3546 (0.4043) Acc D Real: 59.384% 
Loss D Fake: 0.6463 (0.6159) Acc D Fake: 86.747% 
Loss D: 1.001 
Loss G: 0.7456 (0.7806) Acc G: 13.407% 
LR: 2.000e-04 

2023-03-02 01:52:00,471 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3616 (0.4024) Acc D Real: 59.667% 
Loss D Fake: 0.6470 (0.6173) Acc D Fake: 86.599% 
Loss D: 1.009 
Loss G: 0.7449 (0.7790) Acc G: 13.548% 
LR: 2.000e-04 

2023-03-02 01:52:00,478 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3974 (0.4022) Acc D Real: 59.664% 
Loss D Fake: 0.6476 (0.6185) Acc D Fake: 86.463% 
Loss D: 1.045 
Loss G: 0.7441 (0.7776) Acc G: 13.678% 
LR: 2.000e-04 

2023-03-02 01:52:00,485 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3275 (0.3992) Acc D Real: 60.048% 
Loss D Fake: 0.6481 (0.6197) Acc D Fake: 86.338% 
Loss D: 0.976 
Loss G: 0.7440 (0.7762) Acc G: 13.798% 
LR: 2.000e-04 

2023-03-02 01:52:00,492 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3902 (0.3989) Acc D Real: 60.260% 
Loss D Fake: 0.6482 (0.6208) Acc D Fake: 86.222% 
Loss D: 1.038 
Loss G: 0.7435 (0.7750) Acc G: 13.908% 
LR: 2.000e-04 

2023-03-02 01:52:00,499 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3943 (0.3987) Acc D Real: 60.258% 
Loss D Fake: 0.6488 (0.6218) Acc D Fake: 86.115% 
Loss D: 1.043 
Loss G: 0.7428 (0.7738) Acc G: 14.010% 
LR: 2.000e-04 

2023-03-02 01:52:00,506 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4516 (0.4006) Acc D Real: 59.961% 
Loss D Fake: 0.6497 (0.6228) Acc D Fake: 86.016% 
Loss D: 1.101 
Loss G: 0.7412 (0.7726) Acc G: 14.105% 
LR: 2.000e-04 

2023-03-02 01:52:00,513 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3482 (0.3988) Acc D Real: 60.167% 
Loss D Fake: 0.6512 (0.6238) Acc D Fake: 85.923% 
Loss D: 0.999 
Loss G: 0.7399 (0.7715) Acc G: 14.194% 
LR: 2.000e-04 

2023-03-02 01:52:00,520 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3816 (0.3982) Acc D Real: 60.299% 
Loss D Fake: 0.6523 (0.6248) Acc D Fake: 85.837% 
Loss D: 1.034 
Loss G: 0.7387 (0.7704) Acc G: 14.276% 
LR: 2.000e-04 

2023-03-02 01:52:00,528 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4835 (0.4010) Acc D Real: 59.976% 
Loss D Fake: 0.6537 (0.6257) Acc D Fake: 85.756% 
Loss D: 1.137 
Loss G: 0.7364 (0.7693) Acc G: 14.353% 
LR: 2.000e-04 

2023-03-02 01:52:00,536 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3658 (0.3999) Acc D Real: 60.133% 
Loss D Fake: 0.6559 (0.6266) Acc D Fake: 85.680% 
Loss D: 1.022 
Loss G: 0.7342 (0.7682) Acc G: 14.425% 
LR: 2.000e-04 

2023-03-02 01:52:00,544 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3851 (0.3994) Acc D Real: 60.180% 
Loss D Fake: 0.6577 (0.6276) Acc D Fake: 85.609% 
Loss D: 1.043 
Loss G: 0.7323 (0.7671) Acc G: 14.493% 
LR: 2.000e-04 

2023-03-02 01:52:00,552 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.2757 (0.3958) Acc D Real: 60.625% 
Loss D Fake: 0.6590 (0.6285) Acc D Fake: 85.542% 
Loss D: 0.935 
Loss G: 0.7315 (0.7661) Acc G: 14.557% 
LR: 2.000e-04 

2023-03-02 01:52:00,560 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2599 (0.3919) Acc D Real: 61.058% 
Loss D Fake: 0.6592 (0.6294) Acc D Fake: 85.479% 
Loss D: 0.919 
Loss G: 0.7320 (0.7651) Acc G: 14.618% 
LR: 2.000e-04 

2023-03-02 01:52:00,567 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2847 (0.3889) Acc D Real: 61.413% 
Loss D Fake: 0.6582 (0.6302) Acc D Fake: 85.420% 
Loss D: 0.943 
Loss G: 0.7336 (0.7642) Acc G: 14.674% 
LR: 2.000e-04 

2023-03-02 01:52:00,576 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3032 (0.3866) Acc D Real: 61.653% 
Loss D Fake: 0.6565 (0.6309) Acc D Fake: 85.363% 
Loss D: 0.960 
Loss G: 0.7357 (0.7634) Acc G: 14.728% 
LR: 2.000e-04 

2023-03-02 01:52:00,584 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4903 (0.3893) Acc D Real: 61.368% 
Loss D Fake: 0.6550 (0.6315) Acc D Fake: 85.311% 
Loss D: 1.145 
Loss G: 0.7364 (0.7627) Acc G: 14.777% 
LR: 2.000e-04 

2023-03-02 01:52:00,592 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5113 (0.3925) Acc D Real: 61.038% 
Loss D Fake: 0.6552 (0.6321) Acc D Fake: 85.274% 
Loss D: 1.167 
Loss G: 0.7352 (0.7620) Acc G: 14.825% 
LR: 2.000e-04 

2023-03-02 01:52:00,600 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3285 (0.3909) Acc D Real: 61.255% 
Loss D Fake: 0.6564 (0.6327) Acc D Fake: 85.232% 
Loss D: 0.985 
Loss G: 0.7343 (0.7613) Acc G: 14.867% 
LR: 2.000e-04 

2023-03-02 01:52:00,609 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4846 (0.3931) Acc D Real: 61.005% 
Loss D Fake: 0.6573 (0.6333) Acc D Fake: 85.202% 
Loss D: 1.142 
Loss G: 0.7326 (0.7606) Acc G: 14.910% 
LR: 2.000e-04 

2023-03-02 01:52:00,617 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3055 (0.3911) Acc D Real: 61.298% 
Loss D Fake: 0.6589 (0.6340) Acc D Fake: 85.164% 
Loss D: 0.964 
Loss G: 0.7314 (0.7599) Acc G: 14.952% 
LR: 2.000e-04 

2023-03-02 01:52:00,625 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4792 (0.3931) Acc D Real: 61.042% 
Loss D Fake: 0.6600 (0.6346) Acc D Fake: 85.134% 
Loss D: 1.139 
Loss G: 0.7296 (0.7592) Acc G: 14.992% 
LR: 2.000e-04 

2023-03-02 01:52:00,632 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3780 (0.3928) Acc D Real: 61.123% 
Loss D Fake: 0.6619 (0.6352) Acc D Fake: 85.098% 
Loss D: 1.040 
Loss G: 0.7276 (0.7585) Acc G: 15.030% 
LR: 2.000e-04 

2023-03-02 01:52:00,640 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4094 (0.3931) Acc D Real: 61.078% 
Loss D Fake: 0.6637 (0.6358) Acc D Fake: 85.059% 
Loss D: 1.073 
Loss G: 0.7256 (0.7578) Acc G: 15.066% 
LR: 2.000e-04 

2023-03-02 01:52:00,649 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4383 (0.3941) Acc D Real: 60.982% 
Loss D Fake: 0.6657 (0.6365) Acc D Fake: 85.022% 
Loss D: 1.104 
Loss G: 0.7232 (0.7570) Acc G: 15.101% 
LR: 2.000e-04 

2023-03-02 01:52:00,657 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4508 (0.3953) Acc D Real: 60.995% 
Loss D Fake: 0.6682 (0.6371) Acc D Fake: 84.986% 
Loss D: 1.119 
Loss G: 0.7203 (0.7562) Acc G: 15.134% 
LR: 2.000e-04 

2023-03-02 01:52:00,665 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3738 (0.3949) Acc D Real: 61.125% 
Loss D Fake: 0.6709 (0.6378) Acc D Fake: 84.951% 
Loss D: 1.045 
Loss G: 0.7175 (0.7554) Acc G: 15.166% 
LR: 2.000e-04 

2023-03-02 01:52:00,673 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2758 (0.3924) Acc D Real: 61.425% 
Loss D Fake: 0.6730 (0.6386) Acc D Fake: 84.918% 
Loss D: 0.949 
Loss G: 0.7162 (0.7546) Acc G: 15.197% 
LR: 2.000e-04 

2023-03-02 01:52:00,680 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4206 (0.3930) Acc D Real: 61.400% 
Loss D Fake: 0.6739 (0.6393) Acc D Fake: 84.886% 
Loss D: 1.095 
Loss G: 0.7150 (0.7538) Acc G: 15.226% 
LR: 2.000e-04 

2023-03-02 01:52:00,688 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4362 (0.3938) Acc D Real: 61.307% 
Loss D Fake: 0.6754 (0.6400) Acc D Fake: 84.856% 
Loss D: 1.112 
Loss G: 0.7131 (0.7531) Acc G: 15.254% 
LR: 2.000e-04 

2023-03-02 01:52:00,696 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3877 (0.3937) Acc D Real: 61.356% 
Loss D Fake: 0.6772 (0.6407) Acc D Fake: 84.827% 
Loss D: 1.065 
Loss G: 0.7112 (0.7522) Acc G: 15.281% 
LR: 2.000e-04 

2023-03-02 01:52:00,704 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3534 (0.3930) Acc D Real: 61.454% 
Loss D Fake: 0.6789 (0.6414) Acc D Fake: 84.799% 
Loss D: 1.032 
Loss G: 0.7097 (0.7514) Acc G: 15.308% 
LR: 2.000e-04 

2023-03-02 01:52:00,712 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2670 (0.3906) Acc D Real: 61.831% 
Loss D Fake: 0.6799 (0.6421) Acc D Fake: 84.771% 
Loss D: 0.947 
Loss G: 0.7095 (0.7507) Acc G: 15.333% 
LR: 2.000e-04 

2023-03-02 01:52:00,720 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3361 (0.3896) Acc D Real: 61.960% 
Loss D Fake: 0.6796 (0.6428) Acc D Fake: 84.745% 
Loss D: 1.016 
Loss G: 0.7099 (0.7499) Acc G: 15.357% 
LR: 2.000e-04 

2023-03-02 01:52:00,727 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3698 (0.3893) Acc D Real: 62.050% 
Loss D Fake: 0.6791 (0.6435) Acc D Fake: 84.730% 
Loss D: 1.049 
Loss G: 0.7104 (0.7492) Acc G: 15.359% 
LR: 2.000e-04 

2023-03-02 01:52:00,735 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3545 (0.3887) Acc D Real: 62.202% 
Loss D Fake: 0.6787 (0.6441) Acc D Fake: 84.735% 
Loss D: 1.033 
Loss G: 0.7109 (0.7485) Acc G: 15.353% 
LR: 2.000e-04 

2023-03-02 01:52:00,743 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3069 (0.3873) Acc D Real: 62.500% 
Loss D Fake: 0.6780 (0.6447) Acc D Fake: 84.740% 
Loss D: 0.985 
Loss G: 0.7119 (0.7479) Acc G: 15.347% 
LR: 2.000e-04 

2023-03-02 01:52:00,751 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4149 (0.3877) Acc D Real: 62.478% 
Loss D Fake: 0.6770 (0.6452) Acc D Fake: 84.744% 
Loss D: 1.092 
Loss G: 0.7126 (0.7473) Acc G: 15.341% 
LR: 2.000e-04 

2023-03-02 01:52:00,759 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4553 (0.3889) Acc D Real: 62.405% 
Loss D Fake: 0.6768 (0.6457) Acc D Fake: 84.748% 
Loss D: 1.132 
Loss G: 0.7123 (0.7467) Acc G: 15.335% 
LR: 2.000e-04 

2023-03-02 01:52:00,766 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.5110 (0.3909) Acc D Real: 62.217% 
Loss D Fake: 0.6777 (0.6463) Acc D Fake: 84.752% 
Loss D: 1.189 
Loss G: 0.7106 (0.7461) Acc G: 15.330% 
LR: 2.000e-04 

2023-03-02 01:52:00,774 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4550 (0.3919) Acc D Real: 62.130% 
Loss D Fake: 0.6798 (0.6468) Acc D Fake: 84.756% 
Loss D: 1.135 
Loss G: 0.7082 (0.7455) Acc G: 15.324% 
LR: 2.000e-04 

2023-03-02 01:52:00,782 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3747 (0.3916) Acc D Real: 62.193% 
Loss D Fake: 0.6822 (0.6474) Acc D Fake: 84.760% 
Loss D: 1.057 
Loss G: 0.7059 (0.7449) Acc G: 15.319% 
LR: 2.000e-04 

2023-03-02 01:52:00,790 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2941 (0.3901) Acc D Real: 62.393% 
Loss D Fake: 0.6839 (0.6479) Acc D Fake: 84.764% 
Loss D: 0.978 
Loss G: 0.7048 (0.7443) Acc G: 15.314% 
LR: 2.000e-04 

2023-03-02 01:52:00,797 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3941 (0.3902) Acc D Real: 62.467% 
Loss D Fake: 0.6846 (0.6485) Acc D Fake: 84.768% 
Loss D: 1.079 
Loss G: 0.7042 (0.7437) Acc G: 15.309% 
LR: 2.000e-04 

2023-03-02 01:52:00,805 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3238 (0.3892) Acc D Real: 62.608% 
Loss D Fake: 0.6850 (0.6491) Acc D Fake: 84.771% 
Loss D: 1.009 
Loss G: 0.7040 (0.7431) Acc G: 15.305% 
LR: 2.000e-04 

2023-03-02 01:52:00,813 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2675 (0.3873) Acc D Real: 62.833% 
Loss D Fake: 0.6848 (0.6496) Acc D Fake: 84.775% 
Loss D: 0.952 
Loss G: 0.7049 (0.7425) Acc G: 15.300% 
LR: 2.000e-04 

2023-03-02 01:52:00,821 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2904 (0.3859) Acc D Real: 63.044% 
Loss D Fake: 0.6835 (0.6501) Acc D Fake: 84.778% 
Loss D: 0.974 
Loss G: 0.7066 (0.7420) Acc G: 15.296% 
LR: 2.000e-04 

2023-03-02 01:52:00,828 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3739 (0.3857) Acc D Real: 63.117% 
Loss D Fake: 0.6817 (0.6505) Acc D Fake: 84.781% 
Loss D: 1.056 
Loss G: 0.7082 (0.7415) Acc G: 15.291% 
LR: 2.000e-04 

2023-03-02 01:52:00,836 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3442 (0.3852) Acc D Real: 63.210% 
Loss D Fake: 0.6803 (0.6510) Acc D Fake: 84.784% 
Loss D: 1.025 
Loss G: 0.7096 (0.7410) Acc G: 15.287% 
LR: 2.000e-04 

2023-03-02 01:52:00,844 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3138 (0.3841) Acc D Real: 63.310% 
Loss D Fake: 0.6788 (0.6514) Acc D Fake: 84.787% 
Loss D: 0.993 
Loss G: 0.7115 (0.7406) Acc G: 15.283% 
LR: 2.000e-04 

2023-03-02 01:52:00,852 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3866 (0.3842) Acc D Real: 63.390% 
Loss D Fake: 0.6771 (0.6517) Acc D Fake: 84.790% 
Loss D: 1.064 
Loss G: 0.7131 (0.7402) Acc G: 15.279% 
LR: 2.000e-04 

2023-03-02 01:52:00,860 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4222 (0.3847) Acc D Real: 63.396% 
Loss D Fake: 0.6759 (0.6520) Acc D Fake: 84.793% 
Loss D: 1.098 
Loss G: 0.7139 (0.7399) Acc G: 15.275% 
LR: 2.000e-04 

2023-03-02 01:52:00,868 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2809 (0.3833) Acc D Real: 63.562% 
Loss D Fake: 0.6750 (0.6524) Acc D Fake: 84.796% 
Loss D: 0.956 
Loss G: 0.7154 (0.7395) Acc G: 15.272% 
LR: 2.000e-04 

2023-03-02 01:52:00,875 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.2997 (0.3822) Acc D Real: 63.740% 
Loss D Fake: 0.6732 (0.6526) Acc D Fake: 84.799% 
Loss D: 0.973 
Loss G: 0.7177 (0.7392) Acc G: 15.268% 
LR: 2.000e-04 

2023-03-02 01:52:00,883 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4154 (0.3826) Acc D Real: 63.736% 
Loss D Fake: 0.6712 (0.6529) Acc D Fake: 84.801% 
Loss D: 1.087 
Loss G: 0.7193 (0.7390) Acc G: 15.265% 
LR: 2.000e-04 

2023-03-02 01:52:00,891 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4167 (0.3831) Acc D Real: 63.712% 
Loss D Fake: 0.6700 (0.6531) Acc D Fake: 84.804% 
Loss D: 1.087 
Loss G: 0.7201 (0.7387) Acc G: 15.261% 
LR: 2.000e-04 

2023-03-02 01:52:00,899 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3064 (0.3821) Acc D Real: 63.831% 
Loss D Fake: 0.6693 (0.6533) Acc D Fake: 84.806% 
Loss D: 0.976 
Loss G: 0.7213 (0.7385) Acc G: 15.258% 
LR: 2.000e-04 

2023-03-02 01:52:00,907 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3672 (0.3819) Acc D Real: 63.846% 
Loss D Fake: 0.6680 (0.6535) Acc D Fake: 84.818% 
Loss D: 1.035 
Loss G: 0.7227 (0.7383) Acc G: 15.233% 
LR: 2.000e-04 

2023-03-02 01:52:00,915 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3147 (0.3811) Acc D Real: 63.910% 
Loss D Fake: 0.6667 (0.6537) Acc D Fake: 84.841% 
Loss D: 0.981 
Loss G: 0.7241 (0.7381) Acc G: 15.210% 
LR: 2.000e-04 

2023-03-02 01:52:00,922 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3159 (0.3802) Acc D Real: 63.972% 
Loss D Fake: 0.6652 (0.6538) Acc D Fake: 84.864% 
Loss D: 0.981 
Loss G: 0.7259 (0.7380) Acc G: 15.186% 
LR: 2.000e-04 

2023-03-02 01:52:00,930 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3795 (0.3802) Acc D Real: 63.960% 
Loss D Fake: 0.6637 (0.6539) Acc D Fake: 84.886% 
Loss D: 1.043 
Loss G: 0.7274 (0.7378) Acc G: 15.164% 
LR: 2.000e-04 

2023-03-02 01:52:00,937 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4597 (0.3812) Acc D Real: 63.826% 
Loss D Fake: 0.6626 (0.6540) Acc D Fake: 84.907% 
Loss D: 1.122 
Loss G: 0.7279 (0.7377) Acc G: 15.142% 
LR: 2.000e-04 

2023-03-02 01:52:00,945 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3816 (0.3812) Acc D Real: 63.835% 
Loss D Fake: 0.6625 (0.6541) Acc D Fake: 84.928% 
Loss D: 1.044 
Loss G: 0.7280 (0.7376) Acc G: 15.120% 
LR: 2.000e-04 

2023-03-02 01:52:00,953 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4275 (0.3817) Acc D Real: 63.758% 
Loss D Fake: 0.6626 (0.6542) Acc D Fake: 84.949% 
Loss D: 1.090 
Loss G: 0.7275 (0.7375) Acc G: 15.099% 
LR: 2.000e-04 

2023-03-02 01:52:00,961 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4812 (0.3829) Acc D Real: 63.621% 
Loss D Fake: 0.6635 (0.6543) Acc D Fake: 84.969% 
Loss D: 1.145 
Loss G: 0.7259 (0.7374) Acc G: 15.079% 
LR: 2.000e-04 

2023-03-02 01:52:00,968 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3624 (0.3827) Acc D Real: 63.640% 
Loss D Fake: 0.6652 (0.6545) Acc D Fake: 84.988% 
Loss D: 1.028 
Loss G: 0.7244 (0.7372) Acc G: 15.059% 
LR: 2.000e-04 

2023-03-02 01:52:00,976 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3381 (0.3822) Acc D Real: 63.712% 
Loss D Fake: 0.6663 (0.6546) Acc D Fake: 85.007% 
Loss D: 1.004 
Loss G: 0.7235 (0.7371) Acc G: 15.039% 
LR: 2.000e-04 

2023-03-02 01:52:00,984 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3331 (0.3816) Acc D Real: 63.787% 
Loss D Fake: 0.6668 (0.6547) Acc D Fake: 85.026% 
Loss D: 1.000 
Loss G: 0.7233 (0.7369) Acc G: 15.020% 
LR: 2.000e-04 

2023-03-02 01:52:00,991 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4297 (0.3821) Acc D Real: 63.700% 
Loss D Fake: 0.6670 (0.6549) Acc D Fake: 85.044% 
Loss D: 1.097 
Loss G: 0.7227 (0.7367) Acc G: 15.001% 
LR: 2.000e-04 

2023-03-02 01:52:00,999 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4502 (0.3829) Acc D Real: 63.611% 
Loss D Fake: 0.6678 (0.6550) Acc D Fake: 85.062% 
Loss D: 1.118 
Loss G: 0.7215 (0.7366) Acc G: 14.983% 
LR: 2.000e-04 

2023-03-02 01:52:01,007 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4427 (0.3835) Acc D Real: 63.528% 
Loss D Fake: 0.6693 (0.6552) Acc D Fake: 85.079% 
Loss D: 1.112 
Loss G: 0.7197 (0.7364) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,015 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3801 (0.3835) Acc D Real: 63.564% 
Loss D Fake: 0.6710 (0.6553) Acc D Fake: 85.078% 
Loss D: 1.051 
Loss G: 0.7180 (0.7362) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,023 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3801 (0.3835) Acc D Real: 63.528% 
Loss D Fake: 0.6725 (0.6555) Acc D Fake: 85.078% 
Loss D: 1.053 
Loss G: 0.7164 (0.7360) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,030 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3689 (0.3833) Acc D Real: 63.595% 
Loss D Fake: 0.6738 (0.6557) Acc D Fake: 85.077% 
Loss D: 1.043 
Loss G: 0.7152 (0.7358) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,039 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4254 (0.3838) Acc D Real: 63.540% 
Loss D Fake: 0.6751 (0.6559) Acc D Fake: 85.076% 
Loss D: 1.100 
Loss G: 0.7137 (0.7355) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,048 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3500 (0.3834) Acc D Real: 63.576% 
Loss D Fake: 0.6764 (0.6561) Acc D Fake: 85.075% 
Loss D: 1.026 
Loss G: 0.7125 (0.7353) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,057 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3492 (0.3831) Acc D Real: 63.610% 
Loss D Fake: 0.6774 (0.6563) Acc D Fake: 85.074% 
Loss D: 1.027 
Loss G: 0.7117 (0.7351) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,065 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2663 (0.3819) Acc D Real: 63.788% 
Loss D Fake: 0.6777 (0.6566) Acc D Fake: 85.074% 
Loss D: 0.944 
Loss G: 0.7120 (0.7348) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,074 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3878 (0.3819) Acc D Real: 63.803% 
Loss D Fake: 0.6772 (0.6568) Acc D Fake: 85.073% 
Loss D: 1.065 
Loss G: 0.7124 (0.7346) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,083 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3143 (0.3813) Acc D Real: 63.884% 
Loss D Fake: 0.6767 (0.6570) Acc D Fake: 85.072% 
Loss D: 0.991 
Loss G: 0.7131 (0.7344) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,091 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3709 (0.3812) Acc D Real: 63.888% 
Loss D Fake: 0.6759 (0.6571) Acc D Fake: 85.071% 
Loss D: 1.047 
Loss G: 0.7139 (0.7342) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,100 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3391 (0.3808) Acc D Real: 63.927% 
Loss D Fake: 0.6753 (0.6573) Acc D Fake: 85.071% 
Loss D: 1.014 
Loss G: 0.7147 (0.7340) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,109 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4345 (0.3813) Acc D Real: 63.882% 
Loss D Fake: 0.6746 (0.6575) Acc D Fake: 85.070% 
Loss D: 1.109 
Loss G: 0.7149 (0.7338) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,117 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4344 (0.3818) Acc D Real: 63.840% 
Loss D Fake: 0.6748 (0.6577) Acc D Fake: 85.069% 
Loss D: 1.109 
Loss G: 0.7145 (0.7336) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,125 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4346 (0.3823) Acc D Real: 63.760% 
Loss D Fake: 0.6755 (0.6578) Acc D Fake: 85.069% 
Loss D: 1.110 
Loss G: 0.7134 (0.7334) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 01:52:01,132 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3211 (0.3817) Acc D Real: 63.908% 
Loss D Fake: 0.6765 (0.6580) Acc D Fake: 85.068% 
Loss D: 0.998 
Loss G: 0.7127 (0.7332) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 01:52:01,140 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4373 (0.3822) Acc D Real: 63.858% 
Loss D Fake: 0.6771 (0.6582) Acc D Fake: 85.068% 
Loss D: 1.114 
Loss G: 0.7118 (0.7330) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 01:52:01,148 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3176 (0.3816) Acc D Real: 63.909% 
Loss D Fake: 0.6778 (0.6584) Acc D Fake: 85.066% 
Loss D: 0.995 
Loss G: 0.7114 (0.7328) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 01:52:01,156 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4139 (0.3819) Acc D Real: 63.854% 
Loss D Fake: 0.6782 (0.6585) Acc D Fake: 85.051% 
Loss D: 1.092 
Loss G: 0.7108 (0.7326) Acc G: 14.983% 
LR: 2.000e-04 

2023-03-02 01:52:01,163 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3284 (0.3814) Acc D Real: 63.921% 
Loss D Fake: 0.6787 (0.6587) Acc D Fake: 85.035% 
Loss D: 1.007 
Loss G: 0.7106 (0.7324) Acc G: 14.998% 
LR: 2.000e-04 

2023-03-02 01:52:01,172 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2728 (0.3805) Acc D Real: 64.023% 
Loss D Fake: 0.6784 (0.6589) Acc D Fake: 85.020% 
Loss D: 0.951 
Loss G: 0.7115 (0.7323) Acc G: 15.013% 
LR: 2.000e-04 

2023-03-02 01:52:01,180 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3628 (0.3803) Acc D Real: 64.059% 
Loss D Fake: 0.6773 (0.6591) Acc D Fake: 85.005% 
Loss D: 1.040 
Loss G: 0.7127 (0.7321) Acc G: 15.028% 
LR: 2.000e-04 

2023-03-02 01:52:01,188 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3121 (0.3797) Acc D Real: 64.108% 
Loss D Fake: 0.6761 (0.6592) Acc D Fake: 84.990% 
Loss D: 0.988 
Loss G: 0.7142 (0.7319) Acc G: 15.042% 
LR: 2.000e-04 

2023-03-02 01:52:01,196 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3874 (0.3798) Acc D Real: 64.114% 
Loss D Fake: 0.6746 (0.6593) Acc D Fake: 84.979% 
Loss D: 1.062 
Loss G: 0.7156 (0.7318) Acc G: 15.042% 
LR: 2.000e-04 

2023-03-02 01:52:01,203 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3014 (0.3791) Acc D Real: 64.165% 
Loss D Fake: 0.6732 (0.6595) Acc D Fake: 84.979% 
Loss D: 0.975 
Loss G: 0.7174 (0.7317) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 01:52:01,211 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3127 (0.3785) Acc D Real: 64.224% 
Loss D Fake: 0.6713 (0.6596) Acc D Fake: 84.980% 
Loss D: 0.984 
Loss G: 0.7196 (0.7316) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 01:52:01,218 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3995 (0.3787) Acc D Real: 64.187% 
Loss D Fake: 0.6693 (0.6596) Acc D Fake: 84.980% 
Loss D: 1.069 
Loss G: 0.7213 (0.7315) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 01:52:01,226 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2326 (0.3775) Acc D Real: 64.298% 
Loss D Fake: 0.6676 (0.6597) Acc D Fake: 84.980% 
Loss D: 0.900 
Loss G: 0.7239 (0.7314) Acc G: 15.031% 
LR: 2.000e-04 

2023-03-02 01:52:01,233 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3622 (0.3774) Acc D Real: 64.322% 
Loss D Fake: 0.6651 (0.6598) Acc D Fake: 84.983% 
Loss D: 1.027 
Loss G: 0.7264 (0.7314) Acc G: 15.017% 
LR: 2.000e-04 

2023-03-02 01:52:01,241 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3436 (0.3771) Acc D Real: 64.321% 
Loss D Fake: 0.6629 (0.6598) Acc D Fake: 84.993% 
Loss D: 1.006 
Loss G: 0.7286 (0.7313) Acc G: 15.003% 
LR: 2.000e-04 

2023-03-02 01:52:01,249 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4861 (0.3780) Acc D Real: 64.195% 
Loss D Fake: 0.6613 (0.6598) Acc D Fake: 85.006% 
Loss D: 1.147 
Loss G: 0.7295 (0.7313) Acc G: 14.990% 
LR: 2.000e-04 

2023-03-02 01:52:01,256 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3657 (0.3779) Acc D Real: 64.210% 
Loss D Fake: 0.6609 (0.6598) Acc D Fake: 85.015% 
Loss D: 1.027 
Loss G: 0.7300 (0.7313) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 01:52:01,264 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3643 (0.3778) Acc D Real: 64.191% 
Loss D Fake: 0.6604 (0.6598) Acc D Fake: 85.018% 
Loss D: 1.025 
Loss G: 0.7304 (0.7313) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,272 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3668 (0.3777) Acc D Real: 64.197% 
Loss D Fake: 0.6601 (0.6598) Acc D Fake: 85.019% 
Loss D: 1.027 
Loss G: 0.7309 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 01:52:01,279 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4190 (0.3780) Acc D Real: 64.156% 
Loss D Fake: 0.6598 (0.6598) Acc D Fake: 85.019% 
Loss D: 1.079 
Loss G: 0.7307 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 01:52:01,288 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.5344 (0.3792) Acc D Real: 63.978% 
Loss D Fake: 0.6605 (0.6598) Acc D Fake: 85.018% 
Loss D: 1.195 
Loss G: 0.7292 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 01:52:01,296 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2961 (0.3786) Acc D Real: 64.049% 
Loss D Fake: 0.6620 (0.6598) Acc D Fake: 85.018% 
Loss D: 0.958 
Loss G: 0.7282 (0.7313) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 01:52:01,304 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4506 (0.3791) Acc D Real: 63.973% 
Loss D Fake: 0.6629 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.113 
Loss G: 0.7268 (0.7312) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 01:52:01,311 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2554 (0.3782) Acc D Real: 64.062% 
Loss D Fake: 0.6639 (0.6599) Acc D Fake: 85.018% 
Loss D: 0.919 
Loss G: 0.7265 (0.7312) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 01:52:01,319 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3622 (0.3781) Acc D Real: 64.061% 
Loss D Fake: 0.6638 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.026 
Loss G: 0.7267 (0.7312) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 01:52:01,327 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4141 (0.3783) Acc D Real: 64.009% 
Loss D Fake: 0.6637 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.078 
Loss G: 0.7264 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 01:52:01,335 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.2540 (0.3774) Acc D Real: 64.103% 
Loss D Fake: 0.6637 (0.6600) Acc D Fake: 85.018% 
Loss D: 0.918 
Loss G: 0.7271 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 01:52:01,343 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3886 (0.3775) Acc D Real: 64.073% 
Loss D Fake: 0.6629 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.052 
Loss G: 0.7279 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 01:52:01,350 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3385 (0.3772) Acc D Real: 64.101% 
Loss D Fake: 0.6622 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.001 
Loss G: 0.7287 (0.7310) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 01:52:01,358 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2737 (0.3764) Acc D Real: 64.181% 
Loss D Fake: 0.6612 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.935 
Loss G: 0.7303 (0.7310) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 01:52:01,365 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2856 (0.3758) Acc D Real: 64.246% 
Loss D Fake: 0.6594 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.945 
Loss G: 0.7327 (0.7311) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 01:52:01,372 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3378 (0.3755) Acc D Real: 64.261% 
Loss D Fake: 0.6571 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.995 
Loss G: 0.7351 (0.7311) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,380 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3863 (0.3756) Acc D Real: 64.234% 
Loss D Fake: 0.6550 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.041 
Loss G: 0.7371 (0.7311) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,387 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4224 (0.3759) Acc D Real: 64.169% 
Loss D Fake: 0.6537 (0.6599) Acc D Fake: 85.017% 
Loss D: 1.076 
Loss G: 0.7380 (0.7312) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,395 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3896 (0.3760) Acc D Real: 64.146% 
Loss D Fake: 0.6532 (0.6599) Acc D Fake: 85.017% 
Loss D: 1.043 
Loss G: 0.7384 (0.7312) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 01:52:01,402 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2887 (0.3754) Acc D Real: 64.211% 
Loss D Fake: 0.6526 (0.6598) Acc D Fake: 85.017% 
Loss D: 0.941 
Loss G: 0.7395 (0.7313) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,409 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2473 (0.3745) Acc D Real: 64.310% 
Loss D Fake: 0.6511 (0.6598) Acc D Fake: 85.016% 
Loss D: 0.898 
Loss G: 0.7419 (0.7314) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,416 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.5152 (0.3755) Acc D Real: 64.170% 
Loss D Fake: 0.6493 (0.6597) Acc D Fake: 85.016% 
Loss D: 1.164 
Loss G: 0.7429 (0.7314) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,424 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4833 (0.3762) Acc D Real: 64.056% 
Loss D Fake: 0.6491 (0.6596) Acc D Fake: 85.016% 
Loss D: 1.132 
Loss G: 0.7424 (0.7315) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,431 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4735 (0.3769) Acc D Real: 63.963% 
Loss D Fake: 0.6501 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.124 
Loss G: 0.7408 (0.7316) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 01:52:01,439 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4191 (0.3772) Acc D Real: 63.913% 
Loss D Fake: 0.6518 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.071 
Loss G: 0.7386 (0.7316) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,448 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.5376 (0.3783) Acc D Real: 63.754% 
Loss D Fake: 0.6542 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.192 
Loss G: 0.7352 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,455 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3193 (0.3779) Acc D Real: 63.785% 
Loss D Fake: 0.6573 (0.6594) Acc D Fake: 85.016% 
Loss D: 0.977 
Loss G: 0.7324 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,463 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3457 (0.3777) Acc D Real: 63.803% 
Loss D Fake: 0.6593 (0.6594) Acc D Fake: 85.016% 
Loss D: 1.005 
Loss G: 0.7308 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 01:52:01,470 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4319 (0.3780) Acc D Real: 63.777% 
Loss D Fake: 0.6607 (0.6594) Acc D Fake: 85.016% 
Loss D: 1.093 
Loss G: 0.7290 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,478 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2828 (0.3774) Acc D Real: 63.838% 
Loss D Fake: 0.6621 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.945 
Loss G: 0.7281 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,485 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3344 (0.3771) Acc D Real: 63.869% 
Loss D Fake: 0.6624 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.997 
Loss G: 0.7282 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,493 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4194 (0.3774) Acc D Real: 63.811% 
Loss D Fake: 0.6623 (0.6595) Acc D Fake: 85.015% 
Loss D: 1.082 
Loss G: 0.7279 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,500 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3111 (0.3770) Acc D Real: 63.852% 
Loss D Fake: 0.6626 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.974 
Loss G: 0.7279 (0.7315) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 01:52:01,508 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4436 (0.3774) Acc D Real: 63.778% 
Loss D Fake: 0.6627 (0.6595) Acc D Fake: 85.015% 
Loss D: 1.106 
Loss G: 0.7274 (0.7315) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,515 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4530 (0.3779) Acc D Real: 63.710% 
Loss D Fake: 0.6635 (0.6596) Acc D Fake: 85.015% 
Loss D: 1.117 
Loss G: 0.7261 (0.7315) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,523 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3022 (0.3774) Acc D Real: 63.714% 
Loss D Fake: 0.6646 (0.6596) Acc D Fake: 85.015% 
Loss D: 0.967 
Loss G: 0.7254 (0.7314) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 01:52:01,748 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.344 | Generator Loss: 0.725 | Avg: 2.069 
2023-03-02 01:52:01,771 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.249 | Generator Loss: 0.725 | Avg: 1.974 
2023-03-02 01:52:01,794 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.278 | Generator Loss: 0.725 | Avg: 2.003 
2023-03-02 01:52:01,820 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.297 | Generator Loss: 0.725 | Avg: 2.023 
2023-03-02 01:52:01,847 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.299 | Generator Loss: 0.725 | Avg: 2.024 
2023-03-02 01:52:01,874 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.251 | Generator Loss: 0.725 | Avg: 1.976 
2023-03-02 01:52:01,900 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.224 | Generator Loss: 0.725 | Avg: 1.949 
2023-03-02 01:52:01,926 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.178 | Generator Loss: 0.725 | Avg: 1.904 
2023-03-02 01:52:01,953 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.152 | Generator Loss: 0.725 | Avg: 1.878 
2023-03-02 01:52:01,979 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.111 | Generator Loss: 0.725 | Avg: 1.836 
2023-03-02 01:52:02,006 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.078 | Generator Loss: 0.725 | Avg: 1.803 
2023-03-02 01:52:02,032 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.048 | Generator Loss: 0.725 | Avg: 1.773 
2023-03-02 01:52:02,058 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.023 | Generator Loss: 0.725 | Avg: 1.749 
2023-03-02 01:52:02,084 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.005 | Generator Loss: 0.725 | Avg: 1.731 
2023-03-02 01:52:02,111 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.021 | Generator Loss: 0.725 | Avg: 1.747 
2023-03-02 01:52:02,142 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.037 | Generator Loss: 0.725 | Avg: 1.763 
2023-03-02 01:52:02,168 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.053 | Generator Loss: 0.725 | Avg: 1.778 
2023-03-02 01:52:02,201 -                train: [    INFO] - 
Epoch: 13/20
2023-03-02 01:52:02,395 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3018 (0.3269) Acc D Real: 67.760% 
Loss D Fake: 0.6649 (0.6649) Acc D Fake: 85.000% 
Loss D: 0.967 
Loss G: 0.7257 (0.7254) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,402 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4261 (0.3600) Acc D Real: 63.212% 
Loss D Fake: 0.6644 (0.6648) Acc D Fake: 85.000% 
Loss D: 1.091 
Loss G: 0.7258 (0.7255) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,409 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4169 (0.3742) Acc D Real: 61.549% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.082 
Loss G: 0.7253 (0.7255) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,425 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.2560 (0.3506) Acc D Real: 64.750% 
Loss D Fake: 0.6648 (0.6647) Acc D Fake: 85.000% 
Loss D: 0.921 
Loss G: 0.7258 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,432 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4857 (0.3731) Acc D Real: 61.988% 
Loss D Fake: 0.6644 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.150 
Loss G: 0.7256 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,439 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3009 (0.3628) Acc D Real: 63.586% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 0.966 
Loss G: 0.7258 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,446 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4407 (0.3725) Acc D Real: 62.461% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.105 
Loss G: 0.7253 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,453 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4179 (0.3776) Acc D Real: 62.031% 
Loss D Fake: 0.6652 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.083 
Loss G: 0.7245 (0.7254) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,461 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4994 (0.3897) Acc D Real: 60.766% 
Loss D Fake: 0.6664 (0.6649) Acc D Fake: 85.000% 
Loss D: 1.166 
Loss G: 0.7225 (0.7252) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,468 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3758 (0.3885) Acc D Real: 60.767% 
Loss D Fake: 0.6684 (0.6652) Acc D Fake: 85.000% 
Loss D: 1.044 
Loss G: 0.7206 (0.7247) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,475 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2947 (0.3807) Acc D Real: 62.044% 
Loss D Fake: 0.6699 (0.6656) Acc D Fake: 85.000% 
Loss D: 0.965 
Loss G: 0.7196 (0.7243) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:02,481 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4421 (0.3854) Acc D Real: 61.498% 
Loss D Fake: 0.6707 (0.6660) Acc D Fake: 84.976% 
Loss D: 1.113 
Loss G: 0.7183 (0.7238) Acc G: 15.064% 
LR: 2.000e-04 

2023-03-02 01:52:02,488 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2279 (0.3741) Acc D Real: 62.999% 
Loss D Fake: 0.6716 (0.6664) Acc D Fake: 84.881% 
Loss D: 0.899 
Loss G: 0.7184 (0.7235) Acc G: 15.141% 
LR: 2.000e-04 

2023-03-02 01:52:02,495 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4772 (0.3810) Acc D Real: 62.319% 
Loss D Fake: 0.6714 (0.6667) Acc D Fake: 84.806% 
Loss D: 1.149 
Loss G: 0.7180 (0.7231) Acc G: 15.233% 
LR: 2.000e-04 

2023-03-02 01:52:02,502 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3333 (0.3780) Acc D Real: 62.760% 
Loss D Fake: 0.6719 (0.6671) Acc D Fake: 84.714% 
Loss D: 1.005 
Loss G: 0.7177 (0.7228) Acc G: 15.322% 
LR: 2.000e-04 

2023-03-02 01:52:02,509 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3662 (0.3773) Acc D Real: 62.751% 
Loss D Fake: 0.6720 (0.6673) Acc D Fake: 84.632% 
Loss D: 1.038 
Loss G: 0.7176 (0.7224) Acc G: 15.401% 
LR: 2.000e-04 

2023-03-02 01:52:02,516 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3993 (0.3786) Acc D Real: 62.636% 
Loss D Fake: 0.6722 (0.6676) Acc D Fake: 84.560% 
Loss D: 1.071 
Loss G: 0.7173 (0.7222) Acc G: 15.472% 
LR: 2.000e-04 

2023-03-02 01:52:02,523 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3054 (0.3747) Acc D Real: 63.021% 
Loss D Fake: 0.6723 (0.6679) Acc D Fake: 84.496% 
Loss D: 0.978 
Loss G: 0.7176 (0.7219) Acc G: 15.535% 
LR: 2.000e-04 

2023-03-02 01:52:02,530 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2958 (0.3708) Acc D Real: 63.539% 
Loss D Fake: 0.6716 (0.6680) Acc D Fake: 84.438% 
Loss D: 0.967 
Loss G: 0.7187 (0.7218) Acc G: 15.591% 
LR: 2.000e-04 

2023-03-02 01:52:02,537 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2692 (0.3659) Acc D Real: 64.174% 
Loss D Fake: 0.6702 (0.6682) Acc D Fake: 84.385% 
Loss D: 0.939 
Loss G: 0.7206 (0.7217) Acc G: 15.642% 
LR: 2.000e-04 

2023-03-02 01:52:02,544 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3718 (0.3662) Acc D Real: 64.176% 
Loss D Fake: 0.6684 (0.6682) Acc D Fake: 84.337% 
Loss D: 1.040 
Loss G: 0.7224 (0.7217) Acc G: 15.689% 
LR: 2.000e-04 

2023-03-02 01:52:02,550 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3592 (0.3659) Acc D Real: 64.149% 
Loss D Fake: 0.6669 (0.6681) Acc D Fake: 84.293% 
Loss D: 1.026 
Loss G: 0.7239 (0.7218) Acc G: 15.731% 
LR: 2.000e-04 

2023-03-02 01:52:02,557 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3751 (0.3663) Acc D Real: 64.115% 
Loss D Fake: 0.6656 (0.6680) Acc D Fake: 84.253% 
Loss D: 1.041 
Loss G: 0.7252 (0.7220) Acc G: 15.770% 
LR: 2.000e-04 

2023-03-02 01:52:02,564 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4115 (0.3681) Acc D Real: 63.842% 
Loss D Fake: 0.6646 (0.6679) Acc D Fake: 84.217% 
Loss D: 1.076 
Loss G: 0.7259 (0.7221) Acc G: 15.806% 
LR: 2.000e-04 

2023-03-02 01:52:02,571 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3584 (0.3677) Acc D Real: 63.886% 
Loss D Fake: 0.6641 (0.6677) Acc D Fake: 84.183% 
Loss D: 1.022 
Loss G: 0.7265 (0.7223) Acc G: 15.839% 
LR: 2.000e-04 

2023-03-02 01:52:02,578 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3161 (0.3658) Acc D Real: 64.086% 
Loss D Fake: 0.6634 (0.6676) Acc D Fake: 84.151% 
Loss D: 0.979 
Loss G: 0.7275 (0.7225) Acc G: 15.870% 
LR: 2.000e-04 

2023-03-02 01:52:02,585 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3646 (0.3657) Acc D Real: 64.031% 
Loss D Fake: 0.6624 (0.6674) Acc D Fake: 84.122% 
Loss D: 1.027 
Loss G: 0.7285 (0.7227) Acc G: 15.898% 
LR: 2.000e-04 

2023-03-02 01:52:02,593 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3213 (0.3642) Acc D Real: 64.217% 
Loss D Fake: 0.6613 (0.6672) Acc D Fake: 84.095% 
Loss D: 0.983 
Loss G: 0.7299 (0.7230) Acc G: 15.925% 
LR: 2.000e-04 

2023-03-02 01:52:02,600 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.2717 (0.3611) Acc D Real: 64.562% 
Loss D Fake: 0.6598 (0.6669) Acc D Fake: 84.069% 
Loss D: 0.932 
Loss G: 0.7319 (0.7233) Acc G: 15.950% 
LR: 2.000e-04 

2023-03-02 01:52:02,608 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.5039 (0.3657) Acc D Real: 63.871% 
Loss D Fake: 0.6583 (0.6666) Acc D Fake: 84.046% 
Loss D: 1.162 
Loss G: 0.7326 (0.7236) Acc G: 15.973% 
LR: 2.000e-04 

2023-03-02 01:52:02,615 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3229 (0.3644) Acc D Real: 63.994% 
Loss D Fake: 0.6580 (0.6664) Acc D Fake: 84.023% 
Loss D: 0.981 
Loss G: 0.7332 (0.7239) Acc G: 15.994% 
LR: 2.000e-04 

2023-03-02 01:52:02,623 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3396 (0.3636) Acc D Real: 64.148% 
Loss D Fake: 0.6573 (0.6661) Acc D Fake: 84.003% 
Loss D: 0.997 
Loss G: 0.7340 (0.7242) Acc G: 16.015% 
LR: 2.000e-04 

2023-03-02 01:52:02,631 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3576 (0.3635) Acc D Real: 64.263% 
Loss D Fake: 0.6565 (0.6658) Acc D Fake: 83.983% 
Loss D: 1.014 
Loss G: 0.7348 (0.7245) Acc G: 16.034% 
LR: 2.000e-04 

2023-03-02 01:52:02,638 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3586 (0.3633) Acc D Real: 64.198% 
Loss D Fake: 0.6559 (0.6655) Acc D Fake: 83.964% 
Loss D: 1.014 
Loss G: 0.7355 (0.7248) Acc G: 16.052% 
LR: 2.000e-04 

2023-03-02 01:52:02,645 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2302 (0.3596) Acc D Real: 64.631% 
Loss D Fake: 0.6550 (0.6652) Acc D Fake: 83.947% 
Loss D: 0.885 
Loss G: 0.7371 (0.7251) Acc G: 16.069% 
LR: 2.000e-04 

2023-03-02 01:52:02,653 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3236 (0.3587) Acc D Real: 64.686% 
Loss D Fake: 0.6532 (0.6649) Acc D Fake: 83.932% 
Loss D: 0.977 
Loss G: 0.7392 (0.7255) Acc G: 16.077% 
LR: 2.000e-04 

2023-03-02 01:52:02,660 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4077 (0.3599) Acc D Real: 64.445% 
Loss D Fake: 0.6516 (0.6646) Acc D Fake: 83.924% 
Loss D: 1.059 
Loss G: 0.7406 (0.7259) Acc G: 16.073% 
LR: 2.000e-04 

2023-03-02 01:52:02,668 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3198 (0.3589) Acc D Real: 64.497% 
Loss D Fake: 0.6505 (0.6642) Acc D Fake: 83.925% 
Loss D: 0.970 
Loss G: 0.7419 (0.7263) Acc G: 16.058% 
LR: 2.000e-04 

2023-03-02 01:52:02,675 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3862 (0.3596) Acc D Real: 64.423% 
Loss D Fake: 0.6493 (0.6638) Acc D Fake: 83.932% 
Loss D: 1.036 
Loss G: 0.7430 (0.7267) Acc G: 16.039% 
LR: 2.000e-04 

2023-03-02 01:52:02,683 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3881 (0.3603) Acc D Real: 64.295% 
Loss D Fake: 0.6486 (0.6635) Acc D Fake: 83.948% 
Loss D: 1.037 
Loss G: 0.7435 (0.7271) Acc G: 16.023% 
LR: 2.000e-04 

2023-03-02 01:52:02,690 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3907 (0.3610) Acc D Real: 64.187% 
Loss D Fake: 0.6483 (0.6631) Acc D Fake: 83.952% 
Loss D: 1.039 
Loss G: 0.7437 (0.7275) Acc G: 16.018% 
LR: 2.000e-04 

2023-03-02 01:52:02,698 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3921 (0.3617) Acc D Real: 64.059% 
Loss D Fake: 0.6483 (0.6628) Acc D Fake: 83.943% 
Loss D: 1.040 
Loss G: 0.7435 (0.7279) Acc G: 16.027% 
LR: 2.000e-04 

2023-03-02 01:52:02,706 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.1968 (0.3580) Acc D Real: 64.496% 
Loss D Fake: 0.6481 (0.6624) Acc D Fake: 83.933% 
Loss D: 0.845 
Loss G: 0.7447 (0.7283) Acc G: 16.033% 
LR: 2.000e-04 

2023-03-02 01:52:02,713 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2628 (0.3559) Acc D Real: 64.747% 
Loss D Fake: 0.6464 (0.6621) Acc D Fake: 83.925% 
Loss D: 0.909 
Loss G: 0.7471 (0.7287) Acc G: 16.028% 
LR: 2.000e-04 

2023-03-02 01:52:02,722 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3501 (0.3558) Acc D Real: 64.740% 
Loss D Fake: 0.6442 (0.6617) Acc D Fake: 83.932% 
Loss D: 0.994 
Loss G: 0.7494 (0.7292) Acc G: 16.007% 
LR: 2.000e-04 

2023-03-02 01:52:02,730 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3742 (0.3561) Acc D Real: 64.664% 
Loss D Fake: 0.6424 (0.6613) Acc D Fake: 83.951% 
Loss D: 1.017 
Loss G: 0.7513 (0.7296) Acc G: 15.987% 
LR: 2.000e-04 

2023-03-02 01:52:02,738 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3718 (0.3565) Acc D Real: 64.588% 
Loss D Fake: 0.6409 (0.6608) Acc D Fake: 83.980% 
Loss D: 1.013 
Loss G: 0.7527 (0.7301) Acc G: 15.932% 
LR: 2.000e-04 

2023-03-02 01:52:02,745 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3062 (0.3554) Acc D Real: 64.691% 
Loss D Fake: 0.6397 (0.6604) Acc D Fake: 84.033% 
Loss D: 0.946 
Loss G: 0.7543 (0.7306) Acc G: 15.879% 
LR: 2.000e-04 

2023-03-02 01:52:02,753 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3908 (0.3562) Acc D Real: 64.581% 
Loss D Fake: 0.6384 (0.6600) Acc D Fake: 84.085% 
Loss D: 1.029 
Loss G: 0.7556 (0.7311) Acc G: 15.830% 
LR: 2.000e-04 

2023-03-02 01:52:02,761 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3540 (0.3561) Acc D Real: 64.572% 
Loss D Fake: 0.6374 (0.6595) Acc D Fake: 84.132% 
Loss D: 0.991 
Loss G: 0.7567 (0.7316) Acc G: 15.782% 
LR: 2.000e-04 

2023-03-02 01:52:02,768 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3694 (0.3564) Acc D Real: 64.517% 
Loss D Fake: 0.6365 (0.6591) Acc D Fake: 84.170% 
Loss D: 1.006 
Loss G: 0.7575 (0.7321) Acc G: 15.735% 
LR: 2.000e-04 

2023-03-02 01:52:02,775 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.5251 (0.3595) Acc D Real: 64.075% 
Loss D Fake: 0.6363 (0.6587) Acc D Fake: 84.195% 
Loss D: 1.161 
Loss G: 0.7567 (0.7326) Acc G: 15.714% 
LR: 2.000e-04 

2023-03-02 01:52:02,783 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2791 (0.3581) Acc D Real: 64.218% 
Loss D Fake: 0.6372 (0.6583) Acc D Fake: 84.210% 
Loss D: 0.916 
Loss G: 0.7562 (0.7330) Acc G: 15.699% 
LR: 2.000e-04 

2023-03-02 01:52:02,790 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3703 (0.3583) Acc D Real: 64.159% 
Loss D Fake: 0.6375 (0.6579) Acc D Fake: 84.224% 
Loss D: 1.008 
Loss G: 0.7558 (0.7334) Acc G: 15.684% 
LR: 2.000e-04 

2023-03-02 01:52:02,797 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3452 (0.3580) Acc D Real: 64.150% 
Loss D Fake: 0.6378 (0.6575) Acc D Fake: 84.239% 
Loss D: 0.983 
Loss G: 0.7556 (0.7338) Acc G: 15.668% 
LR: 2.000e-04 

2023-03-02 01:52:02,805 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2855 (0.3568) Acc D Real: 64.308% 
Loss D Fake: 0.6377 (0.6572) Acc D Fake: 84.253% 
Loss D: 0.923 
Loss G: 0.7561 (0.7342) Acc G: 15.640% 
LR: 2.000e-04 

2023-03-02 01:52:02,812 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2794 (0.3554) Acc D Real: 64.460% 
Loss D Fake: 0.6369 (0.6568) Acc D Fake: 84.285% 
Loss D: 0.916 
Loss G: 0.7575 (0.7346) Acc G: 15.600% 
LR: 2.000e-04 

2023-03-02 01:52:02,820 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3208 (0.3549) Acc D Real: 64.503% 
Loss D Fake: 0.6355 (0.6565) Acc D Fake: 84.326% 
Loss D: 0.956 
Loss G: 0.7591 (0.7350) Acc G: 15.561% 
LR: 2.000e-04 

2023-03-02 01:52:02,827 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3235 (0.3543) Acc D Real: 64.550% 
Loss D Fake: 0.6341 (0.6561) Acc D Fake: 84.365% 
Loss D: 0.958 
Loss G: 0.7608 (0.7355) Acc G: 15.524% 
LR: 2.000e-04 

2023-03-02 01:52:02,835 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3726 (0.3546) Acc D Real: 64.495% 
Loss D Fake: 0.6327 (0.6557) Acc D Fake: 84.402% 
Loss D: 1.005 
Loss G: 0.7624 (0.7359) Acc G: 15.488% 
LR: 2.000e-04 

2023-03-02 01:52:02,842 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2408 (0.3528) Acc D Real: 64.713% 
Loss D Fake: 0.6312 (0.6553) Acc D Fake: 84.439% 
Loss D: 0.872 
Loss G: 0.7646 (0.7364) Acc G: 15.454% 
LR: 2.000e-04 

2023-03-02 01:52:02,850 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4683 (0.3546) Acc D Real: 64.478% 
Loss D Fake: 0.6294 (0.6549) Acc D Fake: 84.474% 
Loss D: 1.098 
Loss G: 0.7659 (0.7368) Acc G: 15.420% 
LR: 2.000e-04 

2023-03-02 01:52:02,857 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3780 (0.3550) Acc D Real: 64.425% 
Loss D Fake: 0.6287 (0.6545) Acc D Fake: 84.508% 
Loss D: 1.007 
Loss G: 0.7666 (0.7373) Acc G: 15.387% 
LR: 2.000e-04 

2023-03-02 01:52:02,865 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3120 (0.3543) Acc D Real: 64.486% 
Loss D Fake: 0.6281 (0.6541) Acc D Fake: 84.542% 
Loss D: 0.940 
Loss G: 0.7676 (0.7378) Acc G: 15.356% 
LR: 2.000e-04 

2023-03-02 01:52:02,872 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3824 (0.3548) Acc D Real: 64.443% 
Loss D Fake: 0.6273 (0.6537) Acc D Fake: 84.574% 
Loss D: 1.010 
Loss G: 0.7684 (0.7382) Acc G: 15.325% 
LR: 2.000e-04 

2023-03-02 01:52:02,879 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2860 (0.3537) Acc D Real: 64.555% 
Loss D Fake: 0.6265 (0.6533) Acc D Fake: 84.605% 
Loss D: 0.913 
Loss G: 0.7697 (0.7387) Acc G: 15.295% 
LR: 2.000e-04 

2023-03-02 01:52:02,887 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3553 (0.3538) Acc D Real: 64.540% 
Loss D Fake: 0.6253 (0.6529) Acc D Fake: 84.635% 
Loss D: 0.981 
Loss G: 0.7710 (0.7392) Acc G: 15.267% 
LR: 2.000e-04 

2023-03-02 01:52:02,895 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4200 (0.3547) Acc D Real: 64.416% 
Loss D Fake: 0.6245 (0.6525) Acc D Fake: 84.665% 
Loss D: 1.044 
Loss G: 0.7715 (0.7396) Acc G: 15.239% 
LR: 2.000e-04 

2023-03-02 01:52:02,902 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4867 (0.3566) Acc D Real: 64.173% 
Loss D Fake: 0.6245 (0.6521) Acc D Fake: 84.693% 
Loss D: 1.111 
Loss G: 0.7706 (0.7401) Acc G: 15.211% 
LR: 2.000e-04 

2023-03-02 01:52:02,910 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4426 (0.3578) Acc D Real: 64.007% 
Loss D Fake: 0.6258 (0.6517) Acc D Fake: 84.721% 
Loss D: 1.068 
Loss G: 0.7688 (0.7405) Acc G: 15.185% 
LR: 2.000e-04 

2023-03-02 01:52:02,917 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3494 (0.3577) Acc D Real: 64.010% 
Loss D Fake: 0.6274 (0.6513) Acc D Fake: 84.748% 
Loss D: 0.977 
Loss G: 0.7671 (0.7409) Acc G: 15.159% 
LR: 2.000e-04 

2023-03-02 01:52:02,925 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3095 (0.3570) Acc D Real: 64.094% 
Loss D Fake: 0.6285 (0.6510) Acc D Fake: 84.775% 
Loss D: 0.938 
Loss G: 0.7663 (0.7412) Acc G: 15.134% 
LR: 2.000e-04 

2023-03-02 01:52:02,933 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3902 (0.3575) Acc D Real: 64.011% 
Loss D Fake: 0.6291 (0.6507) Acc D Fake: 84.800% 
Loss D: 1.019 
Loss G: 0.7653 (0.7415) Acc G: 15.110% 
LR: 2.000e-04 

2023-03-02 01:52:02,940 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3580 (0.3575) Acc D Real: 64.019% 
Loss D Fake: 0.6299 (0.6505) Acc D Fake: 84.825% 
Loss D: 0.988 
Loss G: 0.7645 (0.7418) Acc G: 15.086% 
LR: 2.000e-04 

2023-03-02 01:52:02,948 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3833 (0.3578) Acc D Real: 63.988% 
Loss D Fake: 0.6305 (0.6502) Acc D Fake: 84.849% 
Loss D: 1.014 
Loss G: 0.7639 (0.7421) Acc G: 15.063% 
LR: 2.000e-04 

2023-03-02 01:52:02,955 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2595 (0.3566) Acc D Real: 64.129% 
Loss D Fake: 0.6307 (0.6499) Acc D Fake: 84.873% 
Loss D: 0.890 
Loss G: 0.7642 (0.7424) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 01:52:02,963 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3695 (0.3567) Acc D Real: 64.092% 
Loss D Fake: 0.6303 (0.6497) Acc D Fake: 84.896% 
Loss D: 1.000 
Loss G: 0.7646 (0.7427) Acc G: 15.019% 
LR: 2.000e-04 

2023-03-02 01:52:02,970 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3301 (0.3564) Acc D Real: 64.123% 
Loss D Fake: 0.6299 (0.6494) Acc D Fake: 84.918% 
Loss D: 0.960 
Loss G: 0.7651 (0.7430) Acc G: 14.997% 
LR: 2.000e-04 

2023-03-02 01:52:02,978 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4028 (0.3570) Acc D Real: 64.053% 
Loss D Fake: 0.6296 (0.6492) Acc D Fake: 84.940% 
Loss D: 1.032 
Loss G: 0.7653 (0.7433) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 01:52:02,985 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4387 (0.3580) Acc D Real: 63.922% 
Loss D Fake: 0.6296 (0.6489) Acc D Fake: 84.961% 
Loss D: 1.068 
Loss G: 0.7649 (0.7435) Acc G: 14.956% 
LR: 2.000e-04 

2023-03-02 01:52:02,994 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3218 (0.3575) Acc D Real: 63.955% 
Loss D Fake: 0.6301 (0.6487) Acc D Fake: 84.982% 
Loss D: 0.952 
Loss G: 0.7645 (0.7438) Acc G: 14.936% 
LR: 2.000e-04 

2023-03-02 01:52:03,001 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3762 (0.3578) Acc D Real: 63.924% 
Loss D Fake: 0.6303 (0.6485) Acc D Fake: 85.003% 
Loss D: 1.006 
Loss G: 0.7642 (0.7440) Acc G: 14.917% 
LR: 2.000e-04 

2023-03-02 01:52:03,008 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4628 (0.3590) Acc D Real: 63.759% 
Loss D Fake: 0.6308 (0.6483) Acc D Fake: 85.022% 
Loss D: 1.094 
Loss G: 0.7631 (0.7443) Acc G: 14.898% 
LR: 2.000e-04 

2023-03-02 01:52:03,016 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3288 (0.3587) Acc D Real: 63.791% 
Loss D Fake: 0.6319 (0.6481) Acc D Fake: 85.042% 
Loss D: 0.961 
Loss G: 0.7621 (0.7445) Acc G: 14.880% 
LR: 2.000e-04 

2023-03-02 01:52:03,026 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4342 (0.3595) Acc D Real: 63.662% 
Loss D Fake: 0.6328 (0.6479) Acc D Fake: 85.061% 
Loss D: 1.067 
Loss G: 0.7607 (0.7447) Acc G: 14.862% 
LR: 2.000e-04 

2023-03-02 01:52:03,034 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3712 (0.3597) Acc D Real: 63.627% 
Loss D Fake: 0.6341 (0.6478) Acc D Fake: 85.079% 
Loss D: 1.005 
Loss G: 0.7592 (0.7448) Acc G: 14.844% 
LR: 2.000e-04 

2023-03-02 01:52:03,042 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4158 (0.3603) Acc D Real: 63.540% 
Loss D Fake: 0.6354 (0.6476) Acc D Fake: 85.097% 
Loss D: 1.051 
Loss G: 0.7576 (0.7450) Acc G: 14.827% 
LR: 2.000e-04 

2023-03-02 01:52:03,050 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4580 (0.3614) Acc D Real: 63.394% 
Loss D Fake: 0.6370 (0.6475) Acc D Fake: 85.115% 
Loss D: 1.095 
Loss G: 0.7553 (0.7451) Acc G: 14.810% 
LR: 2.000e-04 

2023-03-02 01:52:03,059 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4761 (0.3627) Acc D Real: 63.226% 
Loss D Fake: 0.6393 (0.6474) Acc D Fake: 85.132% 
Loss D: 1.115 
Loss G: 0.7523 (0.7452) Acc G: 14.812% 
LR: 2.000e-04 

2023-03-02 01:52:03,067 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3462 (0.3625) Acc D Real: 63.227% 
Loss D Fake: 0.6419 (0.6473) Acc D Fake: 85.130% 
Loss D: 0.988 
Loss G: 0.7497 (0.7452) Acc G: 14.815% 
LR: 2.000e-04 

2023-03-02 01:52:03,075 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3628 (0.3625) Acc D Real: 63.214% 
Loss D Fake: 0.6440 (0.6473) Acc D Fake: 85.129% 
Loss D: 1.007 
Loss G: 0.7475 (0.7452) Acc G: 14.817% 
LR: 2.000e-04 

2023-03-02 01:52:03,084 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3170 (0.3620) Acc D Real: 63.254% 
Loss D Fake: 0.6456 (0.6473) Acc D Fake: 85.128% 
Loss D: 0.963 
Loss G: 0.7461 (0.7452) Acc G: 14.819% 
LR: 2.000e-04 

2023-03-02 01:52:03,092 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4073 (0.3625) Acc D Real: 63.197% 
Loss D Fake: 0.6467 (0.6473) Acc D Fake: 85.126% 
Loss D: 1.054 
Loss G: 0.7447 (0.7452) Acc G: 14.820% 
LR: 2.000e-04 

2023-03-02 01:52:03,101 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2989 (0.3618) Acc D Real: 63.264% 
Loss D Fake: 0.6477 (0.6473) Acc D Fake: 85.125% 
Loss D: 0.947 
Loss G: 0.7441 (0.7452) Acc G: 14.822% 
LR: 2.000e-04 

2023-03-02 01:52:03,108 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3432 (0.3616) Acc D Real: 63.283% 
Loss D Fake: 0.6480 (0.6473) Acc D Fake: 85.124% 
Loss D: 0.991 
Loss G: 0.7439 (0.7452) Acc G: 14.824% 
LR: 2.000e-04 

2023-03-02 01:52:03,117 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4231 (0.3623) Acc D Real: 63.175% 
Loss D Fake: 0.6482 (0.6473) Acc D Fake: 85.122% 
Loss D: 1.071 
Loss G: 0.7433 (0.7452) Acc G: 14.826% 
LR: 2.000e-04 

2023-03-02 01:52:03,125 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3396 (0.3620) Acc D Real: 63.189% 
Loss D Fake: 0.6489 (0.6473) Acc D Fake: 85.121% 
Loss D: 0.989 
Loss G: 0.7427 (0.7452) Acc G: 14.828% 
LR: 2.000e-04 

2023-03-02 01:52:03,133 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2268 (0.3607) Acc D Real: 63.351% 
Loss D Fake: 0.6489 (0.6473) Acc D Fake: 85.120% 
Loss D: 0.876 
Loss G: 0.7435 (0.7452) Acc G: 14.830% 
LR: 2.000e-04 

2023-03-02 01:52:03,140 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2348 (0.3594) Acc D Real: 63.501% 
Loss D Fake: 0.6475 (0.6473) Acc D Fake: 85.119% 
Loss D: 0.882 
Loss G: 0.7457 (0.7452) Acc G: 14.831% 
LR: 2.000e-04 

2023-03-02 01:52:03,150 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4248 (0.3600) Acc D Real: 63.412% 
Loss D Fake: 0.6456 (0.6473) Acc D Fake: 85.118% 
Loss D: 1.070 
Loss G: 0.7473 (0.7452) Acc G: 14.833% 
LR: 2.000e-04 

2023-03-02 01:52:03,159 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4551 (0.3610) Acc D Real: 63.275% 
Loss D Fake: 0.6448 (0.6473) Acc D Fake: 85.116% 
Loss D: 1.100 
Loss G: 0.7476 (0.7452) Acc G: 14.835% 
LR: 2.000e-04 

2023-03-02 01:52:03,166 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3269 (0.3607) Acc D Real: 63.315% 
Loss D Fake: 0.6447 (0.6473) Acc D Fake: 85.115% 
Loss D: 0.972 
Loss G: 0.7479 (0.7452) Acc G: 14.836% 
LR: 2.000e-04 

2023-03-02 01:52:03,174 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3853 (0.3609) Acc D Real: 63.286% 
Loss D Fake: 0.6444 (0.6472) Acc D Fake: 85.114% 
Loss D: 1.030 
Loss G: 0.7481 (0.7453) Acc G: 14.838% 
LR: 2.000e-04 

2023-03-02 01:52:03,182 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4358 (0.3616) Acc D Real: 63.182% 
Loss D Fake: 0.6445 (0.6472) Acc D Fake: 85.113% 
Loss D: 1.080 
Loss G: 0.7476 (0.7453) Acc G: 14.839% 
LR: 2.000e-04 

2023-03-02 01:52:03,190 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3900 (0.3619) Acc D Real: 63.141% 
Loss D Fake: 0.6451 (0.6472) Acc D Fake: 85.112% 
Loss D: 1.035 
Loss G: 0.7469 (0.7453) Acc G: 14.841% 
LR: 2.000e-04 

2023-03-02 01:52:03,199 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3448 (0.3617) Acc D Real: 63.151% 
Loss D Fake: 0.6456 (0.6472) Acc D Fake: 85.111% 
Loss D: 0.990 
Loss G: 0.7464 (0.7453) Acc G: 14.842% 
LR: 2.000e-04 

2023-03-02 01:52:03,207 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4451 (0.3625) Acc D Real: 63.048% 
Loss D Fake: 0.6462 (0.6472) Acc D Fake: 85.110% 
Loss D: 1.091 
Loss G: 0.7454 (0.7453) Acc G: 14.844% 
LR: 2.000e-04 

2023-03-02 01:52:03,217 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3427 (0.3623) Acc D Real: 63.064% 
Loss D Fake: 0.6471 (0.6472) Acc D Fake: 85.109% 
Loss D: 0.990 
Loss G: 0.7445 (0.7453) Acc G: 14.845% 
LR: 2.000e-04 

2023-03-02 01:52:03,226 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3785 (0.3624) Acc D Real: 63.035% 
Loss D Fake: 0.6478 (0.6472) Acc D Fake: 85.108% 
Loss D: 1.026 
Loss G: 0.7438 (0.7453) Acc G: 14.847% 
LR: 2.000e-04 

2023-03-02 01:52:03,235 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3685 (0.3625) Acc D Real: 63.029% 
Loss D Fake: 0.6484 (0.6472) Acc D Fake: 85.107% 
Loss D: 1.017 
Loss G: 0.7432 (0.7453) Acc G: 14.848% 
LR: 2.000e-04 

2023-03-02 01:52:03,243 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3453 (0.3623) Acc D Real: 63.046% 
Loss D Fake: 0.6487 (0.6472) Acc D Fake: 85.106% 
Loss D: 0.994 
Loss G: 0.7431 (0.7452) Acc G: 14.849% 
LR: 2.000e-04 

2023-03-02 01:52:03,250 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3448 (0.3622) Acc D Real: 63.063% 
Loss D Fake: 0.6487 (0.6472) Acc D Fake: 85.105% 
Loss D: 0.994 
Loss G: 0.7432 (0.7452) Acc G: 14.851% 
LR: 2.000e-04 

2023-03-02 01:52:03,258 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4239 (0.3627) Acc D Real: 62.981% 
Loss D Fake: 0.6488 (0.6472) Acc D Fake: 85.104% 
Loss D: 1.073 
Loss G: 0.7427 (0.7452) Acc G: 14.852% 
LR: 2.000e-04 

2023-03-02 01:52:03,266 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3286 (0.3624) Acc D Real: 63.007% 
Loss D Fake: 0.6492 (0.6472) Acc D Fake: 85.103% 
Loss D: 0.978 
Loss G: 0.7426 (0.7452) Acc G: 14.853% 
LR: 2.000e-04 

2023-03-02 01:52:03,273 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4498 (0.3632) Acc D Real: 62.912% 
Loss D Fake: 0.6494 (0.6473) Acc D Fake: 85.102% 
Loss D: 1.099 
Loss G: 0.7418 (0.7452) Acc G: 14.855% 
LR: 2.000e-04 

2023-03-02 01:52:03,284 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.5587 (0.3649) Acc D Real: 62.692% 
Loss D Fake: 0.6508 (0.6473) Acc D Fake: 85.101% 
Loss D: 1.209 
Loss G: 0.7393 (0.7451) Acc G: 14.856% 
LR: 2.000e-04 

2023-03-02 01:52:03,293 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3665 (0.3649) Acc D Real: 62.691% 
Loss D Fake: 0.6532 (0.6473) Acc D Fake: 85.101% 
Loss D: 1.020 
Loss G: 0.7369 (0.7450) Acc G: 14.857% 
LR: 2.000e-04 

2023-03-02 01:52:03,300 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3382 (0.3647) Acc D Real: 62.706% 
Loss D Fake: 0.6550 (0.6474) Acc D Fake: 85.100% 
Loss D: 0.993 
Loss G: 0.7353 (0.7450) Acc G: 14.858% 
LR: 2.000e-04 

2023-03-02 01:52:03,307 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3729 (0.3647) Acc D Real: 62.687% 
Loss D Fake: 0.6564 (0.6475) Acc D Fake: 85.099% 
Loss D: 1.029 
Loss G: 0.7338 (0.7449) Acc G: 14.859% 
LR: 2.000e-04 

2023-03-02 01:52:03,315 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2975 (0.3642) Acc D Real: 62.747% 
Loss D Fake: 0.6574 (0.6476) Acc D Fake: 85.098% 
Loss D: 0.955 
Loss G: 0.7332 (0.7448) Acc G: 14.861% 
LR: 2.000e-04 

2023-03-02 01:52:03,323 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3137 (0.3638) Acc D Real: 62.789% 
Loss D Fake: 0.6577 (0.6476) Acc D Fake: 85.097% 
Loss D: 0.971 
Loss G: 0.7332 (0.7447) Acc G: 14.862% 
LR: 2.000e-04 

2023-03-02 01:52:03,331 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3983 (0.3640) Acc D Real: 62.738% 
Loss D Fake: 0.6576 (0.6477) Acc D Fake: 85.097% 
Loss D: 1.056 
Loss G: 0.7330 (0.7446) Acc G: 14.863% 
LR: 2.000e-04 

2023-03-02 01:52:03,338 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3320 (0.3638) Acc D Real: 62.757% 
Loss D Fake: 0.6577 (0.6478) Acc D Fake: 85.096% 
Loss D: 0.990 
Loss G: 0.7331 (0.7445) Acc G: 14.864% 
LR: 2.000e-04 

2023-03-02 01:52:03,345 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4001 (0.3641) Acc D Real: 62.713% 
Loss D Fake: 0.6577 (0.6479) Acc D Fake: 85.095% 
Loss D: 1.058 
Loss G: 0.7331 (0.7444) Acc G: 14.865% 
LR: 2.000e-04 

2023-03-02 01:52:03,353 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3171 (0.3637) Acc D Real: 62.746% 
Loss D Fake: 0.6577 (0.6480) Acc D Fake: 85.094% 
Loss D: 0.975 
Loss G: 0.7333 (0.7443) Acc G: 14.866% 
LR: 2.000e-04 

2023-03-02 01:52:03,360 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3464 (0.3636) Acc D Real: 62.758% 
Loss D Fake: 0.6572 (0.6480) Acc D Fake: 85.094% 
Loss D: 1.004 
Loss G: 0.7339 (0.7442) Acc G: 14.867% 
LR: 2.000e-04 

2023-03-02 01:52:03,368 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4347 (0.3641) Acc D Real: 62.669% 
Loss D Fake: 0.6570 (0.6481) Acc D Fake: 85.093% 
Loss D: 1.092 
Loss G: 0.7336 (0.7441) Acc G: 14.868% 
LR: 2.000e-04 

2023-03-02 01:52:03,375 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2583 (0.3633) Acc D Real: 62.766% 
Loss D Fake: 0.6571 (0.6482) Acc D Fake: 85.092% 
Loss D: 0.915 
Loss G: 0.7342 (0.7441) Acc G: 14.869% 
LR: 2.000e-04 

2023-03-02 01:52:03,383 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4142 (0.3637) Acc D Real: 62.705% 
Loss D Fake: 0.6565 (0.6482) Acc D Fake: 85.091% 
Loss D: 1.071 
Loss G: 0.7345 (0.7440) Acc G: 14.870% 
LR: 2.000e-04 

2023-03-02 01:52:03,390 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2939 (0.3632) Acc D Real: 62.758% 
Loss D Fake: 0.6562 (0.6483) Acc D Fake: 85.091% 
Loss D: 0.950 
Loss G: 0.7351 (0.7439) Acc G: 14.871% 
LR: 2.000e-04 

2023-03-02 01:52:03,398 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3257 (0.3629) Acc D Real: 62.784% 
Loss D Fake: 0.6554 (0.6484) Acc D Fake: 85.090% 
Loss D: 0.981 
Loss G: 0.7362 (0.7439) Acc G: 14.872% 
LR: 2.000e-04 

2023-03-02 01:52:03,406 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3159 (0.3625) Acc D Real: 62.819% 
Loss D Fake: 0.6543 (0.6484) Acc D Fake: 85.089% 
Loss D: 0.970 
Loss G: 0.7376 (0.7438) Acc G: 14.873% 
LR: 2.000e-04 

2023-03-02 01:52:03,413 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4242 (0.3630) Acc D Real: 62.747% 
Loss D Fake: 0.6532 (0.6484) Acc D Fake: 85.089% 
Loss D: 1.077 
Loss G: 0.7384 (0.7438) Acc G: 14.874% 
LR: 2.000e-04 

2023-03-02 01:52:03,421 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3855 (0.3631) Acc D Real: 62.724% 
Loss D Fake: 0.6528 (0.6485) Acc D Fake: 85.088% 
Loss D: 1.038 
Loss G: 0.7387 (0.7437) Acc G: 14.875% 
LR: 2.000e-04 

2023-03-02 01:52:03,428 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3428 (0.3630) Acc D Real: 62.738% 
Loss D Fake: 0.6525 (0.6485) Acc D Fake: 85.087% 
Loss D: 0.995 
Loss G: 0.7391 (0.7437) Acc G: 14.876% 
LR: 2.000e-04 

2023-03-02 01:52:03,436 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2692 (0.3623) Acc D Real: 62.820% 
Loss D Fake: 0.6519 (0.6485) Acc D Fake: 85.087% 
Loss D: 0.921 
Loss G: 0.7403 (0.7437) Acc G: 14.877% 
LR: 2.000e-04 

2023-03-02 01:52:03,444 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2709 (0.3616) Acc D Real: 62.891% 
Loss D Fake: 0.6504 (0.6485) Acc D Fake: 85.086% 
Loss D: 0.921 
Loss G: 0.7423 (0.7437) Acc G: 14.878% 
LR: 2.000e-04 

2023-03-02 01:52:03,451 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3838 (0.3618) Acc D Real: 62.860% 
Loss D Fake: 0.6486 (0.6485) Acc D Fake: 85.085% 
Loss D: 1.032 
Loss G: 0.7441 (0.7437) Acc G: 14.879% 
LR: 2.000e-04 

2023-03-02 01:52:03,459 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3471 (0.3617) Acc D Real: 62.867% 
Loss D Fake: 0.6472 (0.6485) Acc D Fake: 85.085% 
Loss D: 0.994 
Loss G: 0.7455 (0.7437) Acc G: 14.879% 
LR: 2.000e-04 

2023-03-02 01:52:03,466 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3605 (0.3617) Acc D Real: 62.856% 
Loss D Fake: 0.6461 (0.6485) Acc D Fake: 85.084% 
Loss D: 1.007 
Loss G: 0.7466 (0.7437) Acc G: 14.880% 
LR: 2.000e-04 

2023-03-02 01:52:03,473 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3642 (0.3617) Acc D Real: 62.844% 
Loss D Fake: 0.6453 (0.6485) Acc D Fake: 85.084% 
Loss D: 1.009 
Loss G: 0.7474 (0.7437) Acc G: 14.881% 
LR: 2.000e-04 

2023-03-02 01:52:03,480 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3661 (0.3617) Acc D Real: 62.837% 
Loss D Fake: 0.6447 (0.6485) Acc D Fake: 85.083% 
Loss D: 1.011 
Loss G: 0.7480 (0.7438) Acc G: 14.882% 
LR: 2.000e-04 

2023-03-02 01:52:03,488 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3453 (0.3616) Acc D Real: 62.842% 
Loss D Fake: 0.6441 (0.6484) Acc D Fake: 85.082% 
Loss D: 0.989 
Loss G: 0.7488 (0.7438) Acc G: 14.883% 
LR: 2.000e-04 

2023-03-02 01:52:03,495 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2972 (0.3612) Acc D Real: 62.892% 
Loss D Fake: 0.6433 (0.6484) Acc D Fake: 85.082% 
Loss D: 0.941 
Loss G: 0.7500 (0.7438) Acc G: 14.884% 
LR: 2.000e-04 

2023-03-02 01:52:03,503 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2816 (0.3606) Acc D Real: 62.958% 
Loss D Fake: 0.6420 (0.6484) Acc D Fake: 85.081% 
Loss D: 0.924 
Loss G: 0.7518 (0.7439) Acc G: 14.884% 
LR: 2.000e-04 

2023-03-02 01:52:03,510 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4506 (0.3612) Acc D Real: 62.872% 
Loss D Fake: 0.6406 (0.6483) Acc D Fake: 85.081% 
Loss D: 1.091 
Loss G: 0.7527 (0.7440) Acc G: 14.885% 
LR: 2.000e-04 

2023-03-02 01:52:03,517 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3712 (0.3613) Acc D Real: 62.858% 
Loss D Fake: 0.6402 (0.6482) Acc D Fake: 85.080% 
Loss D: 1.011 
Loss G: 0.7530 (0.7440) Acc G: 14.886% 
LR: 2.000e-04 

2023-03-02 01:52:03,525 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3891 (0.3615) Acc D Real: 62.829% 
Loss D Fake: 0.6400 (0.6482) Acc D Fake: 85.080% 
Loss D: 1.029 
Loss G: 0.7531 (0.7441) Acc G: 14.887% 
LR: 2.000e-04 

2023-03-02 01:52:03,532 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3746 (0.3616) Acc D Real: 62.816% 
Loss D Fake: 0.6400 (0.6481) Acc D Fake: 85.079% 
Loss D: 1.015 
Loss G: 0.7531 (0.7441) Acc G: 14.888% 
LR: 2.000e-04 

2023-03-02 01:52:03,539 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2826 (0.3611) Acc D Real: 62.879% 
Loss D Fake: 0.6399 (0.6481) Acc D Fake: 85.079% 
Loss D: 0.922 
Loss G: 0.7536 (0.7442) Acc G: 14.888% 
LR: 2.000e-04 

2023-03-02 01:52:03,547 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3217 (0.3608) Acc D Real: 62.909% 
Loss D Fake: 0.6391 (0.6480) Acc D Fake: 85.078% 
Loss D: 0.961 
Loss G: 0.7546 (0.7443) Acc G: 14.889% 
LR: 2.000e-04 

2023-03-02 01:52:03,554 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4557 (0.3614) Acc D Real: 62.830% 
Loss D Fake: 0.6385 (0.6480) Acc D Fake: 85.078% 
Loss D: 1.094 
Loss G: 0.7548 (0.7443) Acc G: 14.890% 
LR: 2.000e-04 

2023-03-02 01:52:03,561 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3631 (0.3614) Acc D Real: 62.820% 
Loss D Fake: 0.6385 (0.6479) Acc D Fake: 85.077% 
Loss D: 1.002 
Loss G: 0.7547 (0.7444) Acc G: 14.890% 
LR: 2.000e-04 

2023-03-02 01:52:03,568 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4050 (0.3617) Acc D Real: 62.774% 
Loss D Fake: 0.6388 (0.6478) Acc D Fake: 85.077% 
Loss D: 1.044 
Loss G: 0.7542 (0.7445) Acc G: 14.891% 
LR: 2.000e-04 

2023-03-02 01:52:03,576 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4276 (0.3621) Acc D Real: 62.711% 
Loss D Fake: 0.6395 (0.6478) Acc D Fake: 85.076% 
Loss D: 1.067 
Loss G: 0.7530 (0.7445) Acc G: 14.892% 
LR: 2.000e-04 

2023-03-02 01:52:03,583 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2757 (0.3616) Acc D Real: 62.771% 
Loss D Fake: 0.6404 (0.6477) Acc D Fake: 85.076% 
Loss D: 0.916 
Loss G: 0.7526 (0.7446) Acc G: 14.893% 
LR: 2.000e-04 

2023-03-02 01:52:03,590 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2812 (0.3611) Acc D Real: 62.775% 
Loss D Fake: 0.6403 (0.6477) Acc D Fake: 85.076% 
Loss D: 0.922 
Loss G: 0.7530 (0.7446) Acc G: 14.893% 
LR: 2.000e-04 

2023-03-02 01:52:03,835 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.339 | Generator Loss: 0.753 | Avg: 2.092 
2023-03-02 01:52:03,857 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.222 | Generator Loss: 0.753 | Avg: 1.975 
2023-03-02 01:52:03,880 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.255 | Generator Loss: 0.753 | Avg: 2.008 
2023-03-02 01:52:03,907 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.281 | Generator Loss: 0.753 | Avg: 2.034 
2023-03-02 01:52:03,932 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.282 | Generator Loss: 0.753 | Avg: 2.035 
2023-03-02 01:52:03,958 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.232 | Generator Loss: 0.753 | Avg: 1.985 
2023-03-02 01:52:03,984 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.198 | Generator Loss: 0.753 | Avg: 1.951 
2023-03-02 01:52:04,010 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.150 | Generator Loss: 0.753 | Avg: 1.903 
2023-03-02 01:52:04,036 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.120 | Generator Loss: 0.753 | Avg: 1.872 
2023-03-02 01:52:04,064 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.077 | Generator Loss: 0.753 | Avg: 1.830 
2023-03-02 01:52:04,090 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.043 | Generator Loss: 0.753 | Avg: 1.796 
2023-03-02 01:52:04,116 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.013 | Generator Loss: 0.753 | Avg: 1.766 
2023-03-02 01:52:04,146 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.989 | Generator Loss: 0.753 | Avg: 1.742 
2023-03-02 01:52:04,171 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.969 | Generator Loss: 0.753 | Avg: 1.722 
2023-03-02 01:52:04,196 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.985 | Generator Loss: 0.753 | Avg: 1.738 
2023-03-02 01:52:04,221 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.001 | Generator Loss: 0.753 | Avg: 1.754 
2023-03-02 01:52:04,247 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.017 | Generator Loss: 0.753 | Avg: 1.770 
2023-03-02 01:52:04,279 -                train: [    INFO] - 
Epoch: 14/20
2023-03-02 01:52:04,468 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3609 (0.3702) Acc D Real: 60.833% 
Loss D Fake: 0.6397 (0.6398) Acc D Fake: 85.000% 
Loss D: 1.001 
Loss G: 0.7535 (0.7534) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,476 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3156 (0.3520) Acc D Real: 62.778% 
Loss D Fake: 0.6394 (0.6397) Acc D Fake: 85.000% 
Loss D: 0.955 
Loss G: 0.7540 (0.7536) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,484 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2564 (0.3281) Acc D Real: 65.651% 
Loss D Fake: 0.6388 (0.6395) Acc D Fake: 85.000% 
Loss D: 0.895 
Loss G: 0.7552 (0.7540) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,501 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3878 (0.3400) Acc D Real: 64.365% 
Loss D Fake: 0.6376 (0.6391) Acc D Fake: 85.000% 
Loss D: 1.025 
Loss G: 0.7563 (0.7545) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,508 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3825 (0.3471) Acc D Real: 63.377% 
Loss D Fake: 0.6370 (0.6387) Acc D Fake: 85.000% 
Loss D: 1.019 
Loss G: 0.7568 (0.7549) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,515 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2976 (0.3400) Acc D Real: 64.286% 
Loss D Fake: 0.6365 (0.6384) Acc D Fake: 85.000% 
Loss D: 0.934 
Loss G: 0.7576 (0.7553) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,522 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4533 (0.3542) Acc D Real: 62.546% 
Loss D Fake: 0.6360 (0.6381) Acc D Fake: 85.000% 
Loss D: 1.089 
Loss G: 0.7577 (0.7556) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,529 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.2071 (0.3378) Acc D Real: 64.554% 
Loss D Fake: 0.6358 (0.6379) Acc D Fake: 85.000% 
Loss D: 0.843 
Loss G: 0.7587 (0.7559) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,536 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3376 (0.3378) Acc D Real: 64.427% 
Loss D Fake: 0.6347 (0.6375) Acc D Fake: 85.000% 
Loss D: 0.972 
Loss G: 0.7599 (0.7563) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,543 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3386 (0.3379) Acc D Real: 64.470% 
Loss D Fake: 0.6336 (0.6372) Acc D Fake: 85.000% 
Loss D: 0.972 
Loss G: 0.7612 (0.7568) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,551 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4165 (0.3444) Acc D Real: 63.728% 
Loss D Fake: 0.6327 (0.6368) Acc D Fake: 85.000% 
Loss D: 1.049 
Loss G: 0.7617 (0.7572) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,558 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3474 (0.3447) Acc D Real: 63.722% 
Loss D Fake: 0.6325 (0.6365) Acc D Fake: 85.000% 
Loss D: 0.980 
Loss G: 0.7620 (0.7575) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,565 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3012 (0.3416) Acc D Real: 64.126% 
Loss D Fake: 0.6321 (0.6362) Acc D Fake: 85.000% 
Loss D: 0.933 
Loss G: 0.7627 (0.7579) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,572 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4768 (0.3506) Acc D Real: 63.045% 
Loss D Fake: 0.6318 (0.6359) Acc D Fake: 85.000% 
Loss D: 1.109 
Loss G: 0.7623 (0.7582) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,579 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3687 (0.3517) Acc D Real: 62.923% 
Loss D Fake: 0.6324 (0.6357) Acc D Fake: 85.000% 
Loss D: 1.001 
Loss G: 0.7616 (0.7584) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,587 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3662 (0.3526) Acc D Real: 62.809% 
Loss D Fake: 0.6331 (0.6355) Acc D Fake: 85.000% 
Loss D: 0.999 
Loss G: 0.7608 (0.7585) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,594 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.2835 (0.3487) Acc D Real: 63.319% 
Loss D Fake: 0.6335 (0.6354) Acc D Fake: 85.000% 
Loss D: 0.917 
Loss G: 0.7608 (0.7587) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,601 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2270 (0.3423) Acc D Real: 64.093% 
Loss D Fake: 0.6330 (0.6353) Acc D Fake: 85.000% 
Loss D: 0.860 
Loss G: 0.7620 (0.7588) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,608 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4083 (0.3456) Acc D Real: 63.669% 
Loss D Fake: 0.6319 (0.6351) Acc D Fake: 85.000% 
Loss D: 1.040 
Loss G: 0.7628 (0.7590) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,614 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3442 (0.3456) Acc D Real: 63.700% 
Loss D Fake: 0.6315 (0.6349) Acc D Fake: 85.000% 
Loss D: 0.976 
Loss G: 0.7633 (0.7592) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,621 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3267 (0.3447) Acc D Real: 63.866% 
Loss D Fake: 0.6310 (0.6347) Acc D Fake: 85.000% 
Loss D: 0.958 
Loss G: 0.7641 (0.7595) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,628 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3194 (0.3436) Acc D Real: 64.008% 
Loss D Fake: 0.6302 (0.6346) Acc D Fake: 85.000% 
Loss D: 0.950 
Loss G: 0.7651 (0.7597) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,635 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.2343 (0.3390) Acc D Real: 64.575% 
Loss D Fake: 0.6291 (0.6343) Acc D Fake: 85.000% 
Loss D: 0.863 
Loss G: 0.7670 (0.7600) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,642 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3502 (0.3395) Acc D Real: 64.538% 
Loss D Fake: 0.6273 (0.6340) Acc D Fake: 85.000% 
Loss D: 0.978 
Loss G: 0.7689 (0.7604) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,649 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3673 (0.3406) Acc D Real: 64.391% 
Loss D Fake: 0.6259 (0.6337) Acc D Fake: 85.000% 
Loss D: 0.993 
Loss G: 0.7703 (0.7608) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,656 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.2831 (0.3384) Acc D Real: 64.668% 
Loss D Fake: 0.6247 (0.6334) Acc D Fake: 85.000% 
Loss D: 0.908 
Loss G: 0.7722 (0.7612) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,663 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4141 (0.3411) Acc D Real: 64.345% 
Loss D Fake: 0.6233 (0.6330) Acc D Fake: 85.000% 
Loss D: 1.037 
Loss G: 0.7733 (0.7616) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,671 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3892 (0.3428) Acc D Real: 64.161% 
Loss D Fake: 0.6226 (0.6327) Acc D Fake: 85.000% 
Loss D: 1.012 
Loss G: 0.7738 (0.7620) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,679 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3249 (0.3422) Acc D Real: 64.224% 
Loss D Fake: 0.6224 (0.6323) Acc D Fake: 85.000% 
Loss D: 0.947 
Loss G: 0.7742 (0.7624) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,687 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3957 (0.3439) Acc D Real: 64.017% 
Loss D Fake: 0.6221 (0.6320) Acc D Fake: 85.000% 
Loss D: 1.018 
Loss G: 0.7742 (0.7628) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,694 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3921 (0.3454) Acc D Real: 63.813% 
Loss D Fake: 0.6223 (0.6317) Acc D Fake: 85.000% 
Loss D: 1.014 
Loss G: 0.7738 (0.7632) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,702 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3055 (0.3442) Acc D Real: 63.971% 
Loss D Fake: 0.6226 (0.6314) Acc D Fake: 85.000% 
Loss D: 0.928 
Loss G: 0.7737 (0.7635) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,709 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3148 (0.3433) Acc D Real: 64.107% 
Loss D Fake: 0.6225 (0.6312) Acc D Fake: 85.000% 
Loss D: 0.937 
Loss G: 0.7741 (0.7638) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,716 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2342 (0.3402) Acc D Real: 64.497% 
Loss D Fake: 0.6217 (0.6309) Acc D Fake: 85.000% 
Loss D: 0.856 
Loss G: 0.7756 (0.7641) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,724 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3979 (0.3418) Acc D Real: 64.372% 
Loss D Fake: 0.6204 (0.6306) Acc D Fake: 85.000% 
Loss D: 1.018 
Loss G: 0.7768 (0.7645) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,731 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4137 (0.3438) Acc D Real: 64.131% 
Loss D Fake: 0.6198 (0.6303) Acc D Fake: 85.000% 
Loss D: 1.033 
Loss G: 0.7771 (0.7648) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,738 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3220 (0.3432) Acc D Real: 64.221% 
Loss D Fake: 0.6197 (0.6300) Acc D Fake: 85.000% 
Loss D: 0.942 
Loss G: 0.7774 (0.7652) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,746 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4420 (0.3457) Acc D Real: 63.904% 
Loss D Fake: 0.6196 (0.6298) Acc D Fake: 85.000% 
Loss D: 1.062 
Loss G: 0.7769 (0.7655) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,753 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5087 (0.3498) Acc D Real: 63.439% 
Loss D Fake: 0.6207 (0.6295) Acc D Fake: 85.000% 
Loss D: 1.129 
Loss G: 0.7748 (0.7657) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,760 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3085 (0.3488) Acc D Real: 63.580% 
Loss D Fake: 0.6225 (0.6294) Acc D Fake: 85.000% 
Loss D: 0.931 
Loss G: 0.7730 (0.7659) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,768 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3930 (0.3499) Acc D Real: 63.439% 
Loss D Fake: 0.6238 (0.6292) Acc D Fake: 85.000% 
Loss D: 1.017 
Loss G: 0.7713 (0.7660) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,775 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3691 (0.3503) Acc D Real: 63.391% 
Loss D Fake: 0.6253 (0.6291) Acc D Fake: 85.000% 
Loss D: 0.994 
Loss G: 0.7696 (0.7661) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,783 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2809 (0.3487) Acc D Real: 63.584% 
Loss D Fake: 0.6265 (0.6291) Acc D Fake: 85.000% 
Loss D: 0.907 
Loss G: 0.7687 (0.7661) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,790 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4169 (0.3502) Acc D Real: 63.411% 
Loss D Fake: 0.6271 (0.6290) Acc D Fake: 85.000% 
Loss D: 1.044 
Loss G: 0.7677 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,798 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3498 (0.3502) Acc D Real: 63.423% 
Loss D Fake: 0.6281 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.978 
Loss G: 0.7667 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,806 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3447 (0.3501) Acc D Real: 63.431% 
Loss D Fake: 0.6288 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.973 
Loss G: 0.7660 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,814 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2749 (0.3485) Acc D Real: 63.630% 
Loss D Fake: 0.6291 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.904 
Loss G: 0.7661 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 01:52:04,822 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3504 (0.3486) Acc D Real: 63.658% 
Loss D Fake: 0.6288 (0.6290) Acc D Fake: 84.971% 
Loss D: 0.979 
Loss G: 0.7665 (0.7662) Acc G: 15.033% 
LR: 2.000e-04 

2023-03-02 01:52:04,829 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3051 (0.3477) Acc D Real: 63.780% 
Loss D Fake: 0.6283 (0.6290) Acc D Fake: 84.939% 
Loss D: 0.933 
Loss G: 0.7674 (0.7662) Acc G: 15.066% 
LR: 2.000e-04 

2023-03-02 01:52:04,836 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3279 (0.3473) Acc D Real: 63.841% 
Loss D Fake: 0.6274 (0.6290) Acc D Fake: 84.907% 
Loss D: 0.955 
Loss G: 0.7686 (0.7663) Acc G: 15.097% 
LR: 2.000e-04 

2023-03-02 01:52:04,843 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4334 (0.3490) Acc D Real: 63.632% 
Loss D Fake: 0.6266 (0.6289) Acc D Fake: 84.877% 
Loss D: 1.060 
Loss G: 0.7689 (0.7663) Acc G: 15.127% 
LR: 2.000e-04 

2023-03-02 01:52:04,851 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3259 (0.3485) Acc D Real: 63.683% 
Loss D Fake: 0.6266 (0.6289) Acc D Fake: 84.848% 
Loss D: 0.952 
Loss G: 0.7691 (0.7664) Acc G: 15.156% 
LR: 2.000e-04 

2023-03-02 01:52:04,858 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4352 (0.3502) Acc D Real: 63.487% 
Loss D Fake: 0.6265 (0.6288) Acc D Fake: 84.820% 
Loss D: 1.062 
Loss G: 0.7687 (0.7664) Acc G: 15.184% 
LR: 2.000e-04 

2023-03-02 01:52:04,865 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2761 (0.3488) Acc D Real: 63.658% 
Loss D Fake: 0.6269 (0.6288) Acc D Fake: 84.793% 
Loss D: 0.903 
Loss G: 0.7687 (0.7664) Acc G: 15.211% 
LR: 2.000e-04 

2023-03-02 01:52:04,873 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4332 (0.3503) Acc D Real: 63.461% 
Loss D Fake: 0.6269 (0.6288) Acc D Fake: 84.767% 
Loss D: 1.060 
Loss G: 0.7683 (0.7665) Acc G: 15.237% 
LR: 2.000e-04 

2023-03-02 01:52:04,880 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3274 (0.3499) Acc D Real: 63.527% 
Loss D Fake: 0.6273 (0.6287) Acc D Fake: 84.741% 
Loss D: 0.955 
Loss G: 0.7680 (0.7665) Acc G: 15.262% 
LR: 2.000e-04 

2023-03-02 01:52:04,887 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3273 (0.3495) Acc D Real: 63.583% 
Loss D Fake: 0.6273 (0.6287) Acc D Fake: 84.717% 
Loss D: 0.955 
Loss G: 0.7682 (0.7665) Acc G: 15.286% 
LR: 2.000e-04 

2023-03-02 01:52:04,895 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3397 (0.3494) Acc D Real: 63.589% 
Loss D Fake: 0.6271 (0.6287) Acc D Fake: 84.694% 
Loss D: 0.967 
Loss G: 0.7685 (0.7666) Acc G: 15.310% 
LR: 2.000e-04 

2023-03-02 01:52:04,902 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4343 (0.3508) Acc D Real: 63.411% 
Loss D Fake: 0.6271 (0.6287) Acc D Fake: 84.671% 
Loss D: 1.061 
Loss G: 0.7680 (0.7666) Acc G: 15.332% 
LR: 2.000e-04 

2023-03-02 01:52:04,909 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4091 (0.3517) Acc D Real: 63.311% 
Loss D Fake: 0.6278 (0.6286) Acc D Fake: 84.649% 
Loss D: 1.037 
Loss G: 0.7668 (0.7666) Acc G: 15.354% 
LR: 2.000e-04 

2023-03-02 01:52:04,918 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2302 (0.3498) Acc D Real: 63.543% 
Loss D Fake: 0.6286 (0.6286) Acc D Fake: 84.628% 
Loss D: 0.859 
Loss G: 0.7668 (0.7666) Acc G: 15.376% 
LR: 2.000e-04 

2023-03-02 01:52:04,925 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3635 (0.3500) Acc D Real: 63.514% 
Loss D Fake: 0.6283 (0.6286) Acc D Fake: 84.607% 
Loss D: 0.992 
Loss G: 0.7670 (0.7666) Acc G: 15.396% 
LR: 2.000e-04 

2023-03-02 01:52:04,933 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3895 (0.3506) Acc D Real: 63.429% 
Loss D Fake: 0.6283 (0.6286) Acc D Fake: 84.587% 
Loss D: 1.018 
Loss G: 0.7668 (0.7666) Acc G: 15.416% 
LR: 2.000e-04 

2023-03-02 01:52:04,940 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4309 (0.3518) Acc D Real: 63.276% 
Loss D Fake: 0.6288 (0.6286) Acc D Fake: 84.568% 
Loss D: 1.060 
Loss G: 0.7658 (0.7666) Acc G: 15.435% 
LR: 2.000e-04 

2023-03-02 01:52:04,948 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3204 (0.3514) Acc D Real: 63.329% 
Loss D Fake: 0.6297 (0.6286) Acc D Fake: 84.549% 
Loss D: 0.950 
Loss G: 0.7650 (0.7666) Acc G: 15.454% 
LR: 2.000e-04 

2023-03-02 01:52:04,957 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2593 (0.3500) Acc D Real: 63.497% 
Loss D Fake: 0.6300 (0.6287) Acc D Fake: 84.531% 
Loss D: 0.889 
Loss G: 0.7652 (0.7665) Acc G: 15.472% 
LR: 2.000e-04 

2023-03-02 01:52:04,964 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4304 (0.3512) Acc D Real: 63.338% 
Loss D Fake: 0.6297 (0.6287) Acc D Fake: 84.514% 
Loss D: 1.060 
Loss G: 0.7649 (0.7665) Acc G: 15.489% 
LR: 2.000e-04 

2023-03-02 01:52:04,972 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3626 (0.3513) Acc D Real: 63.305% 
Loss D Fake: 0.6302 (0.6287) Acc D Fake: 84.497% 
Loss D: 0.993 
Loss G: 0.7644 (0.7665) Acc G: 15.506% 
LR: 2.000e-04 

2023-03-02 01:52:04,979 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2670 (0.3501) Acc D Real: 63.465% 
Loss D Fake: 0.6304 (0.6287) Acc D Fake: 84.480% 
Loss D: 0.897 
Loss G: 0.7647 (0.7665) Acc G: 15.523% 
LR: 2.000e-04 

2023-03-02 01:52:04,987 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3404 (0.3500) Acc D Real: 63.477% 
Loss D Fake: 0.6299 (0.6287) Acc D Fake: 84.464% 
Loss D: 0.970 
Loss G: 0.7654 (0.7665) Acc G: 15.539% 
LR: 2.000e-04 

2023-03-02 01:52:04,994 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4122 (0.3509) Acc D Real: 63.373% 
Loss D Fake: 0.6295 (0.6288) Acc D Fake: 84.448% 
Loss D: 1.042 
Loss G: 0.7655 (0.7664) Acc G: 15.555% 
LR: 2.000e-04 

2023-03-02 01:52:05,001 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3880 (0.3514) Acc D Real: 63.318% 
Loss D Fake: 0.6296 (0.6288) Acc D Fake: 84.433% 
Loss D: 1.018 
Loss G: 0.7651 (0.7664) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 01:52:05,008 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2808 (0.3504) Acc D Real: 63.440% 
Loss D Fake: 0.6298 (0.6288) Acc D Fake: 84.418% 
Loss D: 0.911 
Loss G: 0.7654 (0.7664) Acc G: 15.585% 
LR: 2.000e-04 

2023-03-02 01:52:05,016 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3676 (0.3506) Acc D Real: 63.412% 
Loss D Fake: 0.6294 (0.6288) Acc D Fake: 84.403% 
Loss D: 0.997 
Loss G: 0.7658 (0.7664) Acc G: 15.599% 
LR: 2.000e-04 

2023-03-02 01:52:05,024 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4320 (0.3517) Acc D Real: 63.278% 
Loss D Fake: 0.6294 (0.6288) Acc D Fake: 84.389% 
Loss D: 1.061 
Loss G: 0.7653 (0.7664) Acc G: 15.613% 
LR: 2.000e-04 

2023-03-02 01:52:05,032 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3477 (0.3517) Acc D Real: 63.282% 
Loss D Fake: 0.6300 (0.6288) Acc D Fake: 84.376% 
Loss D: 0.978 
Loss G: 0.7646 (0.7664) Acc G: 15.627% 
LR: 2.000e-04 

2023-03-02 01:52:05,040 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2937 (0.3509) Acc D Real: 63.365% 
Loss D Fake: 0.6304 (0.6288) Acc D Fake: 84.371% 
Loss D: 0.924 
Loss G: 0.7646 (0.7663) Acc G: 15.622% 
LR: 2.000e-04 

2023-03-02 01:52:05,049 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3275 (0.3506) Acc D Real: 63.408% 
Loss D Fake: 0.6302 (0.6289) Acc D Fake: 84.379% 
Loss D: 0.958 
Loss G: 0.7649 (0.7663) Acc G: 15.614% 
LR: 2.000e-04 

2023-03-02 01:52:05,056 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4051 (0.3513) Acc D Real: 63.305% 
Loss D Fake: 0.6300 (0.6289) Acc D Fake: 84.387% 
Loss D: 1.035 
Loss G: 0.7648 (0.7663) Acc G: 15.606% 
LR: 2.000e-04 

2023-03-02 01:52:05,064 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4048 (0.3520) Acc D Real: 63.216% 
Loss D Fake: 0.6304 (0.6289) Acc D Fake: 84.394% 
Loss D: 1.035 
Loss G: 0.7640 (0.7663) Acc G: 15.599% 
LR: 2.000e-04 

2023-03-02 01:52:05,071 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2742 (0.3510) Acc D Real: 63.324% 
Loss D Fake: 0.6310 (0.6289) Acc D Fake: 84.402% 
Loss D: 0.905 
Loss G: 0.7638 (0.7662) Acc G: 15.591% 
LR: 2.000e-04 

2023-03-02 01:52:05,079 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3189 (0.3506) Acc D Real: 63.379% 
Loss D Fake: 0.6309 (0.6289) Acc D Fake: 84.409% 
Loss D: 0.950 
Loss G: 0.7642 (0.7662) Acc G: 15.584% 
LR: 2.000e-04 

2023-03-02 01:52:05,086 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3632 (0.3508) Acc D Real: 63.361% 
Loss D Fake: 0.6305 (0.6290) Acc D Fake: 84.416% 
Loss D: 0.994 
Loss G: 0.7646 (0.7662) Acc G: 15.577% 
LR: 2.000e-04 

2023-03-02 01:52:05,093 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3699 (0.3510) Acc D Real: 63.338% 
Loss D Fake: 0.6302 (0.6290) Acc D Fake: 84.423% 
Loss D: 1.000 
Loss G: 0.7648 (0.7662) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 01:52:05,101 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3630 (0.3511) Acc D Real: 63.314% 
Loss D Fake: 0.6302 (0.6290) Acc D Fake: 84.430% 
Loss D: 0.993 
Loss G: 0.7647 (0.7662) Acc G: 15.564% 
LR: 2.000e-04 

2023-03-02 01:52:05,108 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2499 (0.3500) Acc D Real: 63.461% 
Loss D Fake: 0.6300 (0.6290) Acc D Fake: 84.436% 
Loss D: 0.880 
Loss G: 0.7655 (0.7662) Acc G: 15.557% 
LR: 2.000e-04 

2023-03-02 01:52:05,116 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3035 (0.3494) Acc D Real: 63.528% 
Loss D Fake: 0.6290 (0.6290) Acc D Fake: 84.442% 
Loss D: 0.932 
Loss G: 0.7669 (0.7662) Acc G: 15.551% 
LR: 2.000e-04 

2023-03-02 01:52:05,123 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2558 (0.3484) Acc D Real: 63.658% 
Loss D Fake: 0.6277 (0.6290) Acc D Fake: 84.449% 
Loss D: 0.884 
Loss G: 0.7689 (0.7662) Acc G: 15.545% 
LR: 2.000e-04 

2023-03-02 01:52:05,130 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3715 (0.3487) Acc D Real: 63.628% 
Loss D Fake: 0.6260 (0.6289) Acc D Fake: 84.455% 
Loss D: 0.997 
Loss G: 0.7707 (0.7662) Acc G: 15.539% 
LR: 2.000e-04 

2023-03-02 01:52:05,138 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3637 (0.3488) Acc D Real: 63.594% 
Loss D Fake: 0.6247 (0.6289) Acc D Fake: 84.461% 
Loss D: 0.988 
Loss G: 0.7719 (0.7663) Acc G: 15.533% 
LR: 2.000e-04 

2023-03-02 01:52:05,145 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3258 (0.3486) Acc D Real: 63.629% 
Loss D Fake: 0.6238 (0.6288) Acc D Fake: 84.467% 
Loss D: 0.950 
Loss G: 0.7730 (0.7664) Acc G: 15.527% 
LR: 2.000e-04 

2023-03-02 01:52:05,152 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4357 (0.3495) Acc D Real: 63.516% 
Loss D Fake: 0.6231 (0.6288) Acc D Fake: 84.472% 
Loss D: 1.059 
Loss G: 0.7733 (0.7665) Acc G: 15.521% 
LR: 2.000e-04 

2023-03-02 01:52:05,160 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2499 (0.3484) Acc D Real: 63.642% 
Loss D Fake: 0.6229 (0.6287) Acc D Fake: 84.478% 
Loss D: 0.873 
Loss G: 0.7741 (0.7665) Acc G: 15.516% 
LR: 2.000e-04 

2023-03-02 01:52:05,167 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2602 (0.3475) Acc D Real: 63.771% 
Loss D Fake: 0.6218 (0.6286) Acc D Fake: 84.484% 
Loss D: 0.882 
Loss G: 0.7758 (0.7666) Acc G: 15.510% 
LR: 2.000e-04 

2023-03-02 01:52:05,175 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2905 (0.3469) Acc D Real: 63.866% 
Loss D Fake: 0.6201 (0.6286) Acc D Fake: 84.489% 
Loss D: 0.911 
Loss G: 0.7782 (0.7668) Acc G: 15.505% 
LR: 2.000e-04 

2023-03-02 01:52:05,182 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4350 (0.3478) Acc D Real: 63.748% 
Loss D Fake: 0.6185 (0.6285) Acc D Fake: 84.494% 
Loss D: 1.054 
Loss G: 0.7794 (0.7669) Acc G: 15.500% 
LR: 2.000e-04 

2023-03-02 01:52:05,189 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3884 (0.3482) Acc D Real: 63.690% 
Loss D Fake: 0.6180 (0.6283) Acc D Fake: 84.499% 
Loss D: 1.006 
Loss G: 0.7797 (0.7670) Acc G: 15.495% 
LR: 2.000e-04 

2023-03-02 01:52:05,196 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4327 (0.3491) Acc D Real: 63.567% 
Loss D Fake: 0.6181 (0.6282) Acc D Fake: 84.504% 
Loss D: 1.051 
Loss G: 0.7789 (0.7671) Acc G: 15.490% 
LR: 2.000e-04 

2023-03-02 01:52:05,204 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3622 (0.3492) Acc D Real: 63.548% 
Loss D Fake: 0.6191 (0.6282) Acc D Fake: 84.509% 
Loss D: 0.981 
Loss G: 0.7777 (0.7672) Acc G: 15.485% 
LR: 2.000e-04 

2023-03-02 01:52:05,211 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4323 (0.3501) Acc D Real: 63.448% 
Loss D Fake: 0.6203 (0.6281) Acc D Fake: 84.514% 
Loss D: 1.053 
Loss G: 0.7758 (0.7673) Acc G: 15.480% 
LR: 2.000e-04 

2023-03-02 01:52:05,219 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3444 (0.3500) Acc D Real: 63.454% 
Loss D Fake: 0.6219 (0.6280) Acc D Fake: 84.519% 
Loss D: 0.966 
Loss G: 0.7741 (0.7674) Acc G: 15.475% 
LR: 2.000e-04 

2023-03-02 01:52:05,226 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3491 (0.3500) Acc D Real: 63.462% 
Loss D Fake: 0.6232 (0.6280) Acc D Fake: 84.524% 
Loss D: 0.972 
Loss G: 0.7727 (0.7674) Acc G: 15.471% 
LR: 2.000e-04 

2023-03-02 01:52:05,234 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2764 (0.3493) Acc D Real: 63.542% 
Loss D Fake: 0.6240 (0.6279) Acc D Fake: 84.528% 
Loss D: 0.900 
Loss G: 0.7722 (0.7675) Acc G: 15.466% 
LR: 2.000e-04 

2023-03-02 01:52:05,241 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2843 (0.3487) Acc D Real: 63.629% 
Loss D Fake: 0.6240 (0.6279) Acc D Fake: 84.533% 
Loss D: 0.908 
Loss G: 0.7727 (0.7675) Acc G: 15.462% 
LR: 2.000e-04 

2023-03-02 01:52:05,248 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3021 (0.3482) Acc D Real: 63.687% 
Loss D Fake: 0.6233 (0.6278) Acc D Fake: 84.537% 
Loss D: 0.925 
Loss G: 0.7738 (0.7676) Acc G: 15.457% 
LR: 2.000e-04 

2023-03-02 01:52:05,256 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3696 (0.3484) Acc D Real: 63.660% 
Loss D Fake: 0.6224 (0.6278) Acc D Fake: 84.541% 
Loss D: 0.992 
Loss G: 0.7745 (0.7677) Acc G: 15.453% 
LR: 2.000e-04 

2023-03-02 01:52:05,263 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2611 (0.3476) Acc D Real: 63.762% 
Loss D Fake: 0.6217 (0.6277) Acc D Fake: 84.546% 
Loss D: 0.883 
Loss G: 0.7759 (0.7677) Acc G: 15.449% 
LR: 2.000e-04 

2023-03-02 01:52:05,271 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3663 (0.3478) Acc D Real: 63.741% 
Loss D Fake: 0.6205 (0.6277) Acc D Fake: 84.550% 
Loss D: 0.987 
Loss G: 0.7772 (0.7678) Acc G: 15.445% 
LR: 2.000e-04 

2023-03-02 01:52:05,278 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4085 (0.3483) Acc D Real: 63.658% 
Loss D Fake: 0.6198 (0.6276) Acc D Fake: 84.554% 
Loss D: 1.028 
Loss G: 0.7774 (0.7679) Acc G: 15.441% 
LR: 2.000e-04 

2023-03-02 01:52:05,285 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2586 (0.3475) Acc D Real: 63.757% 
Loss D Fake: 0.6197 (0.6275) Acc D Fake: 84.558% 
Loss D: 0.878 
Loss G: 0.7780 (0.7680) Acc G: 15.437% 
LR: 2.000e-04 

2023-03-02 01:52:05,292 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3431 (0.3475) Acc D Real: 63.753% 
Loss D Fake: 0.6190 (0.6275) Acc D Fake: 84.562% 
Loss D: 0.962 
Loss G: 0.7788 (0.7681) Acc G: 15.433% 
LR: 2.000e-04 

2023-03-02 01:52:05,300 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3918 (0.3479) Acc D Real: 63.697% 
Loss D Fake: 0.6186 (0.6274) Acc D Fake: 84.566% 
Loss D: 1.010 
Loss G: 0.7789 (0.7682) Acc G: 15.429% 
LR: 2.000e-04 

2023-03-02 01:52:05,308 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2347 (0.3469) Acc D Real: 63.821% 
Loss D Fake: 0.6184 (0.6273) Acc D Fake: 84.570% 
Loss D: 0.853 
Loss G: 0.7798 (0.7683) Acc G: 15.425% 
LR: 2.000e-04 

2023-03-02 01:52:05,316 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.5076 (0.3483) Acc D Real: 63.649% 
Loss D Fake: 0.6179 (0.6272) Acc D Fake: 84.573% 
Loss D: 1.126 
Loss G: 0.7794 (0.7684) Acc G: 15.422% 
LR: 2.000e-04 

2023-03-02 01:52:05,324 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3702 (0.3485) Acc D Real: 63.624% 
Loss D Fake: 0.6187 (0.6271) Acc D Fake: 84.566% 
Loss D: 0.989 
Loss G: 0.7784 (0.7685) Acc G: 15.432% 
LR: 2.000e-04 

2023-03-02 01:52:05,335 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3673 (0.3486) Acc D Real: 63.602% 
Loss D Fake: 0.6195 (0.6271) Acc D Fake: 84.556% 
Loss D: 0.987 
Loss G: 0.7774 (0.7686) Acc G: 15.442% 
LR: 2.000e-04 

2023-03-02 01:52:05,344 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4544 (0.3495) Acc D Real: 63.491% 
Loss D Fake: 0.6206 (0.6270) Acc D Fake: 84.545% 
Loss D: 1.075 
Loss G: 0.7755 (0.7686) Acc G: 15.453% 
LR: 2.000e-04 

2023-03-02 01:52:05,353 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4137 (0.3501) Acc D Real: 63.420% 
Loss D Fake: 0.6225 (0.6270) Acc D Fake: 84.521% 
Loss D: 1.036 
Loss G: 0.7729 (0.7687) Acc G: 15.477% 
LR: 2.000e-04 

2023-03-02 01:52:05,361 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2940 (0.3496) Acc D Real: 63.466% 
Loss D Fake: 0.6246 (0.6270) Acc D Fake: 84.497% 
Loss D: 0.919 
Loss G: 0.7709 (0.7687) Acc G: 15.501% 
LR: 2.000e-04 

2023-03-02 01:52:05,368 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3265 (0.3494) Acc D Real: 63.490% 
Loss D Fake: 0.6258 (0.6270) Acc D Fake: 84.474% 
Loss D: 0.952 
Loss G: 0.7697 (0.7687) Acc G: 15.524% 
LR: 2.000e-04 

2023-03-02 01:52:05,376 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2521 (0.3486) Acc D Real: 63.584% 
Loss D Fake: 0.6264 (0.6270) Acc D Fake: 84.451% 
Loss D: 0.879 
Loss G: 0.7697 (0.7687) Acc G: 15.547% 
LR: 2.000e-04 

2023-03-02 01:52:05,384 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3393 (0.3485) Acc D Real: 63.591% 
Loss D Fake: 0.6262 (0.6269) Acc D Fake: 84.428% 
Loss D: 0.966 
Loss G: 0.7700 (0.7687) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 01:52:05,392 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4094 (0.3490) Acc D Real: 63.525% 
Loss D Fake: 0.6262 (0.6269) Acc D Fake: 84.406% 
Loss D: 1.036 
Loss G: 0.7695 (0.7687) Acc G: 15.592% 
LR: 2.000e-04 

2023-03-02 01:52:05,399 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3403 (0.3490) Acc D Real: 63.530% 
Loss D Fake: 0.6267 (0.6269) Acc D Fake: 84.384% 
Loss D: 0.967 
Loss G: 0.7690 (0.7687) Acc G: 15.614% 
LR: 2.000e-04 

2023-03-02 01:52:05,407 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.2975 (0.3486) Acc D Real: 63.581% 
Loss D Fake: 0.6270 (0.6269) Acc D Fake: 84.363% 
Loss D: 0.924 
Loss G: 0.7691 (0.7687) Acc G: 15.636% 
LR: 2.000e-04 

2023-03-02 01:52:05,415 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4319 (0.3492) Acc D Real: 63.488% 
Loss D Fake: 0.6270 (0.6269) Acc D Fake: 84.341% 
Loss D: 1.059 
Loss G: 0.7684 (0.7687) Acc G: 15.657% 
LR: 2.000e-04 

2023-03-02 01:52:05,424 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3488 (0.3492) Acc D Real: 63.486% 
Loss D Fake: 0.6278 (0.6269) Acc D Fake: 84.320% 
Loss D: 0.977 
Loss G: 0.7675 (0.7687) Acc G: 15.678% 
LR: 2.000e-04 

2023-03-02 01:52:05,432 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3044 (0.3489) Acc D Real: 63.526% 
Loss D Fake: 0.6283 (0.6270) Acc D Fake: 84.300% 
Loss D: 0.933 
Loss G: 0.7673 (0.7687) Acc G: 15.698% 
LR: 2.000e-04 

2023-03-02 01:52:05,440 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2942 (0.3484) Acc D Real: 63.572% 
Loss D Fake: 0.6282 (0.6270) Acc D Fake: 84.292% 
Loss D: 0.922 
Loss G: 0.7677 (0.7687) Acc G: 15.706% 
LR: 2.000e-04 

2023-03-02 01:52:05,448 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4521 (0.3492) Acc D Real: 63.471% 
Loss D Fake: 0.6281 (0.6270) Acc D Fake: 84.285% 
Loss D: 1.080 
Loss G: 0.7672 (0.7687) Acc G: 15.713% 
LR: 2.000e-04 

2023-03-02 01:52:05,457 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2497 (0.3485) Acc D Real: 63.562% 
Loss D Fake: 0.6285 (0.6270) Acc D Fake: 84.278% 
Loss D: 0.878 
Loss G: 0.7673 (0.7687) Acc G: 15.720% 
LR: 2.000e-04 

2023-03-02 01:52:05,465 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4468 (0.3492) Acc D Real: 63.455% 
Loss D Fake: 0.6285 (0.6270) Acc D Fake: 84.271% 
Loss D: 1.075 
Loss G: 0.7668 (0.7686) Acc G: 15.728% 
LR: 2.000e-04 

2023-03-02 01:52:05,472 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4077 (0.3496) Acc D Real: 63.400% 
Loss D Fake: 0.6293 (0.6270) Acc D Fake: 84.264% 
Loss D: 1.037 
Loss G: 0.7655 (0.7686) Acc G: 15.735% 
LR: 2.000e-04 

2023-03-02 01:52:05,480 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4059 (0.3501) Acc D Real: 63.341% 
Loss D Fake: 0.6307 (0.6270) Acc D Fake: 84.257% 
Loss D: 1.037 
Loss G: 0.7636 (0.7686) Acc G: 15.742% 
LR: 2.000e-04 

2023-03-02 01:52:05,487 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2717 (0.3495) Acc D Real: 63.410% 
Loss D Fake: 0.6321 (0.6271) Acc D Fake: 84.250% 
Loss D: 0.904 
Loss G: 0.7625 (0.7685) Acc G: 15.748% 
LR: 2.000e-04 

2023-03-02 01:52:05,494 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3675 (0.3496) Acc D Real: 63.391% 
Loss D Fake: 0.6328 (0.6271) Acc D Fake: 84.243% 
Loss D: 1.000 
Loss G: 0.7615 (0.7685) Acc G: 15.755% 
LR: 2.000e-04 

2023-03-02 01:52:05,502 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3397 (0.3495) Acc D Real: 63.397% 
Loss D Fake: 0.6337 (0.6272) Acc D Fake: 84.237% 
Loss D: 0.973 
Loss G: 0.7606 (0.7684) Acc G: 15.762% 
LR: 2.000e-04 

2023-03-02 01:52:05,510 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3066 (0.3492) Acc D Real: 63.420% 
Loss D Fake: 0.6343 (0.6272) Acc D Fake: 84.230% 
Loss D: 0.941 
Loss G: 0.7603 (0.7684) Acc G: 15.768% 
LR: 2.000e-04 

2023-03-02 01:52:05,518 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3154 (0.3490) Acc D Real: 63.442% 
Loss D Fake: 0.6344 (0.6273) Acc D Fake: 84.224% 
Loss D: 0.950 
Loss G: 0.7602 (0.7683) Acc G: 15.775% 
LR: 2.000e-04 

2023-03-02 01:52:05,525 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3346 (0.3489) Acc D Real: 63.454% 
Loss D Fake: 0.6344 (0.6273) Acc D Fake: 84.218% 
Loss D: 0.969 
Loss G: 0.7603 (0.7683) Acc G: 15.781% 
LR: 2.000e-04 

2023-03-02 01:52:05,532 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4036 (0.3493) Acc D Real: 63.405% 
Loss D Fake: 0.6345 (0.6274) Acc D Fake: 84.211% 
Loss D: 1.038 
Loss G: 0.7598 (0.7682) Acc G: 15.787% 
LR: 2.000e-04 

2023-03-02 01:52:05,540 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4200 (0.3498) Acc D Real: 63.336% 
Loss D Fake: 0.6353 (0.6274) Acc D Fake: 84.205% 
Loss D: 1.055 
Loss G: 0.7585 (0.7681) Acc G: 15.793% 
LR: 2.000e-04 

2023-03-02 01:52:05,547 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2945 (0.3494) Acc D Real: 63.378% 
Loss D Fake: 0.6363 (0.6275) Acc D Fake: 84.199% 
Loss D: 0.931 
Loss G: 0.7577 (0.7681) Acc G: 15.799% 
LR: 2.000e-04 

2023-03-02 01:52:05,554 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3359 (0.3493) Acc D Real: 63.389% 
Loss D Fake: 0.6368 (0.6276) Acc D Fake: 84.182% 
Loss D: 0.973 
Loss G: 0.7573 (0.7680) Acc G: 15.815% 
LR: 2.000e-04 

2023-03-02 01:52:05,562 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2523 (0.3486) Acc D Real: 63.472% 
Loss D Fake: 0.6368 (0.6276) Acc D Fake: 84.165% 
Loss D: 0.889 
Loss G: 0.7579 (0.7679) Acc G: 15.833% 
LR: 2.000e-04 

2023-03-02 01:52:05,569 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3865 (0.3489) Acc D Real: 63.437% 
Loss D Fake: 0.6363 (0.6277) Acc D Fake: 84.137% 
Loss D: 1.023 
Loss G: 0.7582 (0.7678) Acc G: 15.861% 
LR: 2.000e-04 

2023-03-02 01:52:05,576 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3601 (0.3490) Acc D Real: 63.423% 
Loss D Fake: 0.6362 (0.6277) Acc D Fake: 84.109% 
Loss D: 0.996 
Loss G: 0.7581 (0.7678) Acc G: 15.889% 
LR: 2.000e-04 

2023-03-02 01:52:05,583 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3796 (0.3492) Acc D Real: 63.388% 
Loss D Fake: 0.6366 (0.6278) Acc D Fake: 84.081% 
Loss D: 1.016 
Loss G: 0.7573 (0.7677) Acc G: 15.917% 
LR: 2.000e-04 

2023-03-02 01:52:05,591 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2902 (0.3488) Acc D Real: 63.430% 
Loss D Fake: 0.6373 (0.6279) Acc D Fake: 84.054% 
Loss D: 0.927 
Loss G: 0.7569 (0.7676) Acc G: 15.944% 
LR: 2.000e-04 

2023-03-02 01:52:05,598 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2391 (0.3481) Acc D Real: 63.527% 
Loss D Fake: 0.6372 (0.6279) Acc D Fake: 84.027% 
Loss D: 0.876 
Loss G: 0.7577 (0.7676) Acc G: 15.971% 
LR: 2.000e-04 

2023-03-02 01:52:05,605 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3527 (0.3481) Acc D Real: 63.509% 
Loss D Fake: 0.6363 (0.6280) Acc D Fake: 84.001% 
Loss D: 0.989 
Loss G: 0.7586 (0.7675) Acc G: 15.997% 
LR: 2.000e-04 

2023-03-02 01:52:05,612 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.2755 (0.3476) Acc D Real: 63.571% 
Loss D Fake: 0.6355 (0.6280) Acc D Fake: 83.975% 
Loss D: 0.911 
Loss G: 0.7599 (0.7675) Acc G: 16.023% 
LR: 2.000e-04 

2023-03-02 01:52:05,620 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3292 (0.3475) Acc D Real: 63.579% 
Loss D Fake: 0.6343 (0.6281) Acc D Fake: 83.954% 
Loss D: 0.963 
Loss G: 0.7613 (0.7674) Acc G: 16.039% 
LR: 2.000e-04 

2023-03-02 01:52:05,627 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3367 (0.3474) Acc D Real: 63.582% 
Loss D Fake: 0.6332 (0.6281) Acc D Fake: 83.939% 
Loss D: 0.970 
Loss G: 0.7626 (0.7674) Acc G: 16.054% 
LR: 2.000e-04 

2023-03-02 01:52:05,634 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3598 (0.3475) Acc D Real: 63.563% 
Loss D Fake: 0.6323 (0.6281) Acc D Fake: 83.924% 
Loss D: 0.992 
Loss G: 0.7634 (0.7674) Acc G: 16.069% 
LR: 2.000e-04 

2023-03-02 01:52:05,643 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3966 (0.3478) Acc D Real: 63.513% 
Loss D Fake: 0.6320 (0.6281) Acc D Fake: 83.910% 
Loss D: 1.029 
Loss G: 0.7634 (0.7673) Acc G: 16.083% 
LR: 2.000e-04 

2023-03-02 01:52:05,651 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5214 (0.3489) Acc D Real: 63.501% 
Loss D Fake: 0.6326 (0.6282) Acc D Fake: 83.909% 
Loss D: 1.154 
Loss G: 0.7620 (0.7673) Acc G: 16.084% 
LR: 2.000e-04 

2023-03-02 01:52:05,868 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.329 | Generator Loss: 0.762 | Avg: 2.091 
2023-03-02 01:52:05,890 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.203 | Generator Loss: 0.762 | Avg: 1.964 
2023-03-02 01:52:05,913 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.234 | Generator Loss: 0.762 | Avg: 1.996 
2023-03-02 01:52:05,940 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.268 | Generator Loss: 0.762 | Avg: 2.029 
2023-03-02 01:52:05,966 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.267 | Generator Loss: 0.762 | Avg: 2.029 
2023-03-02 01:52:05,993 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.216 | Generator Loss: 0.762 | Avg: 1.978 
2023-03-02 01:52:06,019 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.178 | Generator Loss: 0.762 | Avg: 1.940 
2023-03-02 01:52:06,047 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.130 | Generator Loss: 0.762 | Avg: 1.892 
2023-03-02 01:52:06,074 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.099 | Generator Loss: 0.762 | Avg: 1.860 
2023-03-02 01:52:06,101 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.057 | Generator Loss: 0.762 | Avg: 1.819 
2023-03-02 01:52:06,127 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.024 | Generator Loss: 0.762 | Avg: 1.785 
2023-03-02 01:52:06,157 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.994 | Generator Loss: 0.762 | Avg: 1.756 
2023-03-02 01:52:06,184 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.970 | Generator Loss: 0.762 | Avg: 1.732 
2023-03-02 01:52:06,210 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.951 | Generator Loss: 0.762 | Avg: 1.712 
2023-03-02 01:52:06,237 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.965 | Generator Loss: 0.762 | Avg: 1.727 
2023-03-02 01:52:06,263 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.980 | Generator Loss: 0.762 | Avg: 1.742 
2023-03-02 01:52:06,289 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.997 | Generator Loss: 0.762 | Avg: 1.759 
2023-03-02 01:52:06,324 -                train: [    INFO] - 
Epoch: 15/20
2023-03-02 01:52:06,514 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.2893 (0.2944) Acc D Real: 69.661% 
Loss D Fake: 0.6349 (0.6344) Acc D Fake: 81.667% 
Loss D: 0.924 
Loss G: 0.7599 (0.7603) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 01:52:06,523 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3124 (0.3004) Acc D Real: 68.681% 
Loss D Fake: 0.6353 (0.6347) Acc D Fake: 81.667% 
Loss D: 0.948 
Loss G: 0.7596 (0.7600) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 01:52:06,530 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2504 (0.2879) Acc D Real: 70.182% 
Loss D Fake: 0.6354 (0.6349) Acc D Fake: 81.667% 
Loss D: 0.886 
Loss G: 0.7600 (0.7600) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 01:52:06,547 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3118 (0.2927) Acc D Real: 69.469% 
Loss D Fake: 0.6350 (0.6349) Acc D Fake: 81.365% 
Loss D: 0.947 
Loss G: 0.7604 (0.7601) Acc G: 18.667% 
LR: 2.000e-04 

2023-03-02 01:52:06,554 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2895 (0.2921) Acc D Real: 69.392% 
Loss D Fake: 0.6347 (0.6349) Acc D Fake: 80.859% 
Loss D: 0.924 
Loss G: 0.7610 (0.7602) Acc G: 19.167% 
LR: 2.000e-04 

2023-03-02 01:52:06,562 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3595 (0.3018) Acc D Real: 68.341% 
Loss D Fake: 0.6342 (0.6348) Acc D Fake: 80.499% 
Loss D: 0.994 
Loss G: 0.7614 (0.7604) Acc G: 19.524% 
LR: 2.000e-04 

2023-03-02 01:52:06,569 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2021 (0.2893) Acc D Real: 69.824% 
Loss D Fake: 0.6338 (0.6347) Acc D Fake: 80.436% 
Loss D: 0.836 
Loss G: 0.7626 (0.7607) Acc G: 19.583% 
LR: 2.000e-04 

2023-03-02 01:52:06,575 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.2730 (0.2875) Acc D Real: 70.139% 
Loss D Fake: 0.6325 (0.6344) Acc D Fake: 80.388% 
Loss D: 0.905 
Loss G: 0.7645 (0.7611) Acc G: 19.630% 
LR: 2.000e-04 

2023-03-02 01:52:06,582 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.2519 (0.2839) Acc D Real: 70.620% 
Loss D Fake: 0.6309 (0.6341) Acc D Fake: 80.349% 
Loss D: 0.883 
Loss G: 0.7667 (0.7617) Acc G: 19.667% 
LR: 2.000e-04 

2023-03-02 01:52:06,589 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4106 (0.2955) Acc D Real: 69.205% 
Loss D Fake: 0.6294 (0.6336) Acc D Fake: 80.317% 
Loss D: 1.040 
Loss G: 0.7679 (0.7622) Acc G: 19.697% 
LR: 2.000e-04 

2023-03-02 01:52:06,596 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4072 (0.3048) Acc D Real: 68.121% 
Loss D Fake: 0.6291 (0.6332) Acc D Fake: 80.291% 
Loss D: 1.036 
Loss G: 0.7677 (0.7627) Acc G: 19.722% 
LR: 2.000e-04 

2023-03-02 01:52:06,603 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3625 (0.3092) Acc D Real: 67.660% 
Loss D Fake: 0.6297 (0.6330) Acc D Fake: 80.260% 
Loss D: 0.992 
Loss G: 0.7668 (0.7630) Acc G: 19.744% 
LR: 2.000e-04 

2023-03-02 01:52:06,610 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4793 (0.3214) Acc D Real: 66.187% 
Loss D Fake: 0.6312 (0.6328) Acc D Fake: 80.134% 
Loss D: 1.111 
Loss G: 0.7643 (0.7631) Acc G: 19.881% 
LR: 2.000e-04 

2023-03-02 01:52:06,617 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2502 (0.3166) Acc D Real: 66.771% 
Loss D Fake: 0.6335 (0.6329) Acc D Fake: 80.014% 
Loss D: 0.884 
Loss G: 0.7623 (0.7631) Acc G: 20.000% 
LR: 2.000e-04 

2023-03-02 01:52:06,624 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3159 (0.3166) Acc D Real: 66.758% 
Loss D Fake: 0.6351 (0.6330) Acc D Fake: 79.909% 
Loss D: 0.951 
Loss G: 0.7607 (0.7629) Acc G: 20.104% 
LR: 2.000e-04 

2023-03-02 01:52:06,631 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4974 (0.3272) Acc D Real: 65.417% 
Loss D Fake: 0.6372 (0.6333) Acc D Fake: 79.816% 
Loss D: 1.135 
Loss G: 0.7575 (0.7626) Acc G: 20.196% 
LR: 2.000e-04 

2023-03-02 01:52:06,638 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3391 (0.3279) Acc D Real: 65.336% 
Loss D Fake: 0.6405 (0.6337) Acc D Fake: 79.641% 
Loss D: 0.980 
Loss G: 0.7540 (0.7621) Acc G: 20.370% 
LR: 2.000e-04 

2023-03-02 01:52:06,645 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2985 (0.3263) Acc D Real: 65.573% 
Loss D Fake: 0.6439 (0.6342) Acc D Fake: 79.485% 
Loss D: 0.942 
Loss G: 0.7508 (0.7615) Acc G: 20.526% 
LR: 2.000e-04 

2023-03-02 01:52:06,652 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2685 (0.3234) Acc D Real: 65.919% 
Loss D Fake: 0.6472 (0.6349) Acc D Fake: 79.260% 
Loss D: 0.916 
Loss G: 0.7477 (0.7608) Acc G: 20.750% 
LR: 2.000e-04 

2023-03-02 01:52:06,659 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4082 (0.3275) Acc D Real: 65.432% 
Loss D Fake: 0.6516 (0.6357) Acc D Fake: 79.015% 
Loss D: 1.060 
Loss G: 0.7426 (0.7600) Acc G: 21.017% 
LR: 2.000e-04 

2023-03-02 01:52:06,666 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3972 (0.3306) Acc D Real: 64.979% 
Loss D Fake: 0.6600 (0.6368) Acc D Fake: 78.681% 
Loss D: 1.057 
Loss G: 0.7339 (0.7588) Acc G: 21.349% 
LR: 2.000e-04 

2023-03-02 01:52:06,673 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4000 (0.3337) Acc D Real: 64.563% 
Loss D Fake: 0.6807 (0.6387) Acc D Fake: 78.132% 
Loss D: 1.081 
Loss G: 0.7146 (0.7569) Acc G: 21.880% 
LR: 2.000e-04 

2023-03-02 01:52:06,680 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3370 (0.3338) Acc D Real: 64.570% 
Loss D Fake: 3.6728 (0.7651) Acc D Fake: 74.876% 
Loss D: 4.010 
Loss G: 0.1368 (0.7310) Acc G: 25.135% 
LR: 2.000e-04 

2023-03-02 01:52:06,686 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.2230 (0.3294) Acc D Real: 65.110% 
Loss D Fake: 3.7956 (0.8863) Acc D Fake: 71.881% 
Loss D: 4.019 
Loss G: 0.1183 (0.7065) Acc G: 28.129% 
LR: 2.000e-04 

2023-03-02 01:52:06,693 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.2242 (0.3253) Acc D Real: 65.651% 
Loss D Fake: 3.8407 (1.0000) Acc D Fake: 69.117% 
Loss D: 4.065 
Loss G: 0.1100 (0.6836) Acc G: 30.893% 
LR: 2.000e-04 

2023-03-02 01:52:06,700 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4731 (0.3308) Acc D Real: 65.085% 
Loss D Fake: 3.8553 (1.1057) Acc D Fake: 66.557% 
Loss D: 4.328 
Loss G: 0.1050 (0.6621) Acc G: 33.453% 
LR: 2.000e-04 

2023-03-02 01:52:06,707 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3442 (0.3313) Acc D Real: 65.086% 
Loss D Fake: 3.8532 (1.2038) Acc D Fake: 64.180% 
Loss D: 4.197 
Loss G: 0.1018 (0.6421) Acc G: 35.830% 
LR: 2.000e-04 

2023-03-02 01:52:06,715 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2801 (0.3295) Acc D Real: 65.665% 
Loss D Fake: 3.8407 (1.2948) Acc D Fake: 61.967% 
Loss D: 4.121 
Loss G: 0.0995 (0.6234) Acc G: 38.042% 
LR: 2.000e-04 

2023-03-02 01:52:06,722 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3498 (0.3302) Acc D Real: 66.128% 
Loss D Fake: 3.8210 (1.3790) Acc D Fake: 59.901% 
Loss D: 4.171 
Loss G: 0.0979 (0.6059) Acc G: 40.108% 
LR: 2.000e-04 

2023-03-02 01:52:06,729 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2507 (0.3276) Acc D Real: 66.951% 
Loss D Fake: 3.7964 (1.4569) Acc D Fake: 57.969% 
Loss D: 4.047 
Loss G: 0.0968 (0.5895) Acc G: 42.040% 
LR: 2.000e-04 

2023-03-02 01:52:06,737 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3266 (0.3276) Acc D Real: 67.812% 
Loss D Fake: 3.7683 (1.5292) Acc D Fake: 56.157% 
Loss D: 4.095 
Loss G: 0.0961 (0.5741) Acc G: 43.851% 
LR: 2.000e-04 

2023-03-02 01:52:06,744 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2921 (0.3265) Acc D Real: 68.670% 
Loss D Fake: 3.7375 (1.5961) Acc D Fake: 54.455% 
Loss D: 4.030 
Loss G: 0.0956 (0.5596) Acc G: 45.552% 
LR: 2.000e-04 

2023-03-02 01:52:06,752 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.2486 (0.3242) Acc D Real: 69.522% 
Loss D Fake: 3.7049 (1.6581) Acc D Fake: 52.854% 
Loss D: 3.953 
Loss G: 0.0953 (0.5459) Acc G: 47.154% 
LR: 2.000e-04 

2023-03-02 01:52:06,760 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2468 (0.3220) Acc D Real: 70.329% 
Loss D Fake: 3.6709 (1.7156) Acc D Fake: 51.344% 
Loss D: 3.918 
Loss G: 0.0952 (0.5330) Acc G: 48.664% 
LR: 2.000e-04 

2023-03-02 01:52:06,767 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2814 (0.3209) Acc D Real: 71.092% 
Loss D Fake: 3.6359 (1.7690) Acc D Fake: 49.918% 
Loss D: 3.917 
Loss G: 0.0953 (0.5209) Acc G: 50.090% 
LR: 2.000e-04 

2023-03-02 01:52:06,776 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.2742 (0.3196) Acc D Real: 71.806% 
Loss D Fake: 3.6002 (1.8185) Acc D Fake: 48.568% 
Loss D: 3.874 
Loss G: 0.0955 (0.5094) Acc G: 51.439% 
LR: 2.000e-04 

2023-03-02 01:52:06,783 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2549 (0.3179) Acc D Real: 72.481% 
Loss D Fake: 3.5640 (1.8644) Acc D Fake: 47.290% 
Loss D: 3.819 
Loss G: 0.0958 (0.4985) Acc G: 52.717% 
LR: 2.000e-04 

2023-03-02 01:52:06,790 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2453 (0.3161) Acc D Real: 73.125% 
Loss D Fake: 3.5275 (1.9070) Acc D Fake: 46.078% 
Loss D: 3.773 
Loss G: 0.0961 (0.4882) Acc G: 53.929% 
LR: 2.000e-04 

2023-03-02 01:52:06,797 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.2604 (0.3147) Acc D Real: 73.724% 
Loss D Fake: 3.4909 (1.9466) Acc D Fake: 44.926% 
Loss D: 3.751 
Loss G: 0.0966 (0.4784) Acc G: 55.081% 
LR: 2.000e-04 

2023-03-02 01:52:06,805 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.2420 (0.3129) Acc D Real: 74.318% 
Loss D Fake: 3.4543 (1.9834) Acc D Fake: 43.830% 
Loss D: 3.696 
Loss G: 0.0972 (0.4691) Acc G: 56.176% 
LR: 2.000e-04 

2023-03-02 01:52:06,812 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2630 (0.3117) Acc D Real: 74.876% 
Loss D Fake: 3.4176 (2.0176) Acc D Fake: 42.786% 
Loss D: 3.681 
Loss G: 0.0978 (0.4602) Acc G: 57.220% 
LR: 2.000e-04 

2023-03-02 01:52:06,820 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3001 (0.3114) Acc D Real: 75.389% 
Loss D Fake: 3.3811 (2.0493) Acc D Fake: 41.791% 
Loss D: 3.681 
Loss G: 0.0985 (0.4518) Acc G: 58.215% 
LR: 2.000e-04 

2023-03-02 01:52:06,827 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2841 (0.3108) Acc D Real: 75.898% 
Loss D Fake: 3.3448 (2.0787) Acc D Fake: 40.842% 
Loss D: 3.629 
Loss G: 0.0992 (0.4438) Acc G: 59.164% 
LR: 2.000e-04 

2023-03-02 01:52:06,834 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2141 (0.3087) Acc D Real: 76.403% 
Loss D Fake: 3.3086 (2.1060) Acc D Fake: 39.934% 
Loss D: 3.523 
Loss G: 0.0999 (0.4362) Acc G: 60.072% 
LR: 2.000e-04 

2023-03-02 01:52:06,842 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.1831 (0.3059) Acc D Real: 76.894% 
Loss D Fake: 3.2727 (2.1314) Acc D Fake: 39.066% 
Loss D: 3.456 
Loss G: 0.1008 (0.4289) Acc G: 60.940% 
LR: 2.000e-04 

2023-03-02 01:52:06,850 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2724 (0.3052) Acc D Real: 77.342% 
Loss D Fake: 3.2372 (2.1549) Acc D Fake: 38.235% 
Loss D: 3.510 
Loss G: 0.1016 (0.4219) Acc G: 61.771% 
LR: 2.000e-04 

2023-03-02 01:52:06,857 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2163 (0.3034) Acc D Real: 77.788% 
Loss D Fake: 3.2017 (2.1767) Acc D Fake: 37.438% 
Loss D: 3.418 
Loss G: 0.1026 (0.4153) Acc G: 62.567% 
LR: 2.000e-04 

2023-03-02 01:52:06,865 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2622 (0.3025) Acc D Real: 78.202% 
Loss D Fake: 3.1665 (2.1969) Acc D Fake: 36.674% 
Loss D: 3.429 
Loss G: 0.1036 (0.4089) Acc G: 63.331% 
LR: 2.000e-04 

2023-03-02 01:52:06,874 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2173 (0.3008) Acc D Real: 78.617% 
Loss D Fake: 3.1314 (2.2156) Acc D Fake: 35.941% 
Loss D: 3.349 
Loss G: 0.1046 (0.4028) Acc G: 64.065% 
LR: 2.000e-04 

2023-03-02 01:52:06,882 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2349 (0.2995) Acc D Real: 79.008% 
Loss D Fake: 3.0966 (2.2329) Acc D Fake: 35.236% 
Loss D: 3.331 
Loss G: 0.1057 (0.3970) Acc G: 64.769% 
LR: 2.000e-04 

2023-03-02 01:52:06,891 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2453 (0.2985) Acc D Real: 79.371% 
Loss D Fake: 3.0619 (2.2488) Acc D Fake: 34.558% 
Loss D: 3.307 
Loss G: 0.1069 (0.3914) Acc G: 65.447% 
LR: 2.000e-04 

2023-03-02 01:52:06,899 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2341 (0.2973) Acc D Real: 79.730% 
Loss D Fake: 3.0273 (2.2635) Acc D Fake: 33.906% 
Loss D: 3.261 
Loss G: 0.1082 (0.3861) Acc G: 66.099% 
LR: 2.000e-04 

2023-03-02 01:52:06,908 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2200 (0.2958) Acc D Real: 80.074% 
Loss D Fake: 2.9929 (2.2770) Acc D Fake: 33.278% 
Loss D: 3.213 
Loss G: 0.1095 (0.3810) Acc G: 66.726% 
LR: 2.000e-04 

2023-03-02 01:52:06,916 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2438 (0.2949) Acc D Real: 80.412% 
Loss D Fake: 2.9588 (2.2894) Acc D Fake: 32.673% 
Loss D: 3.203 
Loss G: 0.1109 (0.3760) Acc G: 67.331% 
LR: 2.000e-04 

2023-03-02 01:52:06,924 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2229 (0.2936) Acc D Real: 80.742% 
Loss D Fake: 2.9250 (2.3008) Acc D Fake: 32.090% 
Loss D: 3.148 
Loss G: 0.1123 (0.3713) Acc G: 67.915% 
LR: 2.000e-04 

2023-03-02 01:52:06,932 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2325 (0.2925) Acc D Real: 81.067% 
Loss D Fake: 2.8914 (2.3111) Acc D Fake: 31.527% 
Loss D: 3.124 
Loss G: 0.1137 (0.3668) Acc G: 68.478% 
LR: 2.000e-04 

2023-03-02 01:52:06,939 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2207 (0.2913) Acc D Real: 81.376% 
Loss D Fake: 2.8580 (2.3206) Acc D Fake: 30.983% 
Loss D: 3.079 
Loss G: 0.1152 (0.3625) Acc G: 69.021% 
LR: 2.000e-04 

2023-03-02 01:52:06,946 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.1689 (0.2892) Acc D Real: 81.685% 
Loss D Fake: 2.8250 (2.3291) Acc D Fake: 30.458% 
Loss D: 2.994 
Loss G: 0.1167 (0.3583) Acc G: 69.546% 
LR: 2.000e-04 

2023-03-02 01:52:06,954 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2137 (0.2880) Acc D Real: 81.976% 
Loss D Fake: 2.7924 (2.3368) Acc D Fake: 29.951% 
Loss D: 3.006 
Loss G: 0.1183 (0.3543) Acc G: 70.054% 
LR: 2.000e-04 

2023-03-02 01:52:06,961 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2297 (0.2870) Acc D Real: 82.252% 
Loss D Fake: 2.7600 (2.3438) Acc D Fake: 29.460% 
Loss D: 2.990 
Loss G: 0.1199 (0.3505) Acc G: 70.545% 
LR: 2.000e-04 

2023-03-02 01:52:06,968 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2197 (0.2859) Acc D Real: 82.524% 
Loss D Fake: 2.7278 (2.3500) Acc D Fake: 28.984% 
Loss D: 2.947 
Loss G: 0.1216 (0.3468) Acc G: 71.020% 
LR: 2.000e-04 

2023-03-02 01:52:06,976 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2050 (0.2846) Acc D Real: 82.784% 
Loss D Fake: 2.6959 (2.3555) Acc D Fake: 28.524% 
Loss D: 2.901 
Loss G: 0.1233 (0.3432) Acc G: 71.480% 
LR: 2.000e-04 

2023-03-02 01:52:06,983 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2906 (0.2847) Acc D Real: 83.042% 
Loss D Fake: 2.6641 (2.3603) Acc D Fake: 28.079% 
Loss D: 2.955 
Loss G: 0.1251 (0.3398) Acc G: 71.925% 
LR: 2.000e-04 

2023-03-02 01:52:06,991 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2176 (0.2837) Acc D Real: 83.297% 
Loss D Fake: 2.6324 (2.3645) Acc D Fake: 27.647% 
Loss D: 2.850 
Loss G: 0.1270 (0.3365) Acc G: 72.357% 
LR: 2.000e-04 

2023-03-02 01:52:06,998 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2415 (0.2831) Acc D Real: 83.527% 
Loss D Fake: 2.6009 (2.3681) Acc D Fake: 27.228% 
Loss D: 2.842 
Loss G: 0.1289 (0.3334) Acc G: 72.776% 
LR: 2.000e-04 

2023-03-02 01:52:07,006 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2003 (0.2818) Acc D Real: 83.764% 
Loss D Fake: 2.5696 (2.3711) Acc D Fake: 26.821% 
Loss D: 2.770 
Loss G: 0.1309 (0.3304) Acc G: 73.183% 
LR: 2.000e-04 

2023-03-02 01:52:07,013 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2165 (0.2809) Acc D Real: 83.997% 
Loss D Fake: 2.5386 (2.3735) Acc D Fake: 26.427% 
Loss D: 2.755 
Loss G: 0.1329 (0.3275) Acc G: 73.577% 
LR: 2.000e-04 

2023-03-02 01:52:07,021 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.1871 (0.2795) Acc D Real: 84.220% 
Loss D Fake: 2.5079 (2.3755) Acc D Fake: 26.044% 
Loss D: 2.695 
Loss G: 0.1350 (0.3247) Acc G: 73.960% 
LR: 2.000e-04 

2023-03-02 01:52:07,028 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2120 (0.2785) Acc D Real: 84.435% 
Loss D Fake: 2.4776 (2.3769) Acc D Fake: 25.672% 
Loss D: 2.690 
Loss G: 0.1371 (0.3220) Acc G: 74.332% 
LR: 2.000e-04 

2023-03-02 01:52:07,036 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.2119 (0.2776) Acc D Real: 84.649% 
Loss D Fake: 2.4476 (2.3779) Acc D Fake: 25.310% 
Loss D: 2.659 
Loss G: 0.1393 (0.3194) Acc G: 74.693% 
LR: 2.000e-04 

2023-03-02 01:52:07,044 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1868 (0.2763) Acc D Real: 84.858% 
Loss D Fake: 2.4176 (2.3785) Acc D Fake: 24.959% 
Loss D: 2.604 
Loss G: 0.1416 (0.3170) Acc G: 75.045% 
LR: 2.000e-04 

2023-03-02 01:52:07,052 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.1954 (0.2752) Acc D Real: 85.060% 
Loss D Fake: 2.3880 (2.3786) Acc D Fake: 24.617% 
Loss D: 2.583 
Loss G: 0.1439 (0.3146) Acc G: 75.387% 
LR: 2.000e-04 

2023-03-02 01:52:07,059 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2198 (0.2745) Acc D Real: 85.256% 
Loss D Fake: 2.3585 (2.3783) Acc D Fake: 24.284% 
Loss D: 2.578 
Loss G: 0.1463 (0.3123) Acc G: 75.719% 
LR: 2.000e-04 

2023-03-02 01:52:07,067 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.1818 (0.2732) Acc D Real: 85.451% 
Loss D Fake: 2.3292 (2.3777) Acc D Fake: 23.960% 
Loss D: 2.511 
Loss G: 0.1488 (0.3101) Acc G: 76.043% 
LR: 2.000e-04 

2023-03-02 01:52:07,074 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.1731 (0.2719) Acc D Real: 85.641% 
Loss D Fake: 2.3003 (2.3767) Acc D Fake: 23.645% 
Loss D: 2.473 
Loss G: 0.1513 (0.3080) Acc G: 76.358% 
LR: 2.000e-04 

2023-03-02 01:52:07,082 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2104 (0.2711) Acc D Real: 85.824% 
Loss D Fake: 2.2718 (2.3753) Acc D Fake: 23.338% 
Loss D: 2.482 
Loss G: 0.1539 (0.3060) Acc G: 76.665% 
LR: 2.000e-04 

2023-03-02 01:52:07,089 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.1957 (0.2702) Acc D Real: 86.003% 
Loss D Fake: 2.2434 (2.3736) Acc D Fake: 23.039% 
Loss D: 2.439 
Loss G: 0.1566 (0.3041) Acc G: 76.964% 
LR: 2.000e-04 

2023-03-02 01:52:07,097 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.1955 (0.2692) Acc D Real: 86.177% 
Loss D Fake: 2.2153 (2.3716) Acc D Fake: 22.747% 
Loss D: 2.411 
Loss G: 0.1593 (0.3023) Acc G: 77.256% 
LR: 2.000e-04 

2023-03-02 01:52:07,104 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.2055 (0.2684) Acc D Real: 86.346% 
Loss D Fake: 2.1874 (2.3693) Acc D Fake: 22.463% 
Loss D: 2.393 
Loss G: 0.1622 (0.3005) Acc G: 77.540% 
LR: 2.000e-04 

2023-03-02 01:52:07,111 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2087 (0.2677) Acc D Real: 86.512% 
Loss D Fake: 2.1597 (2.3667) Acc D Fake: 22.186% 
Loss D: 2.368 
Loss G: 0.1651 (0.2989) Acc G: 77.818% 
LR: 2.000e-04 

2023-03-02 01:52:07,119 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2012 (0.2669) Acc D Real: 86.670% 
Loss D Fake: 2.1321 (2.3639) Acc D Fake: 21.915% 
Loss D: 2.333 
Loss G: 0.1681 (0.2973) Acc G: 78.088% 
LR: 2.000e-04 

2023-03-02 01:52:07,126 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.1979 (0.2660) Acc D Real: 86.830% 
Loss D Fake: 2.1049 (2.3607) Acc D Fake: 21.651% 
Loss D: 2.303 
Loss G: 0.1712 (0.2958) Acc G: 78.352% 
LR: 2.000e-04 

2023-03-02 01:52:07,134 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2188 (0.2655) Acc D Real: 86.979% 
Loss D Fake: 2.0780 (2.3574) Acc D Fake: 21.393% 
Loss D: 2.297 
Loss G: 0.1743 (0.2943) Acc G: 78.610% 
LR: 2.000e-04 

2023-03-02 01:52:07,146 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.2084 (0.2648) Acc D Real: 87.129% 
Loss D Fake: 2.0512 (2.3538) Acc D Fake: 21.142% 
Loss D: 2.260 
Loss G: 0.1775 (0.2929) Acc G: 78.862% 
LR: 2.000e-04 

2023-03-02 01:52:07,154 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.1838 (0.2639) Acc D Real: 87.279% 
Loss D Fake: 2.0248 (2.3499) Acc D Fake: 20.896% 
Loss D: 2.209 
Loss G: 0.1808 (0.2916) Acc G: 79.107% 
LR: 2.000e-04 

2023-03-02 01:52:07,162 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2099 (0.2632) Acc D Real: 87.425% 
Loss D Fake: 1.9988 (2.3459) Acc D Fake: 20.656% 
Loss D: 2.209 
Loss G: 0.1842 (0.2904) Acc G: 79.347% 
LR: 2.000e-04 

2023-03-02 01:52:07,169 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.2220 (0.2628) Acc D Real: 87.568% 
Loss D Fake: 1.9729 (2.3417) Acc D Fake: 20.421% 
Loss D: 2.195 
Loss G: 0.1876 (0.2892) Acc G: 79.582% 
LR: 2.000e-04 

2023-03-02 01:52:07,177 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2046 (0.2621) Acc D Real: 87.708% 
Loss D Fake: 1.9470 (2.3372) Acc D Fake: 20.191% 
Loss D: 2.152 
Loss G: 0.1912 (0.2881) Acc G: 79.812% 
LR: 2.000e-04 

2023-03-02 01:52:07,185 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2019 (0.2614) Acc D Real: 87.844% 
Loss D Fake: 1.9214 (2.3326) Acc D Fake: 19.967% 
Loss D: 2.123 
Loss G: 0.1949 (0.2871) Acc G: 80.036% 
LR: 2.000e-04 

2023-03-02 01:52:07,193 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2091 (0.2609) Acc D Real: 87.978% 
Loss D Fake: 1.8962 (2.3278) Acc D Fake: 19.748% 
Loss D: 2.105 
Loss G: 0.1986 (0.2861) Acc G: 80.255% 
LR: 2.000e-04 

2023-03-02 01:52:07,200 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.2023 (0.2602) Acc D Real: 88.106% 
Loss D Fake: 1.8714 (2.3229) Acc D Fake: 19.533% 
Loss D: 2.074 
Loss G: 0.2023 (0.2852) Acc G: 80.470% 
LR: 2.000e-04 

2023-03-02 01:52:07,207 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.1993 (0.2596) Acc D Real: 88.234% 
Loss D Fake: 1.8472 (2.3177) Acc D Fake: 19.323% 
Loss D: 2.047 
Loss G: 0.2061 (0.2844) Acc G: 80.680% 
LR: 2.000e-04 

2023-03-02 01:52:07,215 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2124 (0.2591) Acc D Real: 88.354% 
Loss D Fake: 1.8233 (2.3125) Acc D Fake: 19.117% 
Loss D: 2.036 
Loss G: 0.2100 (0.2836) Acc G: 80.885% 
LR: 2.000e-04 

2023-03-02 01:52:07,222 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2128 (0.2586) Acc D Real: 88.467% 
Loss D Fake: 1.7996 (2.3071) Acc D Fake: 18.916% 
Loss D: 2.012 
Loss G: 0.2139 (0.2828) Acc G: 81.087% 
LR: 2.000e-04 

2023-03-02 01:52:07,230 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2180 (0.2582) Acc D Real: 88.587% 
Loss D Fake: 1.7762 (2.3016) Acc D Fake: 18.719% 
Loss D: 1.994 
Loss G: 0.2180 (0.2822) Acc G: 81.284% 
LR: 2.000e-04 

2023-03-02 01:52:07,237 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2155 (0.2577) Acc D Real: 88.705% 
Loss D Fake: 1.7532 (2.2959) Acc D Fake: 18.526% 
Loss D: 1.969 
Loss G: 0.2221 (0.2815) Acc G: 81.477% 
LR: 2.000e-04 

2023-03-02 01:52:07,244 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2074 (0.2572) Acc D Real: 88.820% 
Loss D Fake: 1.7306 (2.2901) Acc D Fake: 18.337% 
Loss D: 1.938 
Loss G: 0.2262 (0.2810) Acc G: 81.666% 
LR: 2.000e-04 

2023-03-02 01:52:07,253 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2149 (0.2568) Acc D Real: 88.933% 
Loss D Fake: 1.7083 (2.2843) Acc D Fake: 18.152% 
Loss D: 1.923 
Loss G: 0.2304 (0.2805) Acc G: 81.851% 
LR: 2.000e-04 

2023-03-02 01:52:07,260 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2212 (0.2564) Acc D Real: 89.044% 
Loss D Fake: 1.6866 (2.2783) Acc D Fake: 17.970% 
Loss D: 1.908 
Loss G: 0.2346 (0.2800) Acc G: 82.032% 
LR: 2.000e-04 

2023-03-02 01:52:07,268 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2182 (0.2561) Acc D Real: 89.152% 
Loss D Fake: 1.6651 (2.2722) Acc D Fake: 17.792% 
Loss D: 1.883 
Loss G: 0.2389 (0.2796) Acc G: 82.210% 
LR: 2.000e-04 

2023-03-02 01:52:07,275 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2349 (0.2558) Acc D Real: 89.252% 
Loss D Fake: 1.6441 (2.2660) Acc D Fake: 17.618% 
Loss D: 1.879 
Loss G: 0.2433 (0.2792) Acc G: 82.385% 
LR: 2.000e-04 

2023-03-02 01:52:07,283 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2143 (0.2554) Acc D Real: 89.356% 
Loss D Fake: 1.6233 (2.2598) Acc D Fake: 17.447% 
Loss D: 1.838 
Loss G: 0.2477 (0.2789) Acc G: 82.556% 
LR: 2.000e-04 

2023-03-02 01:52:07,291 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2348 (0.2552) Acc D Real: 89.459% 
Loss D Fake: 1.6028 (2.2535) Acc D Fake: 17.279% 
Loss D: 1.838 
Loss G: 0.2521 (0.2787) Acc G: 82.723% 
LR: 2.000e-04 

2023-03-02 01:52:07,298 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2365 (0.2551) Acc D Real: 89.553% 
Loss D Fake: 1.5827 (2.2471) Acc D Fake: 17.115% 
Loss D: 1.819 
Loss G: 0.2567 (0.2785) Acc G: 82.888% 
LR: 2.000e-04 

2023-03-02 01:52:07,305 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2402 (0.2549) Acc D Real: 89.647% 
Loss D Fake: 1.5625 (2.2406) Acc D Fake: 16.953% 
Loss D: 1.803 
Loss G: 0.2614 (0.2783) Acc G: 83.049% 
LR: 2.000e-04 

2023-03-02 01:52:07,313 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.2319 (0.2547) Acc D Real: 89.744% 
Loss D Fake: 1.5427 (2.2341) Acc D Fake: 16.795% 
Loss D: 1.775 
Loss G: 0.2661 (0.2782) Acc G: 83.208% 
LR: 2.000e-04 

2023-03-02 01:52:07,320 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2464 (0.2546) Acc D Real: 89.837% 
Loss D Fake: 1.5232 (2.2275) Acc D Fake: 16.639% 
Loss D: 1.770 
Loss G: 0.2708 (0.2781) Acc G: 83.363% 
LR: 2.000e-04 

2023-03-02 01:52:07,328 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2401 (0.2545) Acc D Real: 89.930% 
Loss D Fake: 1.5042 (2.2209) Acc D Fake: 16.487% 
Loss D: 1.744 
Loss G: 0.2755 (0.2781) Acc G: 83.516% 
LR: 2.000e-04 

2023-03-02 01:52:07,335 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2490 (0.2545) Acc D Real: 90.021% 
Loss D Fake: 1.4858 (2.2142) Acc D Fake: 16.337% 
Loss D: 1.735 
Loss G: 0.2802 (0.2781) Acc G: 83.666% 
LR: 2.000e-04 

2023-03-02 01:52:07,343 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2320 (0.2542) Acc D Real: 90.108% 
Loss D Fake: 1.4678 (2.2075) Acc D Fake: 16.189% 
Loss D: 1.700 
Loss G: 0.2850 (0.2782) Acc G: 83.813% 
LR: 2.000e-04 

2023-03-02 01:52:07,350 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2438 (0.2542) Acc D Real: 90.196% 
Loss D Fake: 1.4499 (2.2007) Acc D Fake: 16.045% 
Loss D: 1.694 
Loss G: 0.2899 (0.2783) Acc G: 83.957% 
LR: 2.000e-04 

2023-03-02 01:52:07,358 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2497 (0.2541) Acc D Real: 90.283% 
Loss D Fake: 1.4324 (2.1939) Acc D Fake: 15.903% 
Loss D: 1.682 
Loss G: 0.2947 (0.2784) Acc G: 84.099% 
LR: 2.000e-04 

2023-03-02 01:52:07,365 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2548 (0.2541) Acc D Real: 90.368% 
Loss D Fake: 1.4155 (2.1871) Acc D Fake: 15.763% 
Loss D: 1.670 
Loss G: 0.2995 (0.2786) Acc G: 84.239% 
LR: 2.000e-04 

2023-03-02 01:52:07,373 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.2302 (0.2539) Acc D Real: 90.452% 
Loss D Fake: 1.3989 (2.1802) Acc D Fake: 15.626% 
Loss D: 1.629 
Loss G: 0.3044 (0.2788) Acc G: 84.376% 
LR: 2.000e-04 

2023-03-02 01:52:07,380 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2317 (0.2537) Acc D Real: 90.530% 
Loss D Fake: 1.3820 (2.1734) Acc D Fake: 15.492% 
Loss D: 1.614 
Loss G: 0.3096 (0.2791) Acc G: 84.511% 
LR: 2.000e-04 

2023-03-02 01:52:07,388 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2481 (0.2537) Acc D Real: 90.610% 
Loss D Fake: 1.3651 (2.1665) Acc D Fake: 15.359% 
Loss D: 1.613 
Loss G: 0.3148 (0.2794) Acc G: 84.643% 
LR: 2.000e-04 

2023-03-02 01:52:07,395 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.2680 (0.2538) Acc D Real: 90.689% 
Loss D Fake: 1.3485 (2.1595) Acc D Fake: 15.229% 
Loss D: 1.617 
Loss G: 0.3200 (0.2798) Acc G: 84.773% 
LR: 2.000e-04 

2023-03-02 01:52:07,403 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2683 (0.2539) Acc D Real: 90.766% 
Loss D Fake: 1.3324 (2.1526) Acc D Fake: 15.101% 
Loss D: 1.601 
Loss G: 0.3251 (0.2801) Acc G: 84.901% 
LR: 2.000e-04 

2023-03-02 01:52:07,410 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2610 (0.2540) Acc D Real: 90.843% 
Loss D Fake: 1.3166 (2.1456) Acc D Fake: 14.975% 
Loss D: 1.578 
Loss G: 0.3304 (0.2806) Acc G: 85.027% 
LR: 2.000e-04 

2023-03-02 01:52:07,417 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2669 (0.2541) Acc D Real: 90.917% 
Loss D Fake: 1.3011 (2.1386) Acc D Fake: 14.851% 
Loss D: 1.568 
Loss G: 0.3356 (0.2810) Acc G: 85.151% 
LR: 2.000e-04 

2023-03-02 01:52:07,425 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2503 (0.2541) Acc D Real: 90.992% 
Loss D Fake: 1.2858 (2.1316) Acc D Fake: 14.730% 
Loss D: 1.536 
Loss G: 0.3410 (0.2815) Acc G: 85.272% 
LR: 2.000e-04 

2023-03-02 01:52:07,432 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.2820 (0.2543) Acc D Real: 91.062% 
Loss D Fake: 1.2707 (2.1246) Acc D Fake: 14.610% 
Loss D: 1.553 
Loss G: 0.3463 (0.2820) Acc G: 85.392% 
LR: 2.000e-04 

2023-03-02 01:52:07,439 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2569 (0.2543) Acc D Real: 91.134% 
Loss D Fake: 1.2560 (2.1176) Acc D Fake: 14.492% 
Loss D: 1.513 
Loss G: 0.3516 (0.2826) Acc G: 85.510% 
LR: 2.000e-04 

2023-03-02 01:52:07,447 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2888 (0.2546) Acc D Real: 91.202% 
Loss D Fake: 1.2415 (2.1106) Acc D Fake: 14.376% 
Loss D: 1.530 
Loss G: 0.3569 (0.2832) Acc G: 85.626% 
LR: 2.000e-04 

2023-03-02 01:52:07,454 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.2749 (0.2547) Acc D Real: 91.267% 
Loss D Fake: 1.2276 (2.1036) Acc D Fake: 14.262% 
Loss D: 1.503 
Loss G: 0.3622 (0.2838) Acc G: 85.740% 
LR: 2.000e-04 

2023-03-02 01:52:07,462 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2922 (0.2550) Acc D Real: 91.331% 
Loss D Fake: 1.2141 (2.0966) Acc D Fake: 14.150% 
Loss D: 1.506 
Loss G: 0.3674 (0.2845) Acc G: 85.852% 
LR: 2.000e-04 

2023-03-02 01:52:07,469 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2888 (0.2553) Acc D Real: 91.392% 
Loss D Fake: 1.2010 (2.0896) Acc D Fake: 14.039% 
Loss D: 1.490 
Loss G: 0.3726 (0.2852) Acc G: 85.963% 
LR: 2.000e-04 

2023-03-02 01:52:07,476 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2854 (0.2555) Acc D Real: 91.459% 
Loss D Fake: 1.1882 (2.0826) Acc D Fake: 13.930% 
Loss D: 1.474 
Loss G: 0.3777 (0.2859) Acc G: 86.072% 
LR: 2.000e-04 

2023-03-02 01:52:07,484 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2842 (0.2557) Acc D Real: 91.519% 
Loss D Fake: 1.1758 (2.0757) Acc D Fake: 13.823% 
Loss D: 1.460 
Loss G: 0.3828 (0.2866) Acc G: 86.179% 
LR: 2.000e-04 

2023-03-02 01:52:07,492 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2819 (0.2559) Acc D Real: 91.584% 
Loss D Fake: 1.1637 (2.0687) Acc D Fake: 13.718% 
Loss D: 1.446 
Loss G: 0.3879 (0.2874) Acc G: 86.284% 
LR: 2.000e-04 

2023-03-02 01:52:07,499 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3102 (0.2564) Acc D Real: 91.645% 
Loss D Fake: 1.1519 (2.0617) Acc D Fake: 13.614% 
Loss D: 1.462 
Loss G: 0.3928 (0.2882) Acc G: 86.388% 
LR: 2.000e-04 

2023-03-02 01:52:07,506 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3063 (0.2567) Acc D Real: 91.703% 
Loss D Fake: 1.1406 (2.0548) Acc D Fake: 13.512% 
Loss D: 1.447 
Loss G: 0.3977 (0.2890) Acc G: 86.490% 
LR: 2.000e-04 

2023-03-02 01:52:07,514 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2894 (0.2570) Acc D Real: 91.761% 
Loss D Fake: 1.1295 (2.0479) Acc D Fake: 13.411% 
Loss D: 1.419 
Loss G: 0.4027 (0.2899) Acc G: 86.591% 
LR: 2.000e-04 

2023-03-02 01:52:07,521 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3045 (0.2573) Acc D Real: 91.821% 
Loss D Fake: 1.1185 (2.0410) Acc D Fake: 13.311% 
Loss D: 1.423 
Loss G: 0.4076 (0.2907) Acc G: 86.691% 
LR: 2.000e-04 

2023-03-02 01:52:07,529 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2499 (0.2573) Acc D Real: 91.881% 
Loss D Fake: 1.1076 (2.0342) Acc D Fake: 13.213% 
Loss D: 1.358 
Loss G: 0.4128 (0.2916) Acc G: 86.788% 
LR: 2.000e-04 

2023-03-02 01:52:07,536 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3569 (0.2580) Acc D Real: 91.940% 
Loss D Fake: 1.0967 (2.0273) Acc D Fake: 13.117% 
Loss D: 1.454 
Loss G: 0.4176 (0.2926) Acc G: 86.885% 
LR: 2.000e-04 

2023-03-02 01:52:07,543 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2978 (0.2583) Acc D Real: 91.997% 
Loss D Fake: 1.0868 (2.0205) Acc D Fake: 13.022% 
Loss D: 1.385 
Loss G: 0.4224 (0.2935) Acc G: 86.980% 
LR: 2.000e-04 

2023-03-02 01:52:07,551 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3283 (0.2588) Acc D Real: 92.052% 
Loss D Fake: 1.0769 (2.0137) Acc D Fake: 12.928% 
Loss D: 1.405 
Loss G: 0.4272 (0.2945) Acc G: 87.074% 
LR: 2.000e-04 

2023-03-02 01:52:07,558 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3231 (0.2593) Acc D Real: 92.109% 
Loss D Fake: 1.0674 (2.0070) Acc D Fake: 12.836% 
Loss D: 1.391 
Loss G: 0.4317 (0.2954) Acc G: 87.166% 
LR: 2.000e-04 

2023-03-02 01:52:07,566 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3093 (0.2596) Acc D Real: 92.165% 
Loss D Fake: 1.0585 (2.0002) Acc D Fake: 12.745% 
Loss D: 1.368 
Loss G: 0.4362 (0.2964) Acc G: 87.257% 
LR: 2.000e-04 

2023-03-02 01:52:07,573 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3190 (0.2600) Acc D Real: 92.219% 
Loss D Fake: 1.0497 (1.9935) Acc D Fake: 12.655% 
Loss D: 1.369 
Loss G: 0.4406 (0.2974) Acc G: 87.347% 
LR: 2.000e-04 

2023-03-02 01:52:07,581 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3361 (0.2606) Acc D Real: 92.274% 
Loss D Fake: 1.0413 (1.9869) Acc D Fake: 12.567% 
Loss D: 1.377 
Loss G: 0.4449 (0.2985) Acc G: 87.435% 
LR: 2.000e-04 

2023-03-02 01:52:07,588 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2910 (0.2608) Acc D Real: 92.327% 
Loss D Fake: 1.0331 (1.9803) Acc D Fake: 12.479% 
Loss D: 1.324 
Loss G: 0.4492 (0.2995) Acc G: 87.522% 
LR: 2.000e-04 

2023-03-02 01:52:07,595 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3451 (0.2614) Acc D Real: 92.378% 
Loss D Fake: 1.0250 (1.9737) Acc D Fake: 12.393% 
Loss D: 1.370 
Loss G: 0.4534 (0.3006) Acc G: 87.608% 
LR: 2.000e-04 

2023-03-02 01:52:07,603 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3118 (0.2617) Acc D Real: 92.430% 
Loss D Fake: 1.0172 (1.9671) Acc D Fake: 12.308% 
Loss D: 1.329 
Loss G: 0.4576 (0.3017) Acc G: 87.693% 
LR: 2.000e-04 

2023-03-02 01:52:07,611 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3099 (0.2620) Acc D Real: 92.477% 
Loss D Fake: 1.0094 (1.9606) Acc D Fake: 12.225% 
Loss D: 1.319 
Loss G: 0.4619 (0.3028) Acc G: 87.777% 
LR: 2.000e-04 

2023-03-02 01:52:07,618 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3473 (0.2626) Acc D Real: 92.528% 
Loss D Fake: 1.0016 (1.9541) Acc D Fake: 12.142% 
Loss D: 1.349 
Loss G: 0.4661 (0.3039) Acc G: 87.860% 
LR: 2.000e-04 

2023-03-02 01:52:07,625 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3298 (0.2631) Acc D Real: 92.575% 
Loss D Fake: 0.9942 (1.9477) Acc D Fake: 12.061% 
Loss D: 1.324 
Loss G: 0.4703 (0.3050) Acc G: 87.941% 
LR: 2.000e-04 

2023-03-02 01:52:07,632 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3353 (0.2635) Acc D Real: 92.621% 
Loss D Fake: 0.9870 (1.9413) Acc D Fake: 11.980% 
Loss D: 1.322 
Loss G: 0.4744 (0.3061) Acc G: 88.022% 
LR: 2.000e-04 

2023-03-02 01:52:07,640 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3685 (0.2642) Acc D Real: 92.666% 
Loss D Fake: 0.9799 (1.9349) Acc D Fake: 11.901% 
Loss D: 1.348 
Loss G: 0.4783 (0.3072) Acc G: 88.101% 
LR: 2.000e-04 

2023-03-02 01:52:07,647 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3485 (0.2648) Acc D Real: 92.715% 
Loss D Fake: 0.9734 (1.9286) Acc D Fake: 11.823% 
Loss D: 1.322 
Loss G: 0.4820 (0.3084) Acc G: 88.179% 
LR: 2.000e-04 

2023-03-02 01:52:07,657 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3263 (0.2652) Acc D Real: 92.762% 
Loss D Fake: 0.9671 (1.9223) Acc D Fake: 11.745% 
Loss D: 1.293 
Loss G: 0.4857 (0.3096) Acc G: 88.256% 
LR: 2.000e-04 

2023-03-02 01:52:07,664 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3396 (0.2657) Acc D Real: 92.809% 
Loss D Fake: 0.9608 (1.9161) Acc D Fake: 11.669% 
Loss D: 1.300 
Loss G: 0.4894 (0.3107) Acc G: 88.333% 
LR: 2.000e-04 

2023-03-02 01:52:07,671 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3888 (0.2665) Acc D Real: 92.854% 
Loss D Fake: 0.9549 (1.9099) Acc D Fake: 11.594% 
Loss D: 1.344 
Loss G: 0.4928 (0.3119) Acc G: 88.408% 
LR: 2.000e-04 

2023-03-02 01:52:07,679 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3289 (0.2669) Acc D Real: 92.899% 
Loss D Fake: 0.9496 (1.9037) Acc D Fake: 11.519% 
Loss D: 1.278 
Loss G: 0.4961 (0.3131) Acc G: 88.482% 
LR: 2.000e-04 

2023-03-02 01:52:07,686 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3358 (0.2673) Acc D Real: 92.940% 
Loss D Fake: 0.9441 (1.8976) Acc D Fake: 11.446% 
Loss D: 1.280 
Loss G: 0.4995 (0.3143) Acc G: 88.556% 
LR: 2.000e-04 

2023-03-02 01:52:07,693 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2986 (0.2675) Acc D Real: 92.944% 
Loss D Fake: 0.9384 (1.8915) Acc D Fake: 11.439% 
Loss D: 1.237 
Loss G: 0.5033 (0.3155) Acc G: 88.562% 
LR: 2.000e-04 

2023-03-02 01:52:07,896 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.100 | Generator Loss: 0.503 | Avg: 1.603 
2023-03-02 01:52:07,918 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.165 | Generator Loss: 0.503 | Avg: 1.668 
2023-03-02 01:52:07,942 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.148 | Generator Loss: 0.503 | Avg: 1.651 
2023-03-02 01:52:07,968 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.130 | Generator Loss: 0.503 | Avg: 1.633 
2023-03-02 01:52:07,994 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.131 | Generator Loss: 0.503 | Avg: 1.634 
2023-03-02 01:52:08,020 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.157 | Generator Loss: 0.503 | Avg: 1.660 
2023-03-02 01:52:08,046 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.178 | Generator Loss: 0.503 | Avg: 1.681 
2023-03-02 01:52:08,072 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.203 | Generator Loss: 0.503 | Avg: 1.707 
2023-03-02 01:52:08,098 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.219 | Generator Loss: 0.503 | Avg: 1.723 
2023-03-02 01:52:08,124 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.240 | Generator Loss: 0.503 | Avg: 1.743 
2023-03-02 01:52:08,150 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.257 | Generator Loss: 0.503 | Avg: 1.761 
2023-03-02 01:52:08,176 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.271 | Generator Loss: 0.503 | Avg: 1.774 
2023-03-02 01:52:08,201 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.283 | Generator Loss: 0.503 | Avg: 1.787 
2023-03-02 01:52:08,228 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.294 | Generator Loss: 0.503 | Avg: 1.797 
2023-03-02 01:52:08,254 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.287 | Generator Loss: 0.503 | Avg: 1.790 
2023-03-02 01:52:08,279 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.280 | Generator Loss: 0.503 | Avg: 1.783 
2023-03-02 01:52:08,304 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.271 | Generator Loss: 0.503 | Avg: 1.774 
2023-03-02 01:52:08,337 -                train: [    INFO] - 
Epoch: 16/20
2023-03-02 01:52:08,529 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3972 (0.3875) Acc D Real: 99.922% 
Loss D Fake: 0.9266 (0.9294) Acc D Fake: 0.000% 
Loss D: 1.324 
Loss G: 0.5103 (0.5087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,536 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3507 (0.3752) Acc D Real: 99.948% 
Loss D Fake: 0.9216 (0.9268) Acc D Fake: 0.000% 
Loss D: 1.272 
Loss G: 0.5135 (0.5103) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,544 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3154 (0.3603) Acc D Real: 99.961% 
Loss D Fake: 0.9166 (0.9243) Acc D Fake: 0.000% 
Loss D: 1.232 
Loss G: 0.5168 (0.5119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,564 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4278 (0.3738) Acc D Real: 99.823% 
Loss D Fake: 0.9117 (0.9217) Acc D Fake: 0.000% 
Loss D: 1.340 
Loss G: 0.5198 (0.5135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,571 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4335 (0.3837) Acc D Real: 99.818% 
Loss D Fake: 0.9076 (0.9194) Acc D Fake: 0.000% 
Loss D: 1.341 
Loss G: 0.5221 (0.5149) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,578 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3103 (0.3733) Acc D Real: 99.814% 
Loss D Fake: 0.9042 (0.9172) Acc D Fake: 0.000% 
Loss D: 1.214 
Loss G: 0.5246 (0.5163) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,585 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2945 (0.3634) Acc D Real: 99.818% 
Loss D Fake: 0.9001 (0.9151) Acc D Fake: 0.000% 
Loss D: 1.195 
Loss G: 0.5275 (0.5177) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,592 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3716 (0.3643) Acc D Real: 99.809% 
Loss D Fake: 0.8956 (0.9129) Acc D Fake: 0.000% 
Loss D: 1.267 
Loss G: 0.5305 (0.5191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,599 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3537 (0.3633) Acc D Real: 99.807% 
Loss D Fake: 0.8913 (0.9107) Acc D Fake: 0.000% 
Loss D: 1.245 
Loss G: 0.5334 (0.5205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,606 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3819 (0.3649) Acc D Real: 99.692% 
Loss D Fake: 0.8871 (0.9086) Acc D Fake: 0.000% 
Loss D: 1.269 
Loss G: 0.5362 (0.5220) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,612 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3706 (0.3654) Acc D Real: 99.614% 
Loss D Fake: 0.8830 (0.9065) Acc D Fake: 0.000% 
Loss D: 1.254 
Loss G: 0.5391 (0.5234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,619 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4059 (0.3685) Acc D Real: 99.591% 
Loss D Fake: 0.8790 (0.9043) Acc D Fake: 0.000% 
Loss D: 1.285 
Loss G: 0.5416 (0.5248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,626 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3660 (0.3683) Acc D Real: 99.587% 
Loss D Fake: 0.8755 (0.9023) Acc D Fake: 0.000% 
Loss D: 1.241 
Loss G: 0.5441 (0.5262) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,633 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4337 (0.3727) Acc D Real: 99.549% 
Loss D Fake: 0.8722 (0.9003) Acc D Fake: 0.000% 
Loss D: 1.306 
Loss G: 0.5461 (0.5275) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,640 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3242 (0.3697) Acc D Real: 99.538% 
Loss D Fake: 0.8694 (0.8983) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.5483 (0.5288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,648 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3177 (0.3666) Acc D Real: 99.519% 
Loss D Fake: 0.8659 (0.8964) Acc D Fake: 0.000% 
Loss D: 1.184 
Loss G: 0.5510 (0.5301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,655 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3361 (0.3649) Acc D Real: 99.511% 
Loss D Fake: 0.8620 (0.8945) Acc D Fake: 0.000% 
Loss D: 1.198 
Loss G: 0.5539 (0.5314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,662 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3644 (0.3649) Acc D Real: 99.501% 
Loss D Fake: 0.8579 (0.8926) Acc D Fake: 0.000% 
Loss D: 1.222 
Loss G: 0.5568 (0.5328) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,668 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3802 (0.3657) Acc D Real: 99.477% 
Loss D Fake: 0.8541 (0.8907) Acc D Fake: 0.000% 
Loss D: 1.234 
Loss G: 0.5595 (0.5341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,675 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3530 (0.3651) Acc D Real: 99.472% 
Loss D Fake: 0.8505 (0.8888) Acc D Fake: 0.000% 
Loss D: 1.203 
Loss G: 0.5621 (0.5354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,682 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3907 (0.3662) Acc D Real: 99.427% 
Loss D Fake: 0.8469 (0.8869) Acc D Fake: 0.000% 
Loss D: 1.238 
Loss G: 0.5646 (0.5368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,689 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4192 (0.3685) Acc D Real: 99.404% 
Loss D Fake: 0.8438 (0.8850) Acc D Fake: 0.000% 
Loss D: 1.263 
Loss G: 0.5668 (0.5381) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,696 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4095 (0.3702) Acc D Real: 99.368% 
Loss D Fake: 0.8410 (0.8832) Acc D Fake: 0.000% 
Loss D: 1.250 
Loss G: 0.5687 (0.5393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,703 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3961 (0.3713) Acc D Real: 99.358% 
Loss D Fake: 0.8386 (0.8814) Acc D Fake: 0.000% 
Loss D: 1.235 
Loss G: 0.5705 (0.5406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,710 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3245 (0.3695) Acc D Real: 99.353% 
Loss D Fake: 0.8362 (0.8796) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.5725 (0.5418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,717 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3674 (0.3694) Acc D Real: 99.356% 
Loss D Fake: 0.8335 (0.8779) Acc D Fake: 0.000% 
Loss D: 1.201 
Loss G: 0.5746 (0.5430) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,724 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4508 (0.3723) Acc D Real: 99.340% 
Loss D Fake: 0.8310 (0.8762) Acc D Fake: 0.000% 
Loss D: 1.282 
Loss G: 0.5762 (0.5442) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,731 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3583 (0.3718) Acc D Real: 99.330% 
Loss D Fake: 0.8289 (0.8746) Acc D Fake: 0.000% 
Loss D: 1.187 
Loss G: 0.5778 (0.5454) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,738 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3979 (0.3727) Acc D Real: 99.321% 
Loss D Fake: 0.8268 (0.8730) Acc D Fake: 0.000% 
Loss D: 1.225 
Loss G: 0.5794 (0.5465) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,745 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4174 (0.3741) Acc D Real: 99.299% 
Loss D Fake: 0.8249 (0.8715) Acc D Fake: 0.000% 
Loss D: 1.242 
Loss G: 0.5807 (0.5476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,753 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3871 (0.3745) Acc D Real: 99.297% 
Loss D Fake: 0.8233 (0.8700) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.5820 (0.5487) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,760 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3206 (0.3729) Acc D Real: 99.233% 
Loss D Fake: 0.8214 (0.8685) Acc D Fake: 0.000% 
Loss D: 1.142 
Loss G: 0.5838 (0.5498) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,767 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3881 (0.3733) Acc D Real: 99.223% 
Loss D Fake: 0.8190 (0.8670) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5856 (0.5508) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,775 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3592 (0.3729) Acc D Real: 99.213% 
Loss D Fake: 0.8166 (0.8656) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.5876 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,784 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3926 (0.3735) Acc D Real: 99.203% 
Loss D Fake: 0.8141 (0.8642) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5895 (0.5529) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,792 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4169 (0.3747) Acc D Real: 99.185% 
Loss D Fake: 0.8118 (0.8627) Acc D Fake: 0.000% 
Loss D: 1.229 
Loss G: 0.5911 (0.5539) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,800 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4077 (0.3755) Acc D Real: 99.132% 
Loss D Fake: 0.8099 (0.8614) Acc D Fake: 0.000% 
Loss D: 1.218 
Loss G: 0.5926 (0.5550) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,807 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3482 (0.3748) Acc D Real: 99.125% 
Loss D Fake: 0.8079 (0.8600) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.5944 (0.5560) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,815 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3854 (0.3751) Acc D Real: 99.124% 
Loss D Fake: 0.8056 (0.8586) Acc D Fake: 0.000% 
Loss D: 1.191 
Loss G: 0.5962 (0.5570) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,822 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3663 (0.3749) Acc D Real: 99.122% 
Loss D Fake: 0.8034 (0.8573) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.5980 (0.5580) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,830 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4058 (0.3756) Acc D Real: 99.108% 
Loss D Fake: 0.8012 (0.8559) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5997 (0.5590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,838 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3906 (0.3760) Acc D Real: 99.113% 
Loss D Fake: 0.7992 (0.8546) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6013 (0.5599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,845 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4281 (0.3771) Acc D Real: 99.100% 
Loss D Fake: 0.7973 (0.8533) Acc D Fake: 0.000% 
Loss D: 1.225 
Loss G: 0.6026 (0.5609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,853 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3784 (0.3772) Acc D Real: 99.103% 
Loss D Fake: 0.7958 (0.8520) Acc D Fake: 0.000% 
Loss D: 1.174 
Loss G: 0.6039 (0.5619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,860 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3733 (0.3771) Acc D Real: 99.096% 
Loss D Fake: 0.7941 (0.8508) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.6053 (0.5628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,867 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3629 (0.3768) Acc D Real: 99.066% 
Loss D Fake: 0.7923 (0.8495) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6069 (0.5638) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,875 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3788 (0.3768) Acc D Real: 99.071% 
Loss D Fake: 0.7903 (0.8483) Acc D Fake: 0.000% 
Loss D: 1.169 
Loss G: 0.6085 (0.5647) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,882 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3437 (0.3762) Acc D Real: 99.073% 
Loss D Fake: 0.7883 (0.8471) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6104 (0.5656) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,890 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4053 (0.3767) Acc D Real: 99.057% 
Loss D Fake: 0.7861 (0.8459) Acc D Fake: 0.000% 
Loss D: 1.191 
Loss G: 0.6121 (0.5665) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,897 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4039 (0.3773) Acc D Real: 99.059% 
Loss D Fake: 0.7841 (0.8447) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.6137 (0.5675) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,905 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3435 (0.3766) Acc D Real: 99.065% 
Loss D Fake: 0.7822 (0.8435) Acc D Fake: 0.000% 
Loss D: 1.126 
Loss G: 0.6154 (0.5684) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,913 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3705 (0.3765) Acc D Real: 99.066% 
Loss D Fake: 0.7801 (0.8423) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6172 (0.5693) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,921 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3498 (0.3760) Acc D Real: 99.065% 
Loss D Fake: 0.7780 (0.8411) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6191 (0.5702) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,928 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3765 (0.3760) Acc D Real: 99.072% 
Loss D Fake: 0.7757 (0.8399) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6210 (0.5712) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,936 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4050 (0.3765) Acc D Real: 99.073% 
Loss D Fake: 0.7736 (0.8387) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6227 (0.5721) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,943 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3768 (0.3765) Acc D Real: 99.075% 
Loss D Fake: 0.7717 (0.8375) Acc D Fake: 0.000% 
Loss D: 1.149 
Loss G: 0.6243 (0.5730) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,951 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3939 (0.3768) Acc D Real: 99.060% 
Loss D Fake: 0.7698 (0.8364) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6259 (0.5739) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,958 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4080 (0.3774) Acc D Real: 99.060% 
Loss D Fake: 0.7681 (0.8352) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6273 (0.5748) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,966 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3770 (0.3774) Acc D Real: 99.050% 
Loss D Fake: 0.7665 (0.8341) Acc D Fake: 0.000% 
Loss D: 1.144 
Loss G: 0.6286 (0.5757) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,974 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3924 (0.3776) Acc D Real: 99.045% 
Loss D Fake: 0.7650 (0.8329) Acc D Fake: 0.000% 
Loss D: 1.157 
Loss G: 0.6299 (0.5766) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,981 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4249 (0.3784) Acc D Real: 99.044% 
Loss D Fake: 0.7636 (0.8318) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6310 (0.5775) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,989 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4568 (0.3796) Acc D Real: 99.043% 
Loss D Fake: 0.7626 (0.8307) Acc D Fake: 0.000% 
Loss D: 1.219 
Loss G: 0.6316 (0.5783) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:08,997 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4018 (0.3800) Acc D Real: 99.038% 
Loss D Fake: 0.7621 (0.8296) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6321 (0.5792) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,004 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3735 (0.3799) Acc D Real: 99.040% 
Loss D Fake: 0.7614 (0.8286) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6328 (0.5800) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,012 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3723 (0.3797) Acc D Real: 99.041% 
Loss D Fake: 0.7605 (0.8275) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6338 (0.5808) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,021 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4916 (0.3814) Acc D Real: 99.042% 
Loss D Fake: 0.7596 (0.8265) Acc D Fake: 0.000% 
Loss D: 1.251 
Loss G: 0.6342 (0.5816) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,030 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3915 (0.3816) Acc D Real: 99.046% 
Loss D Fake: 0.7593 (0.8255) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6345 (0.5824) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,037 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4039 (0.3819) Acc D Real: 99.038% 
Loss D Fake: 0.7589 (0.8246) Acc D Fake: 0.000% 
Loss D: 1.163 
Loss G: 0.6348 (0.5831) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,045 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4527 (0.3829) Acc D Real: 99.034% 
Loss D Fake: 0.7585 (0.8236) Acc D Fake: 0.000% 
Loss D: 1.211 
Loss G: 0.6350 (0.5839) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,052 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4135 (0.3833) Acc D Real: 99.028% 
Loss D Fake: 0.7585 (0.8227) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6350 (0.5846) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,060 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3340 (0.3826) Acc D Real: 99.034% 
Loss D Fake: 0.7582 (0.8218) Acc D Fake: 0.000% 
Loss D: 1.092 
Loss G: 0.6355 (0.5853) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,067 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3958 (0.3828) Acc D Real: 99.036% 
Loss D Fake: 0.7576 (0.8209) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6361 (0.5860) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,075 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4557 (0.3838) Acc D Real: 99.032% 
Loss D Fake: 0.7570 (0.8201) Acc D Fake: 0.000% 
Loss D: 1.213 
Loss G: 0.6364 (0.5867) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,083 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4070 (0.3841) Acc D Real: 99.028% 
Loss D Fake: 0.7568 (0.8192) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6366 (0.5874) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,091 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3598 (0.3838) Acc D Real: 99.033% 
Loss D Fake: 0.7565 (0.8184) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6370 (0.5880) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,098 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4064 (0.3841) Acc D Real: 99.036% 
Loss D Fake: 0.7560 (0.8176) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6374 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,106 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3993 (0.3843) Acc D Real: 99.038% 
Loss D Fake: 0.7555 (0.8168) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6379 (0.5893) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,114 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4129 (0.3847) Acc D Real: 99.038% 
Loss D Fake: 0.7550 (0.8160) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6383 (0.5899) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,122 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4018 (0.3849) Acc D Real: 99.041% 
Loss D Fake: 0.7545 (0.8153) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.6387 (0.5905) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,130 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3621 (0.3846) Acc D Real: 99.045% 
Loss D Fake: 0.7540 (0.8145) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6393 (0.5911) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,137 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3413 (0.3841) Acc D Real: 99.050% 
Loss D Fake: 0.7531 (0.8137) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6403 (0.5917) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,145 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4016 (0.3843) Acc D Real: 99.039% 
Loss D Fake: 0.7519 (0.8130) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6413 (0.5923) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,152 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4293 (0.3848) Acc D Real: 99.040% 
Loss D Fake: 0.7509 (0.8123) Acc D Fake: 0.000% 
Loss D: 1.180 
Loss G: 0.6422 (0.5929) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,159 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4138 (0.3852) Acc D Real: 99.040% 
Loss D Fake: 0.7501 (0.8115) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6428 (0.5935) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,167 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4034 (0.3854) Acc D Real: 99.043% 
Loss D Fake: 0.7494 (0.8108) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6434 (0.5941) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,174 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4416 (0.3860) Acc D Real: 99.041% 
Loss D Fake: 0.7488 (0.8101) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6438 (0.5946) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,182 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4701 (0.3870) Acc D Real: 99.035% 
Loss D Fake: 0.7487 (0.8094) Acc D Fake: 0.000% 
Loss D: 1.219 
Loss G: 0.6437 (0.5952) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,189 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4402 (0.3876) Acc D Real: 99.037% 
Loss D Fake: 0.7489 (0.8087) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6433 (0.5957) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,196 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4783 (0.3886) Acc D Real: 99.036% 
Loss D Fake: 0.7495 (0.8081) Acc D Fake: 0.000% 
Loss D: 1.228 
Loss G: 0.6426 (0.5963) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,203 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4601 (0.3894) Acc D Real: 99.038% 
Loss D Fake: 0.7504 (0.8074) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.6418 (0.5968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,211 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4040 (0.3895) Acc D Real: 99.040% 
Loss D Fake: 0.7513 (0.8068) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6411 (0.5972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,218 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3560 (0.3892) Acc D Real: 99.037% 
Loss D Fake: 0.7518 (0.8062) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6409 (0.5977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,225 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4407 (0.3897) Acc D Real: 99.039% 
Loss D Fake: 0.7519 (0.8056) Acc D Fake: 0.000% 
Loss D: 1.193 
Loss G: 0.6407 (0.5982) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,232 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4278 (0.3901) Acc D Real: 99.038% 
Loss D Fake: 0.7523 (0.8051) Acc D Fake: 0.000% 
Loss D: 1.180 
Loss G: 0.6403 (0.5986) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,240 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4056 (0.3903) Acc D Real: 99.041% 
Loss D Fake: 0.7527 (0.8045) Acc D Fake: 0.000% 
Loss D: 1.158 
Loss G: 0.6399 (0.5990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,247 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3678 (0.3900) Acc D Real: 99.046% 
Loss D Fake: 0.7530 (0.8040) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6399 (0.5995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,255 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4015 (0.3902) Acc D Real: 99.049% 
Loss D Fake: 0.7529 (0.8035) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6400 (0.5999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,263 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3491 (0.3897) Acc D Real: 99.053% 
Loss D Fake: 0.7526 (0.8030) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6404 (0.6003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,270 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3708 (0.3895) Acc D Real: 99.052% 
Loss D Fake: 0.7520 (0.8025) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6411 (0.6007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,278 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3291 (0.3890) Acc D Real: 99.054% 
Loss D Fake: 0.7511 (0.8020) Acc D Fake: 0.000% 
Loss D: 1.080 
Loss G: 0.6421 (0.6011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,285 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4281 (0.3893) Acc D Real: 99.054% 
Loss D Fake: 0.7499 (0.8014) Acc D Fake: 0.000% 
Loss D: 1.178 
Loss G: 0.6430 (0.6015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,292 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4030 (0.3895) Acc D Real: 99.050% 
Loss D Fake: 0.7491 (0.8009) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6437 (0.6019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,300 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4276 (0.3898) Acc D Real: 99.052% 
Loss D Fake: 0.7484 (0.8004) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6442 (0.6023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,307 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3375 (0.3893) Acc D Real: 99.057% 
Loss D Fake: 0.7477 (0.7999) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6451 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,315 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4170 (0.3896) Acc D Real: 99.060% 
Loss D Fake: 0.7468 (0.7994) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6458 (0.6032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,322 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3783 (0.3895) Acc D Real: 99.064% 
Loss D Fake: 0.7460 (0.7989) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6466 (0.6036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,330 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4624 (0.3902) Acc D Real: 99.063% 
Loss D Fake: 0.7453 (0.7984) Acc D Fake: 0.000% 
Loss D: 1.208 
Loss G: 0.6469 (0.6040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,337 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3523 (0.3898) Acc D Real: 99.068% 
Loss D Fake: 0.7449 (0.7979) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6475 (0.6044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,345 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4710 (0.3906) Acc D Real: 99.069% 
Loss D Fake: 0.7443 (0.7975) Acc D Fake: 0.000% 
Loss D: 1.215 
Loss G: 0.6477 (0.6048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,352 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4035 (0.3907) Acc D Real: 99.070% 
Loss D Fake: 0.7442 (0.7970) Acc D Fake: 0.000% 
Loss D: 1.148 
Loss G: 0.6479 (0.6051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,360 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4662 (0.3913) Acc D Real: 99.070% 
Loss D Fake: 0.7441 (0.7965) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.6477 (0.6055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,367 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4180 (0.3916) Acc D Real: 99.070% 
Loss D Fake: 0.7445 (0.7960) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6474 (0.6059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,374 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3960 (0.3916) Acc D Real: 99.072% 
Loss D Fake: 0.7448 (0.7956) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6472 (0.6063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,381 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4016 (0.3917) Acc D Real: 99.073% 
Loss D Fake: 0.7449 (0.7951) Acc D Fake: 0.000% 
Loss D: 1.146 
Loss G: 0.6472 (0.6066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,389 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3550 (0.3914) Acc D Real: 99.076% 
Loss D Fake: 0.7448 (0.7947) Acc D Fake: 0.000% 
Loss D: 1.100 
Loss G: 0.6475 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,396 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3524 (0.3911) Acc D Real: 99.075% 
Loss D Fake: 0.7442 (0.7943) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6481 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,403 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4341 (0.3914) Acc D Real: 99.076% 
Loss D Fake: 0.7434 (0.7939) Acc D Fake: 0.000% 
Loss D: 1.178 
Loss G: 0.6487 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,410 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3954 (0.3915) Acc D Real: 99.078% 
Loss D Fake: 0.7429 (0.7934) Acc D Fake: 0.000% 
Loss D: 1.138 
Loss G: 0.6492 (0.6080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,417 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3156 (0.3908) Acc D Real: 99.082% 
Loss D Fake: 0.7422 (0.7930) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6501 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,425 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3578 (0.3906) Acc D Real: 99.085% 
Loss D Fake: 0.7411 (0.7926) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6511 (0.6087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,432 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3930 (0.3906) Acc D Real: 99.087% 
Loss D Fake: 0.7400 (0.7921) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6521 (0.6091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,439 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3963 (0.3906) Acc D Real: 99.088% 
Loss D Fake: 0.7390 (0.7917) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6529 (0.6094) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,446 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4172 (0.3908) Acc D Real: 99.091% 
Loss D Fake: 0.7383 (0.7913) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6535 (0.6098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,453 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3415 (0.3904) Acc D Real: 99.094% 
Loss D Fake: 0.7376 (0.7908) Acc D Fake: 0.000% 
Loss D: 1.079 
Loss G: 0.6542 (0.6101) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,461 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4150 (0.3906) Acc D Real: 99.096% 
Loss D Fake: 0.7367 (0.7904) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6549 (0.6105) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,468 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4446 (0.3911) Acc D Real: 99.095% 
Loss D Fake: 0.7361 (0.7900) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6553 (0.6108) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,475 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4369 (0.3914) Acc D Real: 99.097% 
Loss D Fake: 0.7359 (0.7896) Acc D Fake: 0.000% 
Loss D: 1.173 
Loss G: 0.6554 (0.6112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,482 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4668 (0.3920) Acc D Real: 99.100% 
Loss D Fake: 0.7360 (0.7891) Acc D Fake: 0.000% 
Loss D: 1.203 
Loss G: 0.6550 (0.6115) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,490 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4164 (0.3922) Acc D Real: 99.102% 
Loss D Fake: 0.7366 (0.7887) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6545 (0.6119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,497 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3729 (0.3920) Acc D Real: 99.104% 
Loss D Fake: 0.7370 (0.7884) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6544 (0.6122) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,504 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3533 (0.3917) Acc D Real: 99.109% 
Loss D Fake: 0.7369 (0.7880) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6546 (0.6125) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,513 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4328 (0.3921) Acc D Real: 99.111% 
Loss D Fake: 0.7366 (0.7876) Acc D Fake: 0.000% 
Loss D: 1.169 
Loss G: 0.6547 (0.6128) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,521 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4028 (0.3921) Acc D Real: 99.113% 
Loss D Fake: 0.7366 (0.7872) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6548 (0.6131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,529 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3795 (0.3920) Acc D Real: 99.113% 
Loss D Fake: 0.7365 (0.7868) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6550 (0.6135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,537 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4287 (0.3923) Acc D Real: 99.112% 
Loss D Fake: 0.7363 (0.7864) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6550 (0.6138) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,544 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3863 (0.3923) Acc D Real: 99.113% 
Loss D Fake: 0.7362 (0.7861) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6551 (0.6141) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,552 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4505 (0.3927) Acc D Real: 99.115% 
Loss D Fake: 0.7362 (0.7857) Acc D Fake: 0.000% 
Loss D: 1.187 
Loss G: 0.6550 (0.6144) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,560 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4795 (0.3933) Acc D Real: 99.115% 
Loss D Fake: 0.7367 (0.7854) Acc D Fake: 0.000% 
Loss D: 1.216 
Loss G: 0.6543 (0.6146) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,568 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4832 (0.3940) Acc D Real: 99.118% 
Loss D Fake: 0.7376 (0.7850) Acc D Fake: 0.000% 
Loss D: 1.221 
Loss G: 0.6532 (0.6149) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,575 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3422 (0.3936) Acc D Real: 99.119% 
Loss D Fake: 0.7387 (0.7847) Acc D Fake: 0.000% 
Loss D: 1.081 
Loss G: 0.6526 (0.6152) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,583 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4513 (0.3940) Acc D Real: 99.122% 
Loss D Fake: 0.7392 (0.7844) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6520 (0.6154) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,591 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4199 (0.3942) Acc D Real: 99.125% 
Loss D Fake: 0.7399 (0.7841) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6513 (0.6157) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,599 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3112 (0.3936) Acc D Real: 99.128% 
Loss D Fake: 0.7404 (0.7838) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6513 (0.6159) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,606 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4256 (0.3938) Acc D Real: 99.130% 
Loss D Fake: 0.7403 (0.7835) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6513 (0.6162) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,614 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4143 (0.3940) Acc D Real: 99.132% 
Loss D Fake: 0.7403 (0.7832) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6512 (0.6164) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,622 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3968 (0.3940) Acc D Real: 99.134% 
Loss D Fake: 0.7404 (0.7829) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6511 (0.6167) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,629 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3703 (0.3938) Acc D Real: 99.137% 
Loss D Fake: 0.7405 (0.7826) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.6512 (0.6169) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,637 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3152 (0.3933) Acc D Real: 99.140% 
Loss D Fake: 0.7402 (0.7823) Acc D Fake: 0.000% 
Loss D: 1.055 
Loss G: 0.6518 (0.6171) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,644 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3174 (0.3928) Acc D Real: 99.145% 
Loss D Fake: 0.7392 (0.7820) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.6529 (0.6174) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,652 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3825 (0.3927) Acc D Real: 99.148% 
Loss D Fake: 0.7379 (0.7817) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6541 (0.6176) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,660 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4335 (0.3930) Acc D Real: 99.149% 
Loss D Fake: 0.7368 (0.7814) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.6549 (0.6179) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,668 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4612 (0.3934) Acc D Real: 99.146% 
Loss D Fake: 0.7361 (0.7811) Acc D Fake: 0.000% 
Loss D: 1.197 
Loss G: 0.6553 (0.6181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,676 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4175 (0.3936) Acc D Real: 99.149% 
Loss D Fake: 0.7359 (0.7808) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6554 (0.6183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,683 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3840 (0.3935) Acc D Real: 99.152% 
Loss D Fake: 0.7358 (0.7805) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6555 (0.6186) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,691 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4321 (0.3938) Acc D Real: 99.154% 
Loss D Fake: 0.7357 (0.7803) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6555 (0.6188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,699 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4015 (0.3938) Acc D Real: 99.157% 
Loss D Fake: 0.7358 (0.7800) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6554 (0.6191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,706 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3044 (0.3933) Acc D Real: 99.157% 
Loss D Fake: 0.7356 (0.7797) Acc D Fake: 0.000% 
Loss D: 1.040 
Loss G: 0.6560 (0.6193) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:09,932 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.854 | Generator Loss: 0.656 | Avg: 1.510 
2023-03-02 01:52:09,955 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.944 | Generator Loss: 0.656 | Avg: 1.600 
2023-03-02 01:52:09,978 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.920 | Generator Loss: 0.656 | Avg: 1.576 
2023-03-02 01:52:10,004 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.896 | Generator Loss: 0.656 | Avg: 1.552 
2023-03-02 01:52:10,030 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.895 | Generator Loss: 0.656 | Avg: 1.551 
2023-03-02 01:52:10,056 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.938 | Generator Loss: 0.656 | Avg: 1.594 
2023-03-02 01:52:10,082 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.965 | Generator Loss: 0.656 | Avg: 1.621 
2023-03-02 01:52:10,113 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.004 | Generator Loss: 0.656 | Avg: 1.660 
2023-03-02 01:52:10,139 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.026 | Generator Loss: 0.656 | Avg: 1.682 
2023-03-02 01:52:10,165 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.062 | Generator Loss: 0.656 | Avg: 1.718 
2023-03-02 01:52:10,191 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.089 | Generator Loss: 0.656 | Avg: 1.745 
2023-03-02 01:52:10,218 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.114 | Generator Loss: 0.656 | Avg: 1.770 
2023-03-02 01:52:10,243 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.134 | Generator Loss: 0.656 | Avg: 1.790 
2023-03-02 01:52:10,270 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.150 | Generator Loss: 0.656 | Avg: 1.806 
2023-03-02 01:52:10,297 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.138 | Generator Loss: 0.656 | Avg: 1.794 
2023-03-02 01:52:10,323 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.125 | Generator Loss: 0.656 | Avg: 1.781 
2023-03-02 01:52:10,348 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.112 | Generator Loss: 0.656 | Avg: 1.768 
2023-03-02 01:52:10,382 -                train: [    INFO] - 
Epoch: 17/20
2023-03-02 01:52:10,563 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4032 (0.4052) Acc D Real: 99.323% 
Loss D Fake: 0.7342 (0.7345) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6572 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,573 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5374 (0.4493) Acc D Real: 99.253% 
Loss D Fake: 0.7339 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.271 
Loss G: 0.6569 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,581 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3301 (0.4195) Acc D Real: 99.362% 
Loss D Fake: 0.7344 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6568 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,616 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4379 (0.4232) Acc D Real: 99.417% 
Loss D Fake: 0.7344 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6566 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,623 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3655 (0.4136) Acc D Real: 99.427% 
Loss D Fake: 0.7346 (0.7344) Acc D Fake: 0.000% 
Loss D: 1.100 
Loss G: 0.6566 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,630 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3264 (0.4011) Acc D Real: 99.479% 
Loss D Fake: 0.7343 (0.7344) Acc D Fake: 0.000% 
Loss D: 1.061 
Loss G: 0.6571 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,638 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3871 (0.3993) Acc D Real: 99.486% 
Loss D Fake: 0.7337 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6577 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,645 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4065 (0.4001) Acc D Real: 99.479% 
Loss D Fake: 0.7331 (0.7342) Acc D Fake: 0.000% 
Loss D: 1.140 
Loss G: 0.6582 (0.6571) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,653 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3886 (0.3990) Acc D Real: 99.479% 
Loss D Fake: 0.7325 (0.7340) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6588 (0.6573) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,660 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3324 (0.3929) Acc D Real: 99.498% 
Loss D Fake: 0.7318 (0.7338) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6596 (0.6575) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,668 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3968 (0.3933) Acc D Real: 99.510% 
Loss D Fake: 0.7309 (0.7336) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6604 (0.6577) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,675 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4691 (0.3991) Acc D Real: 99.479% 
Loss D Fake: 0.7302 (0.7333) Acc D Fake: 0.000% 
Loss D: 1.199 
Loss G: 0.6607 (0.6579) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,682 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4188 (0.4005) Acc D Real: 99.479% 
Loss D Fake: 0.7302 (0.7331) Acc D Fake: 0.000% 
Loss D: 1.149 
Loss G: 0.6607 (0.6581) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,689 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3590 (0.3977) Acc D Real: 99.476% 
Loss D Fake: 0.7302 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6608 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,696 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4113 (0.3986) Acc D Real: 99.473% 
Loss D Fake: 0.7300 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6608 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,704 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4343 (0.4007) Acc D Real: 99.479% 
Loss D Fake: 0.7301 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6607 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,711 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3870 (0.3999) Acc D Real: 99.479% 
Loss D Fake: 0.7303 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6605 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,718 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3799 (0.3989) Acc D Real: 99.490% 
Loss D Fake: 0.7304 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6605 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,725 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4637 (0.4021) Acc D Real: 99.477% 
Loss D Fake: 0.7305 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6601 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,732 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4193 (0.4029) Acc D Real: 99.484% 
Loss D Fake: 0.7310 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6596 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,739 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4381 (0.4045) Acc D Real: 99.496% 
Loss D Fake: 0.7316 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.6589 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,746 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4485 (0.4064) Acc D Real: 99.500% 
Loss D Fake: 0.7325 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6580 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,753 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3638 (0.4047) Acc D Real: 99.507% 
Loss D Fake: 0.7333 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6575 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,760 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3818 (0.4037) Acc D Real: 99.513% 
Loss D Fake: 0.7338 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6572 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,767 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3976 (0.4035) Acc D Real: 99.517% 
Loss D Fake: 0.7340 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6570 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,774 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4029 (0.4035) Acc D Real: 99.516% 
Loss D Fake: 0.7341 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6569 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,781 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3819 (0.4027) Acc D Real: 99.513% 
Loss D Fake: 0.7342 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6569 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,789 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3757 (0.4018) Acc D Real: 99.519% 
Loss D Fake: 0.7341 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6570 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,797 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.4320 (0.4028) Acc D Real: 99.523% 
Loss D Fake: 0.7340 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6570 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,804 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3831 (0.4022) Acc D Real: 99.530% 
Loss D Fake: 0.7341 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6570 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,812 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.4713 (0.4043) Acc D Real: 99.528% 
Loss D Fake: 0.7342 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.205 
Loss G: 0.6566 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,819 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3816 (0.4036) Acc D Real: 99.528% 
Loss D Fake: 0.7346 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6564 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,827 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3501 (0.4021) Acc D Real: 99.534% 
Loss D Fake: 0.7347 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6565 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,835 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3882 (0.4017) Acc D Real: 99.540% 
Loss D Fake: 0.7345 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6568 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,842 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3826 (0.4011) Acc D Real: 99.544% 
Loss D Fake: 0.7341 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6571 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,850 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3284 (0.3992) Acc D Real: 99.552% 
Loss D Fake: 0.7336 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.062 
Loss G: 0.6578 (0.6581) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,857 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3941 (0.3990) Acc D Real: 99.559% 
Loss D Fake: 0.7328 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6584 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,865 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3255 (0.3971) Acc D Real: 99.561% 
Loss D Fake: 0.7321 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6594 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,872 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4767 (0.3991) Acc D Real: 99.556% 
Loss D Fake: 0.7312 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.208 
Loss G: 0.6598 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,880 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4414 (0.4002) Acc D Real: 99.559% 
Loss D Fake: 0.7311 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.173 
Loss G: 0.6597 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,887 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4397 (0.4011) Acc D Real: 99.559% 
Loss D Fake: 0.7313 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.171 
Loss G: 0.6593 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,895 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3813 (0.4006) Acc D Real: 99.560% 
Loss D Fake: 0.7317 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6590 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,902 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3447 (0.3994) Acc D Real: 99.564% 
Loss D Fake: 0.7319 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.077 
Loss G: 0.6591 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,910 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3748 (0.3988) Acc D Real: 99.565% 
Loss D Fake: 0.7316 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6595 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,917 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2894 (0.3965) Acc D Real: 99.570% 
Loss D Fake: 0.7310 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.6604 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,925 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4298 (0.3972) Acc D Real: 99.573% 
Loss D Fake: 0.7299 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6611 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,933 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4354 (0.3980) Acc D Real: 99.574% 
Loss D Fake: 0.7294 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6614 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,940 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4521 (0.3991) Acc D Real: 99.575% 
Loss D Fake: 0.7293 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6613 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,948 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3774 (0.3986) Acc D Real: 99.576% 
Loss D Fake: 0.7295 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.107 
Loss G: 0.6612 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,955 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4032 (0.3987) Acc D Real: 99.580% 
Loss D Fake: 0.7295 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6612 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,963 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4113 (0.3990) Acc D Real: 99.582% 
Loss D Fake: 0.7296 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6610 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,970 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4469 (0.3999) Acc D Real: 99.586% 
Loss D Fake: 0.7299 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6606 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,978 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3430 (0.3988) Acc D Real: 99.586% 
Loss D Fake: 0.7303 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.073 
Loss G: 0.6606 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,985 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3854 (0.3986) Acc D Real: 99.587% 
Loss D Fake: 0.7302 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6607 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:10,993 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4166 (0.3989) Acc D Real: 99.589% 
Loss D Fake: 0.7301 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6607 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,000 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4766 (0.4003) Acc D Real: 99.587% 
Loss D Fake: 0.7303 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.6602 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,008 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3926 (0.4001) Acc D Real: 99.591% 
Loss D Fake: 0.7309 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6597 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,016 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3332 (0.3990) Acc D Real: 99.596% 
Loss D Fake: 0.7312 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6597 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,024 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3618 (0.3984) Acc D Real: 99.599% 
Loss D Fake: 0.7310 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6600 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,031 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3092 (0.3969) Acc D Real: 99.601% 
Loss D Fake: 0.7305 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.040 
Loss G: 0.6608 (0.6590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,039 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3895 (0.3968) Acc D Real: 99.603% 
Loss D Fake: 0.7295 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6616 (0.6590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,047 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3688 (0.3963) Acc D Real: 99.605% 
Loss D Fake: 0.7286 (0.7319) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6625 (0.6591) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,055 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3836 (0.3961) Acc D Real: 99.605% 
Loss D Fake: 0.7277 (0.7319) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.6634 (0.6591) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,063 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4125 (0.3964) Acc D Real: 99.607% 
Loss D Fake: 0.7269 (0.7318) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6639 (0.6592) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,070 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4015 (0.3965) Acc D Real: 99.607% 
Loss D Fake: 0.7264 (0.7317) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6643 (0.6593) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,077 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3678 (0.3960) Acc D Real: 99.609% 
Loss D Fake: 0.7260 (0.7316) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6648 (0.6594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,085 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4104 (0.3963) Acc D Real: 99.611% 
Loss D Fake: 0.7255 (0.7315) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6651 (0.6594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,093 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3996 (0.3963) Acc D Real: 99.611% 
Loss D Fake: 0.7252 (0.7314) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6654 (0.6595) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,100 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4281 (0.3968) Acc D Real: 99.613% 
Loss D Fake: 0.7250 (0.7313) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6654 (0.6596) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,108 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4501 (0.3975) Acc D Real: 99.610% 
Loss D Fake: 0.7252 (0.7313) Acc D Fake: 0.000% 
Loss D: 1.175 
Loss G: 0.6650 (0.6597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,116 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3990 (0.3975) Acc D Real: 99.612% 
Loss D Fake: 0.7257 (0.7312) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6646 (0.6598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,124 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3724 (0.3972) Acc D Real: 99.615% 
Loss D Fake: 0.7261 (0.7311) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6643 (0.6598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,131 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3766 (0.3969) Acc D Real: 99.617% 
Loss D Fake: 0.7262 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.103 
Loss G: 0.6643 (0.6599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,139 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3802 (0.3967) Acc D Real: 99.619% 
Loss D Fake: 0.7263 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6643 (0.6599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,147 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4636 (0.3976) Acc D Real: 99.623% 
Loss D Fake: 0.7264 (0.7309) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6639 (0.6600) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,154 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3629 (0.3971) Acc D Real: 99.626% 
Loss D Fake: 0.7268 (0.7309) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6637 (0.6600) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,162 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2715 (0.3955) Acc D Real: 99.627% 
Loss D Fake: 0.7267 (0.7308) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6642 (0.6601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,169 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4329 (0.3960) Acc D Real: 99.629% 
Loss D Fake: 0.7260 (0.7308) Acc D Fake: 0.000% 
Loss D: 1.159 
Loss G: 0.6646 (0.6601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,177 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3749 (0.3957) Acc D Real: 99.631% 
Loss D Fake: 0.7257 (0.7307) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6650 (0.6602) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,185 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3799 (0.3955) Acc D Real: 99.633% 
Loss D Fake: 0.7253 (0.7306) Acc D Fake: 0.000% 
Loss D: 1.105 
Loss G: 0.6655 (0.6603) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,192 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4223 (0.3958) Acc D Real: 99.632% 
Loss D Fake: 0.7248 (0.7306) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6657 (0.6603) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,200 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3445 (0.3952) Acc D Real: 99.633% 
Loss D Fake: 0.7245 (0.7305) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6661 (0.6604) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,208 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3289 (0.3944) Acc D Real: 99.635% 
Loss D Fake: 0.7239 (0.7304) Acc D Fake: 0.000% 
Loss D: 1.053 
Loss G: 0.6669 (0.6605) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,215 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3215 (0.3936) Acc D Real: 99.637% 
Loss D Fake: 0.7229 (0.7303) Acc D Fake: 0.000% 
Loss D: 1.044 
Loss G: 0.6681 (0.6606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,223 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3072 (0.3926) Acc D Real: 99.638% 
Loss D Fake: 0.7215 (0.7302) Acc D Fake: 0.000% 
Loss D: 1.029 
Loss G: 0.6697 (0.6607) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,230 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3840 (0.3925) Acc D Real: 99.639% 
Loss D Fake: 0.7198 (0.7301) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6712 (0.6608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,238 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.5043 (0.3938) Acc D Real: 99.638% 
Loss D Fake: 0.7186 (0.7300) Acc D Fake: 0.000% 
Loss D: 1.223 
Loss G: 0.6717 (0.6609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,245 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4008 (0.3938) Acc D Real: 99.640% 
Loss D Fake: 0.7183 (0.7298) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6720 (0.6611) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,253 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4542 (0.3945) Acc D Real: 99.639% 
Loss D Fake: 0.7183 (0.7297) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6717 (0.6612) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,261 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3496 (0.3940) Acc D Real: 99.641% 
Loss D Fake: 0.7185 (0.7296) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6717 (0.6613) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,269 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3899 (0.3940) Acc D Real: 99.641% 
Loss D Fake: 0.7184 (0.7295) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6718 (0.6614) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,276 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4179 (0.3942) Acc D Real: 99.643% 
Loss D Fake: 0.7184 (0.7293) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6717 (0.6615) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,284 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4706 (0.3950) Acc D Real: 99.642% 
Loss D Fake: 0.7187 (0.7292) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6711 (0.6616) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,292 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3885 (0.3950) Acc D Real: 99.643% 
Loss D Fake: 0.7195 (0.7291) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6704 (0.6617) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,300 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4608 (0.3957) Acc D Real: 99.642% 
Loss D Fake: 0.7202 (0.7290) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6696 (0.6618) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,308 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.5416 (0.3972) Acc D Real: 99.639% 
Loss D Fake: 0.7214 (0.7290) Acc D Fake: 0.000% 
Loss D: 1.263 
Loss G: 0.6679 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,316 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3247 (0.3964) Acc D Real: 99.640% 
Loss D Fake: 0.7232 (0.7289) Acc D Fake: 0.000% 
Loss D: 1.048 
Loss G: 0.6666 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,323 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3850 (0.3963) Acc D Real: 99.642% 
Loss D Fake: 0.7242 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.6659 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,331 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4357 (0.3967) Acc D Real: 99.644% 
Loss D Fake: 0.7249 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.6651 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,338 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4055 (0.3968) Acc D Real: 99.646% 
Loss D Fake: 0.7258 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.131 
Loss G: 0.6642 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,345 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3603 (0.3964) Acc D Real: 99.647% 
Loss D Fake: 0.7266 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.087 
Loss G: 0.6636 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,353 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2971 (0.3955) Acc D Real: 99.649% 
Loss D Fake: 0.7269 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.024 
Loss G: 0.6638 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,360 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3099 (0.3946) Acc D Real: 99.651% 
Loss D Fake: 0.7264 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.036 
Loss G: 0.6646 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,368 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3764 (0.3945) Acc D Real: 99.653% 
Loss D Fake: 0.7254 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6654 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,375 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3149 (0.3937) Acc D Real: 99.655% 
Loss D Fake: 0.7245 (0.7286) Acc D Fake: 0.000% 
Loss D: 1.039 
Loss G: 0.6666 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,383 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3337 (0.3932) Acc D Real: 99.656% 
Loss D Fake: 0.7230 (0.7286) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.6680 (0.6622) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,390 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3972 (0.3932) Acc D Real: 99.656% 
Loss D Fake: 0.7216 (0.7285) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6692 (0.6622) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,398 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4561 (0.3938) Acc D Real: 99.655% 
Loss D Fake: 0.7206 (0.7285) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6698 (0.6623) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,405 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4199 (0.3940) Acc D Real: 99.657% 
Loss D Fake: 0.7203 (0.7284) Acc D Fake: 0.000% 
Loss D: 1.140 
Loss G: 0.6700 (0.6624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,412 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3842 (0.3939) Acc D Real: 99.657% 
Loss D Fake: 0.7201 (0.7283) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6701 (0.6625) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,420 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3060 (0.3931) Acc D Real: 99.660% 
Loss D Fake: 0.7198 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.026 
Loss G: 0.6707 (0.6625) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,427 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4159 (0.3933) Acc D Real: 99.662% 
Loss D Fake: 0.7192 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6712 (0.6626) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,434 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3986 (0.3934) Acc D Real: 99.664% 
Loss D Fake: 0.7188 (0.7281) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6714 (0.6627) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,441 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3478 (0.3930) Acc D Real: 99.665% 
Loss D Fake: 0.7185 (0.7280) Acc D Fake: 0.000% 
Loss D: 1.066 
Loss G: 0.6719 (0.6628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,449 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3825 (0.3929) Acc D Real: 99.667% 
Loss D Fake: 0.7180 (0.7279) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6723 (0.6628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,456 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4425 (0.3933) Acc D Real: 99.667% 
Loss D Fake: 0.7177 (0.7278) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6724 (0.6629) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,464 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3659 (0.3931) Acc D Real: 99.670% 
Loss D Fake: 0.7176 (0.7277) Acc D Fake: 0.000% 
Loss D: 1.084 
Loss G: 0.6725 (0.6630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,471 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3641 (0.3928) Acc D Real: 99.671% 
Loss D Fake: 0.7174 (0.7276) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6728 (0.6631) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,478 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3538 (0.3925) Acc D Real: 99.671% 
Loss D Fake: 0.7171 (0.7276) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.6733 (0.6632) Acc G: 99.583% 
LR: 2.000e-04 

2023-03-02 01:52:11,486 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3612 (0.3923) Acc D Real: 99.557% 
Loss D Fake: 0.7165 (0.7275) Acc D Fake: 0.413% 
Loss D: 1.078 
Loss G: 0.6739 (0.6633) Acc G: 99.091% 
LR: 2.000e-04 

2023-03-02 01:52:11,493 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3945 (0.3923) Acc D Real: 99.381% 
Loss D Fake: 0.7159 (0.7274) Acc D Fake: 0.902% 
Loss D: 1.110 
Loss G: 0.6744 (0.6634) Acc G: 98.579% 
LR: 2.000e-04 

2023-03-02 01:52:11,501 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4318 (0.3926) Acc D Real: 99.175% 
Loss D Fake: 0.7154 (0.7273) Acc D Fake: 1.396% 
Loss D: 1.147 
Loss G: 0.6747 (0.6634) Acc G: 98.062% 
LR: 2.000e-04 

2023-03-02 01:52:11,508 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3787 (0.3925) Acc D Real: 98.950% 
Loss D Fake: 0.7152 (0.7272) Acc D Fake: 1.895% 
Loss D: 1.094 
Loss G: 0.6750 (0.6635) Acc G: 97.554% 
LR: 2.000e-04 

2023-03-02 01:52:11,515 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3715 (0.3923) Acc D Real: 98.731% 
Loss D Fake: 0.7148 (0.7271) Acc D Fake: 2.400% 
Loss D: 1.086 
Loss G: 0.6754 (0.6636) Acc G: 97.040% 
LR: 2.000e-04 

2023-03-02 01:52:11,523 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3800 (0.3922) Acc D Real: 98.488% 
Loss D Fake: 0.7144 (0.7270) Acc D Fake: 2.910% 
Loss D: 1.094 
Loss G: 0.6757 (0.6637) Acc G: 96.534% 
LR: 2.000e-04 

2023-03-02 01:52:11,530 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3563 (0.3919) Acc D Real: 98.263% 
Loss D Fake: 0.7140 (0.7269) Acc D Fake: 3.412% 
Loss D: 1.070 
Loss G: 0.6762 (0.6638) Acc G: 96.024% 
LR: 2.000e-04 

2023-03-02 01:52:11,538 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3822 (0.3919) Acc D Real: 98.005% 
Loss D Fake: 0.7135 (0.7268) Acc D Fake: 3.919% 
Loss D: 1.096 
Loss G: 0.6766 (0.6639) Acc G: 95.521% 
LR: 2.000e-04 

2023-03-02 01:52:11,545 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2889 (0.3911) Acc D Real: 97.862% 
Loss D Fake: 0.7129 (0.7267) Acc D Fake: 4.419% 
Loss D: 1.002 
Loss G: 0.6776 (0.6640) Acc G: 95.013% 
LR: 2.000e-04 

2023-03-02 01:52:11,553 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3996 (0.3911) Acc D Real: 97.605% 
Loss D Fake: 0.7118 (0.7265) Acc D Fake: 4.923% 
Loss D: 1.111 
Loss G: 0.6785 (0.6641) Acc G: 94.513% 
LR: 2.000e-04 

2023-03-02 01:52:11,560 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2938 (0.3904) Acc D Real: 97.435% 
Loss D Fake: 0.7109 (0.7264) Acc D Fake: 5.433% 
Loss D: 1.005 
Loss G: 0.6797 (0.6643) Acc G: 94.008% 
LR: 2.000e-04 

2023-03-02 01:52:11,568 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3642 (0.3902) Acc D Real: 97.204% 
Loss D Fake: 0.7095 (0.7263) Acc D Fake: 5.934% 
Loss D: 1.074 
Loss G: 0.6810 (0.6644) Acc G: 93.497% 
LR: 2.000e-04 

2023-03-02 01:52:11,575 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3527 (0.3899) Acc D Real: 96.974% 
Loss D Fake: 0.7082 (0.7262) Acc D Fake: 6.441% 
Loss D: 1.061 
Loss G: 0.6823 (0.6645) Acc G: 92.995% 
LR: 2.000e-04 

2023-03-02 01:52:11,583 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3252 (0.3894) Acc D Real: 96.788% 
Loss D Fake: 0.7069 (0.7260) Acc D Fake: 6.940% 
Loss D: 1.032 
Loss G: 0.6838 (0.6647) Acc G: 92.500% 
LR: 2.000e-04 

2023-03-02 01:52:11,590 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4558 (0.3899) Acc D Real: 96.477% 
Loss D Fake: 0.7055 (0.7259) Acc D Fake: 7.432% 
Loss D: 1.161 
Loss G: 0.6847 (0.6648) Acc G: 92.000% 
LR: 2.000e-04 

2023-03-02 01:52:11,598 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4037 (0.3900) Acc D Real: 96.211% 
Loss D Fake: 0.7048 (0.7257) Acc D Fake: 7.929% 
Loss D: 1.108 
Loss G: 0.6853 (0.6650) Acc G: 91.507% 
LR: 2.000e-04 

2023-03-02 01:52:11,606 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4588 (0.3905) Acc D Real: 95.893% 
Loss D Fake: 0.7044 (0.7256) Acc D Fake: 8.418% 
Loss D: 1.163 
Loss G: 0.6853 (0.6651) Acc G: 91.022% 
LR: 2.000e-04 

2023-03-02 01:52:11,614 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3213 (0.3900) Acc D Real: 95.716% 
Loss D Fake: 0.7044 (0.7254) Acc D Fake: 8.901% 
Loss D: 1.026 
Loss G: 0.6856 (0.6653) Acc G: 90.543% 
LR: 2.000e-04 

2023-03-02 01:52:11,621 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3357 (0.3896) Acc D Real: 95.520% 
Loss D Fake: 0.7039 (0.7252) Acc D Fake: 9.376% 
Loss D: 1.040 
Loss G: 0.6863 (0.6654) Acc G: 90.072% 
LR: 2.000e-04 

2023-03-02 01:52:11,629 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3491 (0.3893) Acc D Real: 95.323% 
Loss D Fake: 0.7032 (0.7251) Acc D Fake: 9.845% 
Loss D: 1.052 
Loss G: 0.6871 (0.6656) Acc G: 89.607% 
LR: 2.000e-04 

2023-03-02 01:52:11,636 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3892 (0.3893) Acc D Real: 95.083% 
Loss D Fake: 0.7024 (0.7249) Acc D Fake: 10.307% 
Loss D: 1.092 
Loss G: 0.6878 (0.6657) Acc G: 89.149% 
LR: 2.000e-04 

2023-03-02 01:52:11,644 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3647 (0.3892) Acc D Real: 94.886% 
Loss D Fake: 0.7016 (0.7248) Acc D Fake: 10.763% 
Loss D: 1.066 
Loss G: 0.6886 (0.6659) Acc G: 88.697% 
LR: 2.000e-04 

2023-03-02 01:52:11,652 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3339 (0.3888) Acc D Real: 94.704% 
Loss D Fake: 0.7008 (0.7246) Acc D Fake: 11.212% 
Loss D: 1.035 
Loss G: 0.6896 (0.6661) Acc G: 88.252% 
LR: 2.000e-04 

2023-03-02 01:52:11,659 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4855 (0.3894) Acc D Real: 94.394% 
Loss D Fake: 0.7000 (0.7244) Acc D Fake: 11.655% 
Loss D: 1.185 
Loss G: 0.6899 (0.6662) Acc G: 87.812% 
LR: 2.000e-04 

2023-03-02 01:52:11,667 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2813 (0.3887) Acc D Real: 94.265% 
Loss D Fake: 0.6997 (0.7243) Acc D Fake: 12.092% 
Loss D: 0.981 
Loss G: 0.6906 (0.6664) Acc G: 87.379% 
LR: 2.000e-04 

2023-03-02 01:52:11,675 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4636 (0.3892) Acc D Real: 93.982% 
Loss D Fake: 0.6990 (0.7241) Acc D Fake: 12.523% 
Loss D: 1.163 
Loss G: 0.6909 (0.6666) Acc G: 86.941% 
LR: 2.000e-04 

2023-03-02 01:52:11,682 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3760 (0.3891) Acc D Real: 93.777% 
Loss D Fake: 0.6988 (0.7239) Acc D Fake: 12.948% 
Loss D: 1.075 
Loss G: 0.6911 (0.6667) Acc G: 86.508% 
LR: 2.000e-04 

2023-03-02 01:52:11,690 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4099 (0.3893) Acc D Real: 93.541% 
Loss D Fake: 0.6987 (0.7237) Acc D Fake: 13.378% 
Loss D: 1.109 
Loss G: 0.6912 (0.6669) Acc G: 86.081% 
LR: 2.000e-04 

2023-03-02 01:52:11,697 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3409 (0.3889) Acc D Real: 93.370% 
Loss D Fake: 0.6986 (0.7236) Acc D Fake: 13.803% 
Loss D: 1.039 
Loss G: 0.6914 (0.6671) Acc G: 85.660% 
LR: 2.000e-04 

2023-03-02 01:52:11,704 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3633 (0.3888) Acc D Real: 93.188% 
Loss D Fake: 0.6982 (0.7234) Acc D Fake: 14.222% 
Loss D: 1.062 
Loss G: 0.6919 (0.6672) Acc G: 85.244% 
LR: 2.000e-04 

2023-03-02 01:52:11,712 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3663 (0.3886) Acc D Real: 93.003% 
Loss D Fake: 0.6977 (0.7232) Acc D Fake: 14.636% 
Loss D: 1.064 
Loss G: 0.6925 (0.6674) Acc G: 84.834% 
LR: 2.000e-04 

2023-03-02 01:52:11,720 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3878 (0.3886) Acc D Real: 92.791% 
Loss D Fake: 0.6971 (0.7231) Acc D Fake: 15.044% 
Loss D: 1.085 
Loss G: 0.6930 (0.6676) Acc G: 84.430% 
LR: 2.000e-04 

2023-03-02 01:52:11,728 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4084 (0.3887) Acc D Real: 92.567% 
Loss D Fake: 0.6968 (0.7229) Acc D Fake: 15.447% 
Loss D: 1.105 
Loss G: 0.6932 (0.6677) Acc G: 84.031% 
LR: 2.000e-04 

2023-03-02 01:52:11,735 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4856 (0.3894) Acc D Real: 92.276% 
Loss D Fake: 0.6968 (0.7227) Acc D Fake: 15.844% 
Loss D: 1.182 
Loss G: 0.6927 (0.6679) Acc G: 83.636% 
LR: 2.000e-04 

2023-03-02 01:52:11,742 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4042 (0.3895) Acc D Real: 92.054% 
Loss D Fake: 0.6975 (0.7226) Acc D Fake: 16.237% 
Loss D: 1.102 
Loss G: 0.6919 (0.6680) Acc G: 83.247% 
LR: 2.000e-04 

2023-03-02 01:52:11,750 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4449 (0.3898) Acc D Real: 91.816% 
Loss D Fake: 0.6984 (0.7224) Acc D Fake: 16.624% 
Loss D: 1.143 
Loss G: 0.6909 (0.6682) Acc G: 82.874% 
LR: 2.000e-04 

2023-03-02 01:52:11,757 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4314 (0.3901) Acc D Real: 91.597% 
Loss D Fake: 0.6995 (0.7223) Acc D Fake: 16.996% 
Loss D: 1.131 
Loss G: 0.6898 (0.6683) Acc G: 82.505% 
LR: 2.000e-04 

2023-03-02 01:52:11,765 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.6905 (0.3920) Acc D Real: 91.554% 
Loss D Fake: 0.7012 (0.7221) Acc D Fake: 17.030% 
Loss D: 1.392 
Loss G: 0.6869 (0.6684) Acc G: 82.471% 
LR: 2.000e-04 

2023-03-02 01:52:11,995 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.797 | Generator Loss: 0.687 | Avg: 1.484 
2023-03-02 01:52:12,017 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.884 | Generator Loss: 0.687 | Avg: 1.571 
2023-03-02 01:52:12,039 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.861 | Generator Loss: 0.687 | Avg: 1.548 
2023-03-02 01:52:12,066 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.838 | Generator Loss: 0.687 | Avg: 1.525 
2023-03-02 01:52:12,092 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.837 | Generator Loss: 0.687 | Avg: 1.524 
2023-03-02 01:52:12,118 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.886 | Generator Loss: 0.687 | Avg: 1.573 
2023-03-02 01:52:12,143 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.910 | Generator Loss: 0.687 | Avg: 1.597 
2023-03-02 01:52:12,169 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.952 | Generator Loss: 0.687 | Avg: 1.639 
2023-03-02 01:52:12,195 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.975 | Generator Loss: 0.687 | Avg: 1.662 
2023-03-02 01:52:12,221 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.015 | Generator Loss: 0.687 | Avg: 1.702 
2023-03-02 01:52:12,247 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.044 | Generator Loss: 0.687 | Avg: 1.731 
2023-03-02 01:52:12,272 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.073 | Generator Loss: 0.687 | Avg: 1.760 
2023-03-02 01:52:12,298 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.096 | Generator Loss: 0.687 | Avg: 1.783 
2023-03-02 01:52:12,325 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.112 | Generator Loss: 0.687 | Avg: 1.799 
2023-03-02 01:52:12,350 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.098 | Generator Loss: 0.687 | Avg: 1.785 
2023-03-02 01:52:12,376 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.083 | Generator Loss: 0.687 | Avg: 1.770 
2023-03-02 01:52:12,401 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.069 | Generator Loss: 0.687 | Avg: 1.756 
2023-03-02 01:52:12,435 -                train: [    INFO] - 
Epoch: 18/20
2023-03-02 01:52:12,626 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4380 (0.3861) Acc D Real: 63.255% 
Loss D Fake: 0.7066 (0.7054) Acc D Fake: 74.167% 
Loss D: 1.145 
Loss G: 0.6821 (0.6833) Acc G: 26.667% 
LR: 2.000e-04 

2023-03-02 01:52:12,634 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3838 (0.3853) Acc D Real: 63.455% 
Loss D Fake: 0.7087 (0.7065) Acc D Fake: 73.333% 
Loss D: 1.092 
Loss G: 0.6802 (0.6822) Acc G: 27.222% 
LR: 2.000e-04 

2023-03-02 01:52:12,643 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3807 (0.3842) Acc D Real: 63.438% 
Loss D Fake: 0.7104 (0.7075) Acc D Fake: 72.917% 
Loss D: 1.091 
Loss G: 0.6787 (0.6814) Acc G: 27.500% 
LR: 2.000e-04 

2023-03-02 01:52:12,663 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3338 (0.3741) Acc D Real: 65.448% 
Loss D Fake: 0.7117 (0.7083) Acc D Fake: 72.333% 
Loss D: 1.046 
Loss G: 0.6778 (0.6807) Acc G: 28.000% 
LR: 2.000e-04 

2023-03-02 01:52:12,671 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2942 (0.3608) Acc D Real: 67.578% 
Loss D Fake: 0.7122 (0.7090) Acc D Fake: 71.944% 
Loss D: 1.006 
Loss G: 0.6778 (0.6802) Acc G: 28.333% 
LR: 2.000e-04 

2023-03-02 01:52:12,679 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3479 (0.3590) Acc D Real: 67.820% 
Loss D Fake: 0.7119 (0.7094) Acc D Fake: 71.667% 
Loss D: 1.060 
Loss G: 0.6782 (0.6799) Acc G: 28.571% 
LR: 2.000e-04 

2023-03-02 01:52:12,686 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4723 (0.3731) Acc D Real: 66.250% 
Loss D Fake: 0.7117 (0.7097) Acc D Fake: 71.458% 
Loss D: 1.184 
Loss G: 0.6780 (0.6797) Acc G: 28.750% 
LR: 2.000e-04 

2023-03-02 01:52:12,694 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4026 (0.3764) Acc D Real: 65.642% 
Loss D Fake: 0.7121 (0.7099) Acc D Fake: 71.296% 
Loss D: 1.115 
Loss G: 0.6775 (0.6794) Acc G: 28.889% 
LR: 2.000e-04 

2023-03-02 01:52:12,701 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3834 (0.3771) Acc D Real: 65.531% 
Loss D Fake: 0.7125 (0.7102) Acc D Fake: 71.000% 
Loss D: 1.096 
Loss G: 0.6771 (0.6792) Acc G: 29.000% 
LR: 2.000e-04 

2023-03-02 01:52:12,708 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4125 (0.3803) Acc D Real: 65.128% 
Loss D Fake: 0.7130 (0.7105) Acc D Fake: 70.758% 
Loss D: 1.125 
Loss G: 0.6766 (0.6790) Acc G: 29.242% 
LR: 2.000e-04 

2023-03-02 01:52:12,715 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3822 (0.3805) Acc D Real: 65.642% 
Loss D Fake: 0.7135 (0.7107) Acc D Fake: 70.556% 
Loss D: 1.096 
Loss G: 0.6762 (0.6787) Acc G: 29.444% 
LR: 2.000e-04 

2023-03-02 01:52:12,722 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4784 (0.3880) Acc D Real: 64.900% 
Loss D Fake: 0.7141 (0.7110) Acc D Fake: 70.256% 
Loss D: 1.193 
Loss G: 0.6753 (0.6785) Acc G: 29.744% 
LR: 2.000e-04 

2023-03-02 01:52:12,729 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3857 (0.3878) Acc D Real: 65.320% 
Loss D Fake: 0.7151 (0.7113) Acc D Fake: 69.762% 
Loss D: 1.101 
Loss G: 0.6745 (0.6782) Acc G: 30.238% 
LR: 2.000e-04 

2023-03-02 01:52:12,736 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2996 (0.3820) Acc D Real: 66.208% 
Loss D Fake: 0.7157 (0.7116) Acc D Fake: 69.111% 
Loss D: 1.015 
Loss G: 0.6742 (0.6779) Acc G: 30.778% 
LR: 2.000e-04 

2023-03-02 01:52:12,743 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3603 (0.3806) Acc D Real: 66.865% 
Loss D Fake: 0.7157 (0.7118) Acc D Fake: 68.542% 
Loss D: 1.076 
Loss G: 0.6743 (0.6777) Acc G: 31.146% 
LR: 2.000e-04 

2023-03-02 01:52:12,752 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4161 (0.3827) Acc D Real: 67.194% 
Loss D Fake: 0.7156 (0.7120) Acc D Fake: 68.137% 
Loss D: 1.132 
Loss G: 0.6743 (0.6775) Acc G: 31.471% 
LR: 2.000e-04 

2023-03-02 01:52:12,759 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3633 (0.3816) Acc D Real: 67.804% 
Loss D Fake: 0.7157 (0.7122) Acc D Fake: 67.778% 
Loss D: 1.079 
Loss G: 0.6743 (0.6773) Acc G: 31.759% 
LR: 2.000e-04 

2023-03-02 01:52:12,766 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.4700 (0.3863) Acc D Real: 67.569% 
Loss D Fake: 0.7158 (0.7124) Acc D Fake: 67.368% 
Loss D: 1.186 
Loss G: 0.6739 (0.6771) Acc G: 32.193% 
LR: 2.000e-04 

2023-03-02 01:52:12,773 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3981 (0.3869) Acc D Real: 68.151% 
Loss D Fake: 0.7164 (0.7126) Acc D Fake: 66.583% 
Loss D: 1.115 
Loss G: 0.6732 (0.6769) Acc G: 33.000% 
LR: 2.000e-04 

2023-03-02 01:52:12,780 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3620 (0.3857) Acc D Real: 69.187% 
Loss D Fake: 0.7169 (0.7128) Acc D Fake: 63.413% 
Loss D: 1.079 
Loss G: 0.6729 (0.6767) Acc G: 36.190% 
LR: 2.000e-04 

2023-03-02 01:52:12,787 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3463 (0.3839) Acc D Real: 70.530% 
Loss D Fake: 0.7171 (0.7130) Acc D Fake: 60.530% 
Loss D: 1.063 
Loss G: 0.6729 (0.6766) Acc G: 39.091% 
LR: 2.000e-04 

2023-03-02 01:52:12,794 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3866 (0.3840) Acc D Real: 71.517% 
Loss D Fake: 0.7170 (0.7132) Acc D Fake: 57.899% 
Loss D: 1.104 
Loss G: 0.6730 (0.6764) Acc G: 40.290% 
LR: 2.000e-04 

2023-03-02 01:52:12,801 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4526 (0.3869) Acc D Real: 72.437% 
Loss D Fake: 0.7171 (0.7134) Acc D Fake: 55.486% 
Loss D: 1.170 
Loss G: 0.6727 (0.6763) Acc G: 42.778% 
LR: 2.000e-04 

2023-03-02 01:52:12,808 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3797 (0.3866) Acc D Real: 73.515% 
Loss D Fake: 0.7175 (0.7135) Acc D Fake: 53.267% 
Loss D: 1.097 
Loss G: 0.6723 (0.6761) Acc G: 45.067% 
LR: 2.000e-04 

2023-03-02 01:52:12,815 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.4185 (0.3878) Acc D Real: 74.519% 
Loss D Fake: 0.7179 (0.7137) Acc D Fake: 51.218% 
Loss D: 1.136 
Loss G: 0.6718 (0.6759) Acc G: 47.179% 
LR: 2.000e-04 

2023-03-02 01:52:12,822 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3851 (0.3877) Acc D Real: 75.455% 
Loss D Fake: 0.7184 (0.7139) Acc D Fake: 49.321% 
Loss D: 1.104 
Loss G: 0.6714 (0.6758) Acc G: 49.136% 
LR: 2.000e-04 

2023-03-02 01:52:12,829 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4185 (0.3888) Acc D Real: 76.328% 
Loss D Fake: 0.7189 (0.7140) Acc D Fake: 47.560% 
Loss D: 1.137 
Loss G: 0.6709 (0.6756) Acc G: 50.952% 
LR: 2.000e-04 

2023-03-02 01:52:12,836 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3735 (0.3883) Acc D Real: 77.141% 
Loss D Fake: 0.7194 (0.7142) Acc D Fake: 45.920% 
Loss D: 1.093 
Loss G: 0.6705 (0.6754) Acc G: 52.644% 
LR: 2.000e-04 

2023-03-02 01:52:12,844 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.4600 (0.3907) Acc D Real: 77.898% 
Loss D Fake: 0.7198 (0.7144) Acc D Fake: 44.389% 
Loss D: 1.180 
Loss G: 0.6699 (0.6752) Acc G: 54.222% 
LR: 2.000e-04 

2023-03-02 01:52:12,852 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3311 (0.3888) Acc D Real: 78.607% 
Loss D Fake: 0.7204 (0.7146) Acc D Fake: 42.957% 
Loss D: 1.051 
Loss G: 0.6696 (0.6750) Acc G: 55.699% 
LR: 2.000e-04 

2023-03-02 01:52:12,859 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3907 (0.3888) Acc D Real: 79.269% 
Loss D Fake: 0.7206 (0.7148) Acc D Fake: 41.615% 
Loss D: 1.111 
Loss G: 0.6695 (0.6749) Acc G: 57.083% 
LR: 2.000e-04 

2023-03-02 01:52:12,867 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4636 (0.3911) Acc D Real: 79.894% 
Loss D Fake: 0.7208 (0.7150) Acc D Fake: 40.354% 
Loss D: 1.184 
Loss G: 0.6689 (0.6747) Acc G: 58.384% 
LR: 2.000e-04 

2023-03-02 01:52:12,875 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3030 (0.3885) Acc D Real: 80.481% 
Loss D Fake: 0.7214 (0.7152) Acc D Fake: 39.167% 
Loss D: 1.024 
Loss G: 0.6687 (0.6745) Acc G: 59.608% 
LR: 2.000e-04 

2023-03-02 01:52:12,882 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3879 (0.3885) Acc D Real: 81.036% 
Loss D Fake: 0.7214 (0.7153) Acc D Fake: 38.048% 
Loss D: 1.109 
Loss G: 0.6686 (0.6743) Acc G: 60.762% 
LR: 2.000e-04 

2023-03-02 01:52:12,890 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3586 (0.3876) Acc D Real: 81.560% 
Loss D Fake: 0.7214 (0.7155) Acc D Fake: 36.991% 
Loss D: 1.080 
Loss G: 0.6687 (0.6742) Acc G: 61.852% 
LR: 2.000e-04 

2023-03-02 01:52:12,897 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3426 (0.3864) Acc D Real: 82.058% 
Loss D Fake: 0.7212 (0.7157) Acc D Fake: 35.991% 
Loss D: 1.064 
Loss G: 0.6690 (0.6740) Acc G: 62.883% 
LR: 2.000e-04 

2023-03-02 01:52:12,904 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3798 (0.3863) Acc D Real: 82.526% 
Loss D Fake: 0.7208 (0.7158) Acc D Fake: 35.044% 
Loss D: 1.101 
Loss G: 0.6694 (0.6739) Acc G: 63.860% 
LR: 2.000e-04 

2023-03-02 01:52:12,912 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2483 (0.3827) Acc D Real: 82.971% 
Loss D Fake: 0.7202 (0.7159) Acc D Fake: 34.145% 
Loss D: 0.968 
Loss G: 0.6705 (0.6738) Acc G: 64.786% 
LR: 2.000e-04 

2023-03-02 01:52:12,919 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3437 (0.3817) Acc D Real: 83.396% 
Loss D Fake: 0.7188 (0.7160) Acc D Fake: 33.292% 
Loss D: 1.063 
Loss G: 0.6718 (0.6738) Acc G: 65.667% 
LR: 2.000e-04 

2023-03-02 01:52:12,927 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4155 (0.3826) Acc D Real: 83.793% 
Loss D Fake: 0.7176 (0.7160) Acc D Fake: 32.480% 
Loss D: 1.133 
Loss G: 0.6728 (0.6738) Acc G: 66.504% 
LR: 2.000e-04 

2023-03-02 01:52:12,935 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4351 (0.3838) Acc D Real: 84.017% 
Loss D Fake: 0.7168 (0.7161) Acc D Fake: 31.706% 
Loss D: 1.152 
Loss G: 0.6732 (0.6737) Acc G: 66.071% 
LR: 2.000e-04 

2023-03-02 01:52:12,942 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3968 (0.3841) Acc D Real: 84.090% 
Loss D Fake: 0.7166 (0.7161) Acc D Fake: 31.899% 
Loss D: 1.113 
Loss G: 0.6734 (0.6737) Acc G: 65.581% 
LR: 2.000e-04 

2023-03-02 01:52:12,950 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3174 (0.3826) Acc D Real: 84.272% 
Loss D Fake: 0.7163 (0.7161) Acc D Fake: 32.386% 
Loss D: 1.034 
Loss G: 0.6739 (0.6737) Acc G: 65.000% 
LR: 2.000e-04 

2023-03-02 01:52:12,957 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3962 (0.3829) Acc D Real: 84.119% 
Loss D Fake: 0.7158 (0.7161) Acc D Fake: 33.000% 
Loss D: 1.112 
Loss G: 0.6743 (0.6738) Acc G: 64.370% 
LR: 2.000e-04 

2023-03-02 01:52:12,965 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4225 (0.3838) Acc D Real: 83.899% 
Loss D Fake: 0.7155 (0.7161) Acc D Fake: 33.623% 
Loss D: 1.138 
Loss G: 0.6744 (0.6738) Acc G: 63.768% 
LR: 2.000e-04 

2023-03-02 01:52:12,972 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3750 (0.3836) Acc D Real: 83.757% 
Loss D Fake: 0.7155 (0.7160) Acc D Fake: 34.220% 
Loss D: 1.090 
Loss G: 0.6744 (0.6738) Acc G: 63.191% 
LR: 2.000e-04 

2023-03-02 01:52:12,980 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3842 (0.3836) Acc D Real: 83.600% 
Loss D Fake: 0.7154 (0.7160) Acc D Fake: 34.792% 
Loss D: 1.100 
Loss G: 0.6745 (0.6738) Acc G: 62.639% 
LR: 2.000e-04 

2023-03-02 01:52:12,987 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3614 (0.3831) Acc D Real: 83.476% 
Loss D Fake: 0.7153 (0.7160) Acc D Fake: 35.340% 
Loss D: 1.077 
Loss G: 0.6746 (0.6738) Acc G: 62.109% 
LR: 2.000e-04 

2023-03-02 01:52:12,995 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3836 (0.3831) Acc D Real: 83.281% 
Loss D Fake: 0.7151 (0.7160) Acc D Fake: 35.900% 
Loss D: 1.099 
Loss G: 0.6748 (0.6738) Acc G: 61.567% 
LR: 2.000e-04 

2023-03-02 01:52:13,002 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3955 (0.3834) Acc D Real: 83.086% 
Loss D Fake: 0.7150 (0.7160) Acc D Fake: 36.438% 
Loss D: 1.110 
Loss G: 0.6749 (0.6739) Acc G: 61.046% 
LR: 2.000e-04 

2023-03-02 01:52:13,009 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4250 (0.3842) Acc D Real: 82.755% 
Loss D Fake: 0.7150 (0.7160) Acc D Fake: 36.955% 
Loss D: 1.140 
Loss G: 0.6747 (0.6739) Acc G: 60.545% 
LR: 2.000e-04 

2023-03-02 01:52:13,017 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3304 (0.3832) Acc D Real: 82.696% 
Loss D Fake: 0.7152 (0.7159) Acc D Fake: 37.453% 
Loss D: 1.046 
Loss G: 0.6748 (0.6739) Acc G: 60.063% 
LR: 2.000e-04 

2023-03-02 01:52:13,024 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3136 (0.3819) Acc D Real: 82.747% 
Loss D Fake: 0.7148 (0.7159) Acc D Fake: 37.963% 
Loss D: 1.028 
Loss G: 0.6754 (0.6739) Acc G: 59.568% 
LR: 2.000e-04 

2023-03-02 01:52:13,031 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3289 (0.3809) Acc D Real: 82.661% 
Loss D Fake: 0.7140 (0.7159) Acc D Fake: 38.485% 
Loss D: 1.043 
Loss G: 0.6763 (0.6740) Acc G: 59.061% 
LR: 2.000e-04 

2023-03-02 01:52:13,039 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3098 (0.3796) Acc D Real: 82.557% 
Loss D Fake: 0.7129 (0.7158) Acc D Fake: 39.018% 
Loss D: 1.023 
Loss G: 0.6776 (0.6740) Acc G: 58.542% 
LR: 2.000e-04 

2023-03-02 01:52:13,046 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3894 (0.3798) Acc D Real: 82.282% 
Loss D Fake: 0.7116 (0.7158) Acc D Fake: 39.561% 
Loss D: 1.101 
Loss G: 0.6787 (0.6741) Acc G: 58.041% 
LR: 2.000e-04 

2023-03-02 01:52:13,054 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3168 (0.3787) Acc D Real: 82.126% 
Loss D Fake: 0.7104 (0.7157) Acc D Fake: 40.115% 
Loss D: 1.027 
Loss G: 0.6800 (0.6742) Acc G: 57.529% 
LR: 2.000e-04 

2023-03-02 01:52:13,062 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3847 (0.3788) Acc D Real: 81.806% 
Loss D Fake: 0.7091 (0.7156) Acc D Fake: 40.650% 
Loss D: 1.094 
Loss G: 0.6812 (0.6743) Acc G: 57.034% 
LR: 2.000e-04 

2023-03-02 01:52:13,069 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3938 (0.3791) Acc D Real: 81.463% 
Loss D Fake: 0.7081 (0.7154) Acc D Fake: 41.167% 
Loss D: 1.102 
Loss G: 0.6820 (0.6745) Acc G: 56.528% 
LR: 2.000e-04 

2023-03-02 01:52:13,077 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3468 (0.3786) Acc D Real: 81.237% 
Loss D Fake: 0.7073 (0.7153) Acc D Fake: 41.694% 
Loss D: 1.054 
Loss G: 0.6829 (0.6746) Acc G: 56.038% 
LR: 2.000e-04 

2023-03-02 01:52:13,085 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3887 (0.3787) Acc D Real: 80.946% 
Loss D Fake: 0.7064 (0.7152) Acc D Fake: 42.204% 
Loss D: 1.095 
Loss G: 0.6836 (0.6747) Acc G: 55.565% 
LR: 2.000e-04 

2023-03-02 01:52:13,092 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3608 (0.3784) Acc D Real: 80.747% 
Loss D Fake: 0.7057 (0.7150) Acc D Fake: 42.698% 
Loss D: 1.066 
Loss G: 0.6844 (0.6749) Acc G: 55.106% 
LR: 2.000e-04 

2023-03-02 01:52:13,100 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3546 (0.3781) Acc D Real: 80.491% 
Loss D Fake: 0.7049 (0.7148) Acc D Fake: 43.177% 
Loss D: 1.060 
Loss G: 0.6851 (0.6750) Acc G: 54.661% 
LR: 2.000e-04 

2023-03-02 01:52:13,107 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4430 (0.3791) Acc D Real: 80.102% 
Loss D Fake: 0.7043 (0.7147) Acc D Fake: 43.641% 
Loss D: 1.147 
Loss G: 0.6855 (0.6752) Acc G: 54.231% 
LR: 2.000e-04 

2023-03-02 01:52:13,115 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3937 (0.3793) Acc D Real: 79.857% 
Loss D Fake: 0.7041 (0.7145) Acc D Fake: 44.091% 
Loss D: 1.098 
Loss G: 0.6856 (0.6754) Acc G: 53.813% 
LR: 2.000e-04 

2023-03-02 01:52:13,122 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3490 (0.3788) Acc D Real: 79.661% 
Loss D Fake: 0.7040 (0.7144) Acc D Fake: 44.527% 
Loss D: 1.053 
Loss G: 0.6858 (0.6755) Acc G: 53.408% 
LR: 2.000e-04 

2023-03-02 01:52:13,130 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3653 (0.3786) Acc D Real: 79.435% 
Loss D Fake: 0.7037 (0.7142) Acc D Fake: 44.951% 
Loss D: 1.069 
Loss G: 0.6862 (0.6757) Acc G: 52.999% 
LR: 2.000e-04 

2023-03-02 01:52:13,138 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3507 (0.3782) Acc D Real: 79.253% 
Loss D Fake: 0.7033 (0.7140) Acc D Fake: 45.362% 
Loss D: 1.054 
Loss G: 0.6867 (0.6758) Acc G: 52.594% 
LR: 2.000e-04 

2023-03-02 01:52:13,146 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3851 (0.3783) Acc D Real: 78.996% 
Loss D Fake: 0.7028 (0.7139) Acc D Fake: 45.786% 
Loss D: 1.088 
Loss G: 0.6871 (0.6760) Acc G: 52.199% 
LR: 2.000e-04 

2023-03-02 01:52:13,153 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4011 (0.3786) Acc D Real: 78.724% 
Loss D Fake: 0.7025 (0.7137) Acc D Fake: 46.197% 
Loss D: 1.104 
Loss G: 0.6872 (0.6762) Acc G: 51.816% 
LR: 2.000e-04 

2023-03-02 01:52:13,160 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3357 (0.3781) Acc D Real: 78.586% 
Loss D Fake: 0.7023 (0.7136) Acc D Fake: 46.597% 
Loss D: 1.038 
Loss G: 0.6876 (0.6763) Acc G: 51.444% 
LR: 2.000e-04 

2023-03-02 01:52:13,168 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4028 (0.3784) Acc D Real: 78.330% 
Loss D Fake: 0.7019 (0.7134) Acc D Fake: 46.986% 
Loss D: 1.105 
Loss G: 0.6879 (0.6765) Acc G: 51.082% 
LR: 2.000e-04 

2023-03-02 01:52:13,175 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3674 (0.3782) Acc D Real: 78.131% 
Loss D Fake: 0.7017 (0.7132) Acc D Fake: 47.365% 
Loss D: 1.069 
Loss G: 0.6881 (0.6766) Acc G: 50.729% 
LR: 2.000e-04 

2023-03-02 01:52:13,183 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3526 (0.3779) Acc D Real: 77.969% 
Loss D Fake: 0.7014 (0.7131) Acc D Fake: 47.733% 
Loss D: 1.054 
Loss G: 0.6884 (0.6768) Acc G: 50.386% 
LR: 2.000e-04 

2023-03-02 01:52:13,191 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4241 (0.3785) Acc D Real: 77.684% 
Loss D Fake: 0.7011 (0.7129) Acc D Fake: 48.092% 
Loss D: 1.125 
Loss G: 0.6885 (0.6769) Acc G: 50.052% 
LR: 2.000e-04 

2023-03-02 01:52:13,198 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4243 (0.3791) Acc D Real: 77.427% 
Loss D Fake: 0.7013 (0.7128) Acc D Fake: 48.442% 
Loss D: 1.126 
Loss G: 0.6881 (0.6771) Acc G: 49.727% 
LR: 2.000e-04 

2023-03-02 01:52:13,206 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4223 (0.3797) Acc D Real: 77.149% 
Loss D Fake: 0.7018 (0.7126) Acc D Fake: 48.782% 
Loss D: 1.124 
Loss G: 0.6875 (0.6772) Acc G: 49.410% 
LR: 2.000e-04 

2023-03-02 01:52:13,213 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4465 (0.3805) Acc D Real: 76.864% 
Loss D Fake: 0.7026 (0.7125) Acc D Fake: 49.114% 
Loss D: 1.149 
Loss G: 0.6865 (0.6773) Acc G: 49.122% 
LR: 2.000e-04 

2023-03-02 01:52:13,221 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3530 (0.3802) Acc D Real: 76.748% 
Loss D Fake: 0.7035 (0.7124) Acc D Fake: 49.417% 
Loss D: 1.057 
Loss G: 0.6858 (0.6774) Acc G: 48.841% 
LR: 2.000e-04 

2023-03-02 01:52:13,228 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4554 (0.3811) Acc D Real: 76.448% 
Loss D Fake: 0.7042 (0.7123) Acc D Fake: 49.712% 
Loss D: 1.160 
Loss G: 0.6848 (0.6775) Acc G: 48.567% 
LR: 2.000e-04 

2023-03-02 01:52:13,236 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3621 (0.3809) Acc D Real: 76.306% 
Loss D Fake: 0.7052 (0.7122) Acc D Fake: 50.000% 
Loss D: 1.067 
Loss G: 0.6840 (0.6776) Acc G: 48.300% 
LR: 2.000e-04 

2023-03-02 01:52:13,244 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3524 (0.3805) Acc D Real: 76.194% 
Loss D Fake: 0.7059 (0.7121) Acc D Fake: 50.281% 
Loss D: 1.058 
Loss G: 0.6835 (0.6777) Acc G: 48.040% 
LR: 2.000e-04 

2023-03-02 01:52:13,251 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3030 (0.3796) Acc D Real: 76.149% 
Loss D Fake: 0.7062 (0.7121) Acc D Fake: 50.556% 
Loss D: 1.009 
Loss G: 0.6836 (0.6778) Acc G: 47.785% 
LR: 2.000e-04 

2023-03-02 01:52:13,259 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3822 (0.3796) Acc D Real: 75.971% 
Loss D Fake: 0.7059 (0.7120) Acc D Fake: 50.824% 
Loss D: 1.088 
Loss G: 0.6838 (0.6778) Acc G: 47.537% 
LR: 2.000e-04 

2023-03-02 01:52:13,267 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4433 (0.3804) Acc D Real: 75.733% 
Loss D Fake: 0.7059 (0.7119) Acc D Fake: 51.085% 
Loss D: 1.149 
Loss G: 0.6835 (0.6779) Acc G: 47.294% 
LR: 2.000e-04 

2023-03-02 01:52:13,274 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2599 (0.3790) Acc D Real: 75.768% 
Loss D Fake: 0.7060 (0.7119) Acc D Fake: 51.341% 
Loss D: 0.966 
Loss G: 0.6838 (0.6780) Acc G: 47.057% 
LR: 2.000e-04 

2023-03-02 01:52:13,281 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3317 (0.3784) Acc D Real: 75.732% 
Loss D Fake: 0.7054 (0.7118) Acc D Fake: 51.591% 
Loss D: 1.037 
Loss G: 0.6846 (0.6780) Acc G: 46.825% 
LR: 2.000e-04 

2023-03-02 01:52:13,289 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3320 (0.3779) Acc D Real: 75.653% 
Loss D Fake: 0.7045 (0.7117) Acc D Fake: 51.835% 
Loss D: 1.036 
Loss G: 0.6856 (0.6781) Acc G: 46.599% 
LR: 2.000e-04 

2023-03-02 01:52:13,297 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3861 (0.3780) Acc D Real: 75.498% 
Loss D Fake: 0.7035 (0.7116) Acc D Fake: 52.074% 
Loss D: 1.090 
Loss G: 0.6865 (0.6782) Acc G: 46.377% 
LR: 2.000e-04 

2023-03-02 01:52:13,304 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4812 (0.3791) Acc D Real: 75.228% 
Loss D Fake: 0.7030 (0.7115) Acc D Fake: 52.308% 
Loss D: 1.184 
Loss G: 0.6866 (0.6783) Acc G: 46.161% 
LR: 2.000e-04 

2023-03-02 01:52:13,312 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3632 (0.3790) Acc D Real: 75.093% 
Loss D Fake: 0.7030 (0.7114) Acc D Fake: 52.536% 
Loss D: 1.066 
Loss G: 0.6865 (0.6784) Acc G: 45.949% 
LR: 2.000e-04 

2023-03-02 01:52:13,319 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4971 (0.3802) Acc D Real: 74.796% 
Loss D Fake: 0.7033 (0.7113) Acc D Fake: 52.760% 
Loss D: 1.200 
Loss G: 0.6858 (0.6785) Acc G: 45.741% 
LR: 2.000e-04 

2023-03-02 01:52:13,327 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3892 (0.3803) Acc D Real: 74.648% 
Loss D Fake: 0.7043 (0.7113) Acc D Fake: 52.979% 
Loss D: 1.093 
Loss G: 0.6849 (0.6785) Acc G: 45.539% 
LR: 2.000e-04 

2023-03-02 01:52:13,334 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3757 (0.3803) Acc D Real: 74.538% 
Loss D Fake: 0.7051 (0.7112) Acc D Fake: 53.193% 
Loss D: 1.081 
Loss G: 0.6842 (0.6786) Acc G: 45.340% 
LR: 2.000e-04 

2023-03-02 01:52:13,342 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3781 (0.3803) Acc D Real: 74.411% 
Loss D Fake: 0.7057 (0.7111) Acc D Fake: 53.403% 
Loss D: 1.084 
Loss G: 0.6837 (0.6787) Acc G: 45.145% 
LR: 2.000e-04 

2023-03-02 01:52:13,349 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3142 (0.3796) Acc D Real: 74.375% 
Loss D Fake: 0.7060 (0.7111) Acc D Fake: 53.608% 
Loss D: 1.020 
Loss G: 0.6836 (0.6787) Acc G: 44.955% 
LR: 2.000e-04 

2023-03-02 01:52:13,357 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4103 (0.3799) Acc D Real: 74.225% 
Loss D Fake: 0.7061 (0.7110) Acc D Fake: 53.810% 
Loss D: 1.116 
Loss G: 0.6834 (0.6788) Acc G: 44.768% 
LR: 2.000e-04 

2023-03-02 01:52:13,365 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3609 (0.3797) Acc D Real: 74.163% 
Loss D Fake: 0.7062 (0.7110) Acc D Fake: 54.007% 
Loss D: 1.067 
Loss G: 0.6833 (0.6788) Acc G: 44.585% 
LR: 2.000e-04 

2023-03-02 01:52:13,374 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4391 (0.3803) Acc D Real: 73.987% 
Loss D Fake: 0.7064 (0.7109) Acc D Fake: 54.200% 
Loss D: 1.145 
Loss G: 0.6829 (0.6788) Acc G: 44.406% 
LR: 2.000e-04 

2023-03-02 01:52:13,382 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3424 (0.3799) Acc D Real: 73.922% 
Loss D Fake: 0.7069 (0.7109) Acc D Fake: 54.373% 
Loss D: 1.049 
Loss G: 0.6826 (0.6789) Acc G: 44.231% 
LR: 2.000e-04 

2023-03-02 01:52:13,389 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3796 (0.3799) Acc D Real: 73.806% 
Loss D Fake: 0.7071 (0.7109) Acc D Fake: 54.542% 
Loss D: 1.087 
Loss G: 0.6824 (0.6789) Acc G: 44.075% 
LR: 2.000e-04 

2023-03-02 01:52:13,397 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3476 (0.3796) Acc D Real: 73.750% 
Loss D Fake: 0.7072 (0.7108) Acc D Fake: 54.709% 
Loss D: 1.055 
Loss G: 0.6824 (0.6789) Acc G: 43.922% 
LR: 2.000e-04 

2023-03-02 01:52:13,404 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3520 (0.3793) Acc D Real: 73.682% 
Loss D Fake: 0.7071 (0.7108) Acc D Fake: 54.872% 
Loss D: 1.059 
Loss G: 0.6826 (0.6790) Acc G: 43.772% 
LR: 2.000e-04 

2023-03-02 01:52:13,412 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3985 (0.3795) Acc D Real: 73.578% 
Loss D Fake: 0.7069 (0.7108) Acc D Fake: 55.032% 
Loss D: 1.105 
Loss G: 0.6827 (0.6790) Acc G: 43.609% 
LR: 2.000e-04 

2023-03-02 01:52:13,419 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4952 (0.3806) Acc D Real: 73.342% 
Loss D Fake: 0.7071 (0.7107) Acc D Fake: 55.189% 
Loss D: 1.202 
Loss G: 0.6821 (0.6790) Acc G: 43.465% 
LR: 2.000e-04 

2023-03-02 01:52:13,427 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3582 (0.3804) Acc D Real: 73.265% 
Loss D Fake: 0.7079 (0.7107) Acc D Fake: 55.343% 
Loss D: 1.066 
Loss G: 0.6814 (0.6791) Acc G: 43.324% 
LR: 2.000e-04 

2023-03-02 01:52:13,435 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2971 (0.3796) Acc D Real: 73.267% 
Loss D Fake: 0.7083 (0.7107) Acc D Fake: 55.494% 
Loss D: 1.005 
Loss G: 0.6814 (0.6791) Acc G: 43.185% 
LR: 2.000e-04 

2023-03-02 01:52:13,442 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3801 (0.3796) Acc D Real: 73.179% 
Loss D Fake: 0.7081 (0.7106) Acc D Fake: 55.642% 
Loss D: 1.088 
Loss G: 0.6815 (0.6791) Acc G: 43.049% 
LR: 2.000e-04 

2023-03-02 01:52:13,450 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3488 (0.3794) Acc D Real: 73.124% 
Loss D Fake: 0.7079 (0.7106) Acc D Fake: 55.788% 
Loss D: 1.057 
Loss G: 0.6818 (0.6791) Acc G: 42.915% 
LR: 2.000e-04 

2023-03-02 01:52:13,458 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4372 (0.3799) Acc D Real: 72.972% 
Loss D Fake: 0.7078 (0.7106) Acc D Fake: 55.931% 
Loss D: 1.145 
Loss G: 0.6817 (0.6792) Acc G: 42.783% 
LR: 2.000e-04 

2023-03-02 01:52:13,466 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4027 (0.3801) Acc D Real: 72.879% 
Loss D Fake: 0.7080 (0.7106) Acc D Fake: 56.071% 
Loss D: 1.111 
Loss G: 0.6814 (0.6792) Acc G: 42.654% 
LR: 2.000e-04 

2023-03-02 01:52:13,473 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3776 (0.3801) Acc D Real: 72.791% 
Loss D Fake: 0.7084 (0.7106) Acc D Fake: 56.209% 
Loss D: 1.086 
Loss G: 0.6810 (0.6792) Acc G: 42.528% 
LR: 2.000e-04 

2023-03-02 01:52:13,481 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3411 (0.3797) Acc D Real: 72.745% 
Loss D Fake: 0.7086 (0.7105) Acc D Fake: 56.345% 
Loss D: 1.050 
Loss G: 0.6810 (0.6792) Acc G: 42.403% 
LR: 2.000e-04 

2023-03-02 01:52:13,489 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3569 (0.3795) Acc D Real: 72.697% 
Loss D Fake: 0.7085 (0.7105) Acc D Fake: 56.478% 
Loss D: 1.065 
Loss G: 0.6811 (0.6792) Acc G: 42.281% 
LR: 2.000e-04 

2023-03-02 01:52:13,496 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3273 (0.3791) Acc D Real: 72.668% 
Loss D Fake: 0.7082 (0.7105) Acc D Fake: 56.609% 
Loss D: 1.036 
Loss G: 0.6816 (0.6792) Acc G: 42.161% 
LR: 2.000e-04 

2023-03-02 01:52:13,504 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3697 (0.3790) Acc D Real: 72.605% 
Loss D Fake: 0.7077 (0.7105) Acc D Fake: 56.738% 
Loss D: 1.077 
Loss G: 0.6821 (0.6793) Acc G: 42.042% 
LR: 2.000e-04 

2023-03-02 01:52:13,511 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4246 (0.3794) Acc D Real: 72.471% 
Loss D Fake: 0.7073 (0.7104) Acc D Fake: 56.864% 
Loss D: 1.132 
Loss G: 0.6823 (0.6793) Acc G: 41.926% 
LR: 2.000e-04 

2023-03-02 01:52:13,519 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3454 (0.3791) Acc D Real: 72.423% 
Loss D Fake: 0.7072 (0.7104) Acc D Fake: 56.989% 
Loss D: 1.053 
Loss G: 0.6825 (0.6793) Acc G: 41.812% 
LR: 2.000e-04 

2023-03-02 01:52:13,526 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3364 (0.3787) Acc D Real: 72.389% 
Loss D Fake: 0.7069 (0.7104) Acc D Fake: 57.111% 
Loss D: 1.043 
Loss G: 0.6829 (0.6794) Acc G: 41.700% 
LR: 2.000e-04 

2023-03-02 01:52:13,534 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3299 (0.3783) Acc D Real: 72.368% 
Loss D Fake: 0.7063 (0.7104) Acc D Fake: 57.231% 
Loss D: 1.036 
Loss G: 0.6836 (0.6794) Acc G: 41.575% 
LR: 2.000e-04 

2023-03-02 01:52:13,542 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3771 (0.3783) Acc D Real: 72.286% 
Loss D Fake: 0.7056 (0.7103) Acc D Fake: 57.363% 
Loss D: 1.083 
Loss G: 0.6842 (0.6794) Acc G: 41.453% 
LR: 2.000e-04 

2023-03-02 01:52:13,549 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3233 (0.3779) Acc D Real: 72.262% 
Loss D Fake: 0.7049 (0.7103) Acc D Fake: 57.493% 
Loss D: 1.028 
Loss G: 0.6850 (0.6795) Acc G: 41.333% 
LR: 2.000e-04 

2023-03-02 01:52:13,557 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3694 (0.3778) Acc D Real: 72.195% 
Loss D Fake: 0.7041 (0.7102) Acc D Fake: 57.621% 
Loss D: 1.074 
Loss G: 0.6858 (0.6795) Acc G: 41.215% 
LR: 2.000e-04 

2023-03-02 01:52:13,565 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3236 (0.3774) Acc D Real: 72.175% 
Loss D Fake: 0.7033 (0.7102) Acc D Fake: 57.747% 
Loss D: 1.027 
Loss G: 0.6867 (0.6796) Acc G: 41.098% 
LR: 2.000e-04 

2023-03-02 01:52:13,573 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4437 (0.3779) Acc D Real: 72.027% 
Loss D Fake: 0.7025 (0.7101) Acc D Fake: 57.870% 
Loss D: 1.146 
Loss G: 0.6871 (0.6796) Acc G: 40.984% 
LR: 2.000e-04 

2023-03-02 01:52:13,581 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3759 (0.3779) Acc D Real: 71.970% 
Loss D Fake: 0.7023 (0.7100) Acc D Fake: 57.992% 
Loss D: 1.078 
Loss G: 0.6873 (0.6797) Acc G: 40.871% 
LR: 2.000e-04 

2023-03-02 01:52:13,588 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4238 (0.3782) Acc D Real: 71.851% 
Loss D Fake: 0.7021 (0.7100) Acc D Fake: 58.112% 
Loss D: 1.126 
Loss G: 0.6873 (0.6798) Acc G: 40.760% 
LR: 2.000e-04 

2023-03-02 01:52:13,596 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4212 (0.3786) Acc D Real: 71.738% 
Loss D Fake: 0.7024 (0.7099) Acc D Fake: 58.230% 
Loss D: 1.124 
Loss G: 0.6868 (0.6798) Acc G: 40.651% 
LR: 2.000e-04 

2023-03-02 01:52:13,604 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3626 (0.3785) Acc D Real: 71.674% 
Loss D Fake: 0.7029 (0.7099) Acc D Fake: 58.346% 
Loss D: 1.065 
Loss G: 0.6864 (0.6799) Acc G: 40.543% 
LR: 2.000e-04 

2023-03-02 01:52:13,612 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3955 (0.3786) Acc D Real: 71.598% 
Loss D Fake: 0.7033 (0.7098) Acc D Fake: 58.461% 
Loss D: 1.099 
Loss G: 0.6859 (0.6799) Acc G: 40.437% 
LR: 2.000e-04 

2023-03-02 01:52:13,619 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3633 (0.3785) Acc D Real: 71.551% 
Loss D Fake: 0.7037 (0.7098) Acc D Fake: 58.573% 
Loss D: 1.067 
Loss G: 0.6856 (0.6800) Acc G: 40.333% 
LR: 2.000e-04 

2023-03-02 01:52:13,627 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3675 (0.3784) Acc D Real: 71.500% 
Loss D Fake: 0.7040 (0.7097) Acc D Fake: 58.684% 
Loss D: 1.072 
Loss G: 0.6853 (0.6800) Acc G: 40.230% 
LR: 2.000e-04 

2023-03-02 01:52:13,635 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4055 (0.3786) Acc D Real: 71.417% 
Loss D Fake: 0.7043 (0.7097) Acc D Fake: 58.794% 
Loss D: 1.110 
Loss G: 0.6850 (0.6800) Acc G: 40.129% 
LR: 2.000e-04 

2023-03-02 01:52:13,642 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4002 (0.3787) Acc D Real: 71.330% 
Loss D Fake: 0.7047 (0.7097) Acc D Fake: 58.901% 
Loss D: 1.105 
Loss G: 0.6846 (0.6801) Acc G: 40.029% 
LR: 2.000e-04 

2023-03-02 01:52:13,650 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4188 (0.3790) Acc D Real: 71.219% 
Loss D Fake: 0.7052 (0.7096) Acc D Fake: 58.995% 
Loss D: 1.124 
Loss G: 0.6838 (0.6801) Acc G: 39.943% 
LR: 2.000e-04 

2023-03-02 01:52:13,657 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4581 (0.3796) Acc D Real: 71.085% 
Loss D Fake: 0.7062 (0.7096) Acc D Fake: 59.088% 
Loss D: 1.164 
Loss G: 0.6827 (0.6801) Acc G: 39.859% 
LR: 2.000e-04 

2023-03-02 01:52:13,665 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4025 (0.3798) Acc D Real: 71.015% 
Loss D Fake: 0.7074 (0.7096) Acc D Fake: 59.179% 
Loss D: 1.110 
Loss G: 0.6814 (0.6801) Acc G: 39.775% 
LR: 2.000e-04 

2023-03-02 01:52:13,672 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3880 (0.3798) Acc D Real: 70.966% 
Loss D Fake: 0.7087 (0.7096) Acc D Fake: 59.269% 
Loss D: 1.097 
Loss G: 0.6803 (0.6801) Acc G: 39.705% 
LR: 2.000e-04 

2023-03-02 01:52:13,679 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4183 (0.3801) Acc D Real: 70.887% 
Loss D Fake: 0.7098 (0.7096) Acc D Fake: 59.345% 
Loss D: 1.128 
Loss G: 0.6791 (0.6801) Acc G: 39.635% 
LR: 2.000e-04 

2023-03-02 01:52:13,687 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.2889 (0.3795) Acc D Real: 70.909% 
Loss D Fake: 0.7108 (0.7096) Acc D Fake: 59.421% 
Loss D: 1.000 
Loss G: 0.6785 (0.6801) Acc G: 39.567% 
LR: 2.000e-04 

2023-03-02 01:52:13,694 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4046 (0.3797) Acc D Real: 70.838% 
Loss D Fake: 0.7112 (0.7096) Acc D Fake: 59.484% 
Loss D: 1.116 
Loss G: 0.6780 (0.6801) Acc G: 39.500% 
LR: 2.000e-04 

2023-03-02 01:52:13,702 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3676 (0.3796) Acc D Real: 70.815% 
Loss D Fake: 0.7117 (0.7096) Acc D Fake: 59.545% 
Loss D: 1.079 
Loss G: 0.6777 (0.6801) Acc G: 39.445% 
LR: 2.000e-04 

2023-03-02 01:52:13,709 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3454 (0.3793) Acc D Real: 70.802% 
Loss D Fake: 0.7119 (0.7096) Acc D Fake: 59.606% 
Loss D: 1.057 
Loss G: 0.6776 (0.6801) Acc G: 39.391% 
LR: 2.000e-04 

2023-03-02 01:52:13,717 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3571 (0.3792) Acc D Real: 70.795% 
Loss D Fake: 0.7119 (0.7096) Acc D Fake: 59.667% 
Loss D: 1.069 
Loss G: 0.6777 (0.6800) Acc G: 39.338% 
LR: 2.000e-04 

2023-03-02 01:52:13,724 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3851 (0.3792) Acc D Real: 70.754% 
Loss D Fake: 0.7118 (0.7097) Acc D Fake: 59.726% 
Loss D: 1.097 
Loss G: 0.6777 (0.6800) Acc G: 39.285% 
LR: 2.000e-04 

2023-03-02 01:52:13,732 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3980 (0.3793) Acc D Real: 70.692% 
Loss D Fake: 0.7118 (0.7097) Acc D Fake: 59.785% 
Loss D: 1.110 
Loss G: 0.6776 (0.6800) Acc G: 39.233% 
LR: 2.000e-04 

2023-03-02 01:52:13,739 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3777 (0.3793) Acc D Real: 70.664% 
Loss D Fake: 0.7120 (0.7097) Acc D Fake: 59.842% 
Loss D: 1.090 
Loss G: 0.6774 (0.6800) Acc G: 39.182% 
LR: 2.000e-04 

2023-03-02 01:52:13,747 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3067 (0.3788) Acc D Real: 70.677% 
Loss D Fake: 0.7121 (0.7097) Acc D Fake: 59.899% 
Loss D: 1.019 
Loss G: 0.6776 (0.6800) Acc G: 39.132% 
LR: 2.000e-04 

2023-03-02 01:52:13,754 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3372 (0.3786) Acc D Real: 70.665% 
Loss D Fake: 0.7117 (0.7097) Acc D Fake: 59.956% 
Loss D: 1.049 
Loss G: 0.6780 (0.6800) Acc G: 39.082% 
LR: 2.000e-04 

2023-03-02 01:52:13,763 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3842 (0.3786) Acc D Real: 70.623% 
Loss D Fake: 0.7112 (0.7097) Acc D Fake: 60.011% 
Loss D: 1.095 
Loss G: 0.6784 (0.6800) Acc G: 39.022% 
LR: 2.000e-04 

2023-03-02 01:52:13,771 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4748 (0.3792) Acc D Real: 70.494% 
Loss D Fake: 0.7111 (0.7097) Acc D Fake: 60.066% 
Loss D: 1.186 
Loss G: 0.6782 (0.6799) Acc G: 38.973% 
LR: 2.000e-04 

2023-03-02 01:52:13,780 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3695 (0.3792) Acc D Real: 70.474% 
Loss D Fake: 0.7115 (0.7097) Acc D Fake: 60.120% 
Loss D: 1.081 
Loss G: 0.6778 (0.6799) Acc G: 38.926% 
LR: 2.000e-04 

2023-03-02 01:52:13,788 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3792 (0.3792) Acc D Real: 70.430% 
Loss D Fake: 0.7118 (0.7098) Acc D Fake: 60.173% 
Loss D: 1.091 
Loss G: 0.6775 (0.6799) Acc G: 38.879% 
LR: 2.000e-04 

2023-03-02 01:52:13,796 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2625 (0.3784) Acc D Real: 70.475% 
Loss D Fake: 0.7119 (0.7098) Acc D Fake: 60.226% 
Loss D: 0.974 
Loss G: 0.6779 (0.6799) Acc G: 38.832% 
LR: 2.000e-04 

2023-03-02 01:52:13,805 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2650 (0.3777) Acc D Real: 70.516% 
Loss D Fake: 0.7111 (0.7098) Acc D Fake: 60.278% 
Loss D: 0.976 
Loss G: 0.6789 (0.6799) Acc G: 38.775% 
LR: 2.000e-04 

2023-03-02 01:52:13,812 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3269 (0.3774) Acc D Real: 70.520% 
Loss D Fake: 0.7099 (0.7098) Acc D Fake: 60.340% 
Loss D: 1.037 
Loss G: 0.6802 (0.6799) Acc G: 38.719% 
LR: 2.000e-04 

2023-03-02 01:52:13,820 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4849 (0.3781) Acc D Real: 70.506% 
Loss D Fake: 0.7088 (0.7098) Acc D Fake: 60.345% 
Loss D: 1.194 
Loss G: 0.6807 (0.6799) Acc G: 38.714% 
LR: 2.000e-04 

2023-03-02 01:52:14,051 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.788 | Generator Loss: 0.681 | Avg: 1.468 
2023-03-02 01:52:14,075 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.869 | Generator Loss: 0.681 | Avg: 1.550 
2023-03-02 01:52:14,099 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.848 | Generator Loss: 0.681 | Avg: 1.528 
2023-03-02 01:52:14,124 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.827 | Generator Loss: 0.681 | Avg: 1.508 
2023-03-02 01:52:14,153 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.825 | Generator Loss: 0.681 | Avg: 1.506 
2023-03-02 01:52:14,178 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.876 | Generator Loss: 0.681 | Avg: 1.556 
2023-03-02 01:52:14,204 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.896 | Generator Loss: 0.681 | Avg: 1.577 
2023-03-02 01:52:14,230 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.938 | Generator Loss: 0.681 | Avg: 1.619 
2023-03-02 01:52:14,262 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.960 | Generator Loss: 0.681 | Avg: 1.641 
2023-03-02 01:52:14,287 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.002 | Generator Loss: 0.681 | Avg: 1.682 
2023-03-02 01:52:14,313 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.030 | Generator Loss: 0.681 | Avg: 1.711 
2023-03-02 01:52:14,338 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.060 | Generator Loss: 0.681 | Avg: 1.741 
2023-03-02 01:52:14,364 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.084 | Generator Loss: 0.681 | Avg: 1.765 
2023-03-02 01:52:14,389 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.100 | Generator Loss: 0.681 | Avg: 1.780 
2023-03-02 01:52:14,415 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.085 | Generator Loss: 0.681 | Avg: 1.766 
2023-03-02 01:52:14,440 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.070 | Generator Loss: 0.681 | Avg: 1.751 
2023-03-02 01:52:14,465 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.056 | Generator Loss: 0.681 | Avg: 1.737 
2023-03-02 01:52:14,499 -                train: [    INFO] - 
Epoch: 19/20
2023-03-02 01:52:14,676 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4214 (0.4035) Acc D Real: 61.745% 
Loss D Fake: 0.7088 (0.7087) Acc D Fake: 70.000% 
Loss D: 1.130 
Loss G: 0.6804 (0.6806) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 01:52:14,698 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4547 (0.4205) Acc D Real: 58.594% 
Loss D Fake: 0.7093 (0.7089) Acc D Fake: 70.000% 
Loss D: 1.164 
Loss G: 0.6796 (0.6803) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 01:52:14,705 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3826 (0.4111) Acc D Real: 59.688% 
Loss D Fake: 0.7102 (0.7092) Acc D Fake: 70.000% 
Loss D: 1.093 
Loss G: 0.6787 (0.6799) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 01:52:14,712 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4452 (0.4179) Acc D Real: 59.260% 
Loss D Fake: 0.7112 (0.7096) Acc D Fake: 69.667% 
Loss D: 1.156 
Loss G: 0.6776 (0.6794) Acc G: 30.333% 
LR: 2.000e-04 

2023-03-02 01:52:14,719 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3416 (0.4052) Acc D Real: 60.547% 
Loss D Fake: 0.7123 (0.7101) Acc D Fake: 69.444% 
Loss D: 1.054 
Loss G: 0.6767 (0.6790) Acc G: 30.833% 
LR: 2.000e-04 

2023-03-02 01:52:14,726 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3599 (0.3987) Acc D Real: 61.406% 
Loss D Fake: 0.7130 (0.7105) Acc D Fake: 69.048% 
Loss D: 1.073 
Loss G: 0.6761 (0.6786) Acc G: 31.190% 
LR: 2.000e-04 

2023-03-02 01:52:14,736 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3194 (0.3888) Acc D Real: 62.513% 
Loss D Fake: 0.7134 (0.7109) Acc D Fake: 68.750% 
Loss D: 1.033 
Loss G: 0.6759 (0.6782) Acc G: 31.458% 
LR: 2.000e-04 

2023-03-02 01:52:14,745 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3819 (0.3880) Acc D Real: 62.691% 
Loss D Fake: 0.7136 (0.7112) Acc D Fake: 68.333% 
Loss D: 1.096 
Loss G: 0.6758 (0.6780) Acc G: 31.667% 
LR: 2.000e-04 

2023-03-02 01:52:14,753 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3846 (0.3877) Acc D Real: 62.990% 
Loss D Fake: 0.7138 (0.7114) Acc D Fake: 68.000% 
Loss D: 1.098 
Loss G: 0.6756 (0.6777) Acc G: 32.000% 
LR: 2.000e-04 

2023-03-02 01:52:14,762 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.2453 (0.3747) Acc D Real: 64.564% 
Loss D Fake: 0.7137 (0.7116) Acc D Fake: 67.727% 
Loss D: 0.959 
Loss G: 0.6761 (0.6776) Acc G: 32.121% 
LR: 2.000e-04 

2023-03-02 01:52:14,769 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3704 (0.3744) Acc D Real: 65.113% 
Loss D Fake: 0.7130 (0.7117) Acc D Fake: 67.639% 
Loss D: 1.083 
Loss G: 0.6767 (0.6775) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 01:52:14,776 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4018 (0.3765) Acc D Real: 64.996% 
Loss D Fake: 0.7125 (0.7118) Acc D Fake: 67.564% 
Loss D: 1.114 
Loss G: 0.6770 (0.6775) Acc G: 32.179% 
LR: 2.000e-04 

2023-03-02 01:52:14,783 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3669 (0.3758) Acc D Real: 65.160% 
Loss D Fake: 0.7122 (0.7118) Acc D Fake: 67.500% 
Loss D: 1.079 
Loss G: 0.6773 (0.6775) Acc G: 32.143% 
LR: 2.000e-04 

2023-03-02 01:52:14,791 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4427 (0.3803) Acc D Real: 64.660% 
Loss D Fake: 0.7121 (0.7118) Acc D Fake: 67.444% 
Loss D: 1.155 
Loss G: 0.6771 (0.6774) Acc G: 32.111% 
LR: 2.000e-04 

2023-03-02 01:52:14,798 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3465 (0.3782) Acc D Real: 65.020% 
Loss D Fake: 0.7123 (0.7119) Acc D Fake: 67.396% 
Loss D: 1.059 
Loss G: 0.6770 (0.6774) Acc G: 32.083% 
LR: 2.000e-04 

2023-03-02 01:52:14,807 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4217 (0.3807) Acc D Real: 64.770% 
Loss D Fake: 0.7124 (0.7119) Acc D Fake: 67.353% 
Loss D: 1.134 
Loss G: 0.6767 (0.6774) Acc G: 32.157% 
LR: 2.000e-04 

2023-03-02 01:52:14,813 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3701 (0.3801) Acc D Real: 64.933% 
Loss D Fake: 0.7128 (0.7120) Acc D Fake: 67.315% 
Loss D: 1.083 
Loss G: 0.6764 (0.6773) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 01:52:14,820 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3469 (0.3784) Acc D Real: 65.197% 
Loss D Fake: 0.7130 (0.7120) Acc D Fake: 67.281% 
Loss D: 1.060 
Loss G: 0.6763 (0.6773) Acc G: 32.281% 
LR: 2.000e-04 

2023-03-02 01:52:14,827 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3970 (0.3793) Acc D Real: 65.344% 
Loss D Fake: 0.7131 (0.7121) Acc D Fake: 67.250% 
Loss D: 1.110 
Loss G: 0.6762 (0.6772) Acc G: 32.333% 
LR: 2.000e-04 

2023-03-02 01:52:14,834 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2992 (0.3755) Acc D Real: 65.801% 
Loss D Fake: 0.7131 (0.7121) Acc D Fake: 67.222% 
Loss D: 1.012 
Loss G: 0.6764 (0.6772) Acc G: 32.381% 
LR: 2.000e-04 

2023-03-02 01:52:14,841 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3041 (0.3723) Acc D Real: 66.233% 
Loss D Fake: 0.7127 (0.7121) Acc D Fake: 67.197% 
Loss D: 1.017 
Loss G: 0.6770 (0.6772) Acc G: 32.424% 
LR: 2.000e-04 

2023-03-02 01:52:14,848 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3072 (0.3694) Acc D Real: 66.558% 
Loss D Fake: 0.7118 (0.7121) Acc D Fake: 67.246% 
Loss D: 1.019 
Loss G: 0.6780 (0.6772) Acc G: 32.391% 
LR: 2.000e-04 

2023-03-02 01:52:14,855 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3916 (0.3704) Acc D Real: 66.382% 
Loss D Fake: 0.7108 (0.7121) Acc D Fake: 67.292% 
Loss D: 1.102 
Loss G: 0.6788 (0.6773) Acc G: 32.361% 
LR: 2.000e-04 

2023-03-02 01:52:14,862 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4397 (0.3731) Acc D Real: 66.094% 
Loss D Fake: 0.7103 (0.7120) Acc D Fake: 67.333% 
Loss D: 1.150 
Loss G: 0.6791 (0.6773) Acc G: 32.333% 
LR: 2.000e-04 

2023-03-02 01:52:14,868 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3595 (0.3726) Acc D Real: 66.030% 
Loss D Fake: 0.7102 (0.7119) Acc D Fake: 67.372% 
Loss D: 1.070 
Loss G: 0.6792 (0.6774) Acc G: 32.308% 
LR: 2.000e-04 

2023-03-02 01:52:14,876 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3082 (0.3702) Acc D Real: 66.360% 
Loss D Fake: 0.7099 (0.7119) Acc D Fake: 67.407% 
Loss D: 1.018 
Loss G: 0.6796 (0.6775) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 01:52:14,883 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3889 (0.3709) Acc D Real: 66.326% 
Loss D Fake: 0.7094 (0.7118) Acc D Fake: 67.500% 
Loss D: 1.098 
Loss G: 0.6801 (0.6776) Acc G: 32.143% 
LR: 2.000e-04 

2023-03-02 01:52:14,891 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.4041 (0.3720) Acc D Real: 66.097% 
Loss D Fake: 0.7091 (0.7117) Acc D Fake: 67.586% 
Loss D: 1.113 
Loss G: 0.6802 (0.6777) Acc G: 32.069% 
LR: 2.000e-04 

2023-03-02 01:52:14,898 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3116 (0.3700) Acc D Real: 66.278% 
Loss D Fake: 0.7089 (0.7116) Acc D Fake: 67.667% 
Loss D: 1.021 
Loss G: 0.6806 (0.6778) Acc G: 32.000% 
LR: 2.000e-04 

2023-03-02 01:52:14,906 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3829 (0.3704) Acc D Real: 66.240% 
Loss D Fake: 0.7084 (0.7115) Acc D Fake: 67.742% 
Loss D: 1.091 
Loss G: 0.6810 (0.6779) Acc G: 31.935% 
LR: 2.000e-04 

2023-03-02 01:52:14,913 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3844 (0.3709) Acc D Real: 66.180% 
Loss D Fake: 0.7081 (0.7114) Acc D Fake: 67.812% 
Loss D: 1.093 
Loss G: 0.6812 (0.6780) Acc G: 31.875% 
LR: 2.000e-04 

2023-03-02 01:52:14,921 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3927 (0.3715) Acc D Real: 66.053% 
Loss D Fake: 0.7080 (0.7113) Acc D Fake: 67.879% 
Loss D: 1.101 
Loss G: 0.6812 (0.6781) Acc G: 31.818% 
LR: 2.000e-04 

2023-03-02 01:52:14,929 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.4282 (0.3732) Acc D Real: 65.780% 
Loss D Fake: 0.7081 (0.7112) Acc D Fake: 67.941% 
Loss D: 1.136 
Loss G: 0.6809 (0.6782) Acc G: 31.765% 
LR: 2.000e-04 

2023-03-02 01:52:14,936 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3078 (0.3713) Acc D Real: 65.988% 
Loss D Fake: 0.7084 (0.7111) Acc D Fake: 68.000% 
Loss D: 1.016 
Loss G: 0.6809 (0.6782) Acc G: 31.714% 
LR: 2.000e-04 

2023-03-02 01:52:14,943 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3422 (0.3705) Acc D Real: 66.068% 
Loss D Fake: 0.7082 (0.7110) Acc D Fake: 68.056% 
Loss D: 1.050 
Loss G: 0.6811 (0.6783) Acc G: 31.667% 
LR: 2.000e-04 

2023-03-02 01:52:14,951 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3167 (0.3691) Acc D Real: 66.223% 
Loss D Fake: 0.7078 (0.7109) Acc D Fake: 68.108% 
Loss D: 1.025 
Loss G: 0.6817 (0.6784) Acc G: 31.622% 
LR: 2.000e-04 

2023-03-02 01:52:14,958 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2973 (0.3672) Acc D Real: 66.357% 
Loss D Fake: 0.7070 (0.7108) Acc D Fake: 68.158% 
Loss D: 1.004 
Loss G: 0.6827 (0.6785) Acc G: 31.579% 
LR: 2.000e-04 

2023-03-02 01:52:14,965 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3254 (0.3661) Acc D Real: 66.433% 
Loss D Fake: 0.7060 (0.7107) Acc D Fake: 68.205% 
Loss D: 1.031 
Loss G: 0.6838 (0.6787) Acc G: 31.496% 
LR: 2.000e-04 

2023-03-02 01:52:14,973 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3212 (0.3650) Acc D Real: 66.509% 
Loss D Fake: 0.7048 (0.7106) Acc D Fake: 68.292% 
Loss D: 1.026 
Loss G: 0.6850 (0.6788) Acc G: 31.417% 
LR: 2.000e-04 

2023-03-02 01:52:14,980 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3991 (0.3658) Acc D Real: 66.331% 
Loss D Fake: 0.7036 (0.7104) Acc D Fake: 68.374% 
Loss D: 1.103 
Loss G: 0.6859 (0.6790) Acc G: 31.341% 
LR: 2.000e-04 

2023-03-02 01:52:14,988 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4027 (0.3667) Acc D Real: 66.166% 
Loss D Fake: 0.7030 (0.7102) Acc D Fake: 68.452% 
Loss D: 1.106 
Loss G: 0.6864 (0.6792) Acc G: 31.270% 
LR: 2.000e-04 

2023-03-02 01:52:14,997 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3112 (0.3654) Acc D Real: 66.271% 
Loss D Fake: 0.7025 (0.7100) Acc D Fake: 68.527% 
Loss D: 1.014 
Loss G: 0.6870 (0.6793) Acc G: 31.202% 
LR: 2.000e-04 

2023-03-02 01:52:15,005 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4296 (0.3669) Acc D Real: 66.050% 
Loss D Fake: 0.7019 (0.7099) Acc D Fake: 68.598% 
Loss D: 1.132 
Loss G: 0.6873 (0.6795) Acc G: 31.136% 
LR: 2.000e-04 

2023-03-02 01:52:15,012 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3947 (0.3675) Acc D Real: 65.918% 
Loss D Fake: 0.7018 (0.7097) Acc D Fake: 68.667% 
Loss D: 1.097 
Loss G: 0.6873 (0.6797) Acc G: 31.074% 
LR: 2.000e-04 

2023-03-02 01:52:15,020 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3620 (0.3674) Acc D Real: 65.851% 
Loss D Fake: 0.7019 (0.7095) Acc D Fake: 68.732% 
Loss D: 1.064 
Loss G: 0.6872 (0.6799) Acc G: 31.014% 
LR: 2.000e-04 

2023-03-02 01:52:15,027 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3796 (0.3676) Acc D Real: 65.769% 
Loss D Fake: 0.7020 (0.7093) Acc D Fake: 68.794% 
Loss D: 1.082 
Loss G: 0.6871 (0.6800) Acc G: 30.957% 
LR: 2.000e-04 

2023-03-02 01:52:15,035 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3412 (0.3671) Acc D Real: 65.789% 
Loss D Fake: 0.7021 (0.7092) Acc D Fake: 68.854% 
Loss D: 1.043 
Loss G: 0.6870 (0.6802) Acc G: 30.903% 
LR: 2.000e-04 

2023-03-02 01:52:15,043 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3824 (0.3674) Acc D Real: 65.757% 
Loss D Fake: 0.7022 (0.7091) Acc D Fake: 68.912% 
Loss D: 1.085 
Loss G: 0.6868 (0.6803) Acc G: 30.850% 
LR: 2.000e-04 

2023-03-02 01:52:15,050 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3225 (0.3665) Acc D Real: 65.838% 
Loss D Fake: 0.7023 (0.7089) Acc D Fake: 68.967% 
Loss D: 1.025 
Loss G: 0.6869 (0.6804) Acc G: 30.800% 
LR: 2.000e-04 

2023-03-02 01:52:15,058 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3597 (0.3664) Acc D Real: 65.843% 
Loss D Fake: 0.7021 (0.7088) Acc D Fake: 69.020% 
Loss D: 1.062 
Loss G: 0.6871 (0.6806) Acc G: 30.752% 
LR: 2.000e-04 

2023-03-02 01:52:15,066 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3766 (0.3665) Acc D Real: 65.744% 
Loss D Fake: 0.7020 (0.7087) Acc D Fake: 69.071% 
Loss D: 1.079 
Loss G: 0.6871 (0.6807) Acc G: 30.705% 
LR: 2.000e-04 

2023-03-02 01:52:15,073 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3414 (0.3661) Acc D Real: 65.792% 
Loss D Fake: 0.7019 (0.7085) Acc D Fake: 69.117% 
Loss D: 1.043 
Loss G: 0.6873 (0.6808) Acc G: 30.660% 
LR: 2.000e-04 

2023-03-02 01:52:15,081 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3885 (0.3665) Acc D Real: 65.715% 
Loss D Fake: 0.7018 (0.7084) Acc D Fake: 69.133% 
Loss D: 1.090 
Loss G: 0.6873 (0.6809) Acc G: 30.648% 
LR: 2.000e-04 

2023-03-02 01:52:15,089 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4621 (0.3682) Acc D Real: 65.444% 
Loss D Fake: 0.7019 (0.7083) Acc D Fake: 69.149% 
Loss D: 1.164 
Loss G: 0.6868 (0.6810) Acc G: 30.636% 
LR: 2.000e-04 

2023-03-02 01:52:15,096 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3630 (0.3681) Acc D Real: 65.425% 
Loss D Fake: 0.7026 (0.7082) Acc D Fake: 69.164% 
Loss D: 1.066 
Loss G: 0.6862 (0.6811) Acc G: 30.625% 
LR: 2.000e-04 

2023-03-02 01:52:15,104 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3267 (0.3674) Acc D Real: 65.458% 
Loss D Fake: 0.7031 (0.7081) Acc D Fake: 69.179% 
Loss D: 1.030 
Loss G: 0.6858 (0.6812) Acc G: 30.614% 
LR: 2.000e-04 

2023-03-02 01:52:15,112 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3171 (0.3665) Acc D Real: 65.514% 
Loss D Fake: 0.7033 (0.7080) Acc D Fake: 69.193% 
Loss D: 1.020 
Loss G: 0.6858 (0.6813) Acc G: 30.603% 
LR: 2.000e-04 

2023-03-02 01:52:15,119 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4005 (0.3671) Acc D Real: 65.400% 
Loss D Fake: 0.7033 (0.7079) Acc D Fake: 69.206% 
Loss D: 1.104 
Loss G: 0.6856 (0.6814) Acc G: 30.593% 
LR: 2.000e-04 

2023-03-02 01:52:15,127 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3672 (0.3671) Acc D Real: 65.387% 
Loss D Fake: 0.7035 (0.7079) Acc D Fake: 69.220% 
Loss D: 1.071 
Loss G: 0.6854 (0.6814) Acc G: 30.583% 
LR: 2.000e-04 

2023-03-02 01:52:15,135 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4786 (0.3689) Acc D Real: 65.098% 
Loss D Fake: 0.7040 (0.7078) Acc D Fake: 69.232% 
Loss D: 1.183 
Loss G: 0.6844 (0.6815) Acc G: 30.574% 
LR: 2.000e-04 

2023-03-02 01:52:15,143 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2578 (0.3672) Acc D Real: 65.299% 
Loss D Fake: 0.7049 (0.7077) Acc D Fake: 69.218% 
Loss D: 0.963 
Loss G: 0.6840 (0.6815) Acc G: 30.565% 
LR: 2.000e-04 

2023-03-02 01:52:15,150 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2671 (0.3656) Acc D Real: 65.460% 
Loss D Fake: 0.7049 (0.7077) Acc D Fake: 69.204% 
Loss D: 0.972 
Loss G: 0.6843 (0.6816) Acc G: 30.556% 
LR: 2.000e-04 

2023-03-02 01:52:15,158 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3507 (0.3653) Acc D Real: 65.467% 
Loss D Fake: 0.7044 (0.7077) Acc D Fake: 69.190% 
Loss D: 1.055 
Loss G: 0.6848 (0.6816) Acc G: 30.547% 
LR: 2.000e-04 

2023-03-02 01:52:15,166 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4390 (0.3665) Acc D Real: 65.284% 
Loss D Fake: 0.7041 (0.7076) Acc D Fake: 69.177% 
Loss D: 1.143 
Loss G: 0.6847 (0.6817) Acc G: 30.544% 
LR: 2.000e-04 

2023-03-02 01:52:15,174 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2978 (0.3654) Acc D Real: 65.415% 
Loss D Fake: 0.7042 (0.7075) Acc D Fake: 69.164% 
Loss D: 1.002 
Loss G: 0.6849 (0.6817) Acc G: 30.561% 
LR: 2.000e-04 

2023-03-02 01:52:15,181 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3665 (0.3654) Acc D Real: 65.410% 
Loss D Fake: 0.7039 (0.7075) Acc D Fake: 69.152% 
Loss D: 1.070 
Loss G: 0.6852 (0.6818) Acc G: 30.578% 
LR: 2.000e-04 

2023-03-02 01:52:15,189 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4229 (0.3663) Acc D Real: 65.276% 
Loss D Fake: 0.7038 (0.7074) Acc D Fake: 69.140% 
Loss D: 1.127 
Loss G: 0.6850 (0.6818) Acc G: 30.594% 
LR: 2.000e-04 

2023-03-02 01:52:15,196 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3041 (0.3654) Acc D Real: 65.347% 
Loss D Fake: 0.7040 (0.7074) Acc D Fake: 69.128% 
Loss D: 1.008 
Loss G: 0.6850 (0.6819) Acc G: 30.609% 
LR: 2.000e-04 

2023-03-02 01:52:15,204 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4395 (0.3664) Acc D Real: 65.193% 
Loss D Fake: 0.7040 (0.7073) Acc D Fake: 69.117% 
Loss D: 1.144 
Loss G: 0.6847 (0.6819) Acc G: 30.624% 
LR: 2.000e-04 

2023-03-02 01:52:15,213 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3658 (0.3664) Acc D Real: 65.187% 
Loss D Fake: 0.7045 (0.7073) Acc D Fake: 69.106% 
Loss D: 1.070 
Loss G: 0.6842 (0.6819) Acc G: 30.639% 
LR: 2.000e-04 

2023-03-02 01:52:15,220 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3888 (0.3667) Acc D Real: 65.129% 
Loss D Fake: 0.7050 (0.7073) Acc D Fake: 69.095% 
Loss D: 1.094 
Loss G: 0.6837 (0.6820) Acc G: 30.653% 
LR: 2.000e-04 

2023-03-02 01:52:15,228 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3066 (0.3659) Acc D Real: 65.233% 
Loss D Fake: 0.7054 (0.7072) Acc D Fake: 69.085% 
Loss D: 1.012 
Loss G: 0.6835 (0.6820) Acc G: 30.667% 
LR: 2.000e-04 

2023-03-02 01:52:15,236 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4218 (0.3667) Acc D Real: 65.110% 
Loss D Fake: 0.7056 (0.7072) Acc D Fake: 69.074% 
Loss D: 1.127 
Loss G: 0.6830 (0.6820) Acc G: 30.681% 
LR: 2.000e-04 

2023-03-02 01:52:15,243 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3572 (0.3666) Acc D Real: 65.097% 
Loss D Fake: 0.7061 (0.7072) Acc D Fake: 69.065% 
Loss D: 1.063 
Loss G: 0.6825 (0.6820) Acc G: 30.694% 
LR: 2.000e-04 

2023-03-02 01:52:15,251 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2868 (0.3655) Acc D Real: 65.221% 
Loss D Fake: 0.7065 (0.7072) Acc D Fake: 69.033% 
Loss D: 0.993 
Loss G: 0.6824 (0.6820) Acc G: 30.728% 
LR: 2.000e-04 

2023-03-02 01:52:15,259 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3703 (0.3656) Acc D Real: 65.212% 
Loss D Fake: 0.7064 (0.7072) Acc D Fake: 69.002% 
Loss D: 1.077 
Loss G: 0.6825 (0.6820) Acc G: 30.762% 
LR: 2.000e-04 

2023-03-02 01:52:15,266 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3056 (0.3648) Acc D Real: 65.317% 
Loss D Fake: 0.7062 (0.7072) Acc D Fake: 68.972% 
Loss D: 1.012 
Loss G: 0.6828 (0.6820) Acc G: 30.795% 
LR: 2.000e-04 

2023-03-02 01:52:15,274 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3955 (0.3652) Acc D Real: 65.251% 
Loss D Fake: 0.7059 (0.7072) Acc D Fake: 68.943% 
Loss D: 1.101 
Loss G: 0.6829 (0.6820) Acc G: 30.827% 
LR: 2.000e-04 

2023-03-02 01:52:15,281 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3643 (0.3652) Acc D Real: 65.247% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.915% 
Loss D: 1.070 
Loss G: 0.6829 (0.6820) Acc G: 30.858% 
LR: 2.000e-04 

2023-03-02 01:52:15,289 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3740 (0.3653) Acc D Real: 65.218% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.887% 
Loss D: 1.080 
Loss G: 0.6828 (0.6820) Acc G: 30.889% 
LR: 2.000e-04 

2023-03-02 01:52:15,297 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3147 (0.3647) Acc D Real: 65.259% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.860% 
Loss D: 1.021 
Loss G: 0.6829 (0.6821) Acc G: 30.918% 
LR: 2.000e-04 

2023-03-02 01:52:15,304 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2878 (0.3637) Acc D Real: 65.352% 
Loss D Fake: 0.7057 (0.7071) Acc D Fake: 68.833% 
Loss D: 0.993 
Loss G: 0.6834 (0.6821) Acc G: 30.948% 
LR: 2.000e-04 

2023-03-02 01:52:15,312 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4018 (0.3642) Acc D Real: 65.297% 
Loss D Fake: 0.7052 (0.7071) Acc D Fake: 68.808% 
Loss D: 1.107 
Loss G: 0.6836 (0.6821) Acc G: 30.976% 
LR: 2.000e-04 

2023-03-02 01:52:15,319 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4424 (0.3651) Acc D Real: 65.159% 
Loss D Fake: 0.7053 (0.7071) Acc D Fake: 68.782% 
Loss D: 1.148 
Loss G: 0.6832 (0.6821) Acc G: 31.004% 
LR: 2.000e-04 

2023-03-02 01:52:15,327 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3801 (0.3653) Acc D Real: 65.137% 
Loss D Fake: 0.7059 (0.7070) Acc D Fake: 68.758% 
Loss D: 1.086 
Loss G: 0.6825 (0.6821) Acc G: 31.031% 
LR: 2.000e-04 

2023-03-02 01:52:15,335 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2716 (0.3642) Acc D Real: 65.284% 
Loss D Fake: 0.7064 (0.7070) Acc D Fake: 68.734% 
Loss D: 0.978 
Loss G: 0.6824 (0.6821) Acc G: 31.057% 
LR: 2.000e-04 

2023-03-02 01:52:15,342 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4799 (0.3655) Acc D Real: 65.115% 
Loss D Fake: 0.7065 (0.7070) Acc D Fake: 68.710% 
Loss D: 1.186 
Loss G: 0.6818 (0.6821) Acc G: 31.096% 
LR: 2.000e-04 

2023-03-02 01:52:15,350 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3312 (0.3651) Acc D Real: 65.156% 
Loss D Fake: 0.7072 (0.7070) Acc D Fake: 68.669% 
Loss D: 1.038 
Loss G: 0.6812 (0.6821) Acc G: 31.140% 
LR: 2.000e-04 

2023-03-02 01:52:15,357 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3656 (0.3651) Acc D Real: 65.157% 
Loss D Fake: 0.7077 (0.7070) Acc D Fake: 68.628% 
Loss D: 1.073 
Loss G: 0.6808 (0.6821) Acc G: 31.183% 
LR: 2.000e-04 

2023-03-02 01:52:15,365 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3087 (0.3645) Acc D Real: 65.221% 
Loss D Fake: 0.7080 (0.7070) Acc D Fake: 68.588% 
Loss D: 1.017 
Loss G: 0.6806 (0.6821) Acc G: 31.225% 
LR: 2.000e-04 

2023-03-02 01:52:15,372 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3757 (0.3646) Acc D Real: 65.183% 
Loss D Fake: 0.7081 (0.7071) Acc D Fake: 68.549% 
Loss D: 1.084 
Loss G: 0.6804 (0.6821) Acc G: 31.266% 
LR: 2.000e-04 

2023-03-02 01:52:15,380 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3256 (0.3642) Acc D Real: 65.223% 
Loss D Fake: 0.7083 (0.7071) Acc D Fake: 68.511% 
Loss D: 1.034 
Loss G: 0.6804 (0.6820) Acc G: 31.306% 
LR: 2.000e-04 

2023-03-02 01:52:15,388 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3024 (0.3636) Acc D Real: 65.288% 
Loss D Fake: 0.7082 (0.7071) Acc D Fake: 68.456% 
Loss D: 1.011 
Loss G: 0.6806 (0.6820) Acc G: 31.345% 
LR: 2.000e-04 

2023-03-02 01:52:15,395 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4619 (0.3646) Acc D Real: 65.156% 
Loss D Fake: 0.7081 (0.7071) Acc D Fake: 68.402% 
Loss D: 1.170 
Loss G: 0.6802 (0.6820) Acc G: 31.401% 
LR: 2.000e-04 

2023-03-02 01:52:15,403 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3852 (0.3648) Acc D Real: 65.117% 
Loss D Fake: 0.7087 (0.7071) Acc D Fake: 68.349% 
Loss D: 1.094 
Loss G: 0.6796 (0.6820) Acc G: 31.456% 
LR: 2.000e-04 

2023-03-02 01:52:15,410 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3873 (0.3651) Acc D Real: 65.082% 
Loss D Fake: 0.7094 (0.7071) Acc D Fake: 68.297% 
Loss D: 1.097 
Loss G: 0.6788 (0.6819) Acc G: 31.510% 
LR: 2.000e-04 

2023-03-02 01:52:15,418 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3445 (0.3648) Acc D Real: 65.113% 
Loss D Fake: 0.7102 (0.7072) Acc D Fake: 68.230% 
Loss D: 1.055 
Loss G: 0.6781 (0.6819) Acc G: 31.580% 
LR: 2.000e-04 

2023-03-02 01:52:15,425 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3623 (0.3648) Acc D Real: 65.132% 
Loss D Fake: 0.7108 (0.7072) Acc D Fake: 68.163% 
Loss D: 1.073 
Loss G: 0.6775 (0.6819) Acc G: 31.648% 
LR: 2.000e-04 

2023-03-02 01:52:15,433 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2706 (0.3639) Acc D Real: 65.247% 
Loss D Fake: 0.7112 (0.7072) Acc D Fake: 68.082% 
Loss D: 0.982 
Loss G: 0.6774 (0.6818) Acc G: 31.730% 
LR: 2.000e-04 

2023-03-02 01:52:15,441 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3468 (0.3637) Acc D Real: 65.275% 
Loss D Fake: 0.7112 (0.7073) Acc D Fake: 68.002% 
Loss D: 1.058 
Loss G: 0.6774 (0.6818) Acc G: 31.812% 
LR: 2.000e-04 

2023-03-02 01:52:15,448 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4185 (0.3642) Acc D Real: 65.210% 
Loss D Fake: 0.7112 (0.7073) Acc D Fake: 67.923% 
Loss D: 1.130 
Loss G: 0.6771 (0.6817) Acc G: 31.892% 
LR: 2.000e-04 

2023-03-02 01:52:15,456 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2456 (0.3631) Acc D Real: 65.345% 
Loss D Fake: 0.7115 (0.7074) Acc D Fake: 67.846% 
Loss D: 0.957 
Loss G: 0.6771 (0.6817) Acc G: 31.971% 
LR: 2.000e-04 

2023-03-02 01:52:15,463 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3571 (0.3630) Acc D Real: 65.344% 
Loss D Fake: 0.7113 (0.7074) Acc D Fake: 67.755% 
Loss D: 1.068 
Loss G: 0.6772 (0.6816) Acc G: 32.048% 
LR: 2.000e-04 

2023-03-02 01:52:15,471 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3756 (0.3632) Acc D Real: 65.327% 
Loss D Fake: 0.7113 (0.7074) Acc D Fake: 67.665% 
Loss D: 1.087 
Loss G: 0.6770 (0.6816) Acc G: 32.139% 
LR: 2.000e-04 

2023-03-02 01:52:15,478 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3934 (0.3634) Acc D Real: 65.294% 
Loss D Fake: 0.7116 (0.7075) Acc D Fake: 67.561% 
Loss D: 1.105 
Loss G: 0.6765 (0.6815) Acc G: 32.245% 
LR: 2.000e-04 

2023-03-02 01:52:15,485 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3208 (0.3630) Acc D Real: 65.339% 
Loss D Fake: 0.7122 (0.7075) Acc D Fake: 67.460% 
Loss D: 1.033 
Loss G: 0.6760 (0.6815) Acc G: 32.349% 
LR: 2.000e-04 

2023-03-02 01:52:15,493 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4191 (0.3636) Acc D Real: 65.271% 
Loss D Fake: 0.7129 (0.7076) Acc D Fake: 67.344% 
Loss D: 1.132 
Loss G: 0.6751 (0.6814) Acc G: 32.481% 
LR: 2.000e-04 

2023-03-02 01:52:15,500 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3132 (0.3631) Acc D Real: 65.313% 
Loss D Fake: 0.7139 (0.7076) Acc D Fake: 67.200% 
Loss D: 1.027 
Loss G: 0.6742 (0.6814) Acc G: 32.627% 
LR: 2.000e-04 

2023-03-02 01:52:15,508 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3917 (0.3634) Acc D Real: 65.342% 
Loss D Fake: 0.7150 (0.7077) Acc D Fake: 67.029% 
Loss D: 1.107 
Loss G: 0.6729 (0.6813) Acc G: 32.815% 
LR: 2.000e-04 

2023-03-02 01:52:15,515 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3740 (0.3635) Acc D Real: 65.362% 
Loss D Fake: 0.7169 (0.7078) Acc D Fake: 66.770% 
Loss D: 1.091 
Loss G: 0.6711 (0.6812) Acc G: 33.138% 
LR: 2.000e-04 

2023-03-02 01:52:15,524 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3909 (0.3637) Acc D Real: 65.436% 
Loss D Fake: 2.5470 (0.7242) Acc D Fake: 66.174% 
Loss D: 2.938 
Loss G: 0.6717 (0.6811) Acc G: 33.735% 
LR: 2.000e-04 

2023-03-02 01:52:15,532 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3927 (0.3640) Acc D Real: 65.724% 
Loss D Fake: 0.7174 (0.7241) Acc D Fake: 65.589% 
Loss D: 1.110 
Loss G: 0.6702 (0.6810) Acc G: 34.322% 
LR: 2.000e-04 

2023-03-02 01:52:15,539 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3001 (0.3634) Acc D Real: 66.023% 
Loss D Fake: 0.7193 (0.7241) Acc D Fake: 65.013% 
Loss D: 1.019 
Loss G: 0.6685 (0.6809) Acc G: 34.898% 
LR: 2.000e-04 

2023-03-02 01:52:15,547 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3778 (0.3635) Acc D Real: 66.318% 
Loss D Fake: 0.7209 (0.7241) Acc D Fake: 64.448% 
Loss D: 1.099 
Loss G: 0.6671 (0.6808) Acc G: 35.464% 
LR: 2.000e-04 

2023-03-02 01:52:15,554 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2963 (0.3629) Acc D Real: 66.608% 
Loss D Fake: 0.7223 (0.7240) Acc D Fake: 63.892% 
Loss D: 1.019 
Loss G: 0.6661 (0.6807) Acc G: 36.020% 
LR: 2.000e-04 

2023-03-02 01:52:15,562 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.5230 (0.3643) Acc D Real: 66.892% 
Loss D Fake: 0.7235 (0.7240) Acc D Fake: 63.346% 
Loss D: 1.246 
Loss G: 0.6645 (0.6805) Acc G: 36.567% 
LR: 2.000e-04 

2023-03-02 01:52:15,569 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3331 (0.3640) Acc D Real: 67.172% 
Loss D Fake: 0.7252 (0.7241) Acc D Fake: 62.809% 
Loss D: 1.058 
Loss G: 0.6631 (0.6804) Acc G: 37.105% 
LR: 2.000e-04 

2023-03-02 01:52:15,577 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2969 (0.3635) Acc D Real: 67.447% 
Loss D Fake: 0.7263 (0.7241) Acc D Fake: 62.282% 
Loss D: 1.023 
Loss G: 0.6624 (0.6802) Acc G: 37.633% 
LR: 2.000e-04 

2023-03-02 01:52:15,584 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4635 (0.3643) Acc D Real: 67.717% 
Loss D Fake: 0.7271 (0.7241) Acc D Fake: 61.763% 
Loss D: 1.191 
Loss G: 0.6615 (0.6801) Acc G: 38.153% 
LR: 2.000e-04 

2023-03-02 01:52:15,592 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4423 (0.3650) Acc D Real: 67.984% 
Loss D Fake: 0.7283 (0.7241) Acc D Fake: 61.252% 
Loss D: 1.171 
Loss G: 0.6602 (0.6799) Acc G: 38.664% 
LR: 2.000e-04 

2023-03-02 01:52:15,600 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4241 (0.3654) Acc D Real: 68.246% 
Loss D Fake: 0.7298 (0.7242) Acc D Fake: 60.750% 
Loss D: 1.154 
Loss G: 0.6587 (0.6797) Acc G: 39.167% 
LR: 2.000e-04 

2023-03-02 01:52:15,608 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4036 (0.3658) Acc D Real: 68.504% 
Loss D Fake: 0.7314 (0.7242) Acc D Fake: 60.256% 
Loss D: 1.135 
Loss G: 0.6573 (0.6795) Acc G: 39.661% 
LR: 2.000e-04 

2023-03-02 01:52:15,616 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4259 (0.3662) Acc D Real: 68.758% 
Loss D Fake: 0.7329 (0.7243) Acc D Fake: 59.770% 
Loss D: 1.159 
Loss G: 0.6557 (0.6794) Acc G: 40.148% 
LR: 2.000e-04 

2023-03-02 01:52:15,623 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2710 (0.3655) Acc D Real: 69.007% 
Loss D Fake: 0.7344 (0.7244) Acc D Fake: 59.292% 
Loss D: 1.005 
Loss G: 0.6549 (0.6792) Acc G: 40.627% 
LR: 2.000e-04 

2023-03-02 01:52:15,630 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4405 (0.3661) Acc D Real: 69.252% 
Loss D Fake: 0.7351 (0.7245) Acc D Fake: 58.822% 
Loss D: 1.176 
Loss G: 0.6541 (0.6790) Acc G: 41.098% 
LR: 2.000e-04 

2023-03-02 01:52:15,638 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3944 (0.3663) Acc D Real: 69.494% 
Loss D Fake: 0.7361 (0.7246) Acc D Fake: 58.358% 
Loss D: 1.130 
Loss G: 0.6531 (0.6788) Acc G: 41.562% 
LR: 2.000e-04 

2023-03-02 01:52:15,646 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3845 (0.3664) Acc D Real: 69.732% 
Loss D Fake: 0.7370 (0.7247) Acc D Fake: 57.902% 
Loss D: 1.122 
Loss G: 0.6523 (0.6785) Acc G: 42.018% 
LR: 2.000e-04 

2023-03-02 01:52:15,654 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3971 (0.3667) Acc D Real: 69.967% 
Loss D Fake: 0.7379 (0.7248) Acc D Fake: 57.454% 
Loss D: 1.135 
Loss G: 0.6514 (0.6783) Acc G: 42.468% 
LR: 2.000e-04 

2023-03-02 01:52:15,663 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4729 (0.3675) Acc D Real: 70.198% 
Loss D Fake: 0.7390 (0.7249) Acc D Fake: 57.012% 
Loss D: 1.212 
Loss G: 0.6502 (0.6781) Acc G: 42.910% 
LR: 2.000e-04 

2023-03-02 01:52:15,671 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3599 (0.3674) Acc D Real: 70.425% 
Loss D Fake: 0.7404 (0.7250) Acc D Fake: 56.576% 
Loss D: 1.100 
Loss G: 0.6491 (0.6779) Acc G: 43.346% 
LR: 2.000e-04 

2023-03-02 01:52:15,679 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4333 (0.3679) Acc D Real: 70.648% 
Loss D Fake: 0.7416 (0.7251) Acc D Fake: 56.148% 
Loss D: 1.175 
Loss G: 0.6479 (0.6777) Acc G: 43.775% 
LR: 2.000e-04 

2023-03-02 01:52:15,687 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3744 (0.3680) Acc D Real: 70.869% 
Loss D Fake: 0.7429 (0.7253) Acc D Fake: 55.726% 
Loss D: 1.117 
Loss G: 0.6468 (0.6774) Acc G: 44.198% 
LR: 2.000e-04 

2023-03-02 01:52:15,695 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4682 (0.3687) Acc D Real: 71.086% 
Loss D Fake: 0.7442 (0.7254) Acc D Fake: 55.310% 
Loss D: 1.212 
Loss G: 0.6453 (0.6772) Acc G: 44.614% 
LR: 2.000e-04 

2023-03-02 01:52:15,702 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3969 (0.3689) Acc D Real: 71.299% 
Loss D Fake: 0.7459 (0.7255) Acc D Fake: 54.900% 
Loss D: 1.143 
Loss G: 0.6439 (0.6770) Acc G: 45.025% 
LR: 2.000e-04 

2023-03-02 01:52:15,710 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3403 (0.3687) Acc D Real: 71.510% 
Loss D Fake: 0.7473 (0.7257) Acc D Fake: 54.496% 
Loss D: 1.088 
Loss G: 0.6428 (0.6767) Acc G: 45.429% 
LR: 2.000e-04 

2023-03-02 01:52:15,717 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4033 (0.3690) Acc D Real: 71.718% 
Loss D Fake: 0.7484 (0.7259) Acc D Fake: 54.099% 
Loss D: 1.152 
Loss G: 0.6417 (0.6764) Acc G: 45.827% 
LR: 2.000e-04 

2023-03-02 01:52:15,725 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4644 (0.3697) Acc D Real: 71.923% 
Loss D Fake: 0.7497 (0.7260) Acc D Fake: 53.707% 
Loss D: 1.214 
Loss G: 0.6404 (0.6762) Acc G: 46.220% 
LR: 2.000e-04 

2023-03-02 01:52:15,733 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3912 (0.3698) Acc D Real: 72.124% 
Loss D Fake: 0.7513 (0.7262) Acc D Fake: 53.320% 
Loss D: 1.143 
Loss G: 0.6390 (0.6759) Acc G: 46.607% 
LR: 2.000e-04 

2023-03-02 01:52:15,741 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3177 (0.3695) Acc D Real: 72.323% 
Loss D Fake: 0.7525 (0.7264) Acc D Fake: 52.939% 
Loss D: 1.070 
Loss G: 0.6383 (0.6756) Acc G: 46.988% 
LR: 2.000e-04 

2023-03-02 01:52:15,748 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3207 (0.3691) Acc D Real: 72.520% 
Loss D Fake: 0.7531 (0.7266) Acc D Fake: 52.564% 
Loss D: 1.074 
Loss G: 0.6381 (0.6754) Acc G: 47.364% 
LR: 2.000e-04 

2023-03-02 01:52:15,756 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4252 (0.3695) Acc D Real: 72.713% 
Loss D Fake: 0.7532 (0.7268) Acc D Fake: 52.194% 
Loss D: 1.178 
Loss G: 0.6378 (0.6751) Acc G: 47.735% 
LR: 2.000e-04 

2023-03-02 01:52:15,763 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3191 (0.3692) Acc D Real: 72.904% 
Loss D Fake: 0.7536 (0.7270) Acc D Fake: 51.829% 
Loss D: 1.073 
Loss G: 0.6377 (0.6749) Acc G: 48.100% 
LR: 2.000e-04 

2023-03-02 01:52:15,771 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3319 (0.3689) Acc D Real: 73.092% 
Loss D Fake: 0.7534 (0.7272) Acc D Fake: 51.469% 
Loss D: 1.085 
Loss G: 0.6379 (0.6746) Acc G: 48.461% 
LR: 2.000e-04 

2023-03-02 01:52:15,778 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4068 (0.3692) Acc D Real: 73.277% 
Loss D Fake: 0.7532 (0.7273) Acc D Fake: 51.114% 
Loss D: 1.160 
Loss G: 0.6380 (0.6743) Acc G: 48.816% 
LR: 2.000e-04 

2023-03-02 01:52:15,785 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2995 (0.3687) Acc D Real: 73.460% 
Loss D Fake: 0.7530 (0.7275) Acc D Fake: 50.764% 
Loss D: 1.052 
Loss G: 0.6384 (0.6741) Acc G: 49.167% 
LR: 2.000e-04 

2023-03-02 01:52:15,793 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3490 (0.3685) Acc D Real: 73.640% 
Loss D Fake: 0.7524 (0.7277) Acc D Fake: 50.418% 
Loss D: 1.101 
Loss G: 0.6390 (0.6739) Acc G: 49.512% 
LR: 2.000e-04 

2023-03-02 01:52:15,801 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3934 (0.3687) Acc D Real: 73.818% 
Loss D Fake: 0.7517 (0.7278) Acc D Fake: 50.078% 
Loss D: 1.145 
Loss G: 0.6395 (0.6736) Acc G: 49.854% 
LR: 2.000e-04 

2023-03-02 01:52:15,808 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4072 (0.3690) Acc D Real: 73.993% 
Loss D Fake: 0.7514 (0.7280) Acc D Fake: 49.742% 
Loss D: 1.159 
Loss G: 0.6396 (0.6734) Acc G: 50.190% 
LR: 2.000e-04 

2023-03-02 01:52:15,815 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3365 (0.3688) Acc D Real: 74.166% 
Loss D Fake: 0.7513 (0.7282) Acc D Fake: 49.410% 
Loss D: 1.088 
Loss G: 0.6398 (0.6732) Acc G: 50.522% 
LR: 2.000e-04 

2023-03-02 01:52:15,823 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3425 (0.3686) Acc D Real: 74.337% 
Loss D Fake: 0.7510 (0.7283) Acc D Fake: 49.083% 
Loss D: 1.093 
Loss G: 0.6402 (0.6730) Acc G: 50.850% 
LR: 2.000e-04 

2023-03-02 01:52:15,830 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4121 (0.3689) Acc D Real: 74.506% 
Loss D Fake: 0.7506 (0.7285) Acc D Fake: 48.760% 
Loss D: 1.163 
Loss G: 0.6404 (0.6727) Acc G: 51.173% 
LR: 2.000e-04 

2023-03-02 01:52:15,838 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3647 (0.3688) Acc D Real: 74.673% 
Loss D Fake: 0.7504 (0.7286) Acc D Fake: 48.441% 
Loss D: 1.115 
Loss G: 0.6405 (0.6725) Acc G: 51.492% 
LR: 2.000e-04 

2023-03-02 01:52:15,845 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4102 (0.3691) Acc D Real: 74.836% 
Loss D Fake: 0.7503 (0.7287) Acc D Fake: 48.127% 
Loss D: 1.160 
Loss G: 0.6405 (0.6723) Acc G: 51.807% 
LR: 2.000e-04 

2023-03-02 01:52:15,853 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3715 (0.3691) Acc D Real: 74.998% 
Loss D Fake: 0.7504 (0.7289) Acc D Fake: 47.816% 
Loss D: 1.122 
Loss G: 0.6404 (0.6721) Acc G: 52.118% 
LR: 2.000e-04 

2023-03-02 01:52:15,860 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3760 (0.3692) Acc D Real: 75.159% 
Loss D Fake: 0.7505 (0.7290) Acc D Fake: 47.510% 
Loss D: 1.127 
Loss G: 0.6403 (0.6719) Acc G: 52.425% 
LR: 2.000e-04 

2023-03-02 01:52:15,868 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4957 (0.3700) Acc D Real: 75.317% 
Loss D Fake: 0.7509 (0.7292) Acc D Fake: 47.207% 
Loss D: 1.247 
Loss G: 0.6395 (0.6717) Acc G: 52.728% 
LR: 2.000e-04 

2023-03-02 01:52:15,875 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2512 (0.3692) Acc D Real: 75.332% 
Loss D Fake: 0.7516 (0.7293) Acc D Fake: 47.179% 
Loss D: 1.003 
Loss G: 0.6393 (0.6715) Acc G: 52.756% 
LR: 2.000e-04 

2023-03-02 01:52:16,104 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.845 | Generator Loss: 0.639 | Avg: 1.485 
2023-03-02 01:52:16,127 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.939 | Generator Loss: 0.639 | Avg: 1.579 
2023-03-02 01:52:16,150 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.914 | Generator Loss: 0.639 | Avg: 1.553 
2023-03-02 01:52:16,178 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.889 | Generator Loss: 0.639 | Avg: 1.528 
2023-03-02 01:52:16,208 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.890 | Generator Loss: 0.639 | Avg: 1.529 
2023-03-02 01:52:16,235 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.933 | Generator Loss: 0.639 | Avg: 1.572 
2023-03-02 01:52:16,262 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.962 | Generator Loss: 0.639 | Avg: 1.602 
2023-03-02 01:52:16,288 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.003 | Generator Loss: 0.639 | Avg: 1.642 
2023-03-02 01:52:16,314 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.027 | Generator Loss: 0.639 | Avg: 1.666 
2023-03-02 01:52:16,342 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.062 | Generator Loss: 0.639 | Avg: 1.702 
2023-03-02 01:52:16,369 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.090 | Generator Loss: 0.639 | Avg: 1.730 
2023-03-02 01:52:16,403 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.115 | Generator Loss: 0.639 | Avg: 1.754 
2023-03-02 01:52:16,430 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.136 | Generator Loss: 0.639 | Avg: 1.775 
2023-03-02 01:52:16,457 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.152 | Generator Loss: 0.639 | Avg: 1.791 
2023-03-02 01:52:16,483 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.140 | Generator Loss: 0.639 | Avg: 1.779 
2023-03-02 01:52:16,510 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.127 | Generator Loss: 0.639 | Avg: 1.766 
2023-03-02 01:52:16,537 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.113 | Generator Loss: 0.639 | Avg: 1.753 
2023-03-02 01:52:16,571 -                train: [    INFO] - 
Epoch: 20/20
2023-03-02 01:52:16,746 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3939 (0.3941) Acc D Real: 100.000% 
Loss D Fake: 0.7518 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.146 
Loss G: 0.6391 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,754 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3385 (0.3756) Acc D Real: 100.000% 
Loss D Fake: 0.7520 (0.7518) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6391 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,764 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2320 (0.3397) Acc D Real: 100.000% 
Loss D Fake: 0.7516 (0.7517) Acc D Fake: 0.000% 
Loss D: 0.984 
Loss G: 0.6399 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,782 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3918 (0.3501) Acc D Real: 100.000% 
Loss D Fake: 0.7505 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.142 
Loss G: 0.6407 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,789 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3524 (0.3505) Acc D Real: 100.000% 
Loss D Fake: 0.7497 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6414 (0.6399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,796 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3867 (0.3556) Acc D Real: 100.000% 
Loss D Fake: 0.7489 (0.7509) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6420 (0.6402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,803 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4078 (0.3622) Acc D Real: 99.993% 
Loss D Fake: 0.7485 (0.7506) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.6422 (0.6405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,810 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3208 (0.3576) Acc D Real: 99.994% 
Loss D Fake: 0.7483 (0.7503) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6426 (0.6407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,817 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4903 (0.3708) Acc D Real: 99.995% 
Loss D Fake: 0.7480 (0.7501) Acc D Fake: 0.000% 
Loss D: 1.238 
Loss G: 0.6423 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,824 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4605 (0.3790) Acc D Real: 99.991% 
Loss D Fake: 0.7487 (0.7500) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.6414 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,831 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4104 (0.3816) Acc D Real: 99.991% 
Loss D Fake: 0.7499 (0.7500) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6403 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,838 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4430 (0.3863) Acc D Real: 99.988% 
Loss D Fake: 0.7512 (0.7501) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6390 (0.6407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,845 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3360 (0.3827) Acc D Real: 99.985% 
Loss D Fake: 0.7527 (0.7502) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6379 (0.6405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,852 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3492 (0.3805) Acc D Real: 99.986% 
Loss D Fake: 0.7536 (0.7505) Acc D Fake: 0.000% 
Loss D: 1.103 
Loss G: 0.6372 (0.6403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,859 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3224 (0.3769) Acc D Real: 99.987% 
Loss D Fake: 0.7542 (0.7507) Acc D Fake: 0.000% 
Loss D: 1.077 
Loss G: 0.6370 (0.6401) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,866 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3893 (0.3776) Acc D Real: 99.985% 
Loss D Fake: 0.7544 (0.7509) Acc D Fake: 0.000% 
Loss D: 1.144 
Loss G: 0.6367 (0.6399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,873 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3618 (0.3767) Acc D Real: 99.980% 
Loss D Fake: 0.7546 (0.7511) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6366 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,880 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3721 (0.3765) Acc D Real: 99.978% 
Loss D Fake: 0.7547 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6365 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,887 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3679 (0.3761) Acc D Real: 99.979% 
Loss D Fake: 0.7548 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6364 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,894 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2969 (0.3723) Acc D Real: 99.980% 
Loss D Fake: 0.7547 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6368 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,900 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2707 (0.3677) Acc D Real: 99.979% 
Loss D Fake: 0.7538 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.025 
Loss G: 0.6379 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,907 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3290 (0.3660) Acc D Real: 99.980% 
Loss D Fake: 0.7525 (0.7518) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6391 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,914 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3629 (0.3659) Acc D Real: 99.980% 
Loss D Fake: 0.7511 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.114 
Loss G: 0.6403 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,921 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4516 (0.3693) Acc D Real: 99.981% 
Loss D Fake: 0.7501 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.202 
Loss G: 0.6408 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,928 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3346 (0.3680) Acc D Real: 99.980% 
Loss D Fake: 0.7496 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.084 
Loss G: 0.6413 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,935 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3527 (0.3674) Acc D Real: 99.981% 
Loss D Fake: 0.7490 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6419 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,942 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4125 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7486 (0.7514) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.6421 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,950 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.4289 (0.3711) Acc D Real: 99.980% 
Loss D Fake: 0.7486 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6419 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,957 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3981 (0.3720) Acc D Real: 99.977% 
Loss D Fake: 0.7490 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6414 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,964 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4575 (0.3747) Acc D Real: 99.978% 
Loss D Fake: 0.7496 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.6405 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,972 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3668 (0.3745) Acc D Real: 99.977% 
Loss D Fake: 0.7507 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.118 
Loss G: 0.6396 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,979 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3583 (0.3740) Acc D Real: 99.978% 
Loss D Fake: 0.7516 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6389 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,987 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3980 (0.3747) Acc D Real: 99.977% 
Loss D Fake: 0.7523 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6383 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:16,994 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4085 (0.3757) Acc D Real: 99.978% 
Loss D Fake: 0.7531 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6375 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,001 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3907 (0.3761) Acc D Real: 99.975% 
Loss D Fake: 0.7540 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.145 
Loss G: 0.6367 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,009 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4047 (0.3769) Acc D Real: 99.976% 
Loss D Fake: 0.7550 (0.7514) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6358 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,016 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4622 (0.3791) Acc D Real: 99.977% 
Loss D Fake: 0.7562 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.218 
Loss G: 0.6345 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,024 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3317 (0.3779) Acc D Real: 99.976% 
Loss D Fake: 0.7575 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6336 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,031 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4429 (0.3795) Acc D Real: 99.977% 
Loss D Fake: 0.7585 (0.7519) Acc D Fake: 0.000% 
Loss D: 1.201 
Loss G: 0.6325 (0.6390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,039 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3491 (0.3788) Acc D Real: 99.977% 
Loss D Fake: 0.7597 (0.7521) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.6317 (0.6388) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,046 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3664 (0.3785) Acc D Real: 99.976% 
Loss D Fake: 0.7606 (0.7523) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6310 (0.6386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,054 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4235 (0.3795) Acc D Real: 99.977% 
Loss D Fake: 0.7613 (0.7525) Acc D Fake: 0.000% 
Loss D: 1.185 
Loss G: 0.6304 (0.6384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,061 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3194 (0.3782) Acc D Real: 99.978% 
Loss D Fake: 0.7619 (0.7527) Acc D Fake: 0.000% 
Loss D: 1.081 
Loss G: 0.6300 (0.6382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,068 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3824 (0.3783) Acc D Real: 99.978% 
Loss D Fake: 0.7622 (0.7529) Acc D Fake: 0.000% 
Loss D: 1.145 
Loss G: 0.6298 (0.6380) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,076 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4692 (0.3802) Acc D Real: 99.978% 
Loss D Fake: 0.7627 (0.7531) Acc D Fake: 0.000% 
Loss D: 1.232 
Loss G: 0.6290 (0.6379) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,083 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3105 (0.3787) Acc D Real: 99.979% 
Loss D Fake: 0.7635 (0.7533) Acc D Fake: 0.000% 
Loss D: 1.074 
Loss G: 0.6286 (0.6377) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,091 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3568 (0.3783) Acc D Real: 99.979% 
Loss D Fake: 0.7638 (0.7536) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6284 (0.6375) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,099 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2958 (0.3766) Acc D Real: 99.980% 
Loss D Fake: 0.7637 (0.7538) Acc D Fake: 0.000% 
Loss D: 1.059 
Loss G: 0.6288 (0.6373) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,108 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3871 (0.3768) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7540) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6291 (0.6371) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,118 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2932 (0.3752) Acc D Real: 99.981% 
Loss D Fake: 0.7627 (0.7541) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.6298 (0.6370) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,128 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3644 (0.3750) Acc D Real: 99.981% 
Loss D Fake: 0.7619 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.126 
Loss G: 0.6304 (0.6369) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,136 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4719 (0.3768) Acc D Real: 99.981% 
Loss D Fake: 0.7614 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.233 
Loss G: 0.6305 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,145 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3731 (0.3767) Acc D Real: 99.982% 
Loss D Fake: 0.7616 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6304 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,154 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3352 (0.3760) Acc D Real: 99.979% 
Loss D Fake: 0.7616 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6305 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,162 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3319 (0.3752) Acc D Real: 99.980% 
Loss D Fake: 0.7613 (0.7548) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6308 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,170 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4385 (0.3763) Acc D Real: 99.980% 
Loss D Fake: 0.7610 (0.7549) Acc D Fake: 0.000% 
Loss D: 1.200 
Loss G: 0.6308 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,177 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3985 (0.3767) Acc D Real: 99.980% 
Loss D Fake: 0.7613 (0.7550) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6305 (0.6362) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,185 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3447 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7616 (0.7551) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6303 (0.6361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,193 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.5042 (0.3783) Acc D Real: 99.981% 
Loss D Fake: 0.7621 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.266 
Loss G: 0.6295 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,202 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2959 (0.3769) Acc D Real: 99.980% 
Loss D Fake: 0.7630 (0.7554) Acc D Fake: 0.000% 
Loss D: 1.059 
Loss G: 0.6290 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,210 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3311 (0.3762) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6290 (0.6358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,218 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3972 (0.3765) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7556) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6289 (0.6357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,226 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3419 (0.3760) Acc D Real: 99.980% 
Loss D Fake: 0.7634 (0.7557) Acc D Fake: 0.000% 
Loss D: 1.105 
Loss G: 0.6289 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,234 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2930 (0.3747) Acc D Real: 99.980% 
Loss D Fake: 0.7631 (0.7559) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.6293 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,242 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4356 (0.3756) Acc D Real: 99.980% 
Loss D Fake: 0.7627 (0.7560) Acc D Fake: 0.000% 
Loss D: 1.198 
Loss G: 0.6294 (0.6354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,250 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4165 (0.3762) Acc D Real: 99.981% 
Loss D Fake: 0.7628 (0.7561) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6292 (0.6353) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,257 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3876 (0.3764) Acc D Real: 99.981% 
Loss D Fake: 0.7631 (0.7562) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6289 (0.6352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,265 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3353 (0.3758) Acc D Real: 99.981% 
Loss D Fake: 0.7633 (0.7563) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6288 (0.6351) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,272 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3965 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7634 (0.7564) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6287 (0.6350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,280 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4041 (0.3765) Acc D Real: 99.982% 
Loss D Fake: 0.7637 (0.7565) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6284 (0.6349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,288 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3896 (0.3767) Acc D Real: 99.981% 
Loss D Fake: 0.7641 (0.7566) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6280 (0.6348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,295 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3344 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7645 (0.7567) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6278 (0.6347) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,303 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2557 (0.3745) Acc D Real: 99.982% 
Loss D Fake: 0.7643 (0.7568) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.6284 (0.6346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,310 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3646 (0.3743) Acc D Real: 99.982% 
Loss D Fake: 0.7634 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6291 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,318 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3311 (0.3738) Acc D Real: 99.982% 
Loss D Fake: 0.7626 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6298 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,325 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4028 (0.3741) Acc D Real: 99.982% 
Loss D Fake: 0.7618 (0.7570) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6304 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,332 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3246 (0.3735) Acc D Real: 99.983% 
Loss D Fake: 0.7612 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6311 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,340 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4335 (0.3743) Acc D Real: 99.983% 
Loss D Fake: 0.7605 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6314 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,348 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4486 (0.3752) Acc D Real: 99.983% 
Loss D Fake: 0.7605 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.6311 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,356 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2420 (0.3736) Acc D Real: 99.983% 
Loss D Fake: 0.7606 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.003 
Loss G: 0.6315 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,364 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4192 (0.3741) Acc D Real: 99.983% 
Loss D Fake: 0.7600 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6318 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,372 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3532 (0.3739) Acc D Real: 99.984% 
Loss D Fake: 0.7598 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6320 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,380 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3875 (0.3740) Acc D Real: 99.984% 
Loss D Fake: 0.7596 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6321 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,387 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3641 (0.3739) Acc D Real: 99.984% 
Loss D Fake: 0.7595 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6322 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,395 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3427 (0.3735) Acc D Real: 99.984% 
Loss D Fake: 0.7593 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6324 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,403 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3998 (0.3738) Acc D Real: 99.983% 
Loss D Fake: 0.7592 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.159 
Loss G: 0.6324 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,411 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3634 (0.3737) Acc D Real: 99.983% 
Loss D Fake: 0.7592 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6324 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,418 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4360 (0.3744) Acc D Real: 99.984% 
Loss D Fake: 0.7594 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.195 
Loss G: 0.6320 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,426 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4063 (0.3748) Acc D Real: 99.984% 
Loss D Fake: 0.7599 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6315 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,433 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4416 (0.3755) Acc D Real: 99.984% 
Loss D Fake: 0.7607 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.202 
Loss G: 0.6306 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,441 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3884 (0.3757) Acc D Real: 99.984% 
Loss D Fake: 0.7618 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6297 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,448 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4021 (0.3759) Acc D Real: 99.984% 
Loss D Fake: 0.7628 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6288 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,456 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4045 (0.3762) Acc D Real: 99.984% 
Loss D Fake: 0.7639 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6278 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,464 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2886 (0.3753) Acc D Real: 99.984% 
Loss D Fake: 0.7648 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.053 
Loss G: 0.6274 (0.6338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,471 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3285 (0.3748) Acc D Real: 99.984% 
Loss D Fake: 0.7650 (0.7578) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6274 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,479 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3144 (0.3742) Acc D Real: 99.984% 
Loss D Fake: 0.7648 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.079 
Loss G: 0.6278 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,486 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3239 (0.3737) Acc D Real: 99.984% 
Loss D Fake: 0.7641 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.088 
Loss G: 0.6285 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,494 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4194 (0.3742) Acc D Real: 99.984% 
Loss D Fake: 0.7633 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.183 
Loss G: 0.6289 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,502 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3647 (0.3741) Acc D Real: 99.984% 
Loss D Fake: 0.7630 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6292 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,509 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3351 (0.3737) Acc D Real: 99.985% 
Loss D Fake: 0.7626 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.6297 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,517 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2379 (0.3723) Acc D Real: 99.985% 
Loss D Fake: 0.7617 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.000 
Loss G: 0.6309 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,524 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3938 (0.3726) Acc D Real: 99.985% 
Loss D Fake: 0.7602 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6320 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,532 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3324 (0.3722) Acc D Real: 99.984% 
Loss D Fake: 0.7590 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.091 
Loss G: 0.6330 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,540 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3303 (0.3718) Acc D Real: 99.985% 
Loss D Fake: 0.7578 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.088 
Loss G: 0.6341 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,548 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2930 (0.3710) Acc D Real: 99.985% 
Loss D Fake: 0.7565 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.049 
Loss G: 0.6354 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,555 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3672 (0.3710) Acc D Real: 99.984% 
Loss D Fake: 0.7550 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.122 
Loss G: 0.6366 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,563 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3693 (0.3710) Acc D Real: 99.984% 
Loss D Fake: 0.7538 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6376 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,571 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3126 (0.3704) Acc D Real: 99.984% 
Loss D Fake: 0.7527 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.065 
Loss G: 0.6387 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,579 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3695 (0.3704) Acc D Real: 99.984% 
Loss D Fake: 0.7515 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6397 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,587 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4455 (0.3711) Acc D Real: 99.985% 
Loss D Fake: 0.7506 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.196 
Loss G: 0.6401 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,595 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3178 (0.3706) Acc D Real: 99.984% 
Loss D Fake: 0.7502 (0.7578) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6406 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,602 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3891 (0.3708) Acc D Real: 99.984% 
Loss D Fake: 0.7496 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6410 (0.6338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,610 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2840 (0.3700) Acc D Real: 99.984% 
Loss D Fake: 0.7490 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.033 
Loss G: 0.6418 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,617 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3039 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7480 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6429 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,624 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3381 (0.3692) Acc D Real: 99.983% 
Loss D Fake: 0.7467 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6440 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,632 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4388 (0.3698) Acc D Real: 99.983% 
Loss D Fake: 0.7457 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.185 
Loss G: 0.6446 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,639 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3771 (0.3698) Acc D Real: 99.983% 
Loss D Fake: 0.7453 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.122 
Loss G: 0.6449 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,647 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3958 (0.3701) Acc D Real: 99.983% 
Loss D Fake: 0.7451 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6449 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,655 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3235 (0.3697) Acc D Real: 99.983% 
Loss D Fake: 0.7450 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6452 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,662 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3429 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7446 (0.7570) Acc D Fake: 0.000% 
Loss D: 1.087 
Loss G: 0.6456 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,669 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3715 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7441 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6459 (0.6346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,677 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3884 (0.3696) Acc D Real: 99.982% 
Loss D Fake: 0.7439 (0.7568) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6460 (0.6347) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,684 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3426 (0.3694) Acc D Real: 99.982% 
Loss D Fake: 0.7438 (0.7567) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6462 (0.6348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,692 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3255 (0.3691) Acc D Real: 99.983% 
Loss D Fake: 0.7434 (0.7565) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6466 (0.6349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,699 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3727 (0.3691) Acc D Real: 99.983% 
Loss D Fake: 0.7429 (0.7564) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6470 (0.6350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,707 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3586 (0.3690) Acc D Real: 99.983% 
Loss D Fake: 0.7426 (0.7563) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6473 (0.6351) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,714 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3393 (0.3688) Acc D Real: 99.983% 
Loss D Fake: 0.7422 (0.7562) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6477 (0.6352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,722 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3702 (0.3688) Acc D Real: 99.983% 
Loss D Fake: 0.7418 (0.7561) Acc D Fake: 0.000% 
Loss D: 1.112 
Loss G: 0.6480 (0.6353) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,729 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4137 (0.3691) Acc D Real: 99.982% 
Loss D Fake: 0.7416 (0.7560) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6479 (0.6354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,737 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3787 (0.3692) Acc D Real: 99.982% 
Loss D Fake: 0.7418 (0.7559) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6477 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,747 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4794 (0.3700) Acc D Real: 99.982% 
Loss D Fake: 0.7423 (0.7558) Acc D Fake: 0.000% 
Loss D: 1.222 
Loss G: 0.6468 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,756 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3006 (0.3695) Acc D Real: 99.982% 
Loss D Fake: 0.7433 (0.7557) Acc D Fake: 0.000% 
Loss D: 1.044 
Loss G: 0.6462 (0.6356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,766 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2601 (0.3687) Acc D Real: 99.982% 
Loss D Fake: 0.7436 (0.7556) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.6464 (0.6357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,773 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4238 (0.3691) Acc D Real: 99.982% 
Loss D Fake: 0.7433 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.6464 (0.6358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,781 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2940 (0.3686) Acc D Real: 99.982% 
Loss D Fake: 0.7434 (0.7554) Acc D Fake: 0.000% 
Loss D: 1.037 
Loss G: 0.6465 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,788 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3820 (0.3686) Acc D Real: 99.981% 
Loss D Fake: 0.7431 (0.7553) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6467 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,796 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3809 (0.3687) Acc D Real: 99.980% 
Loss D Fake: 0.7429 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6468 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,803 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3516 (0.3686) Acc D Real: 99.981% 
Loss D Fake: 0.7428 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6470 (0.6361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,810 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4329 (0.3691) Acc D Real: 99.980% 
Loss D Fake: 0.7427 (0.7551) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6468 (0.6362) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,817 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3546 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7431 (0.7550) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.6466 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,825 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3695 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7433 (0.7549) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6463 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,832 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4819 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7438 (0.7548) Acc D Fake: 0.000% 
Loss D: 1.226 
Loss G: 0.6455 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,839 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3567 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7448 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6447 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,847 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4427 (0.3702) Acc D Real: 99.980% 
Loss D Fake: 0.7457 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.6437 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,854 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3703 (0.3702) Acc D Real: 99.980% 
Loss D Fake: 0.7470 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6426 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,861 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3562 (0.3701) Acc D Real: 99.979% 
Loss D Fake: 0.7481 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6418 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,870 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3310 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7488 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.080 
Loss G: 0.6412 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,877 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3565 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7493 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6409 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,884 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3363 (0.3695) Acc D Real: 99.980% 
Loss D Fake: 0.7495 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6408 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,891 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4031 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7496 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6406 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,899 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3871 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7500 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6402 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,906 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3348 (0.3696) Acc D Real: 99.980% 
Loss D Fake: 0.7504 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6400 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,913 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3699 (0.3696) Acc D Real: 99.980% 
Loss D Fake: 0.7505 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6399 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,920 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3759 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7507 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6397 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,928 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2994 (0.3692) Acc D Real: 99.979% 
Loss D Fake: 0.7508 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.050 
Loss G: 0.6398 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,935 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4256 (0.3696) Acc D Real: 99.979% 
Loss D Fake: 0.7506 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6398 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:17,942 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2504 (0.3688) Acc D Real: 99.979% 
Loss D Fake: 0.7505 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.001 
Loss G: 0.6403 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 01:52:18,159 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.825 | Generator Loss: 0.640 | Avg: 1.465 
2023-03-02 01:52:18,182 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.912 | Generator Loss: 0.640 | Avg: 1.552 
2023-03-02 01:52:18,205 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.889 | Generator Loss: 0.640 | Avg: 1.529 
2023-03-02 01:52:18,232 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.867 | Generator Loss: 0.640 | Avg: 1.507 
2023-03-02 01:52:18,258 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.865 | Generator Loss: 0.640 | Avg: 1.506 
2023-03-02 01:52:18,285 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.911 | Generator Loss: 0.640 | Avg: 1.551 
2023-03-02 01:52:18,312 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.935 | Generator Loss: 0.640 | Avg: 1.575 
2023-03-02 01:52:18,338 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.975 | Generator Loss: 0.640 | Avg: 1.615 
2023-03-02 01:52:18,365 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.997 | Generator Loss: 0.640 | Avg: 1.638 
2023-03-02 01:52:18,391 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.035 | Generator Loss: 0.640 | Avg: 1.675 
2023-03-02 01:52:18,418 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.062 | Generator Loss: 0.640 | Avg: 1.703 
2023-03-02 01:52:18,444 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.089 | Generator Loss: 0.640 | Avg: 1.729 
2023-03-02 01:52:18,474 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.110 | Generator Loss: 0.640 | Avg: 1.751 
2023-03-02 01:52:18,501 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.126 | Generator Loss: 0.640 | Avg: 1.766 
2023-03-02 01:52:18,528 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.113 | Generator Loss: 0.640 | Avg: 1.753 
2023-03-02 01:52:18,554 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.099 | Generator Loss: 0.640 | Avg: 1.740 
2023-03-02 01:52:18,580 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.086 | Generator Loss: 0.640 | Avg: 1.726 
2023-03-02 01:52:18,616 -                train: [    INFO] - Best Metric: At 2 Epoch Gen 0.849 Dis
2023-03-02 01:52:18,616 -                train: [    INFO] - MODEL TRAINING COMPLETED. 
 BEST RESULT SAVED
2023-03-02 01:52:19,074 -                train: [    INFO] - TEST [11/44]: Discriminator Loss: 0.619 | Generator Loss: 0.849 | Avg: 1.468 
2023-03-02 01:52:19,096 -                train: [    INFO] - TEST [21/44]: Discriminator Loss: 0.617 | Generator Loss: 0.849 | Avg: 1.465 
2023-03-02 01:52:19,118 -                train: [    INFO] - TEST [31/44]: Discriminator Loss: 0.615 | Generator Loss: 0.849 | Avg: 1.464 
2023-03-02 01:52:19,143 -                train: [    INFO] - TEST [41/44]: Discriminator Loss: 0.616 | Generator Loss: 0.849 | Avg: 1.464 
2023-03-02 01:52:19,550 -         Optimization: [    INFO] - 
 Batch: 1/121
2023-03-02 01:52:21,833 -         Optimization: [    INFO] - Batch [1/121]: Anomaly Score: 466.257 label: 0.0
2023-03-02 01:52:21,834 -         Optimization: [    INFO] - 
 Batch: 2/121
2023-03-02 01:52:24,106 -         Optimization: [    INFO] - Batch [2/121]: Anomaly Score: 529.977 label: 0.0
2023-03-02 01:52:24,107 -         Optimization: [    INFO] - 
 Batch: 3/121
2023-03-02 01:52:26,367 -         Optimization: [    INFO] - Batch [3/121]: Anomaly Score: 504.801 label: 0.0
2023-03-02 01:52:26,368 -         Optimization: [    INFO] - 
 Batch: 4/121
2023-03-02 01:52:28,629 -         Optimization: [    INFO] - Batch [4/121]: Anomaly Score: 443.575 label: 0.0
2023-03-02 01:52:28,634 -         Optimization: [    INFO] - 
 Batch: 5/121
2023-03-02 01:52:30,896 -         Optimization: [    INFO] - Batch [5/121]: Anomaly Score: 504.627 label: 0.0
2023-03-02 01:52:30,897 -         Optimization: [    INFO] - 
 Batch: 6/121
2023-03-02 01:52:33,151 -         Optimization: [    INFO] - Batch [6/121]: Anomaly Score: 389.892 label: 0.0
2023-03-02 01:52:33,152 -         Optimization: [    INFO] - 
 Batch: 7/121
2023-03-02 01:52:35,419 -         Optimization: [    INFO] - Batch [7/121]: Anomaly Score: 413.785 label: 0.0
2023-03-02 01:52:35,420 -         Optimization: [    INFO] - 
 Batch: 8/121
2023-03-02 01:52:37,651 -         Optimization: [    INFO] - Batch [8/121]: Anomaly Score: 510.531 label: 0.0
2023-03-02 01:52:37,652 -         Optimization: [    INFO] - 
 Batch: 9/121
2023-03-02 01:52:39,890 -         Optimization: [    INFO] - Batch [9/121]: Anomaly Score: 463.740 label: 0.0
2023-03-02 01:52:39,891 -         Optimization: [    INFO] - 
 Batch: 10/121
2023-03-02 01:52:42,137 -         Optimization: [    INFO] - Batch [10/121]: Anomaly Score: 358.456 label: 0.0
2023-03-02 01:52:42,138 -         Optimization: [    INFO] - 
 Batch: 11/121
2023-03-02 01:52:44,598 -         Optimization: [    INFO] - Batch [11/121]: Anomaly Score: 389.799 label: 0.0
2023-03-02 01:52:44,599 -         Optimization: [    INFO] - 
 Batch: 12/121
2023-03-02 01:52:46,968 -         Optimization: [    INFO] - Batch [12/121]: Anomaly Score: 351.639 label: 0.0
2023-03-02 01:52:46,969 -         Optimization: [    INFO] - 
 Batch: 13/121
2023-03-02 01:52:49,242 -         Optimization: [    INFO] - Batch [13/121]: Anomaly Score: 499.466 label: 0.0
2023-03-02 01:52:49,243 -         Optimization: [    INFO] - 
 Batch: 14/121
2023-03-02 01:52:51,542 -         Optimization: [    INFO] - Batch [14/121]: Anomaly Score: 420.870 label: 0.0
2023-03-02 01:52:51,543 -         Optimization: [    INFO] - 
 Batch: 15/121
2023-03-02 01:52:53,843 -         Optimization: [    INFO] - Batch [15/121]: Anomaly Score: 481.751 label: 0.0
2023-03-02 01:52:53,844 -         Optimization: [    INFO] - 
 Batch: 16/121
2023-03-02 01:52:56,169 -         Optimization: [    INFO] - Batch [16/121]: Anomaly Score: 524.861 label: 0.0
2023-03-02 01:52:56,171 -         Optimization: [    INFO] - 
 Batch: 17/121
2023-03-02 01:52:58,440 -         Optimization: [    INFO] - Batch [17/121]: Anomaly Score: 478.398 label: 0.0
2023-03-02 01:52:58,441 -         Optimization: [    INFO] - 
 Batch: 18/121
2023-03-02 01:53:00,705 -         Optimization: [    INFO] - Batch [18/121]: Anomaly Score: 463.407 label: 0.0
2023-03-02 01:53:00,706 -         Optimization: [    INFO] - 
 Batch: 19/121
2023-03-02 01:53:02,973 -         Optimization: [    INFO] - Batch [19/121]: Anomaly Score: 512.908 label: 0.0
2023-03-02 01:53:02,974 -         Optimization: [    INFO] - 
 Batch: 20/121
2023-03-02 01:53:05,269 -         Optimization: [    INFO] - Batch [20/121]: Anomaly Score: 487.421 label: 0.0
2023-03-02 01:53:05,270 -         Optimization: [    INFO] - 
 Batch: 21/121
2023-03-02 01:53:07,562 -         Optimization: [    INFO] - Batch [21/121]: Anomaly Score: 536.308 label: 0.0
2023-03-02 01:53:07,562 -         Optimization: [    INFO] - 
 Batch: 22/121
2023-03-02 01:53:09,729 -         Optimization: [    INFO] - Batch [22/121]: Anomaly Score: 502.256 label: 0.0
2023-03-02 01:53:09,730 -         Optimization: [    INFO] - 
 Batch: 23/121
2023-03-02 01:53:11,951 -         Optimization: [    INFO] - Batch [23/121]: Anomaly Score: 516.259 label: 0.0
2023-03-02 01:53:11,952 -         Optimization: [    INFO] - 
 Batch: 24/121
2023-03-02 01:53:14,239 -         Optimization: [    INFO] - Batch [24/121]: Anomaly Score: 482.294 label: 0.0
2023-03-02 01:53:14,240 -         Optimization: [    INFO] - 
 Batch: 25/121
2023-03-02 01:53:16,468 -         Optimization: [    INFO] - Batch [25/121]: Anomaly Score: 444.781 label: 0.0
2023-03-02 01:53:16,470 -         Optimization: [    INFO] - 
 Batch: 26/121
2023-03-02 01:53:18,661 -         Optimization: [    INFO] - Batch [26/121]: Anomaly Score: 450.907 label: 0.0
2023-03-02 01:53:18,662 -         Optimization: [    INFO] - 
 Batch: 27/121
2023-03-02 01:53:20,962 -         Optimization: [    INFO] - Batch [27/121]: Anomaly Score: 398.580 label: 0.0
2023-03-02 01:53:20,963 -         Optimization: [    INFO] - 
 Batch: 28/121
2023-03-02 01:53:23,207 -         Optimization: [    INFO] - Batch [28/121]: Anomaly Score: 500.339 label: 0.0
2023-03-02 01:53:23,208 -         Optimization: [    INFO] - 
 Batch: 29/121
2023-03-02 01:53:25,479 -         Optimization: [    INFO] - Batch [29/121]: Anomaly Score: 470.945 label: 0.0
2023-03-02 01:53:25,481 -         Optimization: [    INFO] - 
 Batch: 30/121
2023-03-02 01:53:27,761 -         Optimization: [    INFO] - Batch [30/121]: Anomaly Score: 401.075 label: 0.0
2023-03-02 01:53:27,763 -         Optimization: [    INFO] - 
 Batch: 31/121
2023-03-02 01:53:30,059 -         Optimization: [    INFO] - Batch [31/121]: Anomaly Score: 277.443 label: 0.0
2023-03-02 01:53:30,061 -         Optimization: [    INFO] - 
 Batch: 32/121
2023-03-02 01:53:32,355 -         Optimization: [    INFO] - Batch [32/121]: Anomaly Score: 250.701 label: 0.0
2023-03-02 01:53:32,357 -         Optimization: [    INFO] - 
 Batch: 33/121
2023-03-02 01:53:34,691 -         Optimization: [    INFO] - Batch [33/121]: Anomaly Score: 278.422 label: 0.0
2023-03-02 01:53:34,692 -         Optimization: [    INFO] - 
 Batch: 34/121
2023-03-02 01:53:36,999 -         Optimization: [    INFO] - Batch [34/121]: Anomaly Score: 351.372 label: 0.0
2023-03-02 01:53:37,000 -         Optimization: [    INFO] - 
 Batch: 35/121
2023-03-02 01:53:39,286 -         Optimization: [    INFO] - Batch [35/121]: Anomaly Score: 351.406 label: 0.0
2023-03-02 01:53:39,287 -         Optimization: [    INFO] - 
 Batch: 36/121
2023-03-02 01:53:41,540 -         Optimization: [    INFO] - Batch [36/121]: Anomaly Score: 409.445 label: 0.0
2023-03-02 01:53:41,542 -         Optimization: [    INFO] - 
 Batch: 37/121
2023-03-02 01:53:43,796 -         Optimization: [    INFO] - Batch [37/121]: Anomaly Score: 388.781 label: 0.0
2023-03-02 01:53:43,798 -         Optimization: [    INFO] - 
 Batch: 38/121
2023-03-02 01:53:46,047 -         Optimization: [    INFO] - Batch [38/121]: Anomaly Score: 387.812 label: 0.0
2023-03-02 01:53:46,049 -         Optimization: [    INFO] - 
 Batch: 39/121
2023-03-02 01:53:48,396 -         Optimization: [    INFO] - Batch [39/121]: Anomaly Score: 263.026 label: 0.0
2023-03-02 01:53:48,398 -         Optimization: [    INFO] - 
 Batch: 40/121
2023-03-02 01:53:50,703 -         Optimization: [    INFO] - Batch [40/121]: Anomaly Score: 414.920 label: 0.0
2023-03-02 01:53:50,705 -         Optimization: [    INFO] - 
 Batch: 41/121
2023-03-02 01:53:52,970 -         Optimization: [    INFO] - Batch [41/121]: Anomaly Score: 356.912 label: 0.0
2023-03-02 01:53:52,971 -         Optimization: [    INFO] - 
 Batch: 42/121
2023-03-02 01:53:55,226 -         Optimization: [    INFO] - Batch [42/121]: Anomaly Score: 233.457 label: 0.0
2023-03-02 01:53:55,227 -         Optimization: [    INFO] - 
 Batch: 43/121
2023-03-02 01:53:57,485 -         Optimization: [    INFO] - Batch [43/121]: Anomaly Score: 259.992 label: 0.0
2023-03-02 01:53:57,486 -         Optimization: [    INFO] - 
 Batch: 44/121
2023-03-02 01:53:59,773 -         Optimization: [    INFO] - Batch [44/121]: Anomaly Score: 225.597 label: 0.0
2023-03-02 01:53:59,775 -         Optimization: [    INFO] - 
 Batch: 45/121
2023-03-02 01:54:02,041 -         Optimization: [    INFO] - Batch [45/121]: Anomaly Score: 271.384 label: 0.0
2023-03-02 01:54:02,043 -         Optimization: [    INFO] - 
 Batch: 46/121
2023-03-02 01:54:04,289 -         Optimization: [    INFO] - Batch [46/121]: Anomaly Score: 377.532 label: 0.0
2023-03-02 01:54:04,291 -         Optimization: [    INFO] - 
 Batch: 47/121
2023-03-02 01:54:06,564 -         Optimization: [    INFO] - Batch [47/121]: Anomaly Score: 324.280 label: 0.0
2023-03-02 01:54:06,566 -         Optimization: [    INFO] - 
 Batch: 48/121
2023-03-02 01:54:08,786 -         Optimization: [    INFO] - Batch [48/121]: Anomaly Score: 402.056 label: 0.0
2023-03-02 01:54:08,788 -         Optimization: [    INFO] - 
 Batch: 49/121
2023-03-02 01:54:11,016 -         Optimization: [    INFO] - Batch [49/121]: Anomaly Score: 414.724 label: 0.0
2023-03-02 01:54:11,017 -         Optimization: [    INFO] - 
 Batch: 50/121
2023-03-02 01:54:13,187 -         Optimization: [    INFO] - Batch [50/121]: Anomaly Score: 48.807 label: 0.0
2023-03-02 01:54:13,188 -         Optimization: [    INFO] - 
 Batch: 51/121
2023-03-02 01:54:15,332 -         Optimization: [    INFO] - Batch [51/121]: Anomaly Score: 306.523 label: 0.0
2023-03-02 01:54:15,334 -         Optimization: [    INFO] - 
 Batch: 52/121
2023-03-02 01:54:17,517 -         Optimization: [    INFO] - Batch [52/121]: Anomaly Score: 18.860 label: 0.0
2023-03-02 01:54:17,518 -         Optimization: [    INFO] - 
 Batch: 53/121
2023-03-02 01:54:19,711 -         Optimization: [    INFO] - Batch [53/121]: Anomaly Score: 280.499 label: 0.0
2023-03-02 01:54:19,713 -         Optimization: [    INFO] - 
 Batch: 54/121
2023-03-02 01:54:21,915 -         Optimization: [    INFO] - Batch [54/121]: Anomaly Score: 278.947 label: 0.0
2023-03-02 01:54:21,917 -         Optimization: [    INFO] - 
 Batch: 55/121
2023-03-02 01:54:24,099 -         Optimization: [    INFO] - Batch [55/121]: Anomaly Score: 293.226 label: 0.0
2023-03-02 01:54:24,101 -         Optimization: [    INFO] - 
 Batch: 56/121
2023-03-02 01:54:26,279 -         Optimization: [    INFO] - Batch [56/121]: Anomaly Score: 262.013 label: 0.0
2023-03-02 01:54:26,281 -         Optimization: [    INFO] - 
 Batch: 57/121
2023-03-02 01:54:28,461 -         Optimization: [    INFO] - Batch [57/121]: Anomaly Score: 271.355 label: 0.0
2023-03-02 01:54:28,463 -         Optimization: [    INFO] - 
 Batch: 58/121
2023-03-02 01:54:30,671 -         Optimization: [    INFO] - Batch [58/121]: Anomaly Score: 367.570 label: 0.0
2023-03-02 01:54:30,672 -         Optimization: [    INFO] - 
 Batch: 59/121
2023-03-02 01:54:32,983 -         Optimization: [    INFO] - Batch [59/121]: Anomaly Score: 259.245 label: 0.0
2023-03-02 01:54:32,984 -         Optimization: [    INFO] - 
 Batch: 60/121
2023-03-02 01:54:35,313 -         Optimization: [    INFO] - Batch [60/121]: Anomaly Score: 275.737 label: 1.0
2023-03-02 01:54:35,315 -         Optimization: [    INFO] - 
 Batch: 61/121
2023-03-02 01:54:37,590 -         Optimization: [    INFO] - Batch [61/121]: Anomaly Score: 267.439 label: 1.0
2023-03-02 01:54:37,591 -         Optimization: [    INFO] - 
 Batch: 62/121
2023-03-02 01:54:39,864 -         Optimization: [    INFO] - Batch [62/121]: Anomaly Score: 296.469 label: 1.0
2023-03-02 01:54:39,865 -         Optimization: [    INFO] - 
 Batch: 63/121
2023-03-02 01:54:42,139 -         Optimization: [    INFO] - Batch [63/121]: Anomaly Score: 346.789 label: 1.0
2023-03-02 01:54:42,141 -         Optimization: [    INFO] - 
 Batch: 64/121
2023-03-02 01:54:44,410 -         Optimization: [    INFO] - Batch [64/121]: Anomaly Score: 281.667 label: 1.0
2023-03-02 01:54:44,411 -         Optimization: [    INFO] - 
 Batch: 65/121
2023-03-02 01:54:46,679 -         Optimization: [    INFO] - Batch [65/121]: Anomaly Score: 255.648 label: 1.0
2023-03-02 01:54:46,681 -         Optimization: [    INFO] - 
 Batch: 66/121
2023-03-02 01:54:49,034 -         Optimization: [    INFO] - Batch [66/121]: Anomaly Score: 258.009 label: 1.0
2023-03-02 01:54:49,036 -         Optimization: [    INFO] - 
 Batch: 67/121
2023-03-02 01:54:51,306 -         Optimization: [    INFO] - Batch [67/121]: Anomaly Score: 278.039 label: 0.0
2023-03-02 01:54:51,307 -         Optimization: [    INFO] - 
 Batch: 68/121
2023-03-02 01:54:53,551 -         Optimization: [    INFO] - Batch [68/121]: Anomaly Score: 360.976 label: 0.0
2023-03-02 01:54:53,553 -         Optimization: [    INFO] - 
 Batch: 69/121
2023-03-02 01:54:55,767 -         Optimization: [    INFO] - Batch [69/121]: Anomaly Score: 295.035 label: 0.0
2023-03-02 01:54:55,769 -         Optimization: [    INFO] - 
 Batch: 70/121
2023-03-02 01:54:57,999 -         Optimization: [    INFO] - Batch [70/121]: Anomaly Score: 216.804 label: 0.0
2023-03-02 01:54:58,001 -         Optimization: [    INFO] - 
 Batch: 71/121
2023-03-02 01:55:00,221 -         Optimization: [    INFO] - Batch [71/121]: Anomaly Score: 291.379 label: 0.0
2023-03-02 01:55:00,223 -         Optimization: [    INFO] - 
 Batch: 72/121
2023-03-02 01:55:02,439 -         Optimization: [    INFO] - Batch [72/121]: Anomaly Score: 272.524 label: 0.0
2023-03-02 01:55:02,441 -         Optimization: [    INFO] - 
 Batch: 73/121
2023-03-02 01:55:04,667 -         Optimization: [    INFO] - Batch [73/121]: Anomaly Score: 333.014 label: 0.0
2023-03-02 01:55:04,669 -         Optimization: [    INFO] - 
 Batch: 74/121
2023-03-02 01:55:06,877 -         Optimization: [    INFO] - Batch [74/121]: Anomaly Score: 364.602 label: 0.0
2023-03-02 01:55:06,878 -         Optimization: [    INFO] - 
 Batch: 75/121
2023-03-02 01:55:09,066 -         Optimization: [    INFO] - Batch [75/121]: Anomaly Score: 350.230 label: 0.0
2023-03-02 01:55:09,067 -         Optimization: [    INFO] - 
 Batch: 76/121
2023-03-02 01:55:11,252 -         Optimization: [    INFO] - Batch [76/121]: Anomaly Score: 364.371 label: 0.0
2023-03-02 01:55:11,253 -         Optimization: [    INFO] - 
 Batch: 77/121
2023-03-02 01:55:13,443 -         Optimization: [    INFO] - Batch [77/121]: Anomaly Score: 414.719 label: 0.0
2023-03-02 01:55:13,445 -         Optimization: [    INFO] - 
 Batch: 78/121
2023-03-02 01:55:15,644 -         Optimization: [    INFO] - Batch [78/121]: Anomaly Score: 463.004 label: 0.0
2023-03-02 01:55:15,645 -         Optimization: [    INFO] - 
 Batch: 79/121
2023-03-02 01:55:17,829 -         Optimization: [    INFO] - Batch [79/121]: Anomaly Score: 390.393 label: 0.0
2023-03-02 01:55:17,830 -         Optimization: [    INFO] - 
 Batch: 80/121
2023-03-02 01:55:19,998 -         Optimization: [    INFO] - Batch [80/121]: Anomaly Score: 423.657 label: 0.0
2023-03-02 01:55:20,000 -         Optimization: [    INFO] - 
 Batch: 81/121
2023-03-02 01:55:22,212 -         Optimization: [    INFO] - Batch [81/121]: Anomaly Score: 394.235 label: 0.0
2023-03-02 01:55:22,214 -         Optimization: [    INFO] - 
 Batch: 82/121
2023-03-02 01:55:24,394 -         Optimization: [    INFO] - Batch [82/121]: Anomaly Score: 416.885 label: 0.0
2023-03-02 01:55:24,396 -         Optimization: [    INFO] - 
 Batch: 83/121
2023-03-02 01:55:26,641 -         Optimization: [    INFO] - Batch [83/121]: Anomaly Score: 386.050 label: 0.0
2023-03-02 01:55:26,642 -         Optimization: [    INFO] - 
 Batch: 84/121
2023-03-02 01:55:28,881 -         Optimization: [    INFO] - Batch [84/121]: Anomaly Score: 423.716 label: 0.0
2023-03-02 01:55:28,883 -         Optimization: [    INFO] - 
 Batch: 85/121
2023-03-02 01:55:31,200 -         Optimization: [    INFO] - Batch [85/121]: Anomaly Score: 513.518 label: 0.0
2023-03-02 01:55:31,202 -         Optimization: [    INFO] - 
 Batch: 86/121
2023-03-02 01:55:33,478 -         Optimization: [    INFO] - Batch [86/121]: Anomaly Score: 483.231 label: 0.0
2023-03-02 01:55:33,480 -         Optimization: [    INFO] - 
 Batch: 87/121
2023-03-02 01:55:35,758 -         Optimization: [    INFO] - Batch [87/121]: Anomaly Score: 409.316 label: 0.0
2023-03-02 01:55:35,759 -         Optimization: [    INFO] - 
 Batch: 88/121
2023-03-02 01:55:38,023 -         Optimization: [    INFO] - Batch [88/121]: Anomaly Score: 455.813 label: 0.0
2023-03-02 01:55:38,024 -         Optimization: [    INFO] - 
 Batch: 89/121
2023-03-02 01:55:40,252 -         Optimization: [    INFO] - Batch [89/121]: Anomaly Score: 374.110 label: 0.0
2023-03-02 01:55:40,253 -         Optimization: [    INFO] - 
 Batch: 90/121
2023-03-02 01:55:42,422 -         Optimization: [    INFO] - Batch [90/121]: Anomaly Score: 470.483 label: 0.0
2023-03-02 01:55:42,424 -         Optimization: [    INFO] - 
 Batch: 91/121
2023-03-02 01:55:44,668 -         Optimization: [    INFO] - Batch [91/121]: Anomaly Score: 502.853 label: 0.0
2023-03-02 01:55:44,669 -         Optimization: [    INFO] - 
 Batch: 92/121
2023-03-02 01:55:46,919 -         Optimization: [    INFO] - Batch [92/121]: Anomaly Score: 535.836 label: 0.0
2023-03-02 01:55:46,920 -         Optimization: [    INFO] - 
 Batch: 93/121
2023-03-02 01:55:49,096 -         Optimization: [    INFO] - Batch [93/121]: Anomaly Score: 543.372 label: 0.0
2023-03-02 01:55:49,098 -         Optimization: [    INFO] - 
 Batch: 94/121
2023-03-02 01:55:51,274 -         Optimization: [    INFO] - Batch [94/121]: Anomaly Score: 453.029 label: 0.0
2023-03-02 01:55:51,276 -         Optimization: [    INFO] - 
 Batch: 95/121
2023-03-02 01:55:53,476 -         Optimization: [    INFO] - Batch [95/121]: Anomaly Score: 504.012 label: 0.0
2023-03-02 01:55:53,478 -         Optimization: [    INFO] - 
 Batch: 96/121
2023-03-02 01:55:55,674 -         Optimization: [    INFO] - Batch [96/121]: Anomaly Score: 531.092 label: 0.0
2023-03-02 01:55:55,675 -         Optimization: [    INFO] - 
 Batch: 97/121
2023-03-02 01:55:57,885 -         Optimization: [    INFO] - Batch [97/121]: Anomaly Score: 500.644 label: 0.0
2023-03-02 01:55:57,887 -         Optimization: [    INFO] - 
 Batch: 98/121
2023-03-02 01:56:00,095 -         Optimization: [    INFO] - Batch [98/121]: Anomaly Score: 502.354 label: 0.0
2023-03-02 01:56:00,096 -         Optimization: [    INFO] - 
 Batch: 99/121
2023-03-02 01:56:02,285 -         Optimization: [    INFO] - Batch [99/121]: Anomaly Score: 512.126 label: 0.0
2023-03-02 01:56:02,287 -         Optimization: [    INFO] - 
 Batch: 100/121
2023-03-02 01:56:04,462 -         Optimization: [    INFO] - Batch [100/121]: Anomaly Score: 503.317 label: 1.0
2023-03-02 01:56:04,464 -         Optimization: [    INFO] - 
 Batch: 101/121
2023-03-02 01:56:06,706 -         Optimization: [    INFO] - Batch [101/121]: Anomaly Score: 534.251 label: 1.0
2023-03-02 01:56:06,707 -         Optimization: [    INFO] - 
 Batch: 102/121
2023-03-02 01:56:08,870 -         Optimization: [    INFO] - Batch [102/121]: Anomaly Score: 510.888 label: 1.0
2023-03-02 01:56:08,871 -         Optimization: [    INFO] - 
 Batch: 103/121
2023-03-02 01:56:11,047 -         Optimization: [    INFO] - Batch [103/121]: Anomaly Score: 545.569 label: 1.0
2023-03-02 01:56:11,049 -         Optimization: [    INFO] - 
 Batch: 104/121
2023-03-02 01:56:13,239 -         Optimization: [    INFO] - Batch [104/121]: Anomaly Score: 568.716 label: 1.0
2023-03-02 01:56:13,241 -         Optimization: [    INFO] - 
 Batch: 105/121
2023-03-02 01:56:15,490 -         Optimization: [    INFO] - Batch [105/121]: Anomaly Score: 518.923 label: 1.0
2023-03-02 01:56:15,492 -         Optimization: [    INFO] - 
 Batch: 106/121
2023-03-02 01:56:17,770 -         Optimization: [    INFO] - Batch [106/121]: Anomaly Score: 538.571 label: 1.0
2023-03-02 01:56:17,771 -         Optimization: [    INFO] - 
 Batch: 107/121
2023-03-02 01:56:20,180 -         Optimization: [    INFO] - Batch [107/121]: Anomaly Score: 546.892 label: 1.0
2023-03-02 01:56:20,182 -         Optimization: [    INFO] - 
 Batch: 108/121
2023-03-02 01:56:22,338 -         Optimization: [    INFO] - Batch [108/121]: Anomaly Score: 495.599 label: 0.0
2023-03-02 01:56:22,340 -         Optimization: [    INFO] - 
 Batch: 109/121
2023-03-02 01:56:24,521 -         Optimization: [    INFO] - Batch [109/121]: Anomaly Score: 527.425 label: 0.0
2023-03-02 01:56:24,522 -         Optimization: [    INFO] - 
 Batch: 110/121
2023-03-02 01:56:26,778 -         Optimization: [    INFO] - Batch [110/121]: Anomaly Score: 536.287 label: 0.0
2023-03-02 01:56:26,780 -         Optimization: [    INFO] - 
 Batch: 111/121
2023-03-02 01:56:29,133 -         Optimization: [    INFO] - Batch [111/121]: Anomaly Score: 534.817 label: 0.0
2023-03-02 01:56:29,135 -         Optimization: [    INFO] - 
 Batch: 112/121
2023-03-02 01:56:31,380 -         Optimization: [    INFO] - Batch [112/121]: Anomaly Score: 561.760 label: 0.0
2023-03-02 01:56:31,381 -         Optimization: [    INFO] - 
 Batch: 113/121
2023-03-02 01:56:33,634 -         Optimization: [    INFO] - Batch [113/121]: Anomaly Score: 537.073 label: 0.0
2023-03-02 01:56:33,635 -         Optimization: [    INFO] - 
 Batch: 114/121
2023-03-02 01:56:35,927 -         Optimization: [    INFO] - Batch [114/121]: Anomaly Score: 491.927 label: 0.0
2023-03-02 01:56:35,929 -         Optimization: [    INFO] - 
 Batch: 115/121
2023-03-02 01:56:38,225 -         Optimization: [    INFO] - Batch [115/121]: Anomaly Score: 550.126 label: 0.0
2023-03-02 01:56:38,226 -         Optimization: [    INFO] - 
 Batch: 116/121
2023-03-02 01:56:40,441 -         Optimization: [    INFO] - Batch [116/121]: Anomaly Score: 500.297 label: 0.0
2023-03-02 01:56:40,443 -         Optimization: [    INFO] - 
 Batch: 117/121
2023-03-02 01:56:42,692 -         Optimization: [    INFO] - Batch [117/121]: Anomaly Score: 514.858 label: 0.0
2023-03-02 01:56:42,693 -         Optimization: [    INFO] - 
 Batch: 118/121
2023-03-02 01:56:44,958 -         Optimization: [    INFO] - Batch [118/121]: Anomaly Score: 565.347 label: 0.0
2023-03-02 01:56:44,960 -         Optimization: [    INFO] - 
 Batch: 119/121
2023-03-02 01:56:47,221 -         Optimization: [    INFO] - Batch [119/121]: Anomaly Score: 504.058 label: 0.0
2023-03-02 01:56:47,223 -         Optimization: [    INFO] - 
 Batch: 120/121
2023-03-02 01:56:49,463 -         Optimization: [    INFO] - Batch [120/121]: Anomaly Score: 506.758 label: 0.0
2023-03-02 01:56:49,464 -         Optimization: [    INFO] - 
 Batch: 121/121
2023-03-02 01:56:51,679 -         Optimization: [    INFO] - Batch [121/121]: Anomaly Score: 530.517 label: 0.0
2023-03-02 01:59:59,439 -                train: [    INFO] - Device: cuda:0
2023-03-02 02:00:01,975 -                train: [    INFO] - 
Epoch: 1/20
2023-03-02 02:00:02,187 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.6836 (0.6860) Acc D Real: 69.010% 
Loss D Fake: 0.7038 (0.7025) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6815 (0.6828) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,194 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.6795 (0.6839) Acc D Real: 75.035% 
Loss D Fake: 0.7064 (0.7038) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.6790 (0.6815) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,201 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.6795 (0.6828) Acc D Real: 76.953% 
Loss D Fake: 0.7089 (0.7051) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6765 (0.6803) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,218 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.6767 (0.6816) Acc D Real: 78.531% 
Loss D Fake: 0.7114 (0.7064) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6741 (0.6790) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,225 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.6726 (0.6801) Acc D Real: 81.806% 
Loss D Fake: 0.7139 (0.7076) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.6717 (0.6778) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,232 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.6717 (0.6789) Acc D Real: 83.906% 
Loss D Fake: 0.7164 (0.7089) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6693 (0.6766) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,239 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.6653 (0.6772) Acc D Real: 85.794% 
Loss D Fake: 0.7189 (0.7101) Acc D Fake: 0.000% 
Loss D: 1.384 
Loss G: 0.6669 (0.6754) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,246 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.6615 (0.6754) Acc D Real: 87.367% 
Loss D Fake: 0.7215 (0.7114) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.6645 (0.6742) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,253 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.6637 (0.6743) Acc D Real: 88.620% 
Loss D Fake: 0.7241 (0.7127) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6621 (0.6730) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,260 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.6643 (0.6733) Acc D Real: 89.654% 
Loss D Fake: 0.7267 (0.7139) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6597 (0.6718) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,267 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.6627 (0.6725) Acc D Real: 90.516% 
Loss D Fake: 0.7292 (0.7152) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6573 (0.6706) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,275 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.6583 (0.6714) Acc D Real: 91.246% 
Loss D Fake: 0.7318 (0.7165) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6549 (0.6694) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,282 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.6540 (0.6701) Acc D Real: 91.871% 
Loss D Fake: 0.7343 (0.7178) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6526 (0.6682) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,289 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.6518 (0.6689) Acc D Real: 92.413% 
Loss D Fake: 0.7369 (0.7190) Acc D Fake: 0.000% 
Loss D: 1.389 
Loss G: 0.6502 (0.6670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,296 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.6489 (0.6677) Acc D Real: 92.887% 
Loss D Fake: 0.7395 (0.7203) Acc D Fake: 0.000% 
Loss D: 1.388 
Loss G: 0.6478 (0.6658) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,304 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.6480 (0.6665) Acc D Real: 93.306% 
Loss D Fake: 0.7422 (0.7216) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6453 (0.6646) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,311 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.6411 (0.6651) Acc D Real: 93.678% 
Loss D Fake: 0.7449 (0.7229) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.6428 (0.6633) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,318 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.6423 (0.6639) Acc D Real: 94.010% 
Loss D Fake: 0.7478 (0.7242) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6402 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,325 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.6404 (0.6627) Acc D Real: 94.310% 
Loss D Fake: 0.7507 (0.7255) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6376 (0.6609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,332 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.6361 (0.6615) Acc D Real: 94.581% 
Loss D Fake: 0.7536 (0.7269) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6350 (0.6597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,339 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.6330 (0.6602) Acc D Real: 94.827% 
Loss D Fake: 0.7567 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6322 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,346 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.6304 (0.6589) Acc D Real: 95.052% 
Loss D Fake: 0.7599 (0.7296) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6293 (0.6572) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,353 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.6271 (0.6575) Acc D Real: 95.258% 
Loss D Fake: 0.7632 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6264 (0.6559) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,361 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.6250 (0.6562) Acc D Real: 95.448% 
Loss D Fake: 0.7666 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6235 (0.6546) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,368 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.6204 (0.6549) Acc D Real: 95.623% 
Loss D Fake: 0.7700 (0.7339) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.6206 (0.6533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,376 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.6178 (0.6535) Acc D Real: 95.785% 
Loss D Fake: 0.7735 (0.7353) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6174 (0.6519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,384 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.6142 (0.6521) Acc D Real: 95.936% 
Loss D Fake: 0.7773 (0.7368) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.6142 (0.6506) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,392 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.6120 (0.6507) Acc D Real: 96.076% 
Loss D Fake: 0.7811 (0.7384) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.6110 (0.6492) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,400 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.6094 (0.6493) Acc D Real: 96.207% 
Loss D Fake: 0.7849 (0.7399) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.6078 (0.6479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,408 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.6048 (0.6479) Acc D Real: 96.329% 
Loss D Fake: 0.7889 (0.7415) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.6044 (0.6465) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,415 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5990 (0.6464) Acc D Real: 96.444% 
Loss D Fake: 0.7930 (0.7431) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.6009 (0.6450) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,423 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.5943 (0.6448) Acc D Real: 96.551% 
Loss D Fake: 0.7973 (0.7447) Acc D Fake: 0.000% 
Loss D: 1.392 
Loss G: 0.5974 (0.6436) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,432 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5915 (0.6432) Acc D Real: 96.653% 
Loss D Fake: 0.8017 (0.7464) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5938 (0.6421) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,440 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5873 (0.6416) Acc D Real: 96.749% 
Loss D Fake: 0.8064 (0.7481) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5899 (0.6406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,448 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.5790 (0.6399) Acc D Real: 96.839% 
Loss D Fake: 0.8113 (0.7499) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.5859 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,456 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5766 (0.6382) Acc D Real: 96.924% 
Loss D Fake: 0.8166 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.5816 (0.6376) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,464 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5715 (0.6364) Acc D Real: 97.005% 
Loss D Fake: 0.8223 (0.7535) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5770 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,472 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5619 (0.6345) Acc D Real: 97.082% 
Loss D Fake: 0.8286 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.391 
Loss G: 0.5720 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,480 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5587 (0.6326) Acc D Real: 97.155% 
Loss D Fake: 0.8356 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.394 
Loss G: 0.5664 (0.6326) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,488 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5470 (0.6305) Acc D Real: 97.224% 
Loss D Fake: 0.8434 (0.7596) Acc D Fake: 0.000% 
Loss D: 1.390 
Loss G: 0.5602 (0.6309) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,496 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.5275 (0.6281) Acc D Real: 97.290% 
Loss D Fake: 0.8524 (0.7618) Acc D Fake: 0.000% 
Loss D: 1.380 
Loss G: 0.5533 (0.6290) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,504 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.5337 (0.6259) Acc D Real: 97.353% 
Loss D Fake: 0.8628 (0.7641) Acc D Fake: 0.000% 
Loss D: 1.397 
Loss G: 0.5454 (0.6271) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,511 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.5080 (0.6232) Acc D Real: 97.414% 
Loss D Fake: 0.8749 (0.7666) Acc D Fake: 0.000% 
Loss D: 1.383 
Loss G: 0.5362 (0.6250) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,519 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.5145 (0.6208) Acc D Real: 97.471% 
Loss D Fake: 0.8894 (0.7694) Acc D Fake: 0.000% 
Loss D: 1.404 
Loss G: 0.5258 (0.6228) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,526 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4730 (0.6176) Acc D Real: 97.526% 
Loss D Fake: 0.9064 (0.7724) Acc D Fake: 0.000% 
Loss D: 1.379 
Loss G: 0.5135 (0.6204) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,534 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4377 (0.6137) Acc D Real: 97.579% 
Loss D Fake: 0.9277 (0.7757) Acc D Fake: 0.000% 
Loss D: 1.365 
Loss G: 0.4988 (0.6178) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,541 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4545 (0.6104) Acc D Real: 97.629% 
Loss D Fake: 0.9545 (0.7794) Acc D Fake: 0.000% 
Loss D: 1.409 
Loss G: 0.4827 (0.6150) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,548 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4537 (0.6072) Acc D Real: 97.678% 
Loss D Fake: 0.9839 (0.7836) Acc D Fake: 0.000% 
Loss D: 1.438 
Loss G: 0.4666 (0.6120) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,556 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4151 (0.6034) Acc D Real: 97.724% 
Loss D Fake: 1.0126 (0.7881) Acc D Fake: 0.000% 
Loss D: 1.428 
Loss G: 0.4546 (0.6088) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,563 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3986 (0.5994) Acc D Real: 97.769% 
Loss D Fake: 1.0307 (0.7929) Acc D Fake: 0.000% 
Loss D: 1.429 
Loss G: 0.4478 (0.6057) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,571 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3840 (0.5952) Acc D Real: 97.811% 
Loss D Fake: 1.0388 (0.7976) Acc D Fake: 0.000% 
Loss D: 1.423 
Loss G: 0.4464 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,579 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3857 (0.5913) Acc D Real: 97.853% 
Loss D Fake: 1.0347 (0.8021) Acc D Fake: 0.000% 
Loss D: 1.420 
Loss G: 0.4511 (0.5998) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,587 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4319 (0.5883) Acc D Real: 97.893% 
Loss D Fake: 1.0210 (0.8062) Acc D Fake: 0.000% 
Loss D: 1.453 
Loss G: 0.4581 (0.5971) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,594 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3924 (0.5848) Acc D Real: 97.931% 
Loss D Fake: 1.0056 (0.8098) Acc D Fake: 0.000% 
Loss D: 1.398 
Loss G: 0.4662 (0.5948) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,602 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3948 (0.5814) Acc D Real: 97.968% 
Loss D Fake: 0.9907 (0.8130) Acc D Fake: 0.000% 
Loss D: 1.386 
Loss G: 0.4728 (0.5926) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,609 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3971 (0.5781) Acc D Real: 98.003% 
Loss D Fake: 0.9804 (0.8159) Acc D Fake: 0.000% 
Loss D: 1.377 
Loss G: 0.4774 (0.5906) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,617 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4296 (0.5756) Acc D Real: 98.038% 
Loss D Fake: 0.9739 (0.8187) Acc D Fake: 0.000% 
Loss D: 1.403 
Loss G: 0.4801 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,624 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4266 (0.5730) Acc D Real: 98.071% 
Loss D Fake: 0.9697 (0.8212) Acc D Fake: 0.000% 
Loss D: 1.396 
Loss G: 0.4825 (0.5869) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,632 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4742 (0.5714) Acc D Real: 98.103% 
Loss D Fake: 0.9655 (0.8236) Acc D Fake: 0.000% 
Loss D: 1.440 
Loss G: 0.4846 (0.5852) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,639 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3661 (0.5680) Acc D Real: 98.134% 
Loss D Fake: 0.9629 (0.8259) Acc D Fake: 0.000% 
Loss D: 1.329 
Loss G: 0.4854 (0.5835) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,646 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3856 (0.5651) Acc D Real: 98.164% 
Loss D Fake: 0.9624 (0.8281) Acc D Fake: 0.000% 
Loss D: 1.348 
Loss G: 0.4854 (0.5819) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,654 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4303 (0.5629) Acc D Real: 98.194% 
Loss D Fake: 0.9628 (0.8303) Acc D Fake: 0.000% 
Loss D: 1.393 
Loss G: 0.4852 (0.5804) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,662 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4231 (0.5608) Acc D Real: 98.222% 
Loss D Fake: 0.9639 (0.8323) Acc D Fake: 0.000% 
Loss D: 1.387 
Loss G: 0.4838 (0.5789) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,670 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3778 (0.5579) Acc D Real: 98.249% 
Loss D Fake: 0.9676 (0.8344) Acc D Fake: 0.000% 
Loss D: 1.345 
Loss G: 0.4814 (0.5774) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,677 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3998 (0.5556) Acc D Real: 98.276% 
Loss D Fake: 0.9721 (0.8365) Acc D Fake: 0.000% 
Loss D: 1.372 
Loss G: 0.4796 (0.5759) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,685 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4198 (0.5535) Acc D Real: 98.301% 
Loss D Fake: 0.9748 (0.8386) Acc D Fake: 0.000% 
Loss D: 1.395 
Loss G: 0.4787 (0.5745) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,693 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4348 (0.5518) Acc D Real: 98.326% 
Loss D Fake: 0.9753 (0.8406) Acc D Fake: 0.000% 
Loss D: 1.410 
Loss G: 0.4795 (0.5731) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,701 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3827 (0.5493) Acc D Real: 98.351% 
Loss D Fake: 0.9721 (0.8425) Acc D Fake: 0.000% 
Loss D: 1.355 
Loss G: 0.4821 (0.5717) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,709 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3489 (0.5465) Acc D Real: 98.374% 
Loss D Fake: 0.9678 (0.8443) Acc D Fake: 0.000% 
Loss D: 1.317 
Loss G: 0.4836 (0.5705) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,716 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4033 (0.5445) Acc D Real: 98.397% 
Loss D Fake: 0.9659 (0.8460) Acc D Fake: 0.000% 
Loss D: 1.369 
Loss G: 0.4850 (0.5693) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,723 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3797 (0.5422) Acc D Real: 98.419% 
Loss D Fake: 0.9646 (0.8476) Acc D Fake: 0.000% 
Loss D: 1.344 
Loss G: 0.4845 (0.5681) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,731 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4079 (0.5403) Acc D Real: 98.441% 
Loss D Fake: 0.9667 (0.8493) Acc D Fake: 0.000% 
Loss D: 1.375 
Loss G: 0.4841 (0.5670) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,738 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3603 (0.5379) Acc D Real: 98.462% 
Loss D Fake: 0.9660 (0.8508) Acc D Fake: 0.000% 
Loss D: 1.326 
Loss G: 0.4859 (0.5659) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,746 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3903 (0.5359) Acc D Real: 98.483% 
Loss D Fake: 0.9612 (0.8523) Acc D Fake: 0.000% 
Loss D: 1.351 
Loss G: 0.4891 (0.5648) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,754 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3699 (0.5337) Acc D Real: 98.503% 
Loss D Fake: 0.9555 (0.8537) Acc D Fake: 0.000% 
Loss D: 1.325 
Loss G: 0.4918 (0.5639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,761 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3785 (0.5317) Acc D Real: 98.522% 
Loss D Fake: 0.9510 (0.8549) Acc D Fake: 0.000% 
Loss D: 1.330 
Loss G: 0.4947 (0.5630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,769 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3997 (0.5300) Acc D Real: 98.541% 
Loss D Fake: 0.9461 (0.8561) Acc D Fake: 0.000% 
Loss D: 1.346 
Loss G: 0.4974 (0.5621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,776 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4227 (0.5287) Acc D Real: 98.559% 
Loss D Fake: 0.9416 (0.8572) Acc D Fake: 0.000% 
Loss D: 1.364 
Loss G: 0.5000 (0.5613) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,784 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3914 (0.5270) Acc D Real: 98.577% 
Loss D Fake: 0.9392 (0.8582) Acc D Fake: 0.000% 
Loss D: 1.331 
Loss G: 0.4996 (0.5606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,792 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4126 (0.5255) Acc D Real: 98.595% 
Loss D Fake: 0.9415 (0.8592) Acc D Fake: 0.000% 
Loss D: 1.354 
Loss G: 0.4990 (0.5598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,799 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4222 (0.5243) Acc D Real: 98.612% 
Loss D Fake: 0.9433 (0.8603) Acc D Fake: 0.000% 
Loss D: 1.366 
Loss G: 0.4970 (0.5590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,807 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3728 (0.5225) Acc D Real: 98.629% 
Loss D Fake: 0.9475 (0.8613) Acc D Fake: 0.000% 
Loss D: 1.320 
Loss G: 0.4959 (0.5583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,815 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4051 (0.5211) Acc D Real: 98.645% 
Loss D Fake: 0.9480 (0.8624) Acc D Fake: 0.000% 
Loss D: 1.353 
Loss G: 0.4968 (0.5576) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,823 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4589 (0.5203) Acc D Real: 98.661% 
Loss D Fake: 0.9470 (0.8634) Acc D Fake: 0.000% 
Loss D: 1.406 
Loss G: 0.4964 (0.5568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,830 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3686 (0.5186) Acc D Real: 98.677% 
Loss D Fake: 0.9481 (0.8643) Acc D Fake: 0.000% 
Loss D: 1.317 
Loss G: 0.4968 (0.5561) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,839 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3712 (0.5169) Acc D Real: 98.692% 
Loss D Fake: 0.9471 (0.8653) Acc D Fake: 0.000% 
Loss D: 1.318 
Loss G: 0.4974 (0.5555) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,847 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3269 (0.5147) Acc D Real: 98.707% 
Loss D Fake: 0.9454 (0.8662) Acc D Fake: 0.000% 
Loss D: 1.272 
Loss G: 0.4990 (0.5548) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,854 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3760 (0.5132) Acc D Real: 98.721% 
Loss D Fake: 0.9421 (0.8671) Acc D Fake: 0.000% 
Loss D: 1.318 
Loss G: 0.5007 (0.5542) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,862 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3750 (0.5116) Acc D Real: 98.736% 
Loss D Fake: 0.9394 (0.8679) Acc D Fake: 0.000% 
Loss D: 1.314 
Loss G: 0.5014 (0.5536) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,869 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3853 (0.5102) Acc D Real: 98.749% 
Loss D Fake: 0.9387 (0.8686) Acc D Fake: 0.000% 
Loss D: 1.324 
Loss G: 0.5011 (0.5530) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,877 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3381 (0.5084) Acc D Real: 98.763% 
Loss D Fake: 0.9400 (0.8694) Acc D Fake: 0.000% 
Loss D: 1.278 
Loss G: 0.4992 (0.5525) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,885 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3336 (0.5065) Acc D Real: 98.776% 
Loss D Fake: 0.9437 (0.8702) Acc D Fake: 0.000% 
Loss D: 1.277 
Loss G: 0.4962 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,894 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2952 (0.5042) Acc D Real: 98.789% 
Loss D Fake: 0.9485 (0.8710) Acc D Fake: 0.000% 
Loss D: 1.244 
Loss G: 0.4923 (0.5512) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,903 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2983 (0.5021) Acc D Real: 98.802% 
Loss D Fake: 0.9545 (0.8719) Acc D Fake: 0.000% 
Loss D: 1.253 
Loss G: 0.4866 (0.5505) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,913 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2908 (0.4999) Acc D Real: 98.815% 
Loss D Fake: 0.9636 (0.8729) Acc D Fake: 0.000% 
Loss D: 1.254 
Loss G: 0.4789 (0.5498) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,922 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2683 (0.4975) Acc D Real: 98.827% 
Loss D Fake: 0.9779 (0.8740) Acc D Fake: 0.000% 
Loss D: 1.246 
Loss G: 0.4769 (0.5490) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,930 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2185 (0.4946) Acc D Real: 98.839% 
Loss D Fake: 0.9711 (0.8749) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.4907 (0.5485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,938 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2246 (0.4919) Acc D Real: 98.850% 
Loss D Fake: 0.9395 (0.8756) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.5041 (0.5480) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,945 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2023 (0.4890) Acc D Real: 98.862% 
Loss D Fake: 0.9216 (0.8761) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.5130 (0.5477) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,953 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2183 (0.4863) Acc D Real: 98.873% 
Loss D Fake: 0.9107 (0.8764) Acc D Fake: 0.000% 
Loss D: 1.129 
Loss G: 0.5195 (0.5474) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,960 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2282 (0.4838) Acc D Real: 98.884% 
Loss D Fake: 0.9031 (0.8767) Acc D Fake: 0.000% 
Loss D: 1.131 
Loss G: 0.5250 (0.5472) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,968 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2061 (0.4811) Acc D Real: 98.895% 
Loss D Fake: 0.8963 (0.8769) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.5297 (0.5470) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,975 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2030 (0.4784) Acc D Real: 98.906% 
Loss D Fake: 0.8900 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.5339 (0.5469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,983 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.1701 (0.4755) Acc D Real: 98.916% 
Loss D Fake: 0.8842 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.054 
Loss G: 0.5374 (0.5468) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,990 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.1931 (0.4728) Acc D Real: 98.926% 
Loss D Fake: 0.8790 (0.8771) Acc D Fake: 0.000% 
Loss D: 1.072 
Loss G: 0.5405 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:02,998 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.1685 (0.4700) Acc D Real: 98.936% 
Loss D Fake: 0.8741 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.043 
Loss G: 0.5430 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,005 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.1780 (0.4673) Acc D Real: 98.946% 
Loss D Fake: 0.8698 (0.8770) Acc D Fake: 0.000% 
Loss D: 1.048 
Loss G: 0.5450 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,013 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.1694 (0.4645) Acc D Real: 98.956% 
Loss D Fake: 0.8664 (0.8769) Acc D Fake: 0.000% 
Loss D: 1.036 
Loss G: 0.5469 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,020 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.1746 (0.4619) Acc D Real: 98.965% 
Loss D Fake: 0.8638 (0.8768) Acc D Fake: 0.000% 
Loss D: 1.038 
Loss G: 0.5486 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,028 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1877 (0.4594) Acc D Real: 98.975% 
Loss D Fake: 0.8625 (0.8766) Acc D Fake: 0.000% 
Loss D: 1.050 
Loss G: 0.5502 (0.5467) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,035 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.1594 (0.4568) Acc D Real: 98.984% 
Loss D Fake: 0.8604 (0.8765) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.5554 (0.5468) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,043 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1644 (0.4542) Acc D Real: 98.992% 
Loss D Fake: 0.8511 (0.8763) Acc D Fake: 0.000% 
Loss D: 1.015 
Loss G: 0.5623 (0.5469) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,050 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.1610 (0.4516) Acc D Real: 99.001% 
Loss D Fake: 0.8417 (0.8760) Acc D Fake: 0.000% 
Loss D: 1.003 
Loss G: 0.5681 (0.5471) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,058 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.1693 (0.4491) Acc D Real: 99.009% 
Loss D Fake: 0.8344 (0.8756) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.5730 (0.5473) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,065 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.1803 (0.4468) Acc D Real: 99.017% 
Loss D Fake: 0.8281 (0.8752) Acc D Fake: 0.000% 
Loss D: 1.008 
Loss G: 0.5772 (0.5476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,073 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.1397 (0.4442) Acc D Real: 99.025% 
Loss D Fake: 0.8226 (0.8747) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5813 (0.5479) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,080 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1630 (0.4418) Acc D Real: 99.032% 
Loss D Fake: 0.8174 (0.8743) Acc D Fake: 0.000% 
Loss D: 0.980 
Loss G: 0.5851 (0.5482) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,087 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.1453 (0.4393) Acc D Real: 99.039% 
Loss D Fake: 0.8127 (0.8737) Acc D Fake: 0.000% 
Loss D: 0.958 
Loss G: 0.5886 (0.5485) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,095 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.1457 (0.4369) Acc D Real: 99.045% 
Loss D Fake: 0.8086 (0.8732) Acc D Fake: 0.000% 
Loss D: 0.954 
Loss G: 0.5917 (0.5489) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,102 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.1577 (0.4346) Acc D Real: 99.053% 
Loss D Fake: 0.8047 (0.8726) Acc D Fake: 0.000% 
Loss D: 0.962 
Loss G: 0.5948 (0.5493) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,110 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.1545 (0.4323) Acc D Real: 99.059% 
Loss D Fake: 0.8012 (0.8720) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.5974 (0.5497) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,117 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.1355 (0.4299) Acc D Real: 99.065% 
Loss D Fake: 0.7982 (0.8714) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6001 (0.5501) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,125 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.1437 (0.4276) Acc D Real: 99.069% 
Loss D Fake: 0.7951 (0.8708) Acc D Fake: 0.000% 
Loss D: 0.939 
Loss G: 0.6025 (0.5505) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,132 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.1522 (0.4254) Acc D Real: 99.075% 
Loss D Fake: 0.7922 (0.8702) Acc D Fake: 0.000% 
Loss D: 0.944 
Loss G: 0.6049 (0.5509) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,140 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.1367 (0.4231) Acc D Real: 99.081% 
Loss D Fake: 0.7894 (0.8696) Acc D Fake: 0.000% 
Loss D: 0.926 
Loss G: 0.6074 (0.5514) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,147 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.1475 (0.4209) Acc D Real: 99.085% 
Loss D Fake: 0.7867 (0.8689) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6094 (0.5518) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,155 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1517 (0.4188) Acc D Real: 99.087% 
Loss D Fake: 0.7848 (0.8682) Acc D Fake: 0.000% 
Loss D: 0.936 
Loss G: 0.6113 (0.5523) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,163 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.1720 (0.4169) Acc D Real: 99.088% 
Loss D Fake: 0.7830 (0.8676) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6131 (0.5528) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,171 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.1553 (0.4149) Acc D Real: 99.093% 
Loss D Fake: 0.7812 (0.8669) Acc D Fake: 0.000% 
Loss D: 0.937 
Loss G: 0.6149 (0.5533) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,179 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.1254 (0.4127) Acc D Real: 99.096% 
Loss D Fake: 0.7799 (0.8663) Acc D Fake: 0.000% 
Loss D: 0.905 
Loss G: 0.6164 (0.5537) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,186 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.1386 (0.4106) Acc D Real: 99.102% 
Loss D Fake: 0.7787 (0.8656) Acc D Fake: 0.000% 
Loss D: 0.917 
Loss G: 0.6185 (0.5542) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,194 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.1272 (0.4085) Acc D Real: 99.104% 
Loss D Fake: 0.7762 (0.8649) Acc D Fake: 0.000% 
Loss D: 0.903 
Loss G: 0.6210 (0.5547) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,202 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.1424 (0.4065) Acc D Real: 99.106% 
Loss D Fake: 0.7736 (0.8642) Acc D Fake: 0.000% 
Loss D: 0.916 
Loss G: 0.6238 (0.5552) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,209 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.1318 (0.4044) Acc D Real: 99.109% 
Loss D Fake: 0.7713 (0.8636) Acc D Fake: 0.000% 
Loss D: 0.903 
Loss G: 0.6264 (0.5558) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,217 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.1414 (0.4025) Acc D Real: 99.113% 
Loss D Fake: 0.7703 (0.8629) Acc D Fake: 0.000% 
Loss D: 0.912 
Loss G: 0.6281 (0.5563) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,225 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.1791 (0.4009) Acc D Real: 99.115% 
Loss D Fake: 0.7795 (0.8623) Acc D Fake: 0.000% 
Loss D: 0.959 
Loss G: 0.6187 (0.5568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,232 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.1454 (0.3990) Acc D Real: 99.121% 
Loss D Fake: 1.5749 (0.8674) Acc D Fake: 0.000% 
Loss D: 1.720 
Loss G: 0.6206 (0.5572) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,240 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.1360 (0.3971) Acc D Real: 99.127% 
Loss D Fake: 0.7818 (0.8668) Acc D Fake: 0.000% 
Loss D: 0.918 
Loss G: 0.6307 (0.5578) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,247 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1362 (0.3953) Acc D Real: 99.127% 
Loss D Fake: 0.7695 (0.8661) Acc D Fake: 0.000% 
Loss D: 0.906 
Loss G: 0.6337 (0.5583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,255 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.1425 (0.3935) Acc D Real: 99.126% 
Loss D Fake: 0.7669 (0.8654) Acc D Fake: 0.000% 
Loss D: 0.909 
Loss G: 0.6335 (0.5588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,264 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.1240 (0.3916) Acc D Real: 99.127% 
Loss D Fake: 0.7684 (0.8647) Acc D Fake: 0.000% 
Loss D: 0.892 
Loss G: 0.6312 (0.5593) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,272 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.1867 (0.3901) Acc D Real: 99.121% 
Loss D Fake: 0.7732 (0.8641) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6269 (0.5598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,279 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.1521 (0.3885) Acc D Real: 99.120% 
Loss D Fake: 0.7819 (0.8635) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6201 (0.5602) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,287 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.1968 (0.3872) Acc D Real: 99.107% 
Loss D Fake: 0.7974 (0.8631) Acc D Fake: 0.000% 
Loss D: 0.994 
Loss G: 0.6103 (0.5606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,296 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.1459 (0.3855) Acc D Real: 99.105% 
Loss D Fake: 0.8309 (0.8628) Acc D Fake: 0.000% 
Loss D: 0.977 
Loss G: 0.5979 (0.5608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,303 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.1491 (0.3839) Acc D Real: 99.101% 
Loss D Fake: 0.9029 (0.8631) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.5969 (0.5611) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,311 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.1870 (0.3826) Acc D Real: 99.098% 
Loss D Fake: 0.8286 (0.8629) Acc D Fake: 0.000% 
Loss D: 1.016 
Loss G: 0.6047 (0.5614) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,319 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2036 (0.3814) Acc D Real: 99.087% 
Loss D Fake: 0.8058 (0.8625) Acc D Fake: 0.000% 
Loss D: 1.009 
Loss G: 0.6095 (0.5617) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,327 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2023 (0.3802) Acc D Real: 99.081% 
Loss D Fake: 0.7959 (0.8620) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6121 (0.5620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,334 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.1637 (0.3787) Acc D Real: 99.073% 
Loss D Fake: 0.7909 (0.8616) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6134 (0.5624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,342 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2269 (0.3777) Acc D Real: 99.070% 
Loss D Fake: 0.7887 (0.8611) Acc D Fake: 0.000% 
Loss D: 1.016 
Loss G: 0.6133 (0.5627) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,350 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.1669 (0.3764) Acc D Real: 99.066% 
Loss D Fake: 0.7886 (0.8606) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6124 (0.5630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,357 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.2496 (0.3755) Acc D Real: 99.056% 
Loss D Fake: 0.7896 (0.8602) Acc D Fake: 0.000% 
Loss D: 1.039 
Loss G: 0.6108 (0.5633) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,365 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.1731 (0.3742) Acc D Real: 99.052% 
Loss D Fake: 0.7918 (0.8597) Acc D Fake: 0.000% 
Loss D: 0.965 
Loss G: 0.6088 (0.5636) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,372 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2101 (0.3732) Acc D Real: 99.048% 
Loss D Fake: 0.7947 (0.8593) Acc D Fake: 0.000% 
Loss D: 1.005 
Loss G: 0.6061 (0.5639) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,380 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2150 (0.3722) Acc D Real: 99.046% 
Loss D Fake: 0.7989 (0.8589) Acc D Fake: 0.000% 
Loss D: 1.014 
Loss G: 0.6032 (0.5642) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,387 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.1119 (0.3705) Acc D Real: 99.045% 
Loss D Fake: 0.8031 (0.8586) Acc D Fake: 0.000% 
Loss D: 0.915 
Loss G: 0.6005 (0.5644) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:03,616 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.005 | Generator Loss: 0.585 | Avg: 1.589 
2023-03-02 02:00:03,639 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.029 | Generator Loss: 0.585 | Avg: 1.614 
2023-03-02 02:00:03,663 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.035 | Generator Loss: 0.585 | Avg: 1.620 
2023-03-02 02:00:03,693 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.020 | Generator Loss: 0.585 | Avg: 1.605 
2023-03-02 02:00:03,720 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.028 | Generator Loss: 0.585 | Avg: 1.612 
2023-03-02 02:00:03,747 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.016 | Generator Loss: 0.585 | Avg: 1.601 
2023-03-02 02:00:03,775 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.029 | Generator Loss: 0.585 | Avg: 1.614 
2023-03-02 02:00:03,801 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.026 | Generator Loss: 0.585 | Avg: 1.611 
2023-03-02 02:00:03,828 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.025 | Generator Loss: 0.585 | Avg: 1.610 
2023-03-02 02:00:03,855 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.014 | Generator Loss: 0.585 | Avg: 1.599 
2023-03-02 02:00:03,883 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.007 | Generator Loss: 0.585 | Avg: 1.592 
2023-03-02 02:00:03,910 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.998 | Generator Loss: 0.585 | Avg: 1.582 
2023-03-02 02:00:03,937 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.992 | Generator Loss: 0.585 | Avg: 1.577 
2023-03-02 02:00:03,964 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.989 | Generator Loss: 0.585 | Avg: 1.574 
2023-03-02 02:00:03,992 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.997 | Generator Loss: 0.585 | Avg: 1.582 
2023-03-02 02:00:04,018 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.005 | Generator Loss: 0.585 | Avg: 1.589 
2023-03-02 02:00:04,045 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.008 | Generator Loss: 0.585 | Avg: 1.593 
2023-03-02 02:00:04,082 -                train: [    INFO] - Best Loss 100000000000.000 to 0.795
2023-03-02 02:00:04,082 -                train: [    INFO] - 
Epoch: 2/20
2023-03-02 02:00:04,268 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.1485 (0.1985) Acc D Real: 98.854% 
Loss D Fake: 0.8117 (0.8095) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.5951 (0.5964) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,276 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2223 (0.2064) Acc D Real: 98.646% 
Loss D Fake: 0.8272 (0.8154) Acc D Fake: 0.000% 
Loss D: 1.049 
Loss G: 0.5721 (0.5883) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,284 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.1955 (0.2037) Acc D Real: 98.789% 
Loss D Fake: 1.0132 (0.8648) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.5671 (0.5830) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,303 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.1768 (0.1983) Acc D Real: 98.760% 
Loss D Fake: 0.8503 (0.8619) Acc D Fake: 0.000% 
Loss D: 1.027 
Loss G: 0.5934 (0.5851) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,311 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.1828 (0.1958) Acc D Real: 98.715% 
Loss D Fake: 0.8056 (0.8525) Acc D Fake: 0.000% 
Loss D: 0.988 
Loss G: 0.6064 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,318 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.1981 (0.1961) Acc D Real: 98.735% 
Loss D Fake: 0.7903 (0.8436) Acc D Fake: 0.000% 
Loss D: 0.988 
Loss G: 0.6134 (0.5922) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,325 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2291 (0.2002) Acc D Real: 98.763% 
Loss D Fake: 0.7822 (0.8360) Acc D Fake: 0.000% 
Loss D: 1.011 
Loss G: 0.6175 (0.5954) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,332 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.1628 (0.1961) Acc D Real: 98.796% 
Loss D Fake: 0.7774 (0.8295) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6202 (0.5981) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,339 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.1998 (0.1964) Acc D Real: 98.870% 
Loss D Fake: 0.7744 (0.8239) Acc D Fake: 0.000% 
Loss D: 0.974 
Loss G: 0.6218 (0.6005) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,346 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.1837 (0.1953) Acc D Real: 98.902% 
Loss D Fake: 0.7727 (0.8193) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6224 (0.6025) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,353 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2245 (0.1977) Acc D Real: 98.937% 
Loss D Fake: 0.7724 (0.8154) Acc D Fake: 0.000% 
Loss D: 0.997 
Loss G: 0.6222 (0.6041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,360 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.1680 (0.1954) Acc D Real: 98.962% 
Loss D Fake: 0.7729 (0.8121) Acc D Fake: 0.000% 
Loss D: 0.941 
Loss G: 0.6213 (0.6054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,368 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.1199 (0.1900) Acc D Real: 98.951% 
Loss D Fake: 0.7744 (0.8094) Acc D Fake: 0.000% 
Loss D: 0.894 
Loss G: 0.6201 (0.6065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,375 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.1640 (0.1883) Acc D Real: 98.986% 
Loss D Fake: 0.7761 (0.8072) Acc D Fake: 0.000% 
Loss D: 0.940 
Loss G: 0.6186 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,383 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.1820 (0.1879) Acc D Real: 98.978% 
Loss D Fake: 0.7783 (0.8054) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6169 (0.6079) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,391 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.1416 (0.1852) Acc D Real: 98.986% 
Loss D Fake: 0.7812 (0.8040) Acc D Fake: 0.000% 
Loss D: 0.923 
Loss G: 0.6140 (0.6083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,398 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.1739 (0.1846) Acc D Real: 99.002% 
Loss D Fake: 0.7863 (0.8030) Acc D Fake: 0.000% 
Loss D: 0.960 
Loss G: 0.6103 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,405 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.1535 (0.1829) Acc D Real: 98.991% 
Loss D Fake: 0.7924 (0.8024) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6066 (0.6083) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,413 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.1991 (0.1837) Acc D Real: 99.003% 
Loss D Fake: 0.7990 (0.8023) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6031 (0.6080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,420 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.1465 (0.1820) Acc D Real: 99.000% 
Loss D Fake: 0.8061 (0.8024) Acc D Fake: 0.000% 
Loss D: 0.953 
Loss G: 0.6004 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,427 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.1660 (0.1812) Acc D Real: 99.015% 
Loss D Fake: 0.8115 (0.8029) Acc D Fake: 0.000% 
Loss D: 0.977 
Loss G: 0.5996 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,434 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.1354 (0.1792) Acc D Real: 99.033% 
Loss D Fake: 0.8127 (0.8033) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.6009 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,442 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.1632 (0.1786) Acc D Real: 99.034% 
Loss D Fake: 0.8100 (0.8036) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.6034 (0.6069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,449 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.1508 (0.1775) Acc D Real: 99.029% 
Loss D Fake: 0.8054 (0.8036) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6058 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,456 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.1454 (0.1762) Acc D Real: 99.040% 
Loss D Fake: 0.8009 (0.8035) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6084 (0.6069) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,463 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.1512 (0.1753) Acc D Real: 99.059% 
Loss D Fake: 0.7951 (0.8032) Acc D Fake: 0.000% 
Loss D: 0.946 
Loss G: 0.6097 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,471 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.1231 (0.1734) Acc D Real: 99.062% 
Loss D Fake: 0.7898 (0.8027) Acc D Fake: 0.000% 
Loss D: 0.913 
Loss G: 0.6008 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,478 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.1232 (0.1717) Acc D Real: 99.075% 
Loss D Fake: 0.9352 (0.8073) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6031 (0.6066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,486 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.1439 (0.1708) Acc D Real: 99.085% 
Loss D Fake: 1.0233 (0.8145) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.5690 (0.6054) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,493 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.1366 (0.1697) Acc D Real: 99.099% 
Loss D Fake: 0.8957 (0.8171) Acc D Fake: 0.000% 
Loss D: 1.032 
Loss G: 0.5956 (0.6051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,501 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.1528 (0.1691) Acc D Real: 99.102% 
Loss D Fake: 0.8033 (0.8167) Acc D Fake: 0.000% 
Loss D: 0.956 
Loss G: 0.6108 (0.6052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,509 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.1349 (0.1681) Acc D Real: 99.096% 
Loss D Fake: 0.7865 (0.8158) Acc D Fake: 0.000% 
Loss D: 0.921 
Loss G: 0.6175 (0.6056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,516 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1751 (0.1683) Acc D Real: 99.102% 
Loss D Fake: 0.7795 (0.8147) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6209 (0.6061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,523 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.1789 (0.1686) Acc D Real: 99.101% 
Loss D Fake: 0.7763 (0.8136) Acc D Fake: 0.000% 
Loss D: 0.955 
Loss G: 0.6223 (0.6065) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,531 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.1455 (0.1680) Acc D Real: 99.087% 
Loss D Fake: 0.7757 (0.8126) Acc D Fake: 0.000% 
Loss D: 0.921 
Loss G: 0.6221 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,538 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.1677 (0.1680) Acc D Real: 99.088% 
Loss D Fake: 0.7776 (0.8116) Acc D Fake: 0.000% 
Loss D: 0.945 
Loss G: 0.6201 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,546 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.1955 (0.1687) Acc D Real: 99.105% 
Loss D Fake: 0.7823 (0.8109) Acc D Fake: 0.000% 
Loss D: 0.978 
Loss G: 0.6161 (0.6075) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,553 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.1440 (0.1681) Acc D Real: 99.091% 
Loss D Fake: 0.7912 (0.8103) Acc D Fake: 0.000% 
Loss D: 0.935 
Loss G: 0.6097 (0.6076) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,560 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.1661 (0.1680) Acc D Real: 99.085% 
Loss D Fake: 0.8067 (0.8103) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.5993 (0.6074) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,569 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.1493 (0.1676) Acc D Real: 99.088% 
Loss D Fake: 0.8419 (0.8110) Acc D Fake: 0.000% 
Loss D: 0.991 
Loss G: 0.5827 (0.6068) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,577 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.1501 (0.1671) Acc D Real: 99.082% 
Loss D Fake: 0.9965 (0.8154) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.5763 (0.6061) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,584 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2210 (0.1684) Acc D Real: 99.081% 
Loss D Fake: 0.9158 (0.8178) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.5862 (0.6056) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,592 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2160 (0.1695) Acc D Real: 99.066% 
Loss D Fake: 0.8548 (0.8186) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.5918 (0.6053) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,599 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2073 (0.1703) Acc D Real: 99.050% 
Loss D Fake: 0.8439 (0.8192) Acc D Fake: 0.000% 
Loss D: 1.051 
Loss G: 0.5903 (0.6050) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,607 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2069 (0.1711) Acc D Real: 99.043% 
Loss D Fake: 0.8481 (0.8198) Acc D Fake: 0.000% 
Loss D: 1.055 
Loss G: 0.5852 (0.6045) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,615 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2072 (0.1719) Acc D Real: 99.047% 
Loss D Fake: 0.8526 (0.8205) Acc D Fake: 0.000% 
Loss D: 1.060 
Loss G: 0.5805 (0.6040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,622 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2023 (0.1725) Acc D Real: 99.010% 
Loss D Fake: 0.8484 (0.8211) Acc D Fake: 0.000% 
Loss D: 1.051 
Loss G: 0.5781 (0.6035) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,630 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3299 (0.1757) Acc D Real: 98.975% 
Loss D Fake: 0.8384 (0.8214) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.5808 (0.6030) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,637 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2426 (0.1771) Acc D Real: 98.953% 
Loss D Fake: 0.8247 (0.8215) Acc D Fake: 0.000% 
Loss D: 1.067 
Loss G: 0.5878 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,645 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3100 (0.1797) Acc D Real: 98.910% 
Loss D Fake: 0.8111 (0.8213) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.5957 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,653 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2634 (0.1813) Acc D Real: 98.875% 
Loss D Fake: 0.7998 (0.8209) Acc D Fake: 0.000% 
Loss D: 1.063 
Loss G: 0.6027 (0.6026) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,660 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.1828 (0.1813) Acc D Real: 98.867% 
Loss D Fake: 0.7906 (0.8203) Acc D Fake: 0.000% 
Loss D: 0.973 
Loss G: 0.6094 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,668 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2495 (0.1826) Acc D Real: 98.837% 
Loss D Fake: 0.7825 (0.8196) Acc D Fake: 0.000% 
Loss D: 1.032 
Loss G: 0.6153 (0.6029) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,675 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.1775 (0.1825) Acc D Real: 98.812% 
Loss D Fake: 0.7756 (0.8188) Acc D Fake: 0.000% 
Loss D: 0.953 
Loss G: 0.6208 (0.6033) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,683 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2100 (0.1830) Acc D Real: 98.779% 
Loss D Fake: 0.7693 (0.8179) Acc D Fake: 0.000% 
Loss D: 0.979 
Loss G: 0.6259 (0.6037) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,691 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2188 (0.1836) Acc D Real: 98.747% 
Loss D Fake: 0.7636 (0.8170) Acc D Fake: 0.000% 
Loss D: 0.982 
Loss G: 0.6305 (0.6041) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,699 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2132 (0.1841) Acc D Real: 98.722% 
Loss D Fake: 0.7585 (0.8160) Acc D Fake: 0.000% 
Loss D: 0.972 
Loss G: 0.6348 (0.6047) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,706 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.2241 (0.1848) Acc D Real: 98.693% 
Loss D Fake: 0.7542 (0.8149) Acc D Fake: 0.000% 
Loss D: 0.978 
Loss G: 0.6379 (0.6052) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,714 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.1813 (0.1847) Acc D Real: 98.672% 
Loss D Fake: 0.7511 (0.8139) Acc D Fake: 0.000% 
Loss D: 0.932 
Loss G: 0.6407 (0.6058) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,721 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2026 (0.1850) Acc D Real: 98.658% 
Loss D Fake: 0.7481 (0.8128) Acc D Fake: 0.000% 
Loss D: 0.951 
Loss G: 0.6434 (0.6064) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,729 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2031 (0.1853) Acc D Real: 98.622% 
Loss D Fake: 0.7451 (0.8117) Acc D Fake: 0.000% 
Loss D: 0.948 
Loss G: 0.6461 (0.6071) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,736 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2218 (0.1859) Acc D Real: 98.585% 
Loss D Fake: 0.7422 (0.8106) Acc D Fake: 0.000% 
Loss D: 0.964 
Loss G: 0.6489 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,744 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.1951 (0.1860) Acc D Real: 98.574% 
Loss D Fake: 0.7392 (0.8095) Acc D Fake: 0.000% 
Loss D: 0.934 
Loss G: 0.6515 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,751 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.1608 (0.1856) Acc D Real: 98.571% 
Loss D Fake: 0.7366 (0.8084) Acc D Fake: 0.000% 
Loss D: 0.897 
Loss G: 0.6539 (0.6091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,758 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.1931 (0.1858) Acc D Real: 98.550% 
Loss D Fake: 0.7342 (0.8072) Acc D Fake: 0.000% 
Loss D: 0.927 
Loss G: 0.6558 (0.6098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:04,766 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.1922 (0.1859) Acc D Real: 98.532% 
Loss D Fake: 0.7326 (0.8061) Acc D Fake: 0.000% 
Loss D: 0.925 
Loss G: 0.6567 (0.6105) Acc G: 99.950% 
LR: 2.000e-04 

2023-03-02 02:00:04,773 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.1689 (0.1856) Acc D Real: 98.521% 
Loss D Fake: 0.7326 (0.8050) Acc D Fake: 0.049% 
Loss D: 0.901 
Loss G: 0.6551 (0.6112) Acc G: 99.858% 
LR: 2.000e-04 

2023-03-02 02:00:04,781 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.2177 (0.1861) Acc D Real: 98.508% 
Loss D Fake: 0.7386 (0.8041) Acc D Fake: 0.145% 
Loss D: 0.956 
Loss G: 0.6425 (0.6116) Acc G: 99.763% 
LR: 2.000e-04 

2023-03-02 02:00:04,788 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.1395 (0.1854) Acc D Real: 98.508% 
Loss D Fake: 0.7863 (0.8038) Acc D Fake: 0.262% 
Loss D: 0.926 
Loss G: 0.6589 (0.6123) Acc G: 99.624% 
LR: 2.000e-04 

2023-03-02 02:00:04,796 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.1873 (0.1854) Acc D Real: 98.482% 
Loss D Fake: 0.7269 (0.8027) Acc D Fake: 0.376% 
Loss D: 0.914 
Loss G: 0.6625 (0.6130) Acc G: 99.488% 
LR: 2.000e-04 

2023-03-02 02:00:04,804 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1855 (0.1854) Acc D Real: 98.453% 
Loss D Fake: 0.7261 (0.8017) Acc D Fake: 0.509% 
Loss D: 0.912 
Loss G: 0.6627 (0.6137) Acc G: 99.356% 
LR: 2.000e-04 

2023-03-02 02:00:04,812 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.1677 (0.1852) Acc D Real: 98.433% 
Loss D Fake: 0.7259 (0.8006) Acc D Fake: 0.639% 
Loss D: 0.894 
Loss G: 0.6629 (0.6144) Acc G: 99.205% 
LR: 2.000e-04 

2023-03-02 02:00:04,821 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2021 (0.1854) Acc D Real: 98.411% 
Loss D Fake: 0.7254 (0.7996) Acc D Fake: 0.788% 
Loss D: 0.927 
Loss G: 0.6636 (0.6151) Acc G: 99.058% 
LR: 2.000e-04 

2023-03-02 02:00:04,828 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.1717 (0.1852) Acc D Real: 98.390% 
Loss D Fake: 0.7245 (0.7986) Acc D Fake: 0.933% 
Loss D: 0.896 
Loss G: 0.6647 (0.6157) Acc G: 98.915% 
LR: 2.000e-04 

2023-03-02 02:00:04,836 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2193 (0.1857) Acc D Real: 98.357% 
Loss D Fake: 0.7230 (0.7976) Acc D Fake: 1.075% 
Loss D: 0.942 
Loss G: 0.6663 (0.6164) Acc G: 98.754% 
LR: 2.000e-04 

2023-03-02 02:00:04,844 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2061 (0.1859) Acc D Real: 98.319% 
Loss D Fake: 0.7211 (0.7966) Acc D Fake: 1.255% 
Loss D: 0.927 
Loss G: 0.6684 (0.6171) Acc G: 98.575% 
LR: 2.000e-04 

2023-03-02 02:00:04,851 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.1582 (0.1856) Acc D Real: 98.303% 
Loss D Fake: 0.7187 (0.7956) Acc D Fake: 1.432% 
Loss D: 0.877 
Loss G: 0.6707 (0.6177) Acc G: 98.380% 
LR: 2.000e-04 

2023-03-02 02:00:04,859 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2083 (0.1859) Acc D Real: 98.251% 
Loss D Fake: 0.7164 (0.7946) Acc D Fake: 1.624% 
Loss D: 0.925 
Loss G: 0.6730 (0.6184) Acc G: 98.169% 
LR: 2.000e-04 

2023-03-02 02:00:04,867 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.1723 (0.1857) Acc D Real: 98.225% 
Loss D Fake: 0.7143 (0.7936) Acc D Fake: 1.833% 
Loss D: 0.887 
Loss G: 0.6747 (0.6191) Acc G: 97.941% 
LR: 2.000e-04 

2023-03-02 02:00:04,875 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.1891 (0.1858) Acc D Real: 98.203% 
Loss D Fake: 0.7127 (0.7926) Acc D Fake: 2.058% 
Loss D: 0.902 
Loss G: 0.6764 (0.6198) Acc G: 97.720% 
LR: 2.000e-04 

2023-03-02 02:00:04,882 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.1542 (0.1854) Acc D Real: 98.194% 
Loss D Fake: 0.7109 (0.7916) Acc D Fake: 2.276% 
Loss D: 0.865 
Loss G: 0.6782 (0.6206) Acc G: 97.240% 
LR: 2.000e-04 

2023-03-02 02:00:04,889 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.1749 (0.1852) Acc D Real: 98.168% 
Loss D Fake: 0.7090 (0.7906) Acc D Fake: 2.912% 
Loss D: 0.884 
Loss G: 0.6800 (0.6213) Acc G: 96.570% 
LR: 2.000e-04 

2023-03-02 02:00:04,897 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.1669 (0.1850) Acc D Real: 98.156% 
Loss D Fake: 0.7073 (0.7896) Acc D Fake: 3.591% 
Loss D: 0.874 
Loss G: 0.6816 (0.6220) Acc G: 95.857% 
LR: 2.000e-04 

2023-03-02 02:00:04,904 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.1651 (0.1848) Acc D Real: 98.140% 
Loss D Fake: 0.7058 (0.7886) Acc D Fake: 4.314% 
Loss D: 0.871 
Loss G: 0.6829 (0.6227) Acc G: 95.121% 
LR: 2.000e-04 

2023-03-02 02:00:04,913 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.1590 (0.1845) Acc D Real: 98.128% 
Loss D Fake: 0.7047 (0.7877) Acc D Fake: 5.039% 
Loss D: 0.864 
Loss G: 0.6836 (0.6234) Acc G: 94.365% 
LR: 2.000e-04 

2023-03-02 02:00:04,921 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.1904 (0.1846) Acc D Real: 98.097% 
Loss D Fake: 0.7050 (0.7867) Acc D Fake: 5.670% 
Loss D: 0.895 
Loss G: 0.6806 (0.6241) Acc G: 93.989% 
LR: 2.000e-04 

2023-03-02 02:00:04,928 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.1721 (0.1844) Acc D Real: 98.068% 
Loss D Fake: 0.7147 (0.7859) Acc D Fake: 5.871% 
Loss D: 0.887 
Loss G: 0.6679 (0.6246) Acc G: 93.792% 
LR: 2.000e-04 

2023-03-02 02:00:04,936 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.1535 (0.1841) Acc D Real: 98.061% 
Loss D Fake: 0.7378 (0.7854) Acc D Fake: 6.067% 
Loss D: 0.891 
Loss G: 0.6847 (0.6253) Acc G: 93.188% 
LR: 2.000e-04 

2023-03-02 02:00:04,944 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.1670 (0.1839) Acc D Real: 98.035% 
Loss D Fake: 0.6983 (0.7844) Acc D Fake: 6.833% 
Loss D: 0.865 
Loss G: 0.6919 (0.6260) Acc G: 92.430% 
LR: 2.000e-04 

2023-03-02 02:00:04,952 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2157 (0.1842) Acc D Real: 97.976% 
Loss D Fake: 0.6950 (0.7834) Acc D Fake: 7.582% 
Loss D: 0.911 
Loss G: 0.6940 (0.6267) Acc G: 91.689% 
LR: 2.000e-04 

2023-03-02 02:00:04,960 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.1518 (0.1839) Acc D Real: 97.961% 
Loss D Fake: 0.6934 (0.7824) Acc D Fake: 8.315% 
Loss D: 0.845 
Loss G: 0.6955 (0.6275) Acc G: 90.964% 
LR: 2.000e-04 

2023-03-02 02:00:04,967 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.1615 (0.1836) Acc D Real: 97.938% 
Loss D Fake: 0.6919 (0.7815) Acc D Fake: 9.032% 
Loss D: 0.853 
Loss G: 0.6970 (0.6282) Acc G: 90.255% 
LR: 2.000e-04 

2023-03-02 02:00:04,975 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.1800 (0.1836) Acc D Real: 97.903% 
Loss D Fake: 0.6903 (0.7805) Acc D Fake: 9.734% 
Loss D: 0.870 
Loss G: 0.6987 (0.6290) Acc G: 89.561% 
LR: 2.000e-04 

2023-03-02 02:00:04,983 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.1690 (0.1834) Acc D Real: 97.877% 
Loss D Fake: 0.6886 (0.7795) Acc D Fake: 10.421% 
Loss D: 0.858 
Loss G: 0.7006 (0.6297) Acc G: 88.881% 
LR: 2.000e-04 

2023-03-02 02:00:04,991 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.1865 (0.1835) Acc D Real: 97.842% 
Loss D Fake: 0.6867 (0.7786) Acc D Fake: 11.094% 
Loss D: 0.873 
Loss G: 0.7026 (0.6305) Acc G: 88.198% 
LR: 2.000e-04 

2023-03-02 02:00:04,998 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.1916 (0.1836) Acc D Real: 97.801% 
Loss D Fake: 0.6847 (0.7776) Acc D Fake: 11.770% 
Loss D: 0.876 
Loss G: 0.7046 (0.6313) Acc G: 87.530% 
LR: 2.000e-04 

2023-03-02 02:00:05,006 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.1315 (0.1830) Acc D Real: 97.794% 
Loss D Fake: 0.6828 (0.7766) Acc D Fake: 12.432% 
Loss D: 0.814 
Loss G: 0.7067 (0.6320) Acc G: 86.874% 
LR: 2.000e-04 

2023-03-02 02:00:05,013 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.1386 (0.1826) Acc D Real: 97.782% 
Loss D Fake: 0.6807 (0.7757) Acc D Fake: 13.081% 
Loss D: 0.819 
Loss G: 0.7089 (0.6328) Acc G: 86.233% 
LR: 2.000e-04 

2023-03-02 02:00:05,021 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2482 (0.1832) Acc D Real: 97.673% 
Loss D Fake: 0.6786 (0.7747) Acc D Fake: 13.717% 
Loss D: 0.927 
Loss G: 0.7111 (0.6336) Acc G: 85.604% 
LR: 2.000e-04 

2023-03-02 02:00:05,028 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.1914 (0.1833) Acc D Real: 97.629% 
Loss D Fake: 0.6766 (0.7737) Acc D Fake: 14.340% 
Loss D: 0.868 
Loss G: 0.7131 (0.6344) Acc G: 84.987% 
LR: 2.000e-04 

2023-03-02 02:00:05,037 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.1777 (0.1833) Acc D Real: 97.584% 
Loss D Fake: 0.6747 (0.7727) Acc D Fake: 14.967% 
Loss D: 0.852 
Loss G: 0.7150 (0.6352) Acc G: 84.366% 
LR: 2.000e-04 

2023-03-02 02:00:05,044 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2078 (0.1835) Acc D Real: 97.520% 
Loss D Fake: 0.6732 (0.7718) Acc D Fake: 15.583% 
Loss D: 0.881 
Loss G: 0.7163 (0.6360) Acc G: 83.758% 
LR: 2.000e-04 

2023-03-02 02:00:05,052 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.1738 (0.1834) Acc D Real: 97.493% 
Loss D Fake: 0.6722 (0.7708) Acc D Fake: 16.186% 
Loss D: 0.846 
Loss G: 0.7170 (0.6367) Acc G: 83.161% 
LR: 2.000e-04 

2023-03-02 02:00:05,060 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.1576 (0.1832) Acc D Real: 97.474% 
Loss D Fake: 0.6718 (0.7699) Acc D Fake: 16.778% 
Loss D: 0.829 
Loss G: 0.7173 (0.6375) Acc G: 82.575% 
LR: 2.000e-04 

2023-03-02 02:00:05,067 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.1450 (0.1828) Acc D Real: 97.453% 
Loss D Fake: 0.6718 (0.7689) Acc D Fake: 17.358% 
Loss D: 0.817 
Loss G: 0.7173 (0.6383) Acc G: 82.000% 
LR: 2.000e-04 

2023-03-02 02:00:05,075 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.1494 (0.1825) Acc D Real: 97.450% 
Loss D Fake: 0.6718 (0.7680) Acc D Fake: 17.928% 
Loss D: 0.821 
Loss G: 0.7177 (0.6390) Acc G: 81.436% 
LR: 2.000e-04 

2023-03-02 02:00:05,082 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.1364 (0.1821) Acc D Real: 97.449% 
Loss D Fake: 0.6709 (0.7671) Acc D Fake: 18.488% 
Loss D: 0.807 
Loss G: 0.7195 (0.6397) Acc G: 80.883% 
LR: 2.000e-04 

2023-03-02 02:00:05,090 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.1683 (0.1819) Acc D Real: 97.417% 
Loss D Fake: 0.6687 (0.7662) Acc D Fake: 19.037% 
Loss D: 0.837 
Loss G: 0.7220 (0.6405) Acc G: 80.332% 
LR: 2.000e-04 

2023-03-02 02:00:05,097 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.1354 (0.1815) Acc D Real: 97.407% 
Loss D Fake: 0.6660 (0.7653) Acc D Fake: 19.591% 
Loss D: 0.801 
Loss G: 0.7253 (0.6413) Acc G: 79.783% 
LR: 2.000e-04 

2023-03-02 02:00:05,105 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1616 (0.1813) Acc D Real: 97.386% 
Loss D Fake: 0.6628 (0.7644) Acc D Fake: 20.135% 
Loss D: 0.824 
Loss G: 0.7285 (0.6421) Acc G: 79.245% 
LR: 2.000e-04 

2023-03-02 02:00:05,112 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.1914 (0.1814) Acc D Real: 97.328% 
Loss D Fake: 0.6605 (0.7635) Acc D Fake: 20.670% 
Loss D: 0.852 
Loss G: 0.7300 (0.6428) Acc G: 78.716% 
LR: 2.000e-04 

2023-03-02 02:00:05,120 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1805 (0.1814) Acc D Real: 97.293% 
Loss D Fake: 0.6603 (0.7626) Acc D Fake: 21.195% 
Loss D: 0.841 
Loss G: 0.7283 (0.6436) Acc G: 78.196% 
LR: 2.000e-04 

2023-03-02 02:00:05,127 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.1610 (0.1812) Acc D Real: 97.266% 
Loss D Fake: 0.6647 (0.7617) Acc D Fake: 21.696% 
Loss D: 0.826 
Loss G: 0.7238 (0.6443) Acc G: 77.729% 
LR: 2.000e-04 

2023-03-02 02:00:05,135 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.1429 (0.1809) Acc D Real: 97.249% 
Loss D Fake: 0.6678 (0.7609) Acc D Fake: 22.145% 
Loss D: 0.811 
Loss G: 0.7302 (0.6450) Acc G: 77.227% 
LR: 2.000e-04 

2023-03-02 02:00:05,143 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.1758 (0.1809) Acc D Real: 97.209% 
Loss D Fake: 0.6558 (0.7600) Acc D Fake: 22.644% 
Loss D: 0.832 
Loss G: 0.7384 (0.6458) Acc G: 76.734% 
LR: 2.000e-04 

2023-03-02 02:00:05,151 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.1593 (0.1807) Acc D Real: 97.191% 
Loss D Fake: 0.6502 (0.7590) Acc D Fake: 23.134% 
Loss D: 0.809 
Loss G: 0.7431 (0.6467) Acc G: 76.249% 
LR: 2.000e-04 

2023-03-02 02:00:05,159 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1691 (0.1806) Acc D Real: 97.164% 
Loss D Fake: 0.6467 (0.7581) Acc D Fake: 23.616% 
Loss D: 0.816 
Loss G: 0.7465 (0.6475) Acc G: 75.772% 
LR: 2.000e-04 

2023-03-02 02:00:05,166 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.1413 (0.1802) Acc D Real: 97.152% 
Loss D Fake: 0.6439 (0.7571) Acc D Fake: 24.090% 
Loss D: 0.785 
Loss G: 0.7495 (0.6484) Acc G: 75.304% 
LR: 2.000e-04 

2023-03-02 02:00:05,174 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.1508 (0.1800) Acc D Real: 97.143% 
Loss D Fake: 0.6414 (0.7562) Acc D Fake: 24.556% 
Loss D: 0.792 
Loss G: 0.7522 (0.6492) Acc G: 74.843% 
LR: 2.000e-04 

2023-03-02 02:00:05,181 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.1846 (0.1800) Acc D Real: 97.092% 
Loss D Fake: 0.6391 (0.7552) Acc D Fake: 25.028% 
Loss D: 0.824 
Loss G: 0.7548 (0.6501) Acc G: 74.376% 
LR: 2.000e-04 

2023-03-02 02:00:05,192 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.1561 (0.1798) Acc D Real: 97.072% 
Loss D Fake: 0.6370 (0.7542) Acc D Fake: 25.492% 
Loss D: 0.793 
Loss G: 0.7571 (0.6510) Acc G: 73.916% 
LR: 2.000e-04 

2023-03-02 02:00:05,200 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.1678 (0.1797) Acc D Real: 97.038% 
Loss D Fake: 0.6351 (0.7533) Acc D Fake: 25.949% 
Loss D: 0.803 
Loss G: 0.7592 (0.6519) Acc G: 73.465% 
LR: 2.000e-04 

2023-03-02 02:00:05,208 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.1421 (0.1794) Acc D Real: 97.020% 
Loss D Fake: 0.6333 (0.7523) Acc D Fake: 26.398% 
Loss D: 0.775 
Loss G: 0.7614 (0.6528) Acc G: 73.020% 
LR: 2.000e-04 

2023-03-02 02:00:05,215 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.1694 (0.1794) Acc D Real: 97.001% 
Loss D Fake: 0.6315 (0.7513) Acc D Fake: 26.840% 
Loss D: 0.801 
Loss G: 0.7634 (0.6536) Acc G: 72.583% 
LR: 2.000e-04 

2023-03-02 02:00:05,224 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.1641 (0.1792) Acc D Real: 96.977% 
Loss D Fake: 0.6300 (0.7504) Acc D Fake: 27.275% 
Loss D: 0.794 
Loss G: 0.7648 (0.6545) Acc G: 72.152% 
LR: 2.000e-04 

2023-03-02 02:00:05,231 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.1538 (0.1790) Acc D Real: 96.951% 
Loss D Fake: 0.6291 (0.7494) Acc D Fake: 27.703% 
Loss D: 0.783 
Loss G: 0.7657 (0.6554) Acc G: 71.728% 
LR: 2.000e-04 

2023-03-02 02:00:05,239 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1938 (0.1792) Acc D Real: 96.890% 
Loss D Fake: 0.6284 (0.7485) Acc D Fake: 28.125% 
Loss D: 0.822 
Loss G: 0.7668 (0.6563) Acc G: 71.311% 
LR: 2.000e-04 

2023-03-02 02:00:05,246 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.1825 (0.1792) Acc D Real: 96.845% 
Loss D Fake: 0.6274 (0.7475) Acc D Fake: 28.540% 
Loss D: 0.810 
Loss G: 0.7686 (0.6571) Acc G: 70.900% 
LR: 2.000e-04 

2023-03-02 02:00:05,254 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.1429 (0.1789) Acc D Real: 96.834% 
Loss D Fake: 0.6252 (0.7466) Acc D Fake: 28.949% 
Loss D: 0.768 
Loss G: 0.7723 (0.6580) Acc G: 70.488% 
LR: 2.000e-04 

2023-03-02 02:00:05,261 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2082 (0.1791) Acc D Real: 96.775% 
Loss D Fake: 0.6219 (0.7456) Acc D Fake: 29.364% 
Loss D: 0.830 
Loss G: 0.7755 (0.6589) Acc G: 70.077% 
LR: 2.000e-04 

2023-03-02 02:00:05,269 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.1555 (0.1789) Acc D Real: 96.755% 
Loss D Fake: 0.6194 (0.7447) Acc D Fake: 29.773% 
Loss D: 0.775 
Loss G: 0.7785 (0.6598) Acc G: 69.673% 
LR: 2.000e-04 

2023-03-02 02:00:05,276 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.1816 (0.1790) Acc D Real: 96.721% 
Loss D Fake: 0.6171 (0.7437) Acc D Fake: 30.175% 
Loss D: 0.799 
Loss G: 0.7808 (0.6607) Acc G: 69.274% 
LR: 2.000e-04 

2023-03-02 02:00:05,284 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.1384 (0.1787) Acc D Real: 96.715% 
Loss D Fake: 0.6153 (0.7428) Acc D Fake: 30.572% 
Loss D: 0.754 
Loss G: 0.7835 (0.6617) Acc G: 68.881% 
LR: 2.000e-04 

2023-03-02 02:00:05,292 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.1943 (0.1788) Acc D Real: 96.659% 
Loss D Fake: 0.6128 (0.7418) Acc D Fake: 30.963% 
Loss D: 0.807 
Loss G: 0.7865 (0.6626) Acc G: 68.495% 
LR: 2.000e-04 

2023-03-02 02:00:05,299 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.1542 (0.1786) Acc D Real: 96.638% 
Loss D Fake: 0.6104 (0.7408) Acc D Fake: 31.348% 
Loss D: 0.765 
Loss G: 0.7895 (0.6635) Acc G: 68.114% 
LR: 2.000e-04 

2023-03-02 02:00:05,307 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.1333 (0.1783) Acc D Real: 96.635% 
Loss D Fake: 0.6079 (0.7399) Acc D Fake: 31.727% 
Loss D: 0.741 
Loss G: 0.7928 (0.6645) Acc G: 67.738% 
LR: 2.000e-04 

2023-03-02 02:00:05,314 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.1972 (0.1784) Acc D Real: 96.578% 
Loss D Fake: 0.6052 (0.7389) Acc D Fake: 32.101% 
Loss D: 0.802 
Loss G: 0.7960 (0.6654) Acc G: 67.368% 
LR: 2.000e-04 

2023-03-02 02:00:05,321 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.1596 (0.1783) Acc D Real: 96.560% 
Loss D Fake: 0.6028 (0.7379) Acc D Fake: 32.470% 
Loss D: 0.762 
Loss G: 0.7991 (0.6664) Acc G: 67.003% 
LR: 2.000e-04 

2023-03-02 02:00:05,328 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1442 (0.1780) Acc D Real: 96.545% 
Loss D Fake: 0.6003 (0.7369) Acc D Fake: 32.833% 
Loss D: 0.744 
Loss G: 0.8022 (0.6673) Acc G: 66.644% 
LR: 2.000e-04 

2023-03-02 02:00:05,336 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.1666 (0.1779) Acc D Real: 96.517% 
Loss D Fake: 0.5979 (0.7359) Acc D Fake: 33.191% 
Loss D: 0.765 
Loss G: 0.8052 (0.6683) Acc G: 66.289% 
LR: 2.000e-04 

2023-03-02 02:00:05,343 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.1714 (0.1779) Acc D Real: 96.480% 
Loss D Fake: 0.5956 (0.7349) Acc D Fake: 33.545% 
Loss D: 0.767 
Loss G: 0.8080 (0.6693) Acc G: 65.940% 
LR: 2.000e-04 

2023-03-02 02:00:05,351 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.1781 (0.1779) Acc D Real: 96.446% 
Loss D Fake: 0.5937 (0.7340) Acc D Fake: 33.893% 
Loss D: 0.772 
Loss G: 0.8103 (0.6703) Acc G: 65.595% 
LR: 2.000e-04 

2023-03-02 02:00:05,358 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.1593 (0.1778) Acc D Real: 96.423% 
Loss D Fake: 0.5921 (0.7330) Acc D Fake: 34.236% 
Loss D: 0.751 
Loss G: 0.8124 (0.6713) Acc G: 65.244% 
LR: 2.000e-04 

2023-03-02 02:00:05,366 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.1373 (0.1775) Acc D Real: 96.414% 
Loss D Fake: 0.5917 (0.7320) Acc D Fake: 34.586% 
Loss D: 0.729 
Loss G: 0.8103 (0.6722) Acc G: 64.897% 
LR: 2.000e-04 

2023-03-02 02:00:05,375 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.1501 (0.1773) Acc D Real: 96.397% 
Loss D Fake: 0.5955 (0.7311) Acc D Fake: 34.932% 
Loss D: 0.746 
Loss G: 0.8161 (0.6732) Acc G: 64.556% 
LR: 2.000e-04 

2023-03-02 02:00:05,383 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.1954 (0.1774) Acc D Real: 96.342% 
Loss D Fake: 0.5859 (0.7301) Acc D Fake: 35.272% 
Loss D: 0.781 
Loss G: 0.8219 (0.6742) Acc G: 64.218% 
LR: 2.000e-04 

2023-03-02 02:00:05,391 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2118 (0.1777) Acc D Real: 96.280% 
Loss D Fake: 0.5829 (0.7291) Acc D Fake: 35.608% 
Loss D: 0.795 
Loss G: 0.8246 (0.6752) Acc G: 63.886% 
LR: 2.000e-04 

2023-03-02 02:00:05,399 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.1332 (0.1774) Acc D Real: 96.270% 
Loss D Fake: 0.5812 (0.7281) Acc D Fake: 35.940% 
Loss D: 0.714 
Loss G: 0.8277 (0.6763) Acc G: 63.558% 
LR: 2.000e-04 

2023-03-02 02:00:05,406 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.1718 (0.1773) Acc D Real: 96.232% 
Loss D Fake: 0.5788 (0.7271) Acc D Fake: 36.267% 
Loss D: 0.751 
Loss G: 0.8304 (0.6773) Acc G: 63.236% 
LR: 2.000e-04 

2023-03-02 02:00:05,413 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.1710 (0.1773) Acc D Real: 96.206% 
Loss D Fake: 0.5774 (0.7261) Acc D Fake: 36.578% 
Loss D: 0.748 
Loss G: 0.8320 (0.6783) Acc G: 62.928% 
LR: 2.000e-04 

2023-03-02 02:00:05,421 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.1481 (0.1771) Acc D Real: 96.195% 
Loss D Fake: 0.5766 (0.7251) Acc D Fake: 36.886% 
Loss D: 0.725 
Loss G: 0.8331 (0.6793) Acc G: 62.624% 
LR: 2.000e-04 

2023-03-02 02:00:05,428 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.1393 (0.1768) Acc D Real: 96.180% 
Loss D Fake: 0.5760 (0.7241) Acc D Fake: 37.190% 
Loss D: 0.715 
Loss G: 0.8343 (0.6804) Acc G: 62.323% 
LR: 2.000e-04 

2023-03-02 02:00:05,436 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.1781 (0.1769) Acc D Real: 96.146% 
Loss D Fake: 0.5752 (0.7232) Acc D Fake: 37.489% 
Loss D: 0.753 
Loss G: 0.8359 (0.6814) Acc G: 62.027% 
LR: 2.000e-04 

2023-03-02 02:00:05,443 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.1436 (0.1766) Acc D Real: 96.126% 
Loss D Fake: 0.5740 (0.7222) Acc D Fake: 37.785% 
Loss D: 0.718 
Loss G: 0.8381 (0.6824) Acc G: 61.734% 
LR: 2.000e-04 

2023-03-02 02:00:05,451 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.1171 (0.1763) Acc D Real: 96.127% 
Loss D Fake: 0.5722 (0.7213) Acc D Fake: 38.077% 
Loss D: 0.689 
Loss G: 0.8411 (0.6834) Acc G: 61.445% 
LR: 2.000e-04 

2023-03-02 02:00:05,463 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.1369 (0.1760) Acc D Real: 96.124% 
Loss D Fake: 0.5701 (0.7203) Acc D Fake: 38.365% 
Loss D: 0.707 
Loss G: 0.8443 (0.6844) Acc G: 61.160% 
LR: 2.000e-04 

2023-03-02 02:00:05,471 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.0638 (0.1753) Acc D Real: 96.125% 
Loss D Fake: 0.5676 (0.7193) Acc D Fake: 38.392% 
Loss D: 0.631 
Loss G: 0.8487 (0.6855) Acc G: 61.134% 
LR: 2.000e-04 

2023-03-02 02:00:05,671 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.632 | Generator Loss: 0.849 | Avg: 1.481 
2023-03-02 02:00:05,693 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.650 | Generator Loss: 0.849 | Avg: 1.499 
2023-03-02 02:00:05,716 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.647 | Generator Loss: 0.849 | Avg: 1.495 
2023-03-02 02:00:05,743 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.641 | Generator Loss: 0.849 | Avg: 1.490 
2023-03-02 02:00:05,769 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.641 | Generator Loss: 0.849 | Avg: 1.490 
2023-03-02 02:00:05,797 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.659 | Generator Loss: 0.849 | Avg: 1.508 
2023-03-02 02:00:05,823 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.663 | Generator Loss: 0.849 | Avg: 1.512 
2023-03-02 02:00:05,850 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.678 | Generator Loss: 0.849 | Avg: 1.526 
2023-03-02 02:00:05,876 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.683 | Generator Loss: 0.849 | Avg: 1.532 
2023-03-02 02:00:05,903 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.693 | Generator Loss: 0.849 | Avg: 1.542 
2023-03-02 02:00:05,928 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.703 | Generator Loss: 0.849 | Avg: 1.551 
2023-03-02 02:00:05,954 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.707 | Generator Loss: 0.849 | Avg: 1.556 
2023-03-02 02:00:05,980 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.713 | Generator Loss: 0.849 | Avg: 1.562 
2023-03-02 02:00:06,007 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.714 | Generator Loss: 0.849 | Avg: 1.562 
2023-03-02 02:00:06,033 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.710 | Generator Loss: 0.849 | Avg: 1.559 
2023-03-02 02:00:06,058 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.706 | Generator Loss: 0.849 | Avg: 1.555 
2023-03-02 02:00:06,084 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.703 | Generator Loss: 0.849 | Avg: 1.552 
2023-03-02 02:00:06,118 -                train: [    INFO] - Best Loss 0.795 to 0.774
2023-03-02 02:00:06,119 -                train: [    INFO] - 
Epoch: 3/20
2023-03-02 02:00:06,298 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.1749 (0.1572) Acc D Real: 92.943% 
Loss D Fake: 0.5605 (0.5623) Acc D Fake: 83.333% 
Loss D: 0.735 
Loss G: 0.8588 (0.8564) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,305 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.1694 (0.1613) Acc D Real: 92.448% 
Loss D Fake: 0.5580 (0.5609) Acc D Fake: 83.333% 
Loss D: 0.727 
Loss G: 0.8598 (0.8576) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,313 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.1344 (0.1545) Acc D Real: 93.477% 
Loss D Fake: 0.5624 (0.5613) Acc D Fake: 83.333% 
Loss D: 0.697 
Loss G: 0.8693 (0.8605) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,331 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.1761 (0.1589) Acc D Real: 93.125% 
Loss D Fake: 0.5491 (0.5588) Acc D Fake: 83.333% 
Loss D: 0.725 
Loss G: 0.8760 (0.8636) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,339 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.1277 (0.1537) Acc D Real: 93.420% 
Loss D Fake: 0.5456 (0.5566) Acc D Fake: 83.333% 
Loss D: 0.673 
Loss G: 0.8811 (0.8665) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,346 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.0937 (0.1451) Acc D Real: 93.981% 
Loss D Fake: 0.5420 (0.5545) Acc D Fake: 83.333% 
Loss D: 0.636 
Loss G: 0.8869 (0.8694) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,353 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.1576 (0.1467) Acc D Real: 93.750% 
Loss D Fake: 0.5380 (0.5525) Acc D Fake: 83.333% 
Loss D: 0.696 
Loss G: 0.8930 (0.8724) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,360 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.1342 (0.1453) Acc D Real: 93.848% 
Loss D Fake: 0.5339 (0.5504) Acc D Fake: 83.333% 
Loss D: 0.668 
Loss G: 0.8997 (0.8754) Acc G: 16.667% 
LR: 2.000e-04 

2023-03-02 02:00:06,368 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.1715 (0.1479) Acc D Real: 93.484% 
Loss D Fake: 0.5294 (0.5483) Acc D Fake: 83.432% 
Loss D: 0.701 
Loss G: 0.9069 (0.8786) Acc G: 16.500% 
LR: 2.000e-04 

2023-03-02 02:00:06,375 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.1672 (0.1496) Acc D Real: 93.305% 
Loss D Fake: 0.5249 (0.5462) Acc D Fake: 83.575% 
Loss D: 0.692 
Loss G: 0.9141 (0.8818) Acc G: 16.364% 
LR: 2.000e-04 

2023-03-02 02:00:06,382 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.1648 (0.1509) Acc D Real: 93.134% 
Loss D Fake: 0.5202 (0.5440) Acc D Fake: 83.832% 
Loss D: 0.685 
Loss G: 0.9214 (0.8851) Acc G: 16.111% 
LR: 2.000e-04 

2023-03-02 02:00:06,391 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.1502 (0.1509) Acc D Real: 93.169% 
Loss D Fake: 0.5231 (0.5424) Acc D Fake: 84.050% 
Loss D: 0.673 
Loss G: 0.9265 (0.8883) Acc G: 15.897% 
LR: 2.000e-04 

2023-03-02 02:00:06,398 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.1291 (0.1493) Acc D Real: 93.300% 
Loss D Fake: 0.5120 (0.5402) Acc D Fake: 84.237% 
Loss D: 0.641 
Loss G: 0.9356 (0.8917) Acc G: 15.714% 
LR: 2.000e-04 

2023-03-02 02:00:06,405 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.1350 (0.1483) Acc D Real: 93.420% 
Loss D Fake: 0.5075 (0.5381) Acc D Fake: 84.399% 
Loss D: 0.642 
Loss G: 0.9431 (0.8951) Acc G: 15.556% 
LR: 2.000e-04 

2023-03-02 02:00:06,411 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.1531 (0.1486) Acc D Real: 93.398% 
Loss D Fake: 0.5030 (0.5359) Acc D Fake: 84.541% 
Loss D: 0.656 
Loss G: 0.9509 (0.8986) Acc G: 15.417% 
LR: 2.000e-04 

2023-03-02 02:00:06,420 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.0955 (0.1455) Acc D Real: 93.627% 
Loss D Fake: 0.4982 (0.5336) Acc D Fake: 84.666% 
Loss D: 0.594 
Loss G: 0.9600 (0.9022) Acc G: 15.294% 
LR: 2.000e-04 

2023-03-02 02:00:06,429 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.1867 (0.1478) Acc D Real: 93.472% 
Loss D Fake: 0.4928 (0.5314) Acc D Fake: 84.777% 
Loss D: 0.679 
Loss G: 0.9697 (0.9059) Acc G: 15.185% 
LR: 2.000e-04 

2023-03-02 02:00:06,438 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.1321 (0.1470) Acc D Real: 93.503% 
Loss D Fake: 0.4876 (0.5291) Acc D Fake: 84.877% 
Loss D: 0.620 
Loss G: 0.9768 (0.9097) Acc G: 15.088% 
LR: 2.000e-04 

2023-03-02 02:00:06,446 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.1046 (0.1449) Acc D Real: 93.682% 
Loss D Fake: 0.4915 (0.5272) Acc D Fake: 84.966% 
Loss D: 0.596 
Loss G: 0.9867 (0.9135) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:06,455 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.1088 (0.1431) Acc D Real: 93.812% 
Loss D Fake: 0.4808 (0.5250) Acc D Fake: 85.047% 
Loss D: 0.590 
Loss G: 0.9875 (0.9170) Acc G: 14.841% 
LR: 2.000e-04 

2023-03-02 02:00:06,464 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2042 (0.1459) Acc D Real: 93.570% 
Loss D Fake: 0.4797 (0.5229) Acc D Fake: 85.196% 
Loss D: 0.684 
Loss G: 0.9892 (0.9203) Acc G: 14.754% 
LR: 2.000e-04 

2023-03-02 02:00:06,472 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.1619 (0.1466) Acc D Real: 93.512% 
Loss D Fake: 0.4790 (0.5210) Acc D Fake: 85.260% 
Loss D: 0.641 
Loss G: 0.9922 (0.9235) Acc G: 14.692% 
LR: 2.000e-04 

2023-03-02 02:00:06,479 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.1639 (0.1473) Acc D Real: 93.468% 
Loss D Fake: 0.4773 (0.5192) Acc D Fake: 85.319% 
Loss D: 0.641 
Loss G: 0.9965 (0.9265) Acc G: 14.635% 
LR: 2.000e-04 

2023-03-02 02:00:06,486 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.1507 (0.1475) Acc D Real: 93.467% 
Loss D Fake: 0.4751 (0.5174) Acc D Fake: 85.373% 
Loss D: 0.626 
Loss G: 1.0015 (0.9295) Acc G: 14.583% 
LR: 2.000e-04 

2023-03-02 02:00:06,492 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.1821 (0.1488) Acc D Real: 93.299% 
Loss D Fake: 0.4726 (0.5157) Acc D Fake: 85.423% 
Loss D: 0.655 
Loss G: 1.0079 (0.9325) Acc G: 14.535% 
LR: 2.000e-04 

2023-03-02 02:00:06,499 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.1235 (0.1479) Acc D Real: 93.370% 
Loss D Fake: 0.4693 (0.5140) Acc D Fake: 85.469% 
Loss D: 0.593 
Loss G: 1.0164 (0.9356) Acc G: 14.491% 
LR: 2.000e-04 

2023-03-02 02:00:06,506 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.1375 (0.1475) Acc D Real: 93.441% 
Loss D Fake: 0.4650 (0.5122) Acc D Fake: 85.512% 
Loss D: 0.602 
Loss G: 1.0266 (0.9389) Acc G: 14.449% 
LR: 2.000e-04 

2023-03-02 02:00:06,514 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.1275 (0.1468) Acc D Real: 93.502% 
Loss D Fake: 0.4600 (0.5104) Acc D Fake: 85.506% 
Loss D: 0.587 
Loss G: 1.0383 (0.9423) Acc G: 14.468% 
LR: 2.000e-04 

2023-03-02 02:00:06,521 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.1227 (0.1460) Acc D Real: 93.566% 
Loss D Fake: 0.4541 (0.5086) Acc D Fake: 85.490% 
Loss D: 0.577 
Loss G: 1.0528 (0.9460) Acc G: 14.486% 
LR: 2.000e-04 

2023-03-02 02:00:06,528 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.1193 (0.1451) Acc D Real: 93.617% 
Loss D Fake: 0.4466 (0.5066) Acc D Fake: 85.474% 
Loss D: 0.566 
Loss G: 1.0717 (0.9500) Acc G: 14.503% 
LR: 2.000e-04 

2023-03-02 02:00:06,536 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.1501 (0.1453) Acc D Real: 93.626% 
Loss D Fake: 0.4367 (0.5044) Acc D Fake: 85.459% 
Loss D: 0.587 
Loss G: 1.0976 (0.9546) Acc G: 14.518% 
LR: 2.000e-04 

2023-03-02 02:00:06,543 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.1221 (0.1446) Acc D Real: 93.662% 
Loss D Fake: 1.1215 (0.5231) Acc D Fake: 84.877% 
Loss D: 1.244 
Loss G: 1.0638 (0.9580) Acc G: 14.482% 
LR: 2.000e-04 

2023-03-02 02:00:06,551 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1422 (0.1445) Acc D Real: 93.650% 
Loss D Fake: 0.4502 (0.5209) Acc D Fake: 84.979% 
Loss D: 0.592 
Loss G: 1.0373 (0.9603) Acc G: 14.350% 
LR: 2.000e-04 

2023-03-02 02:00:06,559 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.1784 (0.1455) Acc D Real: 93.551% 
Loss D Fake: 0.4567 (0.5191) Acc D Fake: 85.170% 
Loss D: 0.635 
Loss G: 1.0241 (0.9621) Acc G: 14.179% 
LR: 2.000e-04 

2023-03-02 02:00:06,567 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2343 (0.1480) Acc D Real: 93.284% 
Loss D Fake: 0.4661 (0.5176) Acc D Fake: 85.396% 
Loss D: 0.700 
Loss G: 0.9842 (0.9627) Acc G: 13.970% 
LR: 2.000e-04 

2023-03-02 02:00:06,574 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3174 (0.1525) Acc D Real: 92.815% 
Loss D Fake: 0.5065 (0.5173) Acc D Fake: 85.611% 
Loss D: 0.824 
Loss G: 0.2593 (0.9437) Acc G: 16.025% 
LR: 2.000e-04 

2023-03-02 02:00:06,582 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2674 (0.1556) Acc D Real: 92.453% 
Loss D Fake: 2.7204 (0.5753) Acc D Fake: 83.490% 
Loss D: 2.988 
Loss G: 0.1988 (0.9241) Acc G: 18.147% 
LR: 2.000e-04 

2023-03-02 02:00:06,589 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2848 (0.1589) Acc D Real: 92.053% 
Loss D Fake: 2.8294 (0.6331) Acc D Fake: 81.392% 
Loss D: 3.114 
Loss G: 0.1835 (0.9051) Acc G: 20.203% 
LR: 2.000e-04 

2023-03-02 02:00:06,597 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.1891 (0.1596) Acc D Real: 91.885% 
Loss D Fake: 2.8489 (0.6885) Acc D Fake: 79.440% 
Loss D: 3.038 
Loss G: 0.1771 (0.8869) Acc G: 22.031% 
LR: 2.000e-04 

2023-03-02 02:00:06,605 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.2040 (0.1607) Acc D Real: 91.681% 
Loss D Fake: 2.8332 (0.7408) Acc D Fake: 77.665% 
Loss D: 3.037 
Loss G: 0.1746 (0.8695) Acc G: 23.707% 
LR: 2.000e-04 

2023-03-02 02:00:06,613 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.1792 (0.1611) Acc D Real: 91.549% 
Loss D Fake: 2.7994 (0.7898) Acc D Fake: 76.084% 
Loss D: 2.979 
Loss G: 0.1741 (0.8530) Acc G: 25.246% 
LR: 2.000e-04 

2023-03-02 02:00:06,621 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2735 (0.1638) Acc D Real: 91.223% 
Loss D Fake: 2.7554 (0.8355) Acc D Fake: 74.586% 
Loss D: 3.029 
Loss G: 0.1749 (0.8372) Acc G: 26.713% 
LR: 2.000e-04 

2023-03-02 02:00:06,628 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.1976 (0.1645) Acc D Real: 91.098% 
Loss D Fake: 2.7053 (0.8780) Acc D Fake: 73.156% 
Loss D: 2.903 
Loss G: 0.1766 (0.8222) Acc G: 28.113% 
LR: 2.000e-04 

2023-03-02 02:00:06,636 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.1648 (0.1645) Acc D Real: 91.032% 
Loss D Fake: 2.6517 (0.9174) Acc D Fake: 71.789% 
Loss D: 2.816 
Loss G: 0.1790 (0.8079) Acc G: 29.451% 
LR: 2.000e-04 

2023-03-02 02:00:06,643 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.1978 (0.1653) Acc D Real: 90.879% 
Loss D Fake: 2.5961 (0.9539) Acc D Fake: 70.482% 
Loss D: 2.794 
Loss G: 0.1820 (0.7943) Acc G: 30.696% 
LR: 2.000e-04 

2023-03-02 02:00:06,651 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.1579 (0.1651) Acc D Real: 90.812% 
Loss D Fake: 2.5395 (0.9877) Acc D Fake: 69.266% 
Loss D: 2.697 
Loss G: 0.1855 (0.7814) Acc G: 31.887% 
LR: 2.000e-04 

2023-03-02 02:00:06,659 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.1932 (0.1657) Acc D Real: 90.710% 
Loss D Fake: 2.4833 (1.0188) Acc D Fake: 68.101% 
Loss D: 2.677 
Loss G: 0.1888 (0.7690) Acc G: 33.028% 
LR: 2.000e-04 

2023-03-02 02:00:06,666 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.1988 (0.1664) Acc D Real: 90.553% 
Loss D Fake: 2.4294 (1.0476) Acc D Fake: 66.983% 
Loss D: 2.628 
Loss G: 0.1924 (0.7572) Acc G: 34.123% 
LR: 2.000e-04 

2023-03-02 02:00:06,674 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2162 (0.1674) Acc D Real: 90.379% 
Loss D Fake: 2.3754 (1.0742) Acc D Fake: 65.910% 
Loss D: 2.592 
Loss G: 0.1964 (0.7460) Acc G: 35.174% 
LR: 2.000e-04 

2023-03-02 02:00:06,681 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2506 (0.1690) Acc D Real: 90.170% 
Loss D Fake: 2.3219 (1.0986) Acc D Fake: 64.879% 
Loss D: 2.572 
Loss G: 0.2007 (0.7353) Acc G: 36.184% 
LR: 2.000e-04 

2023-03-02 02:00:06,689 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2104 (0.1698) Acc D Real: 90.034% 
Loss D Fake: 2.2695 (1.1211) Acc D Fake: 63.888% 
Loss D: 2.480 
Loss G: 0.2053 (0.7251) Acc G: 37.154% 
LR: 2.000e-04 

2023-03-02 02:00:06,696 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2380 (0.1711) Acc D Real: 89.832% 
Loss D Fake: 2.2177 (1.1418) Acc D Fake: 62.934% 
Loss D: 2.456 
Loss G: 0.2102 (0.7154) Acc G: 38.057% 
LR: 2.000e-04 

2023-03-02 02:00:06,704 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.1941 (0.1715) Acc D Real: 89.747% 
Loss D Fake: 2.1670 (1.1608) Acc D Fake: 62.047% 
Loss D: 2.361 
Loss G: 0.2153 (0.7062) Acc G: 38.927% 
LR: 2.000e-04 

2023-03-02 02:00:06,712 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2356 (0.1727) Acc D Real: 89.541% 
Loss D Fake: 2.1175 (1.1782) Acc D Fake: 61.191% 
Loss D: 2.353 
Loss G: 0.2206 (0.6973) Acc G: 39.764% 
LR: 2.000e-04 

2023-03-02 02:00:06,720 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2971 (0.1749) Acc D Real: 89.247% 
Loss D Fake: 2.0691 (1.1941) Acc D Fake: 60.366% 
Loss D: 2.366 
Loss G: 0.2261 (0.6889) Acc G: 40.572% 
LR: 2.000e-04 

2023-03-02 02:00:06,727 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2451 (0.1761) Acc D Real: 89.084% 
Loss D Fake: 2.0222 (1.2086) Acc D Fake: 59.571% 
Loss D: 2.267 
Loss G: 0.2316 (0.6809) Acc G: 41.351% 
LR: 2.000e-04 

2023-03-02 02:00:06,735 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2571 (0.1775) Acc D Real: 88.892% 
Loss D Fake: 1.9769 (1.2219) Acc D Fake: 58.802% 
Loss D: 2.234 
Loss G: 0.2372 (0.6732) Acc G: 42.104% 
LR: 2.000e-04 

2023-03-02 02:00:06,742 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3182 (0.1799) Acc D Real: 88.596% 
Loss D Fake: 1.9328 (1.2339) Acc D Fake: 58.060% 
Loss D: 2.251 
Loss G: 0.2429 (0.6660) Acc G: 42.831% 
LR: 2.000e-04 

2023-03-02 02:00:06,750 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2544 (0.1811) Acc D Real: 88.438% 
Loss D Fake: 1.8899 (1.2449) Acc D Fake: 57.342% 
Loss D: 2.144 
Loss G: 0.2488 (0.6590) Acc G: 43.506% 
LR: 2.000e-04 

2023-03-02 02:00:06,757 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2638 (0.1825) Acc D Real: 88.259% 
Loss D Fake: 1.8483 (1.2548) Acc D Fake: 56.675% 
Loss D: 2.112 
Loss G: 0.2549 (0.6524) Acc G: 44.159% 
LR: 2.000e-04 

2023-03-02 02:00:06,765 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2530 (0.1836) Acc D Real: 88.129% 
Loss D Fake: 1.8072 (1.2637) Acc D Fake: 56.030% 
Loss D: 2.060 
Loss G: 0.2613 (0.6461) Acc G: 44.791% 
LR: 2.000e-04 

2023-03-02 02:00:06,772 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2110 (0.1841) Acc D Real: 88.076% 
Loss D Fake: 1.7667 (1.2717) Acc D Fake: 55.405% 
Loss D: 1.978 
Loss G: 0.2680 (0.6401) Acc G: 45.403% 
LR: 2.000e-04 

2023-03-02 02:00:06,780 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2178 (0.1846) Acc D Real: 87.987% 
Loss D Fake: 1.7275 (1.2788) Acc D Fake: 54.800% 
Loss D: 1.945 
Loss G: 0.2747 (0.6344) Acc G: 45.995% 
LR: 2.000e-04 

2023-03-02 02:00:06,787 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2318 (0.1853) Acc D Real: 87.895% 
Loss D Fake: 1.6892 (1.2851) Acc D Fake: 54.213% 
Loss D: 1.921 
Loss G: 0.2817 (0.6289) Acc G: 46.570% 
LR: 2.000e-04 

2023-03-02 02:00:06,794 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2044 (0.1856) Acc D Real: 87.857% 
Loss D Fake: 1.6516 (1.2907) Acc D Fake: 53.644% 
Loss D: 1.856 
Loss G: 0.2889 (0.6238) Acc G: 47.127% 
LR: 2.000e-04 

2023-03-02 02:00:06,802 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2739 (0.1869) Acc D Real: 87.706% 
Loss D Fake: 1.6149 (1.2955) Acc D Fake: 53.092% 
Loss D: 1.889 
Loss G: 0.2962 (0.6189) Acc G: 47.642% 
LR: 2.000e-04 

2023-03-02 02:00:06,810 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2292 (0.1876) Acc D Real: 87.619% 
Loss D Fake: 1.5792 (1.2997) Acc D Fake: 52.581% 
Loss D: 1.808 
Loss G: 0.3037 (0.6143) Acc G: 48.143% 
LR: 2.000e-04 

2023-03-02 02:00:06,817 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.2762 (0.1888) Acc D Real: 87.483% 
Loss D Fake: 1.5444 (1.3032) Acc D Fake: 52.085% 
Loss D: 1.821 
Loss G: 0.3112 (0.6099) Acc G: 48.628% 
LR: 2.000e-04 

2023-03-02 02:00:06,824 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2755 (0.1901) Acc D Real: 87.360% 
Loss D Fake: 1.5111 (1.3062) Acc D Fake: 51.603% 
Loss D: 1.787 
Loss G: 0.3187 (0.6057) Acc G: 49.100% 
LR: 2.000e-04 

2023-03-02 02:00:06,832 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.2855 (0.1914) Acc D Real: 87.227% 
Loss D Fake: 1.4787 (1.3086) Acc D Fake: 51.134% 
Loss D: 1.764 
Loss G: 0.3262 (0.6018) Acc G: 49.559% 
LR: 2.000e-04 

2023-03-02 02:00:06,839 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1952 (0.1915) Acc D Real: 87.228% 
Loss D Fake: 1.4473 (1.3105) Acc D Fake: 50.679% 
Loss D: 1.642 
Loss G: 0.3341 (0.5981) Acc G: 50.005% 
LR: 2.000e-04 

2023-03-02 02:00:06,847 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.2663 (0.1925) Acc D Real: 87.088% 
Loss D Fake: 1.4162 (1.3120) Acc D Fake: 50.235% 
Loss D: 1.683 
Loss G: 0.3422 (0.5945) Acc G: 50.439% 
LR: 2.000e-04 

2023-03-02 02:00:06,854 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3016 (0.1940) Acc D Real: 86.954% 
Loss D Fake: 1.3857 (1.3130) Acc D Fake: 49.804% 
Loss D: 1.687 
Loss G: 0.3503 (0.5912) Acc G: 50.861% 
LR: 2.000e-04 

2023-03-02 02:00:06,862 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.2776 (0.1951) Acc D Real: 86.867% 
Loss D Fake: 1.3565 (1.3136) Acc D Fake: 49.385% 
Loss D: 1.634 
Loss G: 0.3584 (0.5881) Acc G: 51.249% 
LR: 2.000e-04 

2023-03-02 02:00:06,869 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3445 (0.1971) Acc D Real: 86.673% 
Loss D Fake: 1.3281 (1.3138) Acc D Fake: 48.998% 
Loss D: 1.673 
Loss G: 0.3667 (0.5852) Acc G: 51.628% 
LR: 2.000e-04 

2023-03-02 02:00:06,876 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.1991 (0.1971) Acc D Real: 86.677% 
Loss D Fake: 1.3004 (1.3136) Acc D Fake: 48.621% 
Loss D: 1.500 
Loss G: 0.3751 (0.5825) Acc G: 51.996% 
LR: 2.000e-04 

2023-03-02 02:00:06,884 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2463 (0.1977) Acc D Real: 86.649% 
Loss D Fake: 1.2734 (1.3131) Acc D Fake: 48.255% 
Loss D: 1.520 
Loss G: 0.3835 (0.5799) Acc G: 52.355% 
LR: 2.000e-04 

2023-03-02 02:00:06,891 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3107 (0.1991) Acc D Real: 86.508% 
Loss D Fake: 1.2475 (1.3122) Acc D Fake: 47.897% 
Loss D: 1.558 
Loss G: 0.3921 (0.5776) Acc G: 52.705% 
LR: 2.000e-04 

2023-03-02 02:00:06,898 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.2751 (0.2001) Acc D Real: 86.384% 
Loss D Fake: 1.2220 (1.3111) Acc D Fake: 47.548% 
Loss D: 1.497 
Loss G: 0.4010 (0.5754) Acc G: 53.046% 
LR: 2.000e-04 

2023-03-02 02:00:06,907 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2744 (0.2010) Acc D Real: 86.320% 
Loss D Fake: 1.1967 (1.3097) Acc D Fake: 47.208% 
Loss D: 1.471 
Loss G: 0.4099 (0.5733) Acc G: 53.379% 
LR: 2.000e-04 

2023-03-02 02:00:06,915 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2372 (0.2014) Acc D Real: 86.317% 
Loss D Fake: 1.1723 (1.3080) Acc D Fake: 46.876% 
Loss D: 1.410 
Loss G: 0.4189 (0.5714) Acc G: 53.704% 
LR: 2.000e-04 

2023-03-02 02:00:06,923 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2868 (0.2025) Acc D Real: 86.195% 
Loss D Fake: 1.1488 (1.3061) Acc D Fake: 46.552% 
Loss D: 1.436 
Loss G: 0.4279 (0.5697) Acc G: 54.020% 
LR: 2.000e-04 

2023-03-02 02:00:06,932 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2744 (0.2033) Acc D Real: 86.127% 
Loss D Fake: 1.1259 (1.3040) Acc D Fake: 46.236% 
Loss D: 1.400 
Loss G: 0.4369 (0.5681) Acc G: 54.310% 
LR: 2.000e-04 

2023-03-02 02:00:06,940 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3221 (0.2047) Acc D Real: 86.010% 
Loss D Fake: 1.1028 (1.3016) Acc D Fake: 45.947% 
Loss D: 1.425 
Loss G: 0.4476 (0.5667) Acc G: 54.593% 
LR: 2.000e-04 

2023-03-02 02:00:06,948 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3100 (0.2060) Acc D Real: 85.919% 
Loss D Fake: 1.0783 (1.2990) Acc D Fake: 45.665% 
Loss D: 1.388 
Loss G: 0.4585 (0.5654) Acc G: 54.869% 
LR: 2.000e-04 

2023-03-02 02:00:06,956 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3149 (0.2072) Acc D Real: 85.817% 
Loss D Fake: 1.0554 (1.2962) Acc D Fake: 45.389% 
Loss D: 1.370 
Loss G: 0.4690 (0.5643) Acc G: 55.138% 
LR: 2.000e-04 

2023-03-02 02:00:06,964 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3297 (0.2086) Acc D Real: 85.669% 
Loss D Fake: 1.0340 (1.2932) Acc D Fake: 45.120% 
Loss D: 1.364 
Loss G: 0.4788 (0.5634) Acc G: 55.402% 
LR: 2.000e-04 

2023-03-02 02:00:06,972 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.1511 (0.2080) Acc D Real: 85.724% 
Loss D Fake: 1.0137 (1.2901) Acc D Fake: 44.856% 
Loss D: 1.165 
Loss G: 0.4888 (0.5625) Acc G: 55.641% 
LR: 2.000e-04 

2023-03-02 02:00:06,980 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2852 (0.2088) Acc D Real: 85.654% 
Loss D Fake: 0.9935 (1.2868) Acc D Fake: 44.617% 
Loss D: 1.279 
Loss G: 0.4987 (0.5618) Acc G: 55.874% 
LR: 2.000e-04 

2023-03-02 02:00:06,987 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3566 (0.2104) Acc D Real: 85.568% 
Loss D Fake: 0.9746 (1.2833) Acc D Fake: 44.383% 
Loss D: 1.331 
Loss G: 0.5082 (0.5612) Acc G: 56.103% 
LR: 2.000e-04 

2023-03-02 02:00:06,994 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3115 (0.2115) Acc D Real: 85.515% 
Loss D Fake: 0.9571 (1.2798) Acc D Fake: 44.154% 
Loss D: 1.269 
Loss G: 0.5173 (0.5608) Acc G: 56.326% 
LR: 2.000e-04 

2023-03-02 02:00:07,002 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4159 (0.2137) Acc D Real: 85.335% 
Loss D Fake: 0.9406 (1.2762) Acc D Fake: 43.930% 
Loss D: 1.357 
Loss G: 0.5261 (0.5604) Acc G: 56.545% 
LR: 2.000e-04 

2023-03-02 02:00:07,009 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2964 (0.2146) Acc D Real: 85.289% 
Loss D Fake: 0.9250 (1.2724) Acc D Fake: 43.711% 
Loss D: 1.221 
Loss G: 0.5349 (0.5601) Acc G: 56.759% 
LR: 2.000e-04 

2023-03-02 02:00:07,016 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3239 (0.2158) Acc D Real: 85.175% 
Loss D Fake: 0.9098 (1.2686) Acc D Fake: 43.497% 
Loss D: 1.234 
Loss G: 0.5438 (0.5599) Acc G: 56.969% 
LR: 2.000e-04 

2023-03-02 02:00:07,024 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2454 (0.2161) Acc D Real: 85.116% 
Loss D Fake: 0.8944 (1.2647) Acc D Fake: 43.287% 
Loss D: 1.140 
Loss G: 0.5531 (0.5599) Acc G: 57.174% 
LR: 2.000e-04 

2023-03-02 02:00:07,031 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3171 (0.2171) Acc D Real: 85.056% 
Loss D Fake: 0.8791 (1.2607) Acc D Fake: 43.081% 
Loss D: 1.196 
Loss G: 0.5625 (0.5599) Acc G: 57.375% 
LR: 2.000e-04 

2023-03-02 02:00:07,039 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3879 (0.2189) Acc D Real: 84.914% 
Loss D Fake: 0.8645 (1.2567) Acc D Fake: 42.879% 
Loss D: 1.252 
Loss G: 0.5717 (0.5600) Acc G: 57.572% 
LR: 2.000e-04 

2023-03-02 02:00:07,046 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2580 (0.2193) Acc D Real: 84.958% 
Loss D Fake: 0.8506 (1.2526) Acc D Fake: 42.682% 
Loss D: 1.109 
Loss G: 0.5806 (0.5602) Acc G: 57.748% 
LR: 2.000e-04 

2023-03-02 02:00:07,054 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3362 (0.2204) Acc D Real: 84.855% 
Loss D Fake: 0.8374 (1.2484) Acc D Fake: 42.505% 
Loss D: 1.174 
Loss G: 0.5895 (0.5605) Acc G: 57.920% 
LR: 2.000e-04 

2023-03-02 02:00:07,061 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3231 (0.2214) Acc D Real: 84.804% 
Loss D Fake: 0.8245 (1.2442) Acc D Fake: 42.332% 
Loss D: 1.148 
Loss G: 0.5983 (0.5609) Acc G: 58.089% 
LR: 2.000e-04 

2023-03-02 02:00:07,070 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3376 (0.2226) Acc D Real: 84.727% 
Loss D Fake: 0.8121 (1.2400) Acc D Fake: 42.162% 
Loss D: 1.150 
Loss G: 0.6071 (0.5613) Acc G: 58.255% 
LR: 2.000e-04 

2023-03-02 02:00:07,078 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2896 (0.2232) Acc D Real: 84.713% 
Loss D Fake: 0.8000 (1.2357) Acc D Fake: 41.995% 
Loss D: 1.090 
Loss G: 0.6159 (0.5619) Acc G: 58.418% 
LR: 2.000e-04 

2023-03-02 02:00:07,086 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3729 (0.2247) Acc D Real: 84.672% 
Loss D Fake: 0.7885 (1.2314) Acc D Fake: 41.832% 
Loss D: 1.161 
Loss G: 0.6240 (0.5625) Acc G: 58.577% 
LR: 2.000e-04 

2023-03-02 02:00:07,093 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3221 (0.2256) Acc D Real: 84.609% 
Loss D Fake: 0.7780 (1.2271) Acc D Fake: 41.672% 
Loss D: 1.100 
Loss G: 0.6321 (0.5631) Acc G: 58.734% 
LR: 2.000e-04 

2023-03-02 02:00:07,101 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3695 (0.2270) Acc D Real: 84.521% 
Loss D Fake: 0.7677 (1.2228) Acc D Fake: 41.514% 
Loss D: 1.137 
Loss G: 0.6401 (0.5639) Acc G: 58.887% 
LR: 2.000e-04 

2023-03-02 02:00:07,108 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3300 (0.2279) Acc D Real: 84.470% 
Loss D Fake: 0.7578 (1.2184) Acc D Fake: 41.360% 
Loss D: 1.088 
Loss G: 0.6479 (0.5646) Acc G: 59.038% 
LR: 2.000e-04 

2023-03-02 02:00:07,116 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4350 (0.2298) Acc D Real: 84.337% 
Loss D Fake: 0.7484 (1.2141) Acc D Fake: 41.209% 
Loss D: 1.183 
Loss G: 0.6553 (0.5655) Acc G: 59.185% 
LR: 2.000e-04 

2023-03-02 02:00:07,124 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3527 (0.2310) Acc D Real: 84.307% 
Loss D Fake: 0.7398 (1.2097) Acc D Fake: 41.060% 
Loss D: 1.092 
Loss G: 0.6622 (0.5664) Acc G: 59.331% 
LR: 2.000e-04 

2023-03-02 02:00:07,132 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3654 (0.2322) Acc D Real: 84.229% 
Loss D Fake: 0.7316 (1.2054) Acc D Fake: 40.914% 
Loss D: 1.097 
Loss G: 0.6692 (0.5673) Acc G: 59.473% 
LR: 2.000e-04 

2023-03-02 02:00:07,139 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1548 (0.2315) Acc D Real: 84.253% 
Loss D Fake: 0.7231 (1.2010) Acc D Fake: 40.770% 
Loss D: 0.878 
Loss G: 0.6771 (0.5683) Acc G: 59.613% 
LR: 2.000e-04 

2023-03-02 02:00:07,148 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2420 (0.2316) Acc D Real: 84.248% 
Loss D Fake: 0.7137 (1.1967) Acc D Fake: 40.630% 
Loss D: 0.956 
Loss G: 0.6856 (0.5693) Acc G: 59.735% 
LR: 2.000e-04 

2023-03-02 02:00:07,155 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2587 (0.2318) Acc D Real: 84.268% 
Loss D Fake: 0.7042 (1.1923) Acc D Fake: 40.506% 
Loss D: 0.963 
Loss G: 0.6945 (0.5705) Acc G: 59.856% 
LR: 2.000e-04 

2023-03-02 02:00:07,162 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3416 (0.2328) Acc D Real: 84.199% 
Loss D Fake: 0.6946 (1.1880) Acc D Fake: 40.385% 
Loss D: 1.036 
Loss G: 0.7033 (0.5716) Acc G: 59.974% 
LR: 2.000e-04 

2023-03-02 02:00:07,170 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3981 (0.2342) Acc D Real: 84.131% 
Loss D Fake: 0.6856 (1.1836) Acc D Fake: 40.265% 
Loss D: 1.084 
Loss G: 0.7114 (0.5728) Acc G: 60.090% 
LR: 2.000e-04 

2023-03-02 02:00:07,177 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2721 (0.2345) Acc D Real: 84.125% 
Loss D Fake: 0.6773 (1.1792) Acc D Fake: 40.148% 
Loss D: 0.949 
Loss G: 0.7196 (0.5741) Acc G: 60.204% 
LR: 2.000e-04 

2023-03-02 02:00:07,185 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3410 (0.2355) Acc D Real: 84.019% 
Loss D Fake: 0.6691 (1.1749) Acc D Fake: 40.432% 
Loss D: 1.010 
Loss G: 0.7276 (0.5754) Acc G: 59.875% 
LR: 2.000e-04 

2023-03-02 02:00:07,192 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3368 (0.2363) Acc D Real: 83.896% 
Loss D Fake: 0.6612 (1.1705) Acc D Fake: 40.771% 
Loss D: 0.998 
Loss G: 0.7356 (0.5768) Acc G: 59.509% 
LR: 2.000e-04 

2023-03-02 02:00:07,200 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3659 (0.2374) Acc D Real: 83.731% 
Loss D Fake: 0.6535 (1.1662) Acc D Fake: 41.129% 
Loss D: 1.019 
Loss G: 0.7435 (0.5782) Acc G: 59.121% 
LR: 2.000e-04 

2023-03-02 02:00:07,207 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2614 (0.2376) Acc D Real: 83.654% 
Loss D Fake: 0.6459 (1.1618) Acc D Fake: 41.508% 
Loss D: 0.907 
Loss G: 0.7514 (0.5796) Acc G: 58.725% 
LR: 2.000e-04 

2023-03-02 02:00:07,214 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4104 (0.2390) Acc D Real: 83.434% 
Loss D Fake: 0.6387 (1.1575) Acc D Fake: 41.895% 
Loss D: 1.049 
Loss G: 0.7589 (0.5811) Acc G: 58.323% 
LR: 2.000e-04 

2023-03-02 02:00:07,222 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2625 (0.2392) Acc D Real: 83.354% 
Loss D Fake: 0.6319 (1.1532) Acc D Fake: 42.290% 
Loss D: 0.894 
Loss G: 0.7664 (0.5826) Acc G: 57.926% 
LR: 2.000e-04 

2023-03-02 02:00:07,229 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3588 (0.2402) Acc D Real: 83.178% 
Loss D Fake: 0.6252 (1.1489) Acc D Fake: 42.677% 
Loss D: 0.984 
Loss G: 0.7735 (0.5842) Acc G: 57.523% 
LR: 2.000e-04 

2023-03-02 02:00:07,237 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3962 (0.2415) Acc D Real: 82.977% 
Loss D Fake: 0.6191 (1.1446) Acc D Fake: 43.072% 
Loss D: 1.015 
Loss G: 0.7802 (0.5857) Acc G: 57.127% 
LR: 2.000e-04 

2023-03-02 02:00:07,244 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2482 (0.2415) Acc D Real: 82.917% 
Loss D Fake: 0.6132 (1.1404) Acc D Fake: 43.462% 
Loss D: 0.861 
Loss G: 0.7871 (0.5874) Acc G: 56.710% 
LR: 2.000e-04 

2023-03-02 02:00:07,252 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4200 (0.2429) Acc D Real: 82.699% 
Loss D Fake: 0.6073 (1.1361) Acc D Fake: 43.872% 
Loss D: 1.027 
Loss G: 0.7936 (0.5890) Acc G: 56.299% 
LR: 2.000e-04 

2023-03-02 02:00:07,259 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4535 (0.2446) Acc D Real: 82.450% 
Loss D Fake: 0.6021 (1.1319) Acc D Fake: 44.274% 
Loss D: 1.056 
Loss G: 0.7995 (0.5907) Acc G: 55.882% 
LR: 2.000e-04 

2023-03-02 02:00:07,267 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4049 (0.2458) Acc D Real: 82.249% 
Loss D Fake: 0.5972 (1.1278) Acc D Fake: 44.683% 
Loss D: 1.002 
Loss G: 0.8054 (0.5923) Acc G: 55.472% 
LR: 2.000e-04 

2023-03-02 02:00:07,274 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4422 (0.2474) Acc D Real: 82.001% 
Loss D Fake: 0.5925 (1.1236) Acc D Fake: 45.099% 
Loss D: 1.035 
Loss G: 0.8106 (0.5940) Acc G: 55.042% 
LR: 2.000e-04 

2023-03-02 02:00:07,281 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3291 (0.2480) Acc D Real: 81.887% 
Loss D Fake: 0.5884 (1.1195) Acc D Fake: 45.522% 
Loss D: 0.917 
Loss G: 0.8159 (0.5957) Acc G: 54.618% 
LR: 2.000e-04 

2023-03-02 02:00:07,289 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3310 (0.2486) Acc D Real: 81.778% 
Loss D Fake: 0.5837 (1.1154) Acc D Fake: 45.938% 
Loss D: 0.915 
Loss G: 0.8223 (0.5975) Acc G: 54.201% 
LR: 2.000e-04 

2023-03-02 02:00:07,296 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2730 (0.2488) Acc D Real: 81.716% 
Loss D Fake: 0.5783 (1.1113) Acc D Fake: 46.347% 
Loss D: 0.851 
Loss G: 0.8294 (0.5992) Acc G: 53.791% 
LR: 2.000e-04 

2023-03-02 02:00:07,304 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3766 (0.2498) Acc D Real: 81.559% 
Loss D Fake: 0.5727 (1.1073) Acc D Fake: 46.750% 
Loss D: 0.949 
Loss G: 0.8366 (0.6010) Acc G: 53.386% 
LR: 2.000e-04 

2023-03-02 02:00:07,311 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2798 (0.2500) Acc D Real: 81.497% 
Loss D Fake: 0.5670 (1.1033) Acc D Fake: 47.148% 
Loss D: 0.847 
Loss G: 0.8442 (0.6028) Acc G: 52.988% 
LR: 2.000e-04 

2023-03-02 02:00:07,319 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4228 (0.2513) Acc D Real: 81.304% 
Loss D Fake: 0.5613 (1.0992) Acc D Fake: 47.539% 
Loss D: 0.984 
Loss G: 0.8516 (0.6047) Acc G: 52.595% 
LR: 2.000e-04 

2023-03-02 02:00:07,326 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3233 (0.2518) Acc D Real: 81.206% 
Loss D Fake: 0.5558 (1.0952) Acc D Fake: 47.925% 
Loss D: 0.879 
Loss G: 0.8590 (0.6065) Acc G: 52.209% 
LR: 2.000e-04 

2023-03-02 02:00:07,334 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4140 (0.2530) Acc D Real: 81.022% 
Loss D Fake: 0.5505 (1.0913) Acc D Fake: 48.305% 
Loss D: 0.965 
Loss G: 0.8658 (0.6084) Acc G: 51.827% 
LR: 2.000e-04 

2023-03-02 02:00:07,341 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3984 (0.2540) Acc D Real: 80.855% 
Loss D Fake: 0.5459 (1.0873) Acc D Fake: 48.680% 
Loss D: 0.944 
Loss G: 0.8720 (0.6103) Acc G: 51.452% 
LR: 2.000e-04 

2023-03-02 02:00:07,348 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3364 (0.2546) Acc D Real: 80.747% 
Loss D Fake: 0.5417 (1.0834) Acc D Fake: 49.049% 
Loss D: 0.878 
Loss G: 0.8777 (0.6123) Acc G: 51.082% 
LR: 2.000e-04 

2023-03-02 02:00:07,356 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.2802 (0.2548) Acc D Real: 80.689% 
Loss D Fake: 0.5376 (1.0795) Acc D Fake: 49.413% 
Loss D: 0.818 
Loss G: 0.8836 (0.6142) Acc G: 50.717% 
LR: 2.000e-04 

2023-03-02 02:00:07,363 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4281 (0.2560) Acc D Real: 80.512% 
Loss D Fake: 0.5336 (1.0756) Acc D Fake: 49.772% 
Loss D: 0.962 
Loss G: 0.8891 (0.6161) Acc G: 50.357% 
LR: 2.000e-04 

2023-03-02 02:00:07,371 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2920 (0.2563) Acc D Real: 80.442% 
Loss D Fake: 0.5300 (1.0718) Acc D Fake: 50.125% 
Loss D: 0.822 
Loss G: 0.8944 (0.6181) Acc G: 50.003% 
LR: 2.000e-04 

2023-03-02 02:00:07,378 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2860 (0.2565) Acc D Real: 80.387% 
Loss D Fake: 0.5263 (1.0680) Acc D Fake: 50.474% 
Loss D: 0.812 
Loss G: 0.9000 (0.6201) Acc G: 49.653% 
LR: 2.000e-04 

2023-03-02 02:00:07,385 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3036 (0.2568) Acc D Real: 80.315% 
Loss D Fake: 0.5224 (1.0642) Acc D Fake: 50.818% 
Loss D: 0.826 
Loss G: 0.9059 (0.6221) Acc G: 49.308% 
LR: 2.000e-04 

2023-03-02 02:00:07,393 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3947 (0.2578) Acc D Real: 80.175% 
Loss D Fake: 0.5187 (1.0604) Acc D Fake: 51.157% 
Loss D: 0.913 
Loss G: 0.9109 (0.6240) Acc G: 48.968% 
LR: 2.000e-04 

2023-03-02 02:00:07,400 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4129 (0.2588) Acc D Real: 80.021% 
Loss D Fake: 0.5157 (1.0567) Acc D Fake: 51.492% 
Loss D: 0.929 
Loss G: 0.9151 (0.6260) Acc G: 48.633% 
LR: 2.000e-04 

2023-03-02 02:00:07,408 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3468 (0.2594) Acc D Real: 79.918% 
Loss D Fake: 0.5132 (1.0530) Acc D Fake: 51.822% 
Loss D: 0.860 
Loss G: 0.9188 (0.6280) Acc G: 48.302% 
LR: 2.000e-04 

2023-03-02 02:00:07,415 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3449 (0.2600) Acc D Real: 79.819% 
Loss D Fake: 0.5109 (1.0493) Acc D Fake: 52.147% 
Loss D: 0.856 
Loss G: 0.9224 (0.6300) Acc G: 47.975% 
LR: 2.000e-04 

2023-03-02 02:00:07,422 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5126 (0.2617) Acc D Real: 79.598% 
Loss D Fake: 0.5088 (1.0457) Acc D Fake: 52.469% 
Loss D: 1.021 
Loss G: 0.9250 (0.6320) Acc G: 47.653% 
LR: 2.000e-04 

2023-03-02 02:00:07,430 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3372 (0.2622) Acc D Real: 79.518% 
Loss D Fake: 0.5072 (1.0421) Acc D Fake: 52.785% 
Loss D: 0.844 
Loss G: 0.9279 (0.6340) Acc G: 47.336% 
LR: 2.000e-04 

2023-03-02 02:00:07,437 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3631 (0.2629) Acc D Real: 79.418% 
Loss D Fake: 0.5053 (1.0386) Acc D Fake: 53.098% 
Loss D: 0.868 
Loss G: 0.9309 (0.6359) Acc G: 47.022% 
LR: 2.000e-04 

2023-03-02 02:00:07,444 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3715 (0.2636) Acc D Real: 79.311% 
Loss D Fake: 0.5034 (1.0350) Acc D Fake: 53.407% 
Loss D: 0.875 
Loss G: 0.9339 (0.6379) Acc G: 46.713% 
LR: 2.000e-04 

2023-03-02 02:00:07,452 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4474 (0.2648) Acc D Real: 79.151% 
Loss D Fake: 0.5017 (1.0315) Acc D Fake: 53.711% 
Loss D: 0.949 
Loss G: 0.9360 (0.6398) Acc G: 46.408% 
LR: 2.000e-04 

2023-03-02 02:00:07,459 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3771 (0.2655) Acc D Real: 79.044% 
Loss D Fake: 0.5006 (1.0281) Acc D Fake: 54.012% 
Loss D: 0.878 
Loss G: 0.9379 (0.6418) Acc G: 46.106% 
LR: 2.000e-04 

2023-03-02 02:00:07,467 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4584 (0.2668) Acc D Real: 78.887% 
Loss D Fake: 0.4995 (1.0247) Acc D Fake: 54.308% 
Loss D: 0.958 
Loss G: 0.9392 (0.6437) Acc G: 45.809% 
LR: 2.000e-04 

2023-03-02 02:00:07,474 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2972 (0.2670) Acc D Real: 78.840% 
Loss D Fake: 0.4988 (1.0213) Acc D Fake: 54.601% 
Loss D: 0.796 
Loss G: 0.9407 (0.6456) Acc G: 45.515% 
LR: 2.000e-04 

2023-03-02 02:00:07,482 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2013 (0.2666) Acc D Real: 78.862% 
Loss D Fake: 0.4975 (1.0180) Acc D Fake: 54.891% 
Loss D: 0.699 
Loss G: 0.9434 (0.6475) Acc G: 45.225% 
LR: 2.000e-04 

2023-03-02 02:00:07,489 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2441 (0.2664) Acc D Real: 78.862% 
Loss D Fake: 0.4954 (1.0147) Acc D Fake: 54.917% 
Loss D: 0.740 
Loss G: 0.9477 (0.6494) Acc G: 45.198% 
LR: 2.000e-04 

2023-03-02 02:00:07,693 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.550 | Generator Loss: 0.948 | Avg: 1.497 
2023-03-02 02:00:07,718 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.577 | Generator Loss: 0.948 | Avg: 1.524 
2023-03-02 02:00:07,743 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.571 | Generator Loss: 0.948 | Avg: 1.518 
2023-03-02 02:00:07,770 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.563 | Generator Loss: 0.948 | Avg: 1.511 
2023-03-02 02:00:07,797 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.562 | Generator Loss: 0.948 | Avg: 1.510 
2023-03-02 02:00:07,825 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.641 | Generator Loss: 0.948 | Avg: 1.589 
2023-03-02 02:00:07,854 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.639 | Generator Loss: 0.948 | Avg: 1.587 
2023-03-02 02:00:07,880 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.689 | Generator Loss: 0.948 | Avg: 1.637 
2023-03-02 02:00:07,908 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.700 | Generator Loss: 0.948 | Avg: 1.648 
2023-03-02 02:00:07,934 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.761 | Generator Loss: 0.948 | Avg: 1.709 
2023-03-02 02:00:07,961 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.804 | Generator Loss: 0.948 | Avg: 1.752 
2023-03-02 02:00:07,988 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.857 | Generator Loss: 0.948 | Avg: 1.805 
2023-03-02 02:00:08,014 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.884 | Generator Loss: 0.948 | Avg: 1.831 
2023-03-02 02:00:08,040 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.888 | Generator Loss: 0.948 | Avg: 1.836 
2023-03-02 02:00:08,066 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.868 | Generator Loss: 0.948 | Avg: 1.816 
2023-03-02 02:00:08,092 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.849 | Generator Loss: 0.948 | Avg: 1.797 
2023-03-02 02:00:08,118 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.833 | Generator Loss: 0.948 | Avg: 1.781 
2023-03-02 02:00:08,152 -                train: [    INFO] - 
Epoch: 4/20
2023-03-02 02:00:08,291 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3656 (0.2891) Acc D Real: 72.109% 
Loss D Fake: 0.4892 (0.4908) Acc D Fake: 100.000% 
Loss D: 0.855 
Loss G: 0.9580 (0.9555) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,307 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3455 (0.3079) Acc D Real: 70.347% 
Loss D Fake: 0.4864 (0.4894) Acc D Fake: 100.000% 
Loss D: 0.832 
Loss G: 0.9623 (0.9578) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,315 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3183 (0.3105) Acc D Real: 70.286% 
Loss D Fake: 0.4840 (0.4880) Acc D Fake: 100.000% 
Loss D: 0.802 
Loss G: 0.9663 (0.9599) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,329 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3827 (0.3249) Acc D Real: 68.781% 
Loss D Fake: 0.4817 (0.4868) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9703 (0.9620) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,336 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3079 (0.3221) Acc D Real: 69.123% 
Loss D Fake: 0.4793 (0.4855) Acc D Fake: 100.000% 
Loss D: 0.787 
Loss G: 0.9747 (0.9641) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,343 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2624 (0.3135) Acc D Real: 70.112% 
Loss D Fake: 0.4767 (0.4843) Acc D Fake: 100.000% 
Loss D: 0.739 
Loss G: 0.9794 (0.9663) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,351 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2829 (0.3097) Acc D Real: 70.612% 
Loss D Fake: 0.4740 (0.4830) Acc D Fake: 100.000% 
Loss D: 0.757 
Loss G: 0.9840 (0.9685) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,358 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4135 (0.3212) Acc D Real: 69.433% 
Loss D Fake: 0.4716 (0.4817) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 0.9878 (0.9706) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,365 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3646 (0.3256) Acc D Real: 69.052% 
Loss D Fake: 0.4697 (0.4805) Acc D Fake: 100.000% 
Loss D: 0.834 
Loss G: 0.9907 (0.9726) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,372 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3727 (0.3298) Acc D Real: 68.670% 
Loss D Fake: 0.4683 (0.4794) Acc D Fake: 100.000% 
Loss D: 0.841 
Loss G: 0.9931 (0.9745) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,379 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4063 (0.3362) Acc D Real: 68.077% 
Loss D Fake: 0.4672 (0.4784) Acc D Fake: 100.000% 
Loss D: 0.874 
Loss G: 0.9945 (0.9762) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,386 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3556 (0.3377) Acc D Real: 68.005% 
Loss D Fake: 0.4665 (0.4775) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 0.9957 (0.9777) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,393 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2830 (0.3338) Acc D Real: 68.408% 
Loss D Fake: 0.4658 (0.4766) Acc D Fake: 100.000% 
Loss D: 0.749 
Loss G: 0.9976 (0.9791) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,400 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4211 (0.3396) Acc D Real: 67.833% 
Loss D Fake: 0.4647 (0.4759) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9990 (0.9804) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,407 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4171 (0.3445) Acc D Real: 67.370% 
Loss D Fake: 0.4641 (0.4751) Acc D Fake: 100.000% 
Loss D: 0.881 
Loss G: 1.0000 (0.9816) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,415 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2798 (0.3407) Acc D Real: 67.788% 
Loss D Fake: 0.4636 (0.4744) Acc D Fake: 100.000% 
Loss D: 0.743 
Loss G: 1.0014 (0.9828) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,422 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3462 (0.3410) Acc D Real: 67.818% 
Loss D Fake: 0.4628 (0.4738) Acc D Fake: 100.000% 
Loss D: 0.809 
Loss G: 1.0026 (0.9839) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,430 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3848 (0.3433) Acc D Real: 67.632% 
Loss D Fake: 0.4623 (0.4732) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 1.0034 (0.9849) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,437 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3786 (0.3450) Acc D Real: 67.479% 
Loss D Fake: 0.4619 (0.4726) Acc D Fake: 100.000% 
Loss D: 0.840 
Loss G: 1.0040 (0.9859) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,444 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4384 (0.3495) Acc D Real: 67.029% 
Loss D Fake: 0.4617 (0.4721) Acc D Fake: 100.000% 
Loss D: 0.900 
Loss G: 1.0043 (0.9868) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,451 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2331 (0.3442) Acc D Real: 67.562% 
Loss D Fake: 0.4614 (0.4716) Acc D Fake: 100.000% 
Loss D: 0.694 
Loss G: 1.0054 (0.9876) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,458 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.2806 (0.3414) Acc D Real: 67.844% 
Loss D Fake: 0.4605 (0.4711) Acc D Fake: 100.000% 
Loss D: 0.741 
Loss G: 1.0072 (0.9885) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,465 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4786 (0.3471) Acc D Real: 67.285% 
Loss D Fake: 0.4597 (0.4707) Acc D Fake: 100.000% 
Loss D: 0.938 
Loss G: 1.0080 (0.9893) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,472 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4980 (0.3532) Acc D Real: 66.652% 
Loss D Fake: 0.4597 (0.4702) Acc D Fake: 100.000% 
Loss D: 0.958 
Loss G: 1.0076 (0.9900) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,480 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3932 (0.3547) Acc D Real: 66.522% 
Loss D Fake: 0.4600 (0.4698) Acc D Fake: 100.000% 
Loss D: 0.853 
Loss G: 1.0067 (0.9907) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,488 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4008 (0.3564) Acc D Real: 66.333% 
Loss D Fake: 0.4605 (0.4695) Acc D Fake: 100.000% 
Loss D: 0.861 
Loss G: 1.0063 (0.9912) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,495 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3970 (0.3579) Acc D Real: 66.192% 
Loss D Fake: 0.4606 (0.4692) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0060 (0.9918) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,504 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2879 (0.3555) Acc D Real: 66.467% 
Loss D Fake: 0.4607 (0.4689) Acc D Fake: 100.000% 
Loss D: 0.749 
Loss G: 1.0064 (0.9923) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,512 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3110 (0.3540) Acc D Real: 66.660% 
Loss D Fake: 0.4604 (0.4686) Acc D Fake: 100.000% 
Loss D: 0.771 
Loss G: 1.0070 (0.9928) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,520 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2885 (0.3519) Acc D Real: 66.897% 
Loss D Fake: 0.4600 (0.4683) Acc D Fake: 100.000% 
Loss D: 0.748 
Loss G: 1.0083 (0.9933) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,527 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3976 (0.3533) Acc D Real: 66.779% 
Loss D Fake: 0.4593 (0.4680) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0092 (0.9938) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,535 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2637 (0.3506) Acc D Real: 67.069% 
Loss D Fake: 0.4588 (0.4677) Acc D Fake: 100.000% 
Loss D: 0.722 
Loss G: 1.0104 (0.9943) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,542 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3633 (0.3510) Acc D Real: 67.016% 
Loss D Fake: 0.4580 (0.4675) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 1.0120 (0.9948) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,550 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3514 (0.3510) Acc D Real: 67.040% 
Loss D Fake: 0.4572 (0.4672) Acc D Fake: 100.000% 
Loss D: 0.809 
Loss G: 1.0133 (0.9953) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,557 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2504 (0.3482) Acc D Real: 67.338% 
Loss D Fake: 0.4565 (0.4669) Acc D Fake: 100.000% 
Loss D: 0.707 
Loss G: 1.0149 (0.9959) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,564 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3108 (0.3472) Acc D Real: 67.459% 
Loss D Fake: 0.4555 (0.4666) Acc D Fake: 100.000% 
Loss D: 0.766 
Loss G: 1.0168 (0.9964) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,572 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3262 (0.3466) Acc D Real: 67.537% 
Loss D Fake: 0.4546 (0.4662) Acc D Fake: 100.000% 
Loss D: 0.781 
Loss G: 1.0185 (0.9970) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,579 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3954 (0.3479) Acc D Real: 67.425% 
Loss D Fake: 0.4538 (0.4659) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 1.0196 (0.9976) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,587 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.2417 (0.3452) Acc D Real: 67.702% 
Loss D Fake: 0.4532 (0.4656) Acc D Fake: 100.000% 
Loss D: 0.695 
Loss G: 1.0211 (0.9982) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,594 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4709 (0.3483) Acc D Real: 67.384% 
Loss D Fake: 0.4525 (0.4653) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 1.0220 (0.9987) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,601 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2241 (0.3453) Acc D Real: 67.707% 
Loss D Fake: 0.4520 (0.4650) Acc D Fake: 100.000% 
Loss D: 0.676 
Loss G: 1.0234 (0.9993) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,609 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2642 (0.3434) Acc D Real: 67.902% 
Loss D Fake: 0.4511 (0.4646) Acc D Fake: 100.000% 
Loss D: 0.715 
Loss G: 1.0257 (0.9999) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,616 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3473 (0.3435) Acc D Real: 67.920% 
Loss D Fake: 0.4499 (0.4643) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 1.0275 (1.0006) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,624 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4106 (0.3450) Acc D Real: 67.786% 
Loss D Fake: 0.4492 (0.4640) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 1.0284 (1.0012) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,631 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3899 (0.3460) Acc D Real: 67.714% 
Loss D Fake: 0.4489 (0.4637) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 1.0287 (1.0018) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,638 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3195 (0.3454) Acc D Real: 67.765% 
Loss D Fake: 0.4487 (0.4633) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 1.0297 (1.0024) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,646 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3172 (0.3448) Acc D Real: 67.841% 
Loss D Fake: 0.4481 (0.4630) Acc D Fake: 100.000% 
Loss D: 0.765 
Loss G: 1.0311 (1.0030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,653 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3064 (0.3441) Acc D Real: 67.946% 
Loss D Fake: 0.4474 (0.4627) Acc D Fake: 100.000% 
Loss D: 0.754 
Loss G: 1.0323 (1.0036) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,661 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4224 (0.3456) Acc D Real: 67.775% 
Loss D Fake: 0.4468 (0.4624) Acc D Fake: 100.000% 
Loss D: 0.869 
Loss G: 1.0335 (1.0042) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,668 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2790 (0.3443) Acc D Real: 67.932% 
Loss D Fake: 0.4462 (0.4621) Acc D Fake: 100.000% 
Loss D: 0.725 
Loss G: 1.0348 (1.0048) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,676 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3211 (0.3439) Acc D Real: 67.988% 
Loss D Fake: 0.4455 (0.4617) Acc D Fake: 100.000% 
Loss D: 0.767 
Loss G: 1.0363 (1.0054) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,683 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4240 (0.3454) Acc D Real: 67.852% 
Loss D Fake: 0.4449 (0.4614) Acc D Fake: 100.000% 
Loss D: 0.869 
Loss G: 1.0371 (1.0060) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,691 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3211 (0.3449) Acc D Real: 67.912% 
Loss D Fake: 0.4445 (0.4611) Acc D Fake: 100.000% 
Loss D: 0.766 
Loss G: 1.0378 (1.0066) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,698 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3524 (0.3451) Acc D Real: 67.920% 
Loss D Fake: 0.4443 (0.4608) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 1.0382 (1.0071) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,706 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3133 (0.3445) Acc D Real: 67.999% 
Loss D Fake: 0.4442 (0.4605) Acc D Fake: 100.000% 
Loss D: 0.757 
Loss G: 1.0383 (1.0077) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,713 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3141 (0.3440) Acc D Real: 68.058% 
Loss D Fake: 0.4441 (0.4602) Acc D Fake: 100.000% 
Loss D: 0.758 
Loss G: 1.0387 (1.0082) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,721 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3645 (0.3443) Acc D Real: 68.038% 
Loss D Fake: 0.4438 (0.4599) Acc D Fake: 100.000% 
Loss D: 0.808 
Loss G: 1.0390 (1.0088) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,728 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3258 (0.3440) Acc D Real: 68.071% 
Loss D Fake: 0.4437 (0.4597) Acc D Fake: 100.000% 
Loss D: 0.769 
Loss G: 1.0397 (1.0093) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,736 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2196 (0.3419) Acc D Real: 68.312% 
Loss D Fake: 0.4433 (0.4594) Acc D Fake: 100.000% 
Loss D: 0.663 
Loss G: 1.0406 (1.0098) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,743 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2428 (0.3403) Acc D Real: 68.490% 
Loss D Fake: 0.4427 (0.4591) Acc D Fake: 100.000% 
Loss D: 0.685 
Loss G: 1.0419 (1.0103) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,752 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2707 (0.3392) Acc D Real: 68.625% 
Loss D Fake: 0.4421 (0.4588) Acc D Fake: 100.000% 
Loss D: 0.713 
Loss G: 1.0433 (1.0109) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,759 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3436 (0.3393) Acc D Real: 68.618% 
Loss D Fake: 0.4414 (0.4586) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0445 (1.0114) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,767 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2252 (0.3375) Acc D Real: 68.820% 
Loss D Fake: 0.4408 (0.4583) Acc D Fake: 100.000% 
Loss D: 0.666 
Loss G: 1.0459 (1.0120) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,775 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2742 (0.3365) Acc D Real: 68.922% 
Loss D Fake: 0.4401 (0.4580) Acc D Fake: 100.000% 
Loss D: 0.714 
Loss G: 1.0473 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,783 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3918 (0.3373) Acc D Real: 68.854% 
Loss D Fake: 0.4398 (0.4577) Acc D Fake: 100.000% 
Loss D: 0.832 
Loss G: 1.0466 (1.0130) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,791 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.1998 (0.3353) Acc D Real: 69.083% 
Loss D Fake: 0.4407 (0.4575) Acc D Fake: 100.000% 
Loss D: 0.641 
Loss G: 1.0444 (1.0135) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,798 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3382 (0.3353) Acc D Real: 69.107% 
Loss D Fake: 0.4420 (0.4572) Acc D Fake: 100.000% 
Loss D: 0.780 
Loss G: 1.0416 (1.0139) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,806 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3543 (0.3356) Acc D Real: 69.101% 
Loss D Fake: 0.4436 (0.4571) Acc D Fake: 100.000% 
Loss D: 0.798 
Loss G: 1.0385 (1.0143) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,814 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3306 (0.3355) Acc D Real: 69.132% 
Loss D Fake: 0.4451 (0.4569) Acc D Fake: 100.000% 
Loss D: 0.776 
Loss G: 1.0356 (1.0146) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,821 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3494 (0.3357) Acc D Real: 69.135% 
Loss D Fake: 0.4465 (0.4567) Acc D Fake: 100.000% 
Loss D: 0.796 
Loss G: 1.0331 (1.0148) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,829 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3644 (0.3361) Acc D Real: 69.105% 
Loss D Fake: 0.4479 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.812 
Loss G: 1.0304 (1.0150) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,836 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4088 (0.3371) Acc D Real: 69.032% 
Loss D Fake: 0.4493 (0.4565) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0276 (1.0152) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,844 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2593 (0.3361) Acc D Real: 69.153% 
Loss D Fake: 0.4505 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.710 
Loss G: 1.0260 (1.0154) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,852 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4338 (0.3374) Acc D Real: 69.043% 
Loss D Fake: 0.4512 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 1.0246 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,860 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2642 (0.3364) Acc D Real: 69.137% 
Loss D Fake: 0.4519 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.716 
Loss G: 1.0237 (1.0156) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,868 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2994 (0.3359) Acc D Real: 69.182% 
Loss D Fake: 0.4523 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.752 
Loss G: 1.0234 (1.0157) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,876 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3274 (0.3358) Acc D Real: 69.199% 
Loss D Fake: 0.4524 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.780 
Loss G: 1.0234 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,883 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3695 (0.3362) Acc D Real: 69.176% 
Loss D Fake: 0.4525 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 1.0231 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,891 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3739 (0.3367) Acc D Real: 69.153% 
Loss D Fake: 0.4528 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.827 
Loss G: 1.0224 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,900 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3983 (0.3375) Acc D Real: 69.095% 
Loss D Fake: 0.4534 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.852 
Loss G: 1.0210 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,907 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2854 (0.3368) Acc D Real: 69.160% 
Loss D Fake: 0.4542 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 1.0196 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,915 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3027 (0.3364) Acc D Real: 69.207% 
Loss D Fake: 0.4549 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.758 
Loss G: 1.0183 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,923 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2945 (0.3359) Acc D Real: 69.273% 
Loss D Fake: 0.4556 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 1.0170 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,931 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3216 (0.3358) Acc D Real: 69.292% 
Loss D Fake: 0.4563 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0159 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,939 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.2693 (0.3350) Acc D Real: 69.379% 
Loss D Fake: 0.4569 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.726 
Loss G: 1.0150 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,947 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3206 (0.3348) Acc D Real: 69.410% 
Loss D Fake: 0.4573 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0140 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,955 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.2638 (0.3340) Acc D Real: 69.490% 
Loss D Fake: 0.4579 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.722 
Loss G: 1.0131 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,963 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3123 (0.3338) Acc D Real: 69.522% 
Loss D Fake: 0.4584 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.771 
Loss G: 1.0123 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,970 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3195 (0.3336) Acc D Real: 69.542% 
Loss D Fake: 0.4588 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0116 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,978 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2860 (0.3331) Acc D Real: 69.599% 
Loss D Fake: 0.4592 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.745 
Loss G: 1.0109 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,985 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3253 (0.3330) Acc D Real: 69.614% 
Loss D Fake: 0.4597 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0100 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:08,993 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3900 (0.3336) Acc D Real: 69.572% 
Loss D Fake: 0.4603 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 1.0084 (1.0157) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,000 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.1714 (0.3319) Acc D Real: 69.753% 
Loss D Fake: 0.4612 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.633 
Loss G: 1.0073 (1.0156) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,008 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2911 (0.3315) Acc D Real: 69.794% 
Loss D Fake: 0.4617 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.753 
Loss G: 1.0063 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,015 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3097 (0.3312) Acc D Real: 69.817% 
Loss D Fake: 0.4622 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.772 
Loss G: 1.0054 (1.0154) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,023 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3718 (0.3317) Acc D Real: 69.777% 
Loss D Fake: 0.4628 (0.4565) Acc D Fake: 100.000% 
Loss D: 0.835 
Loss G: 1.0041 (1.0153) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,031 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3729 (0.3321) Acc D Real: 69.734% 
Loss D Fake: 0.4636 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 1.0025 (1.0152) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,038 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2906 (0.3317) Acc D Real: 69.777% 
Loss D Fake: 0.4645 (0.4566) Acc D Fake: 100.000% 
Loss D: 0.755 
Loss G: 1.0007 (1.0150) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,046 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3406 (0.3317) Acc D Real: 69.773% 
Loss D Fake: 0.4655 (0.4567) Acc D Fake: 100.000% 
Loss D: 0.806 
Loss G: 0.9988 (1.0149) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,053 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3178 (0.3316) Acc D Real: 69.795% 
Loss D Fake: 0.4671 (0.4568) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 0.9937 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,061 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3202 (0.3315) Acc D Real: 69.803% 
Loss D Fake: 0.4703 (0.4570) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9877 (1.0144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,068 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2556 (0.3308) Acc D Real: 69.876% 
Loss D Fake: 0.4734 (0.4571) Acc D Fake: 100.000% 
Loss D: 0.729 
Loss G: 0.9818 (1.0141) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,075 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3667 (0.3311) Acc D Real: 69.847% 
Loss D Fake: 0.4764 (0.4573) Acc D Fake: 100.000% 
Loss D: 0.843 
Loss G: 0.9765 (1.0137) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,083 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3281 (0.3311) Acc D Real: 69.849% 
Loss D Fake: 0.4791 (0.4575) Acc D Fake: 100.000% 
Loss D: 0.807 
Loss G: 0.9711 (1.0133) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,090 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3019 (0.3308) Acc D Real: 69.890% 
Loss D Fake: 0.4819 (0.4578) Acc D Fake: 100.000% 
Loss D: 0.784 
Loss G: 0.9674 (1.0129) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,097 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3187 (0.3307) Acc D Real: 69.898% 
Loss D Fake: 0.4812 (0.4580) Acc D Fake: 100.000% 
Loss D: 0.800 
Loss G: 0.9737 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,105 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3413 (0.3308) Acc D Real: 69.888% 
Loss D Fake: 0.4770 (0.4581) Acc D Fake: 100.000% 
Loss D: 0.818 
Loss G: 0.9784 (1.0122) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,112 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2777 (0.3303) Acc D Real: 69.946% 
Loss D Fake: 0.4757 (0.4583) Acc D Fake: 100.000% 
Loss D: 0.753 
Loss G: 0.9794 (1.0119) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,119 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2639 (0.3297) Acc D Real: 70.003% 
Loss D Fake: 0.4755 (0.4585) Acc D Fake: 100.000% 
Loss D: 0.739 
Loss G: 0.9795 (1.0116) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,127 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2574 (0.3290) Acc D Real: 70.071% 
Loss D Fake: 0.4756 (0.4586) Acc D Fake: 100.000% 
Loss D: 0.733 
Loss G: 0.9793 (1.0113) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,134 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2538 (0.3284) Acc D Real: 70.143% 
Loss D Fake: 0.4757 (0.4588) Acc D Fake: 100.000% 
Loss D: 0.730 
Loss G: 0.9790 (1.0110) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,142 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.1451 (0.3267) Acc D Real: 70.309% 
Loss D Fake: 0.4759 (0.4589) Acc D Fake: 100.000% 
Loss D: 0.621 
Loss G: 0.9789 (1.0108) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,150 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3450 (0.3269) Acc D Real: 70.292% 
Loss D Fake: 0.4760 (0.4591) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 0.9787 (1.0105) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,157 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3454 (0.3271) Acc D Real: 70.274% 
Loss D Fake: 0.4762 (0.4592) Acc D Fake: 100.000% 
Loss D: 0.822 
Loss G: 0.9784 (1.0102) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,164 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3069 (0.3269) Acc D Real: 70.280% 
Loss D Fake: 0.4765 (0.4594) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9775 (1.0099) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,172 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2416 (0.3262) Acc D Real: 70.351% 
Loss D Fake: 0.4771 (0.4595) Acc D Fake: 100.000% 
Loss D: 0.719 
Loss G: 0.9767 (1.0096) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,179 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.1904 (0.3250) Acc D Real: 70.467% 
Loss D Fake: 0.4776 (0.4597) Acc D Fake: 100.000% 
Loss D: 0.668 
Loss G: 0.9761 (1.0093) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,186 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3718 (0.3254) Acc D Real: 70.418% 
Loss D Fake: 0.4780 (0.4598) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 0.9755 (1.0091) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,193 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2591 (0.3249) Acc D Real: 70.468% 
Loss D Fake: 0.4784 (0.4600) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9747 (1.0088) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,201 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2592 (0.3243) Acc D Real: 70.517% 
Loss D Fake: 0.4789 (0.4601) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9740 (1.0085) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,208 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4088 (0.3250) Acc D Real: 70.429% 
Loss D Fake: 0.4794 (0.4603) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 0.9732 (1.0082) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,215 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.2083 (0.3241) Acc D Real: 70.517% 
Loss D Fake: 0.4799 (0.4605) Acc D Fake: 100.000% 
Loss D: 0.688 
Loss G: 0.9725 (1.0079) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,223 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3026 (0.3239) Acc D Real: 70.528% 
Loss D Fake: 0.4803 (0.4606) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9718 (1.0076) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,231 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2727 (0.3235) Acc D Real: 70.560% 
Loss D Fake: 0.4809 (0.4608) Acc D Fake: 100.000% 
Loss D: 0.754 
Loss G: 0.9710 (1.0073) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,239 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3153 (0.3234) Acc D Real: 70.556% 
Loss D Fake: 0.4814 (0.4609) Acc D Fake: 100.000% 
Loss D: 0.797 
Loss G: 0.9700 (1.0070) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,246 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2311 (0.3227) Acc D Real: 70.620% 
Loss D Fake: 0.4821 (0.4611) Acc D Fake: 100.000% 
Loss D: 0.713 
Loss G: 0.9689 (1.0067) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,254 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.1713 (0.3215) Acc D Real: 70.741% 
Loss D Fake: 0.4829 (0.4613) Acc D Fake: 100.000% 
Loss D: 0.654 
Loss G: 0.9672 (1.0064) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,262 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2178 (0.3207) Acc D Real: 70.817% 
Loss D Fake: 0.4841 (0.4615) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9655 (1.0061) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,269 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2648 (0.3203) Acc D Real: 70.850% 
Loss D Fake: 0.4851 (0.4616) Acc D Fake: 100.000% 
Loss D: 0.750 
Loss G: 0.9639 (1.0058) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,276 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2535 (0.3198) Acc D Real: 70.892% 
Loss D Fake: 0.4861 (0.4618) Acc D Fake: 100.000% 
Loss D: 0.740 
Loss G: 0.9625 (1.0054) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,284 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2151 (0.3190) Acc D Real: 70.960% 
Loss D Fake: 0.4870 (0.4620) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9612 (1.0051) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,291 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3021 (0.3188) Acc D Real: 70.958% 
Loss D Fake: 0.4879 (0.4622) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9600 (1.0048) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,299 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2721 (0.3185) Acc D Real: 70.981% 
Loss D Fake: 0.4887 (0.4624) Acc D Fake: 100.000% 
Loss D: 0.761 
Loss G: 0.9588 (1.0044) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,307 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.2150 (0.3177) Acc D Real: 71.046% 
Loss D Fake: 0.4896 (0.4626) Acc D Fake: 100.000% 
Loss D: 0.705 
Loss G: 0.9572 (1.0041) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,314 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2134 (0.3170) Acc D Real: 71.115% 
Loss D Fake: 0.4907 (0.4628) Acc D Fake: 100.000% 
Loss D: 0.704 
Loss G: 0.9556 (1.0037) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,321 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2225 (0.3163) Acc D Real: 71.180% 
Loss D Fake: 0.4919 (0.4630) Acc D Fake: 100.000% 
Loss D: 0.714 
Loss G: 0.9536 (1.0034) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,329 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2265 (0.3156) Acc D Real: 71.235% 
Loss D Fake: 0.4935 (0.4632) Acc D Fake: 100.000% 
Loss D: 0.720 
Loss G: 0.9508 (1.0030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,336 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.2464 (0.3151) Acc D Real: 71.269% 
Loss D Fake: 0.4956 (0.4635) Acc D Fake: 100.000% 
Loss D: 0.742 
Loss G: 0.9478 (1.0026) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,344 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.1863 (0.3142) Acc D Real: 71.353% 
Loss D Fake: 0.4976 (0.4637) Acc D Fake: 100.000% 
Loss D: 0.684 
Loss G: 0.9450 (1.0022) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,351 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.2022 (0.3134) Acc D Real: 71.428% 
Loss D Fake: 0.4998 (0.4640) Acc D Fake: 100.000% 
Loss D: 0.702 
Loss G: 0.9414 (1.0017) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,359 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2093 (0.3127) Acc D Real: 71.486% 
Loss D Fake: 0.5026 (0.4643) Acc D Fake: 100.000% 
Loss D: 0.712 
Loss G: 0.9373 (1.0013) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,366 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2614 (0.3123) Acc D Real: 71.509% 
Loss D Fake: 0.5063 (0.4645) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 0.9317 (1.0008) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,374 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2776 (0.3121) Acc D Real: 71.519% 
Loss D Fake: 0.5121 (0.4649) Acc D Fake: 100.000% 
Loss D: 0.790 
Loss G: 0.9233 (1.0003) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:09,381 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2054 (0.3113) Acc D Real: 71.584% 
Loss D Fake: 0.5224 (0.4653) Acc D Fake: 99.989% 
Loss D: 0.728 
Loss G: 0.9107 (0.9996) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,388 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2627 (0.3110) Acc D Real: 71.596% 
Loss D Fake: 1.0696 (0.4694) Acc D Fake: 99.844% 
Loss D: 1.332 
Loss G: 0.9662 (0.9994) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,395 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.2443 (0.3105) Acc D Real: 71.626% 
Loss D Fake: 0.4747 (0.4695) Acc D Fake: 99.845% 
Loss D: 0.719 
Loss G: 0.9954 (0.9994) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,403 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2052 (0.3098) Acc D Real: 71.688% 
Loss D Fake: 0.4657 (0.4694) Acc D Fake: 99.846% 
Loss D: 0.671 
Loss G: 0.9861 (0.9993) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,410 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.1906 (0.3090) Acc D Real: 71.762% 
Loss D Fake: 0.4950 (0.4696) Acc D Fake: 99.847% 
Loss D: 0.686 
Loss G: 0.9325 (0.9988) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,417 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2897 (0.3089) Acc D Real: 71.763% 
Loss D Fake: 0.5154 (0.4699) Acc D Fake: 99.848% 
Loss D: 0.805 
Loss G: 0.9119 (0.9983) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,424 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3390 (0.3091) Acc D Real: 71.720% 
Loss D Fake: 0.5254 (0.4703) Acc D Fake: 99.849% 
Loss D: 0.864 
Loss G: 0.9002 (0.9976) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,432 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2372 (0.3086) Acc D Real: 71.764% 
Loss D Fake: 0.5313 (0.4707) Acc D Fake: 99.850% 
Loss D: 0.769 
Loss G: 0.8935 (0.9969) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,439 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3782 (0.3091) Acc D Real: 71.679% 
Loss D Fake: 0.5349 (0.4711) Acc D Fake: 99.851% 
Loss D: 0.913 
Loss G: 0.8891 (0.9962) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,447 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3868 (0.3096) Acc D Real: 71.599% 
Loss D Fake: 0.5374 (0.4715) Acc D Fake: 99.852% 
Loss D: 0.924 
Loss G: 0.8860 (0.9955) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,454 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2620 (0.3093) Acc D Real: 71.624% 
Loss D Fake: 0.5391 (0.4720) Acc D Fake: 99.853% 
Loss D: 0.801 
Loss G: 0.8844 (0.9948) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,461 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4110 (0.3099) Acc D Real: 71.531% 
Loss D Fake: 0.5398 (0.4724) Acc D Fake: 99.854% 
Loss D: 0.951 
Loss G: 0.8834 (0.9941) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,468 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4761 (0.3110) Acc D Real: 71.407% 
Loss D Fake: 0.5404 (0.4728) Acc D Fake: 99.855% 
Loss D: 1.016 
Loss G: 0.8828 (0.9934) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,475 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4679 (0.3120) Acc D Real: 71.393% 
Loss D Fake: 0.5408 (0.4733) Acc D Fake: 99.855% 
Loss D: 1.009 
Loss G: 0.8820 (0.9927) Acc G: 0.011% 
LR: 2.000e-04 

2023-03-02 02:00:09,682 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.595 | Generator Loss: 0.882 | Avg: 1.477 
2023-03-02 02:00:09,706 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.653 | Generator Loss: 0.882 | Avg: 1.535 
2023-03-02 02:00:09,728 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.638 | Generator Loss: 0.882 | Avg: 1.520 
2023-03-02 02:00:09,754 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.624 | Generator Loss: 0.882 | Avg: 1.506 
2023-03-02 02:00:09,779 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.621 | Generator Loss: 0.882 | Avg: 1.503 
2023-03-02 02:00:09,805 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.691 | Generator Loss: 0.882 | Avg: 1.573 
2023-03-02 02:00:09,831 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.697 | Generator Loss: 0.882 | Avg: 1.579 
2023-03-02 02:00:09,856 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.754 | Generator Loss: 0.882 | Avg: 1.636 
2023-03-02 02:00:09,882 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.774 | Generator Loss: 0.882 | Avg: 1.656 
2023-03-02 02:00:09,907 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.839 | Generator Loss: 0.882 | Avg: 1.720 
2023-03-02 02:00:09,939 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.879 | Generator Loss: 0.882 | Avg: 1.761 
2023-03-02 02:00:09,965 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.921 | Generator Loss: 0.882 | Avg: 1.802 
2023-03-02 02:00:09,992 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.957 | Generator Loss: 0.882 | Avg: 1.838 
2023-03-02 02:00:10,018 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.966 | Generator Loss: 0.882 | Avg: 1.848 
2023-03-02 02:00:10,045 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.944 | Generator Loss: 0.882 | Avg: 1.826 
2023-03-02 02:00:10,071 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.925 | Generator Loss: 0.882 | Avg: 1.807 
2023-03-02 02:00:10,097 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.907 | Generator Loss: 0.882 | Avg: 1.789 
2023-03-02 02:00:10,131 -                train: [    INFO] - 
Epoch: 5/20
2023-03-02 02:00:10,299 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4209 (0.4733) Acc D Real: 50.417% 
Loss D Fake: 0.5422 (0.5418) Acc D Fake: 100.000% 
Loss D: 0.963 
Loss G: 0.8796 (0.8803) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,308 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5121 (0.4862) Acc D Real: 49.306% 
Loss D Fake: 0.5432 (0.5423) Acc D Fake: 100.000% 
Loss D: 1.055 
Loss G: 0.8783 (0.8796) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,316 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2773 (0.4340) Acc D Real: 55.404% 
Loss D Fake: 0.5438 (0.5427) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 0.8779 (0.8792) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,333 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3461 (0.4164) Acc D Real: 57.250% 
Loss D Fake: 0.5437 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.890 
Loss G: 0.8782 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,340 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3493 (0.4052) Acc D Real: 58.411% 
Loss D Fake: 0.5433 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.893 
Loss G: 0.8787 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,347 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3799 (0.4016) Acc D Real: 58.698% 
Loss D Fake: 0.5428 (0.5429) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 0.8792 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,354 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.5276 (0.4173) Acc D Real: 56.855% 
Loss D Fake: 0.5425 (0.5429) Acc D Fake: 100.000% 
Loss D: 1.070 
Loss G: 0.8793 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,362 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3850 (0.4138) Acc D Real: 57.193% 
Loss D Fake: 0.5425 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.8790 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,369 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4368 (0.4161) Acc D Real: 56.990% 
Loss D Fake: 0.5427 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.980 
Loss G: 0.8788 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,376 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3667 (0.4116) Acc D Real: 57.727% 
Loss D Fake: 0.5425 (0.5428) Acc D Fake: 100.000% 
Loss D: 0.909 
Loss G: 0.8792 (0.8790) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,382 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4265 (0.4128) Acc D Real: 57.552% 
Loss D Fake: 0.5420 (0.5427) Acc D Fake: 100.000% 
Loss D: 0.969 
Loss G: 0.8800 (0.8791) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,389 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3731 (0.4098) Acc D Real: 57.953% 
Loss D Fake: 0.5413 (0.5426) Acc D Fake: 100.000% 
Loss D: 0.914 
Loss G: 0.8808 (0.8792) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,396 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4854 (0.4152) Acc D Real: 57.370% 
Loss D Fake: 0.5407 (0.5425) Acc D Fake: 100.000% 
Loss D: 1.026 
Loss G: 0.8817 (0.8794) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,403 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4336 (0.4164) Acc D Real: 57.205% 
Loss D Fake: 0.5399 (0.5423) Acc D Fake: 100.000% 
Loss D: 0.974 
Loss G: 0.8826 (0.8796) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,410 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4670 (0.4196) Acc D Real: 56.908% 
Loss D Fake: 0.5391 (0.5421) Acc D Fake: 100.000% 
Loss D: 1.006 
Loss G: 0.8837 (0.8799) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,417 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.5365 (0.4264) Acc D Real: 56.057% 
Loss D Fake: 0.5384 (0.5419) Acc D Fake: 100.000% 
Loss D: 1.075 
Loss G: 0.8842 (0.8801) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,424 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4192 (0.4260) Acc D Real: 56.134% 
Loss D Fake: 0.5380 (0.5417) Acc D Fake: 100.000% 
Loss D: 0.957 
Loss G: 0.8847 (0.8804) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,431 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3251 (0.4207) Acc D Real: 56.790% 
Loss D Fake: 0.5375 (0.5414) Acc D Fake: 100.000% 
Loss D: 0.863 
Loss G: 0.8856 (0.8807) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,439 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3798 (0.4187) Acc D Real: 57.036% 
Loss D Fake: 0.5366 (0.5412) Acc D Fake: 100.000% 
Loss D: 0.916 
Loss G: 0.8868 (0.8810) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,447 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.5011 (0.4226) Acc D Real: 56.657% 
Loss D Fake: 0.5356 (0.5409) Acc D Fake: 100.000% 
Loss D: 1.037 
Loss G: 0.8882 (0.8813) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,454 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4640 (0.4245) Acc D Real: 56.409% 
Loss D Fake: 0.5345 (0.5406) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.8894 (0.8817) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,461 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4616 (0.4261) Acc D Real: 56.205% 
Loss D Fake: 0.5338 (0.5403) Acc D Fake: 100.000% 
Loss D: 0.995 
Loss G: 0.8902 (0.8821) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,467 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4328 (0.4264) Acc D Real: 56.189% 
Loss D Fake: 0.5332 (0.5400) Acc D Fake: 100.000% 
Loss D: 0.966 
Loss G: 0.8909 (0.8824) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,474 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4440 (0.4271) Acc D Real: 56.177% 
Loss D Fake: 0.5327 (0.5398) Acc D Fake: 100.000% 
Loss D: 0.977 
Loss G: 0.8915 (0.8828) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,481 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3770 (0.4252) Acc D Real: 56.372% 
Loss D Fake: 0.5322 (0.5395) Acc D Fake: 100.000% 
Loss D: 0.909 
Loss G: 0.8921 (0.8831) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,488 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4422 (0.4258) Acc D Real: 56.283% 
Loss D Fake: 0.5317 (0.5392) Acc D Fake: 100.000% 
Loss D: 0.974 
Loss G: 0.8926 (0.8835) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,495 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.5258 (0.4294) Acc D Real: 55.956% 
Loss D Fake: 0.5313 (0.5389) Acc D Fake: 100.000% 
Loss D: 1.057 
Loss G: 0.8932 (0.8838) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,502 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3800 (0.4277) Acc D Real: 56.223% 
Loss D Fake: 0.5307 (0.5386) Acc D Fake: 100.000% 
Loss D: 0.911 
Loss G: 0.8942 (0.8842) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,509 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3973 (0.4266) Acc D Real: 56.328% 
Loss D Fake: 0.5299 (0.5383) Acc D Fake: 100.000% 
Loss D: 0.927 
Loss G: 0.8952 (0.8846) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,517 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4892 (0.4287) Acc D Real: 56.129% 
Loss D Fake: 0.5291 (0.5380) Acc D Fake: 100.000% 
Loss D: 1.018 
Loss G: 0.8963 (0.8849) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,525 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.4685 (0.4299) Acc D Real: 56.009% 
Loss D Fake: 0.5284 (0.5377) Acc D Fake: 100.000% 
Loss D: 0.997 
Loss G: 0.8972 (0.8853) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,532 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4733 (0.4312) Acc D Real: 55.870% 
Loss D Fake: 0.5278 (0.5374) Acc D Fake: 100.000% 
Loss D: 1.001 
Loss G: 0.8977 (0.8857) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,540 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3632 (0.4292) Acc D Real: 56.115% 
Loss D Fake: 0.5274 (0.5371) Acc D Fake: 100.000% 
Loss D: 0.891 
Loss G: 0.8983 (0.8861) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,548 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4200 (0.4290) Acc D Real: 56.124% 
Loss D Fake: 0.5270 (0.5368) Acc D Fake: 100.000% 
Loss D: 0.947 
Loss G: 0.8987 (0.8864) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,556 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4724 (0.4302) Acc D Real: 56.060% 
Loss D Fake: 0.5266 (0.5366) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.8995 (0.8868) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,564 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5007 (0.4321) Acc D Real: 55.892% 
Loss D Fake: 0.5259 (0.5363) Acc D Fake: 100.000% 
Loss D: 1.027 
Loss G: 0.9005 (0.8872) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,573 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3939 (0.4311) Acc D Real: 55.999% 
Loss D Fake: 0.5251 (0.5360) Acc D Fake: 100.000% 
Loss D: 0.919 
Loss G: 0.9016 (0.8875) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,580 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4614 (0.4318) Acc D Real: 55.893% 
Loss D Fake: 0.5244 (0.5357) Acc D Fake: 100.000% 
Loss D: 0.986 
Loss G: 0.9024 (0.8879) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,590 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3956 (0.4309) Acc D Real: 55.975% 
Loss D Fake: 0.5240 (0.5354) Acc D Fake: 100.000% 
Loss D: 0.920 
Loss G: 0.9029 (0.8883) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,598 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3970 (0.4301) Acc D Real: 56.104% 
Loss D Fake: 0.5236 (0.5351) Acc D Fake: 100.000% 
Loss D: 0.921 
Loss G: 0.9036 (0.8887) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,607 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2591 (0.4260) Acc D Real: 56.586% 
Loss D Fake: 0.5228 (0.5348) Acc D Fake: 100.000% 
Loss D: 0.782 
Loss G: 0.9052 (0.8891) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,616 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3798 (0.4250) Acc D Real: 56.710% 
Loss D Fake: 0.5216 (0.5345) Acc D Fake: 100.000% 
Loss D: 0.901 
Loss G: 0.9068 (0.8895) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,624 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3733 (0.4238) Acc D Real: 56.870% 
Loss D Fake: 0.5204 (0.5342) Acc D Fake: 100.000% 
Loss D: 0.894 
Loss G: 0.9086 (0.8899) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,631 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4974 (0.4254) Acc D Real: 56.716% 
Loss D Fake: 0.5192 (0.5338) Acc D Fake: 100.000% 
Loss D: 1.017 
Loss G: 0.9103 (0.8904) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,639 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4137 (0.4252) Acc D Real: 56.752% 
Loss D Fake: 0.5180 (0.5335) Acc D Fake: 100.000% 
Loss D: 0.932 
Loss G: 0.9118 (0.8908) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,647 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3433 (0.4234) Acc D Real: 56.989% 
Loss D Fake: 0.5170 (0.5331) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 0.9134 (0.8913) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,654 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3331 (0.4215) Acc D Real: 57.241% 
Loss D Fake: 0.5157 (0.5328) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 0.9154 (0.8918) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,662 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3607 (0.4203) Acc D Real: 57.388% 
Loss D Fake: 0.5144 (0.5324) Acc D Fake: 100.000% 
Loss D: 0.875 
Loss G: 0.9173 (0.8923) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,670 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3656 (0.4192) Acc D Real: 57.509% 
Loss D Fake: 0.5132 (0.5320) Acc D Fake: 100.000% 
Loss D: 0.879 
Loss G: 0.9188 (0.8929) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,677 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4492 (0.4198) Acc D Real: 57.453% 
Loss D Fake: 0.5123 (0.5316) Acc D Fake: 100.000% 
Loss D: 0.961 
Loss G: 0.9200 (0.8934) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,685 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4799 (0.4210) Acc D Real: 57.320% 
Loss D Fake: 0.5117 (0.5313) Acc D Fake: 100.000% 
Loss D: 0.992 
Loss G: 0.9204 (0.8939) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,693 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4701 (0.4219) Acc D Real: 57.238% 
Loss D Fake: 0.5115 (0.5309) Acc D Fake: 100.000% 
Loss D: 0.982 
Loss G: 0.9208 (0.8944) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,700 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3917 (0.4213) Acc D Real: 57.304% 
Loss D Fake: 0.5112 (0.5305) Acc D Fake: 100.000% 
Loss D: 0.903 
Loss G: 0.9212 (0.8949) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,707 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4096 (0.4211) Acc D Real: 57.356% 
Loss D Fake: 0.5108 (0.5302) Acc D Fake: 100.000% 
Loss D: 0.920 
Loss G: 0.9219 (0.8954) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,715 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3150 (0.4192) Acc D Real: 57.600% 
Loss D Fake: 0.5101 (0.5298) Acc D Fake: 100.000% 
Loss D: 0.825 
Loss G: 0.9235 (0.8959) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,722 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4882 (0.4204) Acc D Real: 57.483% 
Loss D Fake: 0.5091 (0.5294) Acc D Fake: 100.000% 
Loss D: 0.997 
Loss G: 0.9246 (0.8964) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,729 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4242 (0.4205) Acc D Real: 57.504% 
Loss D Fake: 0.5085 (0.5291) Acc D Fake: 100.000% 
Loss D: 0.933 
Loss G: 0.9254 (0.8969) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,736 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4152 (0.4204) Acc D Real: 57.542% 
Loss D Fake: 0.5080 (0.5287) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 0.9260 (0.8974) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,744 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4906 (0.4216) Acc D Real: 57.451% 
Loss D Fake: 0.5076 (0.5284) Acc D Fake: 100.000% 
Loss D: 0.998 
Loss G: 0.9266 (0.8979) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,751 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3394 (0.4202) Acc D Real: 57.628% 
Loss D Fake: 0.5071 (0.5280) Acc D Fake: 100.000% 
Loss D: 0.847 
Loss G: 0.9277 (0.8984) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,758 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4744 (0.4211) Acc D Real: 57.546% 
Loss D Fake: 0.5063 (0.5277) Acc D Fake: 100.000% 
Loss D: 0.981 
Loss G: 0.9288 (0.8989) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,765 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3996 (0.4208) Acc D Real: 57.608% 
Loss D Fake: 0.5054 (0.5273) Acc D Fake: 100.000% 
Loss D: 0.905 
Loss G: 0.9304 (0.8994) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,773 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4994 (0.4220) Acc D Real: 57.513% 
Loss D Fake: 0.5043 (0.5270) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9319 (0.8999) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,780 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4139 (0.4219) Acc D Real: 57.572% 
Loss D Fake: 0.5034 (0.5266) Acc D Fake: 100.000% 
Loss D: 0.917 
Loss G: 0.9333 (0.9004) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,788 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4261 (0.4219) Acc D Real: 57.570% 
Loss D Fake: 0.5026 (0.5262) Acc D Fake: 100.000% 
Loss D: 0.929 
Loss G: 0.9343 (0.9009) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,795 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3583 (0.4210) Acc D Real: 57.704% 
Loss D Fake: 0.5020 (0.5259) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 0.9354 (0.9014) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,803 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4104 (0.4208) Acc D Real: 57.755% 
Loss D Fake: 0.5012 (0.5255) Acc D Fake: 100.000% 
Loss D: 0.912 
Loss G: 0.9367 (0.9019) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,810 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3326 (0.4195) Acc D Real: 57.917% 
Loss D Fake: 0.5002 (0.5251) Acc D Fake: 100.000% 
Loss D: 0.833 
Loss G: 0.9382 (0.9025) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,818 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3718 (0.4189) Acc D Real: 57.999% 
Loss D Fake: 0.4992 (0.5248) Acc D Fake: 100.000% 
Loss D: 0.871 
Loss G: 0.9398 (0.9030) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,825 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4641 (0.4195) Acc D Real: 57.939% 
Loss D Fake: 0.4983 (0.5244) Acc D Fake: 100.000% 
Loss D: 0.962 
Loss G: 0.9410 (0.9035) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,832 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3884 (0.4191) Acc D Real: 58.002% 
Loss D Fake: 0.4976 (0.5240) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9422 (0.9041) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,839 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3873 (0.4186) Acc D Real: 58.057% 
Loss D Fake: 0.4968 (0.5237) Acc D Fake: 100.000% 
Loss D: 0.884 
Loss G: 0.9432 (0.9046) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,847 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3309 (0.4174) Acc D Real: 58.207% 
Loss D Fake: 0.4961 (0.5233) Acc D Fake: 100.000% 
Loss D: 0.827 
Loss G: 0.9446 (0.9052) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,854 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3724 (0.4168) Acc D Real: 58.310% 
Loss D Fake: 0.4951 (0.5229) Acc D Fake: 100.000% 
Loss D: 0.868 
Loss G: 0.9465 (0.9057) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,861 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3684 (0.4162) Acc D Real: 58.405% 
Loss D Fake: 0.4938 (0.5225) Acc D Fake: 100.000% 
Loss D: 0.862 
Loss G: 0.9487 (0.9063) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,869 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.5599 (0.4181) Acc D Real: 58.208% 
Loss D Fake: 0.4926 (0.5221) Acc D Fake: 100.000% 
Loss D: 1.052 
Loss G: 0.9499 (0.9068) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,876 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.5646 (0.4200) Acc D Real: 58.022% 
Loss D Fake: 0.4921 (0.5217) Acc D Fake: 100.000% 
Loss D: 1.057 
Loss G: 0.9503 (0.9074) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,884 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2909 (0.4183) Acc D Real: 58.215% 
Loss D Fake: 0.4919 (0.5214) Acc D Fake: 100.000% 
Loss D: 0.783 
Loss G: 0.9512 (0.9079) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,891 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3445 (0.4174) Acc D Real: 58.342% 
Loss D Fake: 0.4911 (0.5210) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 0.9526 (0.9085) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,898 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4089 (0.4173) Acc D Real: 58.376% 
Loss D Fake: 0.4902 (0.5206) Acc D Fake: 100.000% 
Loss D: 0.899 
Loss G: 0.9539 (0.9091) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,906 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4761 (0.4180) Acc D Real: 58.314% 
Loss D Fake: 0.4896 (0.5202) Acc D Fake: 100.000% 
Loss D: 0.966 
Loss G: 0.9546 (0.9096) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,913 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3777 (0.4175) Acc D Real: 58.384% 
Loss D Fake: 0.4892 (0.5199) Acc D Fake: 100.000% 
Loss D: 0.867 
Loss G: 0.9554 (0.9102) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,921 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4023 (0.4173) Acc D Real: 58.408% 
Loss D Fake: 0.4887 (0.5195) Acc D Fake: 100.000% 
Loss D: 0.891 
Loss G: 0.9561 (0.9107) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,928 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3972 (0.4171) Acc D Real: 58.456% 
Loss D Fake: 0.4882 (0.5191) Acc D Fake: 100.000% 
Loss D: 0.885 
Loss G: 0.9570 (0.9113) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,936 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4332 (0.4173) Acc D Real: 58.442% 
Loss D Fake: 0.4877 (0.5188) Acc D Fake: 100.000% 
Loss D: 0.921 
Loss G: 0.9574 (0.9118) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,943 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3750 (0.4168) Acc D Real: 58.505% 
Loss D Fake: 0.4876 (0.5184) Acc D Fake: 100.000% 
Loss D: 0.863 
Loss G: 0.9578 (0.9123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,951 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.5121 (0.4179) Acc D Real: 58.411% 
Loss D Fake: 0.4874 (0.5180) Acc D Fake: 100.000% 
Loss D: 0.999 
Loss G: 0.9580 (0.9128) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,958 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2872 (0.4164) Acc D Real: 58.599% 
Loss D Fake: 0.4871 (0.5177) Acc D Fake: 100.000% 
Loss D: 0.774 
Loss G: 0.9589 (0.9134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,966 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3278 (0.4154) Acc D Real: 58.725% 
Loss D Fake: 0.4864 (0.5173) Acc D Fake: 100.000% 
Loss D: 0.814 
Loss G: 0.9602 (0.9139) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,973 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3034 (0.4142) Acc D Real: 58.870% 
Loss D Fake: 0.4855 (0.5170) Acc D Fake: 100.000% 
Loss D: 0.789 
Loss G: 0.9618 (0.9144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,980 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4648 (0.4148) Acc D Real: 58.836% 
Loss D Fake: 0.4846 (0.5166) Acc D Fake: 100.000% 
Loss D: 0.949 
Loss G: 0.9632 (0.9149) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,988 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4444 (0.4151) Acc D Real: 58.823% 
Loss D Fake: 0.4839 (0.5163) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.9642 (0.9155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:10,995 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4022 (0.4149) Acc D Real: 58.843% 
Loss D Fake: 0.4833 (0.5159) Acc D Fake: 100.000% 
Loss D: 0.886 
Loss G: 0.9652 (0.9160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,004 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4665 (0.4155) Acc D Real: 58.792% 
Loss D Fake: 0.4828 (0.5156) Acc D Fake: 100.000% 
Loss D: 0.949 
Loss G: 0.9658 (0.9165) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,011 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3566 (0.4149) Acc D Real: 58.877% 
Loss D Fake: 0.4825 (0.5152) Acc D Fake: 100.000% 
Loss D: 0.839 
Loss G: 0.9664 (0.9170) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,019 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4097 (0.4148) Acc D Real: 58.906% 
Loss D Fake: 0.4821 (0.5149) Acc D Fake: 100.000% 
Loss D: 0.892 
Loss G: 0.9669 (0.9176) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,027 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3722 (0.4144) Acc D Real: 58.982% 
Loss D Fake: 0.4817 (0.5146) Acc D Fake: 100.000% 
Loss D: 0.854 
Loss G: 0.9678 (0.9181) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,034 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4724 (0.4150) Acc D Real: 58.948% 
Loss D Fake: 0.4812 (0.5142) Acc D Fake: 100.000% 
Loss D: 0.954 
Loss G: 0.9685 (0.9186) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,042 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4497 (0.4153) Acc D Real: 58.923% 
Loss D Fake: 0.4809 (0.5139) Acc D Fake: 100.000% 
Loss D: 0.931 
Loss G: 0.9689 (0.9191) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,049 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.5229 (0.4164) Acc D Real: 58.818% 
Loss D Fake: 0.4808 (0.5136) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9685 (0.9196) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,057 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4578 (0.4168) Acc D Real: 58.791% 
Loss D Fake: 0.4812 (0.5133) Acc D Fake: 100.000% 
Loss D: 0.939 
Loss G: 0.9677 (0.9200) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,065 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2565 (0.4152) Acc D Real: 58.978% 
Loss D Fake: 0.4816 (0.5129) Acc D Fake: 100.000% 
Loss D: 0.738 
Loss G: 0.9675 (0.9205) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,072 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.6553 (0.4175) Acc D Real: 58.749% 
Loss D Fake: 0.4818 (0.5126) Acc D Fake: 100.000% 
Loss D: 1.137 
Loss G: 0.9666 (0.9209) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,080 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3765 (0.4171) Acc D Real: 58.816% 
Loss D Fake: 0.4825 (0.5124) Acc D Fake: 100.000% 
Loss D: 0.859 
Loss G: 0.9657 (0.9214) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,088 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4811 (0.4177) Acc D Real: 58.755% 
Loss D Fake: 0.4829 (0.5121) Acc D Fake: 100.000% 
Loss D: 0.964 
Loss G: 0.9652 (0.9218) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,095 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4527 (0.4181) Acc D Real: 58.738% 
Loss D Fake: 0.4831 (0.5118) Acc D Fake: 100.000% 
Loss D: 0.936 
Loss G: 0.9647 (0.9222) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,102 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3152 (0.4171) Acc D Real: 58.859% 
Loss D Fake: 0.4833 (0.5115) Acc D Fake: 100.000% 
Loss D: 0.798 
Loss G: 0.9651 (0.9226) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,110 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.5022 (0.4179) Acc D Real: 58.794% 
Loss D Fake: 0.4827 (0.5113) Acc D Fake: 100.000% 
Loss D: 0.985 
Loss G: 0.9668 (0.9230) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,117 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4331 (0.4180) Acc D Real: 58.798% 
Loss D Fake: 0.4815 (0.5110) Acc D Fake: 100.000% 
Loss D: 0.915 
Loss G: 0.9687 (0.9234) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,125 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1962 (0.4160) Acc D Real: 59.021% 
Loss D Fake: 0.4803 (0.5107) Acc D Fake: 100.000% 
Loss D: 0.677 
Loss G: 0.9710 (0.9238) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,132 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4595 (0.4164) Acc D Real: 59.000% 
Loss D Fake: 0.4789 (0.5105) Acc D Fake: 100.000% 
Loss D: 0.938 
Loss G: 0.9731 (0.9243) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,139 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3866 (0.4162) Acc D Real: 59.047% 
Loss D Fake: 0.4778 (0.5102) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9747 (0.9247) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,147 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3926 (0.4160) Acc D Real: 59.083% 
Loss D Fake: 0.4769 (0.5099) Acc D Fake: 100.000% 
Loss D: 0.870 
Loss G: 0.9764 (0.9252) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,154 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3745 (0.4156) Acc D Real: 59.136% 
Loss D Fake: 0.4758 (0.5096) Acc D Fake: 100.000% 
Loss D: 0.850 
Loss G: 0.9784 (0.9256) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,161 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4642 (0.4160) Acc D Real: 59.105% 
Loss D Fake: 0.4746 (0.5093) Acc D Fake: 100.000% 
Loss D: 0.939 
Loss G: 0.9804 (0.9261) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,169 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2971 (0.4150) Acc D Real: 59.230% 
Loss D Fake: 0.4734 (0.5090) Acc D Fake: 100.000% 
Loss D: 0.770 
Loss G: 0.9829 (0.9266) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,176 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.5379 (0.4160) Acc D Real: 59.134% 
Loss D Fake: 0.4720 (0.5087) Acc D Fake: 100.000% 
Loss D: 1.010 
Loss G: 0.9848 (0.9271) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,184 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4107 (0.4160) Acc D Real: 59.157% 
Loss D Fake: 0.4711 (0.5083) Acc D Fake: 100.000% 
Loss D: 0.882 
Loss G: 0.9864 (0.9276) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,191 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4029 (0.4159) Acc D Real: 59.172% 
Loss D Fake: 0.4702 (0.5080) Acc D Fake: 100.000% 
Loss D: 0.873 
Loss G: 0.9877 (0.9281) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,198 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3385 (0.4152) Acc D Real: 59.251% 
Loss D Fake: 0.4694 (0.5077) Acc D Fake: 100.000% 
Loss D: 0.808 
Loss G: 0.9891 (0.9286) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,205 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4420 (0.4155) Acc D Real: 59.248% 
Loss D Fake: 0.4687 (0.5074) Acc D Fake: 100.000% 
Loss D: 0.911 
Loss G: 0.9904 (0.9291) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,213 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3004 (0.4145) Acc D Real: 59.363% 
Loss D Fake: 0.4680 (0.5071) Acc D Fake: 100.000% 
Loss D: 0.768 
Loss G: 0.9916 (0.9296) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,220 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2933 (0.4136) Acc D Real: 59.480% 
Loss D Fake: 0.4672 (0.5067) Acc D Fake: 100.000% 
Loss D: 0.760 
Loss G: 0.9931 (0.9301) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,228 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3788 (0.4133) Acc D Real: 59.521% 
Loss D Fake: 0.4664 (0.5064) Acc D Fake: 100.000% 
Loss D: 0.845 
Loss G: 0.9946 (0.9306) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,235 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.5382 (0.4143) Acc D Real: 59.426% 
Loss D Fake: 0.4656 (0.5061) Acc D Fake: 100.000% 
Loss D: 1.004 
Loss G: 0.9954 (0.9312) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,243 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4414 (0.4145) Acc D Real: 59.419% 
Loss D Fake: 0.4653 (0.5058) Acc D Fake: 100.000% 
Loss D: 0.907 
Loss G: 0.9959 (0.9317) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,250 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2513 (0.4132) Acc D Real: 59.569% 
Loss D Fake: 0.4650 (0.5055) Acc D Fake: 100.000% 
Loss D: 0.716 
Loss G: 0.9971 (0.9322) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,257 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3995 (0.4131) Acc D Real: 59.601% 
Loss D Fake: 0.4642 (0.5051) Acc D Fake: 100.000% 
Loss D: 0.864 
Loss G: 0.9984 (0.9327) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,265 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.5013 (0.4138) Acc D Real: 59.538% 
Loss D Fake: 0.4636 (0.5048) Acc D Fake: 100.000% 
Loss D: 0.965 
Loss G: 0.9988 (0.9332) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,272 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4647 (0.4142) Acc D Real: 59.503% 
Loss D Fake: 0.4636 (0.5045) Acc D Fake: 100.000% 
Loss D: 0.928 
Loss G: 0.9989 (0.9337) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,280 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3808 (0.4139) Acc D Real: 59.539% 
Loss D Fake: 0.4634 (0.5042) Acc D Fake: 100.000% 
Loss D: 0.844 
Loss G: 0.9995 (0.9342) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,287 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.5595 (0.4150) Acc D Real: 59.432% 
Loss D Fake: 0.4629 (0.5039) Acc D Fake: 100.000% 
Loss D: 1.022 
Loss G: 1.0006 (0.9347) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,295 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3405 (0.4145) Acc D Real: 59.515% 
Loss D Fake: 0.4623 (0.5036) Acc D Fake: 100.000% 
Loss D: 0.803 
Loss G: 1.0018 (0.9352) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,302 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4448 (0.4147) Acc D Real: 59.505% 
Loss D Fake: 0.4616 (0.5033) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0029 (0.9357) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,309 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.5326 (0.4155) Acc D Real: 59.427% 
Loss D Fake: 0.4611 (0.5029) Acc D Fake: 100.000% 
Loss D: 0.994 
Loss G: 1.0036 (0.9362) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,317 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3963 (0.4154) Acc D Real: 59.455% 
Loss D Fake: 0.4608 (0.5026) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0041 (0.9367) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,324 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2335 (0.4141) Acc D Real: 59.604% 
Loss D Fake: 0.4605 (0.5023) Acc D Fake: 100.000% 
Loss D: 0.694 
Loss G: 1.0051 (0.9372) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,331 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3716 (0.4138) Acc D Real: 59.650% 
Loss D Fake: 0.4598 (0.5020) Acc D Fake: 100.000% 
Loss D: 0.831 
Loss G: 1.0065 (0.9377) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,339 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3648 (0.4134) Acc D Real: 59.704% 
Loss D Fake: 0.4590 (0.5017) Acc D Fake: 100.000% 
Loss D: 0.824 
Loss G: 1.0080 (0.9382) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,346 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3816 (0.4132) Acc D Real: 59.746% 
Loss D Fake: 0.4583 (0.5014) Acc D Fake: 100.000% 
Loss D: 0.840 
Loss G: 1.0089 (0.9387) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,353 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4390 (0.4134) Acc D Real: 59.744% 
Loss D Fake: 0.4579 (0.5011) Acc D Fake: 100.000% 
Loss D: 0.897 
Loss G: 1.0096 (0.9392) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,361 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4692 (0.4138) Acc D Real: 59.720% 
Loss D Fake: 0.4575 (0.5008) Acc D Fake: 100.000% 
Loss D: 0.927 
Loss G: 1.0103 (0.9397) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,368 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2991 (0.4130) Acc D Real: 59.818% 
Loss D Fake: 0.4571 (0.5005) Acc D Fake: 100.000% 
Loss D: 0.756 
Loss G: 1.0111 (0.9402) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,376 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.5042 (0.4136) Acc D Real: 59.761% 
Loss D Fake: 0.4568 (0.5002) Acc D Fake: 100.000% 
Loss D: 0.961 
Loss G: 1.0113 (0.9407) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,384 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4265 (0.4137) Acc D Real: 59.773% 
Loss D Fake: 0.4568 (0.4999) Acc D Fake: 100.000% 
Loss D: 0.883 
Loss G: 1.0114 (0.9412) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,392 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4316 (0.4138) Acc D Real: 59.775% 
Loss D Fake: 0.4567 (0.4996) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 1.0115 (0.9416) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,400 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3288 (0.4132) Acc D Real: 59.847% 
Loss D Fake: 0.4566 (0.4993) Acc D Fake: 100.000% 
Loss D: 0.785 
Loss G: 1.0119 (0.9421) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,407 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3537 (0.4128) Acc D Real: 59.900% 
Loss D Fake: 0.4563 (0.4990) Acc D Fake: 100.000% 
Loss D: 0.810 
Loss G: 1.0125 (0.9426) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,415 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4994 (0.4134) Acc D Real: 59.851% 
Loss D Fake: 0.4561 (0.4987) Acc D Fake: 100.000% 
Loss D: 0.956 
Loss G: 1.0127 (0.9430) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,423 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3890 (0.4133) Acc D Real: 59.882% 
Loss D Fake: 0.4561 (0.4985) Acc D Fake: 100.000% 
Loss D: 0.845 
Loss G: 1.0126 (0.9435) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,430 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4268 (0.4134) Acc D Real: 59.885% 
Loss D Fake: 0.4562 (0.4982) Acc D Fake: 100.000% 
Loss D: 0.883 
Loss G: 1.0121 (0.9440) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,438 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4215 (0.4134) Acc D Real: 59.887% 
Loss D Fake: 0.4565 (0.4979) Acc D Fake: 100.000% 
Loss D: 0.878 
Loss G: 1.0116 (0.9444) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,445 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3538 (0.4130) Acc D Real: 59.934% 
Loss D Fake: 0.4568 (0.4976) Acc D Fake: 100.000% 
Loss D: 0.811 
Loss G: 1.0113 (0.9448) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,452 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4495 (0.4133) Acc D Real: 59.923% 
Loss D Fake: 0.4570 (0.4974) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0106 (0.9453) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,460 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2873 (0.4124) Acc D Real: 60.017% 
Loss D Fake: 0.4573 (0.4971) Acc D Fake: 100.000% 
Loss D: 0.745 
Loss G: 1.0104 (0.9457) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,467 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4211 (0.4125) Acc D Real: 60.017% 
Loss D Fake: 0.4572 (0.4969) Acc D Fake: 100.000% 
Loss D: 0.878 
Loss G: 1.0109 (0.9461) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,474 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3642 (0.4122) Acc D Real: 60.021% 
Loss D Fake: 0.4568 (0.4966) Acc D Fake: 100.000% 
Loss D: 0.821 
Loss G: 1.0122 (0.9465) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:11,720 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.506 | Generator Loss: 1.012 | Avg: 1.518 
2023-03-02 02:00:11,743 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.556 | Generator Loss: 1.012 | Avg: 1.568 
2023-03-02 02:00:11,767 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.543 | Generator Loss: 1.012 | Avg: 1.555 
2023-03-02 02:00:11,794 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.531 | Generator Loss: 1.012 | Avg: 1.543 
2023-03-02 02:00:11,821 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.528 | Generator Loss: 1.012 | Avg: 1.541 
2023-03-02 02:00:11,848 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.613 | Generator Loss: 1.012 | Avg: 1.625 
2023-03-02 02:00:11,874 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.616 | Generator Loss: 1.012 | Avg: 1.628 
2023-03-02 02:00:11,901 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.675 | Generator Loss: 1.012 | Avg: 1.687 
2023-03-02 02:00:11,928 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.693 | Generator Loss: 1.012 | Avg: 1.706 
2023-03-02 02:00:11,955 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.765 | Generator Loss: 1.012 | Avg: 1.778 
2023-03-02 02:00:11,982 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.809 | Generator Loss: 1.012 | Avg: 1.822 
2023-03-02 02:00:12,009 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.864 | Generator Loss: 1.012 | Avg: 1.876 
2023-03-02 02:00:12,036 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.901 | Generator Loss: 1.012 | Avg: 1.913 
2023-03-02 02:00:12,063 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.909 | Generator Loss: 1.012 | Avg: 1.922 
2023-03-02 02:00:12,091 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.886 | Generator Loss: 1.012 | Avg: 1.898 
2023-03-02 02:00:12,117 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.864 | Generator Loss: 1.012 | Avg: 1.876 
2023-03-02 02:00:12,144 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.844 | Generator Loss: 1.012 | Avg: 1.856 
2023-03-02 02:00:12,177 -                train: [    INFO] - 
Epoch: 6/20
2023-03-02 02:00:12,333 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4424 (0.4424) Acc D Real: 58.490% 
Loss D Fake: 0.4553 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.898 
Loss G: 1.0148 (1.0142) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,341 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4752 (0.4533) Acc D Real: 57.153% 
Loss D Fake: 0.4547 (0.4553) Acc D Fake: 100.000% 
Loss D: 0.930 
Loss G: 1.0156 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,348 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4338 (0.4484) Acc D Real: 57.669% 
Loss D Fake: 0.4543 (0.4551) Acc D Fake: 100.000% 
Loss D: 0.888 
Loss G: 1.0164 (1.0151) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,361 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3237 (0.4235) Acc D Real: 60.385% 
Loss D Fake: 0.4539 (0.4548) Acc D Fake: 100.000% 
Loss D: 0.778 
Loss G: 1.0172 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,368 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4305 (0.4247) Acc D Real: 60.391% 
Loss D Fake: 0.4536 (0.4546) Acc D Fake: 100.000% 
Loss D: 0.884 
Loss G: 1.0176 (1.0159) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,375 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.5462 (0.4420) Acc D Real: 58.862% 
Loss D Fake: 0.4535 (0.4545) Acc D Fake: 100.000% 
Loss D: 1.000 
Loss G: 1.0173 (1.0161) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,382 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3948 (0.4361) Acc D Real: 59.460% 
Loss D Fake: 0.4538 (0.4544) Acc D Fake: 100.000% 
Loss D: 0.849 
Loss G: 1.0170 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,389 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4516 (0.4378) Acc D Real: 59.363% 
Loss D Fake: 0.4540 (0.4543) Acc D Fake: 100.000% 
Loss D: 0.906 
Loss G: 1.0165 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,396 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4056 (0.4346) Acc D Real: 59.740% 
Loss D Fake: 0.4543 (0.4543) Acc D Fake: 100.000% 
Loss D: 0.860 
Loss G: 1.0157 (1.0162) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,403 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4599 (0.4369) Acc D Real: 59.531% 
Loss D Fake: 0.4548 (0.4544) Acc D Fake: 100.000% 
Loss D: 0.915 
Loss G: 1.0145 (1.0160) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,410 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4411 (0.4373) Acc D Real: 59.457% 
Loss D Fake: 0.4556 (0.4545) Acc D Fake: 100.000% 
Loss D: 0.897 
Loss G: 1.0132 (1.0158) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,417 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3976 (0.4342) Acc D Real: 59.804% 
Loss D Fake: 0.4563 (0.4546) Acc D Fake: 100.000% 
Loss D: 0.854 
Loss G: 1.0120 (1.0155) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,424 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.5289 (0.4410) Acc D Real: 59.066% 
Loss D Fake: 0.4570 (0.4548) Acc D Fake: 100.000% 
Loss D: 0.986 
Loss G: 1.0103 (1.0151) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,431 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2628 (0.4291) Acc D Real: 60.323% 
Loss D Fake: 0.4579 (0.4550) Acc D Fake: 100.000% 
Loss D: 0.721 
Loss G: 1.0093 (1.0147) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,438 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4228 (0.4287) Acc D Real: 60.312% 
Loss D Fake: 0.4582 (0.4552) Acc D Fake: 100.000% 
Loss D: 0.881 
Loss G: 1.0089 (1.0144) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,447 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3948 (0.4267) Acc D Real: 60.527% 
Loss D Fake: 0.4584 (0.4554) Acc D Fake: 100.000% 
Loss D: 0.853 
Loss G: 1.0086 (1.0140) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,454 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3552 (0.4227) Acc D Real: 60.859% 
Loss D Fake: 0.4586 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.814 
Loss G: 1.0082 (1.0137) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,461 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3995 (0.4215) Acc D Real: 60.962% 
Loss D Fake: 0.4588 (0.4557) Acc D Fake: 100.000% 
Loss D: 0.858 
Loss G: 1.0080 (1.0134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,469 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4600 (0.4234) Acc D Real: 60.776% 
Loss D Fake: 0.4589 (0.4559) Acc D Fake: 100.000% 
Loss D: 0.919 
Loss G: 1.0077 (1.0131) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,476 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3741 (0.4211) Acc D Real: 60.977% 
Loss D Fake: 0.4590 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.833 
Loss G: 1.0078 (1.0129) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,483 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3649 (0.4185) Acc D Real: 61.219% 
Loss D Fake: 0.4588 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.824 
Loss G: 1.0084 (1.0127) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,490 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3039 (0.4135) Acc D Real: 61.825% 
Loss D Fake: 0.4584 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.762 
Loss G: 1.0095 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,497 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3839 (0.4123) Acc D Real: 61.940% 
Loss D Fake: 0.4578 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.842 
Loss G: 1.0106 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,504 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.5001 (0.4158) Acc D Real: 61.577% 
Loss D Fake: 0.4572 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.957 
Loss G: 1.0113 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,510 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3954 (0.4150) Acc D Real: 61.679% 
Loss D Fake: 0.4570 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.852 
Loss G: 1.0116 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,517 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4146 (0.4150) Acc D Real: 61.651% 
Loss D Fake: 0.4568 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.871 
Loss G: 1.0119 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,525 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4532 (0.4164) Acc D Real: 61.483% 
Loss D Fake: 0.4567 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.910 
Loss G: 1.0122 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,532 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2720 (0.4114) Acc D Real: 62.010% 
Loss D Fake: 0.4565 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.729 
Loss G: 1.0126 (1.0123) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,540 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3799 (0.4104) Acc D Real: 62.087% 
Loss D Fake: 0.4563 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.836 
Loss G: 1.0132 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,547 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2719 (0.4059) Acc D Real: 62.544% 
Loss D Fake: 0.4559 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.728 
Loss G: 1.0141 (1.0124) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,554 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5178 (0.4094) Acc D Real: 62.215% 
Loss D Fake: 0.4554 (0.4564) Acc D Fake: 100.000% 
Loss D: 0.973 
Loss G: 1.0147 (1.0125) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,562 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4679 (0.4112) Acc D Real: 62.025% 
Loss D Fake: 0.4552 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.923 
Loss G: 1.0150 (1.0126) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,569 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.1279 (0.4028) Acc D Real: 62.917% 
Loss D Fake: 0.4549 (0.4563) Acc D Fake: 100.000% 
Loss D: 0.583 
Loss G: 1.0161 (1.0127) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,577 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3065 (0.4001) Acc D Real: 63.205% 
Loss D Fake: 0.4542 (0.4562) Acc D Fake: 100.000% 
Loss D: 0.761 
Loss G: 1.0176 (1.0128) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,584 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.5642 (0.4046) Acc D Real: 62.713% 
Loss D Fake: 0.4535 (0.4562) Acc D Fake: 100.000% 
Loss D: 1.018 
Loss G: 1.0185 (1.0130) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,592 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3912 (0.4043) Acc D Real: 62.739% 
Loss D Fake: 0.4531 (0.4561) Acc D Fake: 100.000% 
Loss D: 0.844 
Loss G: 1.0194 (1.0132) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,599 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3663 (0.4033) Acc D Real: 62.834% 
Loss D Fake: 0.4525 (0.4560) Acc D Fake: 100.000% 
Loss D: 0.819 
Loss G: 1.0206 (1.0134) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,607 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5170 (0.4062) Acc D Real: 62.519% 
Loss D Fake: 0.4520 (0.4559) Acc D Fake: 100.000% 
Loss D: 0.969 
Loss G: 1.0212 (1.0136) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,614 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3739 (0.4054) Acc D Real: 62.617% 
Loss D Fake: 0.4517 (0.4558) Acc D Fake: 100.000% 
Loss D: 0.826 
Loss G: 1.0218 (1.0138) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,621 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4581 (0.4067) Acc D Real: 62.497% 
Loss D Fake: 0.4515 (0.4557) Acc D Fake: 100.000% 
Loss D: 0.910 
Loss G: 1.0222 (1.0140) Acc G: 0.000% 
LR: 2.000e-04 

2023-03-02 02:00:12,629 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4059 (0.4066) Acc D Real: 62.521% 
Loss D Fake: 0.4513 (0.4556) Acc D Fake: 100.000% 
Loss D: 0.857 
Loss G: 1.0227 (1.0142) Acc G: 0.040% 
LR: 2.000e-04 

2023-03-02 02:00:12,636 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4479 (0.4076) Acc D Real: 62.402% 
Loss D Fake: 0.4509 (0.4555) Acc D Fake: 99.961% 
Loss D: 0.899 
Loss G: 1.0237 (1.0144) Acc G: 0.078% 
LR: 2.000e-04 

2023-03-02 02:00:12,644 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3377 (0.4060) Acc D Real: 62.577% 
Loss D Fake: 0.4502 (0.4553) Acc D Fake: 99.924% 
Loss D: 0.788 
Loss G: 1.0256 (1.0146) Acc G: 0.114% 
LR: 2.000e-04 

2023-03-02 02:00:12,651 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4346 (0.4067) Acc D Real: 62.477% 
Loss D Fake: 0.4490 (0.4552) Acc D Fake: 99.889% 
Loss D: 0.884 
Loss G: 1.0287 (1.0150) Acc G: 0.148% 
LR: 2.000e-04 

2023-03-02 02:00:12,659 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4855 (0.4084) Acc D Real: 62.275% 
Loss D Fake: 0.4471 (0.4550) Acc D Fake: 99.819% 
Loss D: 0.933 
Loss G: 1.0325 (1.0153) Acc G: 0.217% 
LR: 2.000e-04 

2023-03-02 02:00:12,666 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4528 (0.4093) Acc D Real: 62.160% 
Loss D Fake: 0.4451 (0.4548) Acc D Fake: 99.752% 
Loss D: 0.898 
Loss G: 1.0362 (1.0158) Acc G: 0.284% 
LR: 2.000e-04 

2023-03-02 02:00:12,673 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4487 (0.4101) Acc D Real: 62.067% 
Loss D Fake: 0.4433 (0.4546) Acc D Fake: 99.688% 
Loss D: 0.892 
Loss G: 1.0388 (1.0163) Acc G: 0.347% 
LR: 2.000e-04 

2023-03-02 02:00:12,681 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4840 (0.4116) Acc D Real: 61.880% 
Loss D Fake: 0.4424 (0.4543) Acc D Fake: 99.626% 
Loss D: 0.926 
Loss G: 1.0398 (1.0167) Acc G: 0.408% 
LR: 2.000e-04 

2023-03-02 02:00:12,688 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3388 (0.4102) Acc D Real: 62.033% 
Loss D Fake: 0.4421 (0.4541) Acc D Fake: 99.567% 
Loss D: 0.781 
Loss G: 1.0402 (1.0172) Acc G: 0.467% 
LR: 2.000e-04 

2023-03-02 02:00:12,696 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4001 (0.4100) Acc D Real: 62.042% 
Loss D Fake: 0.4421 (0.4538) Acc D Fake: 99.510% 
Loss D: 0.842 
Loss G: 1.0402 (1.0177) Acc G: 0.523% 
LR: 2.000e-04 

2023-03-02 02:00:12,703 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4325 (0.4104) Acc D Real: 61.988% 
Loss D Fake: 0.4422 (0.4536) Acc D Fake: 99.455% 
Loss D: 0.875 
Loss G: 1.0399 (1.0181) Acc G: 0.577% 
LR: 2.000e-04 

2023-03-02 02:00:12,711 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4908 (0.4119) Acc D Real: 61.825% 
Loss D Fake: 0.4424 (0.4534) Acc D Fake: 99.403% 
Loss D: 0.933 
Loss G: 1.0395 (1.0185) Acc G: 0.629% 
LR: 2.000e-04 

2023-03-02 02:00:12,718 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4044 (0.4118) Acc D Real: 61.851% 
Loss D Fake: 0.4427 (0.4532) Acc D Fake: 99.352% 
Loss D: 0.847 
Loss G: 1.0390 (1.0189) Acc G: 0.679% 
LR: 2.000e-04 

2023-03-02 02:00:12,725 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.5489 (0.4143) Acc D Real: 61.573% 
Loss D Fake: 0.4430 (0.4530) Acc D Fake: 99.303% 
Loss D: 0.992 
Loss G: 1.0382 (1.0192) Acc G: 0.727% 
LR: 2.000e-04 

2023-03-02 02:00:12,733 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4687 (0.4153) Acc D Real: 61.485% 
Loss D Fake: 0.4434 (0.4528) Acc D Fake: 99.256% 
Loss D: 0.912 
Loss G: 1.0373 (1.0196) Acc G: 0.774% 
LR: 2.000e-04 

2023-03-02 02:00:12,740 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4751 (0.4163) Acc D Real: 61.385% 
Loss D Fake: 0.4440 (0.4527) Acc D Fake: 99.211% 
Loss D: 0.919 
Loss G: 1.0364 (1.0198) Acc G: 0.819% 
LR: 2.000e-04 

2023-03-02 02:00:12,748 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4506 (0.4169) Acc D Real: 61.302% 
Loss D Fake: 0.4445 (0.4526) Acc D Fake: 99.167% 
Loss D: 0.895 
Loss G: 1.0354 (1.0201) Acc G: 0.862% 
LR: 2.000e-04 

2023-03-02 02:00:12,755 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4161 (0.4169) Acc D Real: 61.299% 
Loss D Fake: 0.4450 (0.4524) Acc D Fake: 99.124% 
Loss D: 0.861 
Loss G: 1.0345 (1.0204) Acc G: 0.904% 
LR: 2.000e-04 

2023-03-02 02:00:12,764 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2479 (0.4141) Acc D Real: 61.573% 
Loss D Fake: 0.4455 (0.4523) Acc D Fake: 99.083% 
Loss D: 0.693 
Loss G: 1.0338 (1.0206) Acc G: 0.944% 
LR: 2.000e-04 

2023-03-02 02:00:12,773 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3707 (0.4134) Acc D Real: 61.650% 
Loss D Fake: 0.4458 (0.4522) Acc D Fake: 99.044% 
Loss D: 0.817 
Loss G: 1.0331 (1.0208) Acc G: 0.984% 
LR: 2.000e-04 

2023-03-02 02:00:12,781 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3330 (0.4121) Acc D Real: 61.773% 
Loss D Fake: 0.4462 (0.4521) Acc D Fake: 99.005% 
Loss D: 0.779 
Loss G: 1.0325 (1.0210) Acc G: 1.022% 
LR: 2.000e-04 

2023-03-02 02:00:12,789 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2218 (0.4090) Acc D Real: 62.101% 
Loss D Fake: 0.4465 (0.4520) Acc D Fake: 98.968% 
Loss D: 0.668 
Loss G: 1.0321 (1.0212) Acc G: 1.058% 
LR: 2.000e-04 

2023-03-02 02:00:12,796 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3454 (0.4080) Acc D Real: 62.198% 
Loss D Fake: 0.4467 (0.4519) Acc D Fake: 98.932% 
Loss D: 0.792 
Loss G: 1.0318 (1.0213) Acc G: 1.094% 
LR: 2.000e-04 

2023-03-02 02:00:12,805 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3845 (0.4077) Acc D Real: 62.221% 
Loss D Fake: 0.4469 (0.4519) Acc D Fake: 98.897% 
Loss D: 0.831 
Loss G: 1.0313 (1.0215) Acc G: 1.128% 
LR: 2.000e-04 

2023-03-02 02:00:12,813 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3686 (0.4071) Acc D Real: 62.277% 
Loss D Fake: 0.4472 (0.4518) Acc D Fake: 98.864% 
Loss D: 0.816 
Loss G: 1.0308 (1.0216) Acc G: 1.162% 
LR: 2.000e-04 

2023-03-02 02:00:12,821 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4810 (0.4082) Acc D Real: 62.138% 
Loss D Fake: 0.4475 (0.4517) Acc D Fake: 98.831% 
Loss D: 0.929 
Loss G: 1.0302 (1.0217) Acc G: 1.194% 
LR: 2.000e-04 

2023-03-02 02:00:12,829 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3755 (0.4077) Acc D Real: 62.174% 
Loss D Fake: 0.4479 (0.4517) Acc D Fake: 98.799% 
Loss D: 0.823 
Loss G: 1.0295 (1.0219) Acc G: 1.225% 
LR: 2.000e-04 

2023-03-02 02:00:12,837 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4693 (0.4086) Acc D Real: 62.062% 
Loss D Fake: 0.4483 (0.4516) Acc D Fake: 98.768% 
Loss D: 0.918 
Loss G: 1.0287 (1.0220) Acc G: 1.256% 
LR: 2.000e-04 

2023-03-02 02:00:12,845 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3408 (0.4076) Acc D Real: 62.150% 
Loss D Fake: 0.4488 (0.4516) Acc D Fake: 98.738% 
Loss D: 0.790 
Loss G: 1.0280 (1.0220) Acc G: 1.286% 
LR: 2.000e-04 

2023-03-02 02:00:12,853 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4139 (0.4077) Acc D Real: 62.128% 
Loss D Fake: 0.4492 (0.4515) Acc D Fake: 98.709% 
Loss D: 0.863 
Loss G: 1.0272 (1.0221) Acc G: 1.315% 
LR: 2.000e-04 

2023-03-02 02:00:12,860 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4772 (0.4087) Acc D Real: 61.976% 
Loss D Fake: 0.4497 (0.4515) Acc D Fake: 98.681% 
Loss D: 0.927 
Loss G: 1.0263 (1.0222) Acc G: 1.343% 
LR: 2.000e-04 

2023-03-02 02:00:12,868 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3508 (0.4079) Acc D Real: 62.040% 
Loss D Fake: 0.4502 (0.4515) Acc D Fake: 98.630% 
Loss D: 0.801 
Loss G: 1.0254 (1.0222) Acc G: 1.393% 
LR: 2.000e-04 

2023-03-02 02:00:12,875 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3603 (0.4073) Acc D Real: 62.088% 
Loss D Fake: 0.4507 (0.4515) Acc D Fake: 98.581% 
Loss D: 0.811 
Loss G: 1.0246 (1.0223) Acc G: 1.441% 
LR: 2.000e-04 

2023-03-02 02:00:12,883 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4021 (0.4072) Acc D Real: 62.071% 
Loss D Fake: 0.4511 (0.4515) Acc D Fake: 98.533% 
Loss D: 0.853 
Loss G: 1.0238 (1.0223) Acc G: 1.489% 
LR: 2.000e-04 

2023-03-02 02:00:12,890 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3712 (0.4067) Acc D Real: 62.111% 
Loss D Fake: 0.4516 (0.4515) Acc D Fake: 98.487% 
Loss D: 0.823 
Loss G: 1.0230 (1.0223) Acc G: 1.535% 
LR: 2.000e-04 

2023-03-02 02:00:12,898 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2694 (0.4049) Acc D Real: 62.282% 
Loss D Fake: 0.4520 (0.4515) Acc D Fake: 98.442% 
Loss D: 0.721 
Loss G: 1.0223 (1.0223) Acc G: 1.580% 
LR: 2.000e-04 

2023-03-02 02:00:12,906 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4234 (0.4052) Acc D Real: 62.230% 
Loss D Fake: 0.4524 (0.4515) Acc D Fake: 98.397% 
Loss D: 0.876 
Loss G: 1.0215 (1.0223) Acc G: 1.624% 
LR: 2.000e-04 

2023-03-02 02:00:12,913 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3620 (0.4046) Acc D Real: 62.269% 
Loss D Fake: 0.4528 (0.4515) Acc D Fake: 98.354% 
Loss D: 0.815 
Loss G: 1.0209 (1.0223) Acc G: 1.667% 
LR: 2.000e-04 

2023-03-02 02:00:12,921 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4404 (0.4051) Acc D Real: 62.164% 
Loss D Fake: 0.4532 (0.4515) Acc D Fake: 98.312% 
Loss D: 0.894 
Loss G: 1.0201 (1.0222) Acc G: 1.708% 
LR: 2.000e-04 

2023-03-02 02:00:12,928 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3435 (0.4043) Acc D Real: 62.204% 
Loss D Fake: 0.4536 (0.4516) Acc D Fake: 98.272% 
Loss D: 0.797 
Loss G: 1.0194 (1.0222) Acc G: 1.749% 
LR: 2.000e-04 

2023-03-02 02:00:12,936 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3437 (0.4036) Acc D Real: 62.276% 
Loss D Fake: 0.4540 (0.4516) Acc D Fake: 98.232% 
Loss D: 0.798 
Loss G: 1.0188 (1.0221) Acc G: 1.789% 
LR: 2.000e-04 

2023-03-02 02:00:12,943 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3448 (0.4029) Acc D Real: 62.327% 
Loss D Fake: 0.4544 (0.4516) Acc D Fake: 98.193% 
Loss D: 0.799 
Loss G: 1.0183 (1.0221) Acc G: 1.827% 
LR: 2.000e-04 

2023-03-02 02:00:12,950 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3041 (0.4017) Acc D Real: 62.414% 
Loss D Fake: 0.4546 (0.4517) Acc D Fake: 98.155% 
Loss D: 0.759 
Loss G: 1.0179 (1.0221) Acc G: 1.865% 
LR: 2.000e-04 

2023-03-02 02:00:12,958 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4044 (0.4017) Acc D Real: 62.377% 
Loss D Fake: 0.4548 (0.4517) Acc D Fake: 98.118% 
Loss D: 0.859 
Loss G: 1.0177 (1.0220) Acc G: 1.902% 
LR: 2.000e-04 

2023-03-02 02:00:12,965 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3165 (0.4007) Acc D Real: 62.452% 
Loss D Fake: 0.4550 (0.4517) Acc D Fake: 98.081% 
Loss D: 0.771 
Loss G: 1.0174 (1.0219) Acc G: 1.938% 
LR: 2.000e-04 

2023-03-02 02:00:12,973 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3497 (0.4001) Acc D Real: 62.489% 
Loss D Fake: 0.4551 (0.4518) Acc D Fake: 98.046% 
Loss D: 0.805 
Loss G: 1.0172 (1.0219) Acc G: 1.973% 
LR: 2.000e-04 

2023-03-02 02:00:12,980 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3662 (0.3998) Acc D Real: 62.507% 
Loss D Fake: 0.4553 (0.4518) Acc D Fake: 98.011% 
Loss D: 0.821 
Loss G: 1.0170 (1.0218) Acc G: 2.008% 
LR: 2.000e-04 

2023-03-02 02:00:12,987 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4186 (0.4000) Acc D Real: 62.451% 
Loss D Fake: 0.4554 (0.4519) Acc D Fake: 97.978% 
Loss D: 0.874 
Loss G: 1.0166 (1.0218) Acc G: 2.041% 
LR: 2.000e-04 

2023-03-02 02:00:12,995 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2225 (0.3980) Acc D Real: 62.643% 
Loss D Fake: 0.4557 (0.4519) Acc D Fake: 97.944% 
Loss D: 0.678 
Loss G: 1.0162 (1.0217) Acc G: 2.074% 
LR: 2.000e-04 

2023-03-02 02:00:13,002 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2580 (0.3965) Acc D Real: 62.774% 
Loss D Fake: 0.4559 (0.4519) Acc D Fake: 97.912% 
Loss D: 0.714 
Loss G: 1.0161 (1.0217) Acc G: 2.106% 
LR: 2.000e-04 

2023-03-02 02:00:13,009 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.5183 (0.3978) Acc D Real: 62.543% 
Loss D Fake: 0.4560 (0.4520) Acc D Fake: 97.880% 
Loss D: 0.974 
Loss G: 1.0155 (1.0216) Acc G: 2.138% 
LR: 2.000e-04 

2023-03-02 02:00:13,017 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4147 (0.3980) Acc D Real: 62.498% 
Loss D Fake: 0.4565 (0.4520) Acc D Fake: 97.849% 
Loss D: 0.871 
Loss G: 1.0147 (1.0215) Acc G: 2.168% 
LR: 2.000e-04 

2023-03-02 02:00:13,024 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2245 (0.3961) Acc D Real: 62.698% 
Loss D Fake: 0.4569 (0.4521) Acc D Fake: 97.819% 
Loss D: 0.681 
Loss G: 1.0143 (1.0214) Acc G: 2.199% 
LR: 2.000e-04 

2023-03-02 02:00:13,032 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3669 (0.3958) Acc D Real: 62.711% 
Loss D Fake: 0.4571 (0.4521) Acc D Fake: 97.789% 
Loss D: 0.824 
Loss G: 1.0139 (1.0214) Acc G: 2.228% 
LR: 2.000e-04 

2023-03-02 02:00:13,039 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2687 (0.3945) Acc D Real: 62.841% 
Loss D Fake: 0.4573 (0.4522) Acc D Fake: 97.760% 
Loss D: 0.726 
Loss G: 1.0138 (1.0213) Acc G: 2.257% 
LR: 2.000e-04 

2023-03-02 02:00:13,047 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.5208 (0.3958) Acc D Real: 62.649% 
Loss D Fake: 0.4574 (0.4522) Acc D Fake: 97.732% 
Loss D: 0.978 
Loss G: 1.0134 (1.0212) Acc G: 2.285% 
LR: 2.000e-04 

2023-03-02 02:00:13,054 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3530 (0.3953) Acc D Real: 62.658% 
Loss D Fake: 0.4577 (0.4523) Acc D Fake: 97.704% 
Loss D: 0.811 
Loss G: 1.0130 (1.0211) Acc G: 2.313% 
LR: 2.000e-04 

2023-03-02 02:00:13,061 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3136 (0.3945) Acc D Real: 62.721% 
Loss D Fake: 0.4579 (0.4524) Acc D Fake: 97.677% 
Loss D: 0.771 
Loss G: 1.0128 (1.0210) Acc G: 2.340% 
LR: 2.000e-04 

2023-03-02 02:00:13,069 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3554 (0.3941) Acc D Real: 62.742% 
Loss D Fake: 0.4580 (0.4524) Acc D Fake: 97.650% 
Loss D: 0.813 
Loss G: 1.0126 (1.0209) Acc G: 2.367% 
LR: 2.000e-04 

2023-03-02 02:00:13,076 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3736 (0.3939) Acc D Real: 62.748% 
Loss D Fake: 0.4582 (0.4525) Acc D Fake: 97.624% 
Loss D: 0.832 
Loss G: 1.0123 (1.0209) Acc G: 2.393% 
LR: 2.000e-04 

2023-03-02 02:00:13,084 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4293 (0.3943) Acc D Real: 62.689% 
Loss D Fake: 0.4585 (0.4525) Acc D Fake: 97.598% 
Loss D: 0.888 
Loss G: 1.0115 (1.0208) Acc G: 2.418% 
LR: 2.000e-04 

2023-03-02 02:00:13,091 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4232 (0.3946) Acc D Real: 62.645% 
Loss D Fake: 0.4590 (0.4526) Acc D Fake: 97.573% 
Loss D: 0.882 
Loss G: 1.0107 (1.0207) Acc G: 2.443% 
LR: 2.000e-04 

2023-03-02 02:00:13,098 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3843 (0.3945) Acc D Real: 62.641% 
Loss D Fake: 0.4594 (0.4527) Acc D Fake: 97.548% 
Loss D: 0.844 
Loss G: 1.0100 (1.0206) Acc G: 2.468% 
LR: 2.000e-04 

2023-03-02 02:00:13,106 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3815 (0.3943) Acc D Real: 62.628% 
Loss D Fake: 0.4598 (0.4527) Acc D Fake: 97.524% 
Loss D: 0.841 
Loss G: 1.0093 (1.0205) Acc G: 2.492% 
LR: 2.000e-04 

2023-03-02 02:00:13,113 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3947 (0.3943) Acc D Real: 62.629% 
Loss D Fake: 0.4603 (0.4528) Acc D Fake: 97.500% 
Loss D: 0.855 
Loss G: 1.0083 (1.0203) Acc G: 2.516% 
LR: 2.000e-04 

2023-03-02 02:00:13,121 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3807 (0.3942) Acc D Real: 62.742% 
Loss D Fake: 0.4610 (0.4529) Acc D Fake: 97.477% 
Loss D: 0.842 
Loss G: 1.0071 (1.0202) Acc G: 2.539% 
LR: 2.000e-04 

2023-03-02 02:00:13,128 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3202 (0.3935) Acc D Real: 62.872% 
Loss D Fake: 0.4617 (0.4530) Acc D Fake: 97.454% 
Loss D: 0.782 
Loss G: 1.0059 (1.0201) Acc G: 2.562% 
LR: 2.000e-04 

2023-03-02 02:00:13,136 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3915 (0.3935) Acc D Real: 62.970% 
Loss D Fake: 0.4624 (0.4530) Acc D Fake: 97.431% 
Loss D: 0.854 
Loss G: 1.0045 (1.0199) Acc G: 2.591% 
LR: 2.000e-04 

2023-03-02 02:00:13,143 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3721 (0.3933) Acc D Real: 63.028% 
Loss D Fake: 0.4639 (0.4531) Acc D Fake: 97.394% 
Loss D: 0.836 
Loss G: 0.9981 (1.0197) Acc G: 2.628% 
LR: 2.000e-04 

2023-03-02 02:00:13,151 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4477 (0.3938) Acc D Real: 63.063% 
Loss D Fake: 0.4707 (0.4533) Acc D Fake: 97.357% 
Loss D: 0.918 
Loss G: 0.9826 (1.0194) Acc G: 2.664% 
LR: 2.000e-04 

2023-03-02 02:00:13,158 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3957 (0.3938) Acc D Real: 63.107% 
Loss D Fake: 0.4824 (0.4536) Acc D Fake: 97.321% 
Loss D: 0.878 
Loss G: 0.9778 (1.0190) Acc G: 2.700% 
LR: 2.000e-04 

2023-03-02 02:00:13,165 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3479 (0.3934) Acc D Real: 63.253% 
Loss D Fake: 0.4754 (0.4538) Acc D Fake: 97.286% 
Loss D: 0.823 
Loss G: 0.9922 (1.0188) Acc G: 2.735% 
LR: 2.000e-04 

2023-03-02 02:00:13,173 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3796 (0.3933) Acc D Real: 63.374% 
Loss D Fake: 0.4670 (0.4539) Acc D Fake: 97.251% 
Loss D: 0.847 
Loss G: 1.0003 (1.0186) Acc G: 2.770% 
LR: 2.000e-04 

2023-03-02 02:00:13,180 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4295 (0.3936) Acc D Real: 63.393% 
Loss D Fake: 0.4643 (0.4540) Acc D Fake: 97.217% 
Loss D: 0.894 
Loss G: 1.0034 (1.0185) Acc G: 2.803% 
LR: 2.000e-04 

2023-03-02 02:00:13,188 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3144 (0.3929) Acc D Real: 63.524% 
Loss D Fake: 0.4632 (0.4540) Acc D Fake: 97.184% 
Loss D: 0.778 
Loss G: 1.0048 (1.0184) Acc G: 2.837% 
LR: 2.000e-04 

2023-03-02 02:00:13,195 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4128 (0.3931) Acc D Real: 63.555% 
Loss D Fake: 0.4627 (0.4541) Acc D Fake: 97.151% 
Loss D: 0.875 
Loss G: 1.0054 (1.0183) Acc G: 2.869% 
LR: 2.000e-04 

2023-03-02 02:00:13,203 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4745 (0.3938) Acc D Real: 63.524% 
Loss D Fake: 0.4625 (0.4542) Acc D Fake: 97.119% 
Loss D: 0.937 
Loss G: 1.0056 (1.0182) Acc G: 2.902% 
LR: 2.000e-04 

2023-03-02 02:00:13,210 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3490 (0.3934) Acc D Real: 63.609% 
Loss D Fake: 0.4625 (0.4543) Acc D Fake: 97.087% 
Loss D: 0.812 
Loss G: 1.0056 (1.0181) Acc G: 2.933% 
LR: 2.000e-04 

2023-03-02 02:00:13,217 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2247 (0.3920) Acc D Real: 63.819% 
Loss D Fake: 0.4626 (0.4543) Acc D Fake: 97.056% 
Loss D: 0.687 
Loss G: 1.0057 (1.0180) Acc G: 2.964% 
LR: 2.000e-04 

2023-03-02 02:00:13,225 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2774 (0.3911) Acc D Real: 63.954% 
Loss D Fake: 0.4626 (0.4544) Acc D Fake: 97.025% 
Loss D: 0.740 
Loss G: 1.0057 (1.0179) Acc G: 2.995% 
LR: 2.000e-04 

2023-03-02 02:00:13,232 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4049 (0.3912) Acc D Real: 63.995% 
Loss D Fake: 0.4627 (0.4545) Acc D Fake: 96.995% 
Loss D: 0.868 
Loss G: 1.0056 (1.0178) Acc G: 3.025% 
LR: 2.000e-04 

2023-03-02 02:00:13,240 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4271 (0.3915) Acc D Real: 63.983% 
Loss D Fake: 0.4628 (0.4545) Acc D Fake: 96.965% 
Loss D: 0.890 
Loss G: 1.0054 (1.0177) Acc G: 3.055% 
LR: 2.000e-04 

2023-03-02 02:00:13,248 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3056 (0.3908) Acc D Real: 64.086% 
Loss D Fake: 0.4630 (0.4546) Acc D Fake: 96.935% 
Loss D: 0.769 
Loss G: 1.0050 (1.0176) Acc G: 3.084% 
LR: 2.000e-04 

2023-03-02 02:00:13,255 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3390 (0.3904) Acc D Real: 64.183% 
Loss D Fake: 0.4632 (0.4547) Acc D Fake: 96.907% 
Loss D: 0.802 
Loss G: 1.0047 (1.0175) Acc G: 3.112% 
LR: 2.000e-04 

2023-03-02 02:00:13,263 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4288 (0.3907) Acc D Real: 64.195% 
Loss D Fake: 0.4635 (0.4547) Acc D Fake: 96.878% 
Loss D: 0.892 
Loss G: 1.0042 (1.0174) Acc G: 3.141% 
LR: 2.000e-04 

2023-03-02 02:00:13,270 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3952 (0.3907) Acc D Real: 64.220% 
Loss D Fake: 0.4638 (0.4548) Acc D Fake: 96.850% 
Loss D: 0.859 
Loss G: 1.0037 (1.0172) Acc G: 3.168% 
LR: 2.000e-04 

2023-03-02 02:00:13,278 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3459 (0.3903) Acc D Real: 64.312% 
Loss D Fake: 0.4642 (0.4549) Acc D Fake: 96.823% 
Loss D: 0.810 
Loss G: 1.0031 (1.0171) Acc G: 3.196% 
LR: 2.000e-04 

2023-03-02 02:00:13,285 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4903 (0.3911) Acc D Real: 64.276% 
Loss D Fake: 0.4646 (0.4550) Acc D Fake: 96.796% 
Loss D: 0.955 
Loss G: 1.0023 (1.0170) Acc G: 3.223% 
LR: 2.000e-04 

2023-03-02 02:00:13,293 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4500 (0.3916) Acc D Real: 64.224% 
Loss D Fake: 0.4651 (0.4550) Acc D Fake: 96.769% 
Loss D: 0.915 
Loss G: 1.0014 (1.0169) Acc G: 3.249% 
LR: 2.000e-04 

2023-03-02 02:00:13,300 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3272 (0.3911) Acc D Real: 64.319% 
Loss D Fake: 0.4656 (0.4551) Acc D Fake: 96.743% 
Loss D: 0.793 
Loss G: 1.0006 (1.0168) Acc G: 3.275% 
LR: 2.000e-04 

2023-03-02 02:00:13,308 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4261 (0.3913) Acc D Real: 64.356% 
Loss D Fake: 0.4662 (0.4552) Acc D Fake: 96.717% 
Loss D: 0.892 
Loss G: 0.9997 (1.0166) Acc G: 3.301% 
LR: 2.000e-04 

2023-03-02 02:00:13,315 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3141 (0.3908) Acc D Real: 64.451% 
Loss D Fake: 0.4667 (0.4553) Acc D Fake: 96.692% 
Loss D: 0.781 
Loss G: 0.9988 (1.0165) Acc G: 3.326% 
LR: 2.000e-04 

2023-03-02 02:00:13,323 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3663 (0.3906) Acc D Real: 64.524% 
Loss D Fake: 0.4672 (0.4554) Acc D Fake: 96.667% 
Loss D: 0.834 
Loss G: 0.9980 (1.0164) Acc G: 3.351% 
LR: 2.000e-04 

2023-03-02 02:00:13,330 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.2467 (0.3895) Acc D Real: 64.689% 
Loss D Fake: 0.4677 (0.4555) Acc D Fake: 96.642% 
Loss D: 0.714 
Loss G: 0.9974 (1.0162) Acc G: 3.376% 
LR: 2.000e-04 

2023-03-02 02:00:13,338 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3412 (0.3892) Acc D Real: 64.785% 
Loss D Fake: 0.4681 (0.4556) Acc D Fake: 96.618% 
Loss D: 0.809 
Loss G: 0.9968 (1.0161) Acc G: 3.400% 
LR: 2.000e-04 

2023-03-02 02:00:13,345 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2666 (0.3883) Acc D Real: 64.880% 
Loss D Fake: 0.4684 (0.4557) Acc D Fake: 96.594% 
Loss D: 0.735 
Loss G: 0.9963 (1.0159) Acc G: 3.424% 
LR: 2.000e-04 

2023-03-02 02:00:13,352 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3291 (0.3878) Acc D Real: 64.959% 
Loss D Fake: 0.4687 (0.4557) Acc D Fake: 96.570% 
Loss D: 0.798 
Loss G: 0.9959 (1.0158) Acc G: 3.447% 
LR: 2.000e-04 

2023-03-02 02:00:13,360 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3825 (0.3878) Acc D Real: 65.011% 
Loss D Fake: 0.4690 (0.4558) Acc D Fake: 96.547% 
Loss D: 0.852 
Loss G: 0.9954 (1.0157) Acc G: 3.470% 
LR: 2.000e-04 

2023-03-02 02:00:13,367 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3233 (0.3873) Acc D Real: 65.092% 
Loss D Fake: 0.4694 (0.4559) Acc D Fake: 96.524% 
Loss D: 0.793 
Loss G: 0.9950 (1.0155) Acc G: 3.493% 
LR: 2.000e-04 

2023-03-02 02:00:13,375 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3457 (0.3870) Acc D Real: 65.189% 
Loss D Fake: 0.4697 (0.4560) Acc D Fake: 96.489% 
Loss D: 0.815 
Loss G: 0.9945 (1.0154) Acc G: 3.528% 
LR: 2.000e-04 

2023-03-02 02:00:13,382 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2032 (0.3857) Acc D Real: 65.358% 
Loss D Fake: 0.4700 (0.4561) Acc D Fake: 96.455% 
Loss D: 0.673 
Loss G: 0.9941 (1.0152) Acc G: 3.561% 
LR: 2.000e-04 

2023-03-02 02:00:13,390 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3647 (0.3856) Acc D Real: 65.404% 
Loss D Fake: 0.4703 (0.4562) Acc D Fake: 96.422% 
Loss D: 0.835 
Loss G: 0.9937 (1.0151) Acc G: 3.595% 
LR: 2.000e-04 

2023-03-02 02:00:13,397 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3469 (0.3853) Acc D Real: 65.484% 
Loss D Fake: 0.4706 (0.4563) Acc D Fake: 96.389% 
Loss D: 0.818 
Loss G: 0.9932 (1.0149) Acc G: 3.628% 
LR: 2.000e-04 

2023-03-02 02:00:13,405 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2109 (0.3841) Acc D Real: 65.613% 
Loss D Fake: 0.4710 (0.4564) Acc D Fake: 96.356% 
Loss D: 0.682 
Loss G: 0.9927 (1.0148) Acc G: 3.660% 
LR: 2.000e-04 

2023-03-02 02:00:13,413 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3188 (0.3837) Acc D Real: 65.665% 
Loss D Fake: 0.4713 (0.4565) Acc D Fake: 96.324% 
Loss D: 0.790 
Loss G: 0.9923 (1.0146) Acc G: 3.692% 
LR: 2.000e-04 

2023-03-02 02:00:13,421 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.2926 (0.3831) Acc D Real: 65.738% 
Loss D Fake: 0.4716 (0.4566) Acc D Fake: 96.293% 
Loss D: 0.764 
Loss G: 0.9920 (1.0144) Acc G: 3.724% 
LR: 2.000e-04 

2023-03-02 02:00:13,429 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.2165 (0.3819) Acc D Real: 65.872% 
Loss D Fake: 0.4718 (0.4567) Acc D Fake: 96.261% 
Loss D: 0.688 
Loss G: 0.9917 (1.0143) Acc G: 3.755% 
LR: 2.000e-04 

2023-03-02 02:00:13,436 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2811 (0.3813) Acc D Real: 65.980% 
Loss D Fake: 0.4720 (0.4568) Acc D Fake: 96.230% 
Loss D: 0.753 
Loss G: 0.9915 (1.0141) Acc G: 3.786% 
LR: 2.000e-04 

2023-03-02 02:00:13,443 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3131 (0.3808) Acc D Real: 66.070% 
Loss D Fake: 0.4723 (0.4569) Acc D Fake: 96.200% 
Loss D: 0.785 
Loss G: 0.9912 (1.0140) Acc G: 3.816% 
LR: 2.000e-04 

2023-03-02 02:00:13,451 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3487 (0.3806) Acc D Real: 66.127% 
Loss D Fake: 0.4726 (0.4571) Acc D Fake: 96.170% 
Loss D: 0.821 
Loss G: 0.9907 (1.0138) Acc G: 3.846% 
LR: 2.000e-04 

2023-03-02 02:00:13,458 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3652 (0.3805) Acc D Real: 66.172% 
Loss D Fake: 0.4730 (0.4572) Acc D Fake: 96.140% 
Loss D: 0.838 
Loss G: 0.9901 (1.0137) Acc G: 3.875% 
LR: 2.000e-04 

2023-03-02 02:00:13,466 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.2748 (0.3798) Acc D Real: 66.254% 
Loss D Fake: 0.4734 (0.4573) Acc D Fake: 96.111% 
Loss D: 0.748 
Loss G: 0.9895 (1.0135) Acc G: 3.905% 
LR: 2.000e-04 

2023-03-02 02:00:13,473 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.2890 (0.3792) Acc D Real: 66.338% 
Loss D Fake: 0.4739 (0.4574) Acc D Fake: 96.082% 
Loss D: 0.763 
Loss G: 0.9889 (1.0134) Acc G: 3.933% 
LR: 2.000e-04 

2023-03-02 02:00:13,480 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4018 (0.3794) Acc D Real: 66.343% 
Loss D Fake: 0.4743 (0.4575) Acc D Fake: 96.054% 
Loss D: 0.876 
Loss G: 0.9882 (1.0132) Acc G: 3.962% 
LR: 2.000e-04 

2023-03-02 02:00:13,489 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3253 (0.3790) Acc D Real: 66.418% 
Loss D Fake: 0.4749 (0.4576) Acc D Fake: 96.026% 
Loss D: 0.800 
Loss G: 0.9873 (1.0130) Acc G: 3.990% 
LR: 2.000e-04 

2023-03-02 02:00:13,496 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2746 (0.3783) Acc D Real: 66.501% 
Loss D Fake: 0.4756 (0.4577) Acc D Fake: 95.998% 
Loss D: 0.750 
Loss G: 0.9863 (1.0129) Acc G: 4.017% 
LR: 2.000e-04 

2023-03-02 02:00:13,504 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2599 (0.3776) Acc D Real: 66.516% 
Loss D Fake: 0.4763 (0.4578) Acc D Fake: 95.995% 
Loss D: 0.736 
Loss G: 0.9853 (1.0127) Acc G: 4.020% 
LR: 2.000e-04 

2023-03-02 02:00:13,735 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.521 | Generator Loss: 0.984 | Avg: 1.505 
2023-03-02 02:00:13,757 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.556 | Generator Loss: 0.984 | Avg: 1.540 
2023-03-02 02:00:13,780 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.548 | Generator Loss: 0.984 | Avg: 1.532 
2023-03-02 02:00:13,806 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.538 | Generator Loss: 0.984 | Avg: 1.523 
2023-03-02 02:00:13,835 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.537 | Generator Loss: 0.984 | Avg: 1.521 
2023-03-02 02:00:13,865 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.597 | Generator Loss: 0.984 | Avg: 1.581 
2023-03-02 02:00:13,891 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.600 | Generator Loss: 0.984 | Avg: 1.584 
2023-03-02 02:00:13,917 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.655 | Generator Loss: 0.984 | Avg: 1.639 
2023-03-02 02:00:13,943 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.670 | Generator Loss: 0.984 | Avg: 1.654 
2023-03-02 02:00:13,968 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.726 | Generator Loss: 0.984 | Avg: 1.710 
2023-03-02 02:00:13,994 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.764 | Generator Loss: 0.984 | Avg: 1.748 
2023-03-02 02:00:14,020 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.794 | Generator Loss: 0.984 | Avg: 1.778 
2023-03-02 02:00:14,046 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.824 | Generator Loss: 0.984 | Avg: 1.809 
2023-03-02 02:00:14,071 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.829 | Generator Loss: 0.984 | Avg: 1.814 
2023-03-02 02:00:14,096 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.811 | Generator Loss: 0.984 | Avg: 1.796 
2023-03-02 02:00:14,122 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.795 | Generator Loss: 0.984 | Avg: 1.779 
2023-03-02 02:00:14,147 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.780 | Generator Loss: 0.984 | Avg: 1.764 
2023-03-02 02:00:14,179 -                train: [    INFO] - 
Epoch: 7/20
2023-03-02 02:00:14,368 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3742 (0.3410) Acc D Real: 74.036% 
Loss D Fake: 0.4778 (0.4774) Acc D Fake: 90.990% 
Loss D: 0.852 
Loss G: 0.9831 (0.9837) Acc G: 9.141% 
LR: 2.000e-04 

2023-03-02 02:00:14,376 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2703 (0.3174) Acc D Real: 76.562% 
Loss D Fake: 0.4786 (0.4778) Acc D Fake: 90.660% 
Loss D: 0.749 
Loss G: 0.9818 (0.9831) Acc G: 9.427% 
LR: 2.000e-04 

2023-03-02 02:00:14,384 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2033 (0.2889) Acc D Real: 79.245% 
Loss D Fake: 0.4795 (0.4782) Acc D Fake: 90.495% 
Loss D: 0.683 
Loss G: 0.9806 (0.9824) Acc G: 9.570% 
LR: 2.000e-04 

2023-03-02 02:00:14,401 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3895 (0.3090) Acc D Real: 77.073% 
Loss D Fake: 0.4805 (0.4787) Acc D Fake: 90.396% 
Loss D: 0.870 
Loss G: 0.9791 (0.9818) Acc G: 9.656% 
LR: 2.000e-04 

2023-03-02 02:00:14,408 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3328 (0.3130) Acc D Real: 76.372% 
Loss D Fake: 0.4817 (0.4792) Acc D Fake: 90.330% 
Loss D: 0.814 
Loss G: 0.9768 (0.9809) Acc G: 9.714% 
LR: 2.000e-04 

2023-03-02 02:00:14,415 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2464 (0.3035) Acc D Real: 77.574% 
Loss D Fake: 0.4834 (0.4798) Acc D Fake: 90.283% 
Loss D: 0.730 
Loss G: 0.9741 (0.9800) Acc G: 9.754% 
LR: 2.000e-04 

2023-03-02 02:00:14,423 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2479 (0.2965) Acc D Real: 78.158% 
Loss D Fake: 0.4853 (0.4805) Acc D Fake: 90.247% 
Loss D: 0.733 
Loss G: 0.9713 (0.9789) Acc G: 9.785% 
LR: 2.000e-04 

2023-03-02 02:00:14,430 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3058 (0.2976) Acc D Real: 78.177% 
Loss D Fake: 0.4872 (0.4812) Acc D Fake: 90.220% 
Loss D: 0.793 
Loss G: 0.9684 (0.9777) Acc G: 9.809% 
LR: 2.000e-04 

2023-03-02 02:00:14,438 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3135 (0.2991) Acc D Real: 78.167% 
Loss D Fake: 0.4893 (0.4820) Acc D Fake: 90.198% 
Loss D: 0.803 
Loss G: 0.9651 (0.9764) Acc G: 9.828% 
LR: 2.000e-04 

2023-03-02 02:00:14,445 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3770 (0.3062) Acc D Real: 77.727% 
Loss D Fake: 0.4917 (0.4829) Acc D Fake: 90.028% 
Loss D: 0.869 
Loss G: 0.9613 (0.9751) Acc G: 9.995% 
LR: 2.000e-04 

2023-03-02 02:00:14,452 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3824 (0.3126) Acc D Real: 77.005% 
Loss D Fake: 0.4945 (0.4839) Acc D Fake: 89.887% 
Loss D: 0.877 
Loss G: 0.9570 (0.9736) Acc G: 10.135% 
LR: 2.000e-04 

2023-03-02 02:00:14,460 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4144 (0.3204) Acc D Real: 75.974% 
Loss D Fake: 0.4977 (0.4849) Acc D Fake: 89.768% 
Loss D: 0.912 
Loss G: 0.9522 (0.9719) Acc G: 10.252% 
LR: 2.000e-04 

2023-03-02 02:00:14,467 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2261 (0.3137) Acc D Real: 76.626% 
Loss D Fake: 0.5016 (0.4861) Acc D Fake: 89.665% 
Loss D: 0.728 
Loss G: 0.9465 (0.9701) Acc G: 10.353% 
LR: 2.000e-04 

2023-03-02 02:00:14,475 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3154 (0.3138) Acc D Real: 76.705% 
Loss D Fake: 0.5063 (0.4875) Acc D Fake: 89.517% 
Loss D: 0.822 
Loss G: 0.9396 (0.9681) Acc G: 10.528% 
LR: 2.000e-04 

2023-03-02 02:00:14,482 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4160 (0.3202) Acc D Real: 76.146% 
Loss D Fake: 0.5126 (0.4890) Acc D Fake: 89.339% 
Loss D: 0.929 
Loss G: 0.9299 (0.9657) Acc G: 10.703% 
LR: 2.000e-04 

2023-03-02 02:00:14,490 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2582 (0.3165) Acc D Real: 76.688% 
Loss D Fake: 0.5217 (0.4910) Acc D Fake: 89.182% 
Loss D: 0.780 
Loss G: 0.9163 (0.9628) Acc G: 10.861% 
LR: 2.000e-04 

2023-03-02 02:00:14,497 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3180 (0.3166) Acc D Real: 76.704% 
Loss D Fake: 0.5361 (0.4935) Acc D Fake: 88.950% 
Loss D: 0.854 
Loss G: 0.8950 (0.9590) Acc G: 11.091% 
LR: 2.000e-04 

2023-03-02 02:00:14,504 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3352 (0.3176) Acc D Real: 76.414% 
Loss D Fake: 0.5613 (0.4970) Acc D Fake: 88.566% 
Loss D: 0.896 
Loss G: 0.8589 (0.9537) Acc G: 11.472% 
LR: 2.000e-04 

2023-03-02 02:00:14,512 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2217 (0.3128) Acc D Real: 76.852% 
Loss D Fake: 0.6103 (0.5027) Acc D Fake: 87.549% 
Loss D: 0.832 
Loss G: 0.7948 (0.9458) Acc G: 12.482% 
LR: 2.000e-04 

2023-03-02 02:00:14,519 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2679 (0.3107) Acc D Real: 77.054% 
Loss D Fake: 4.8658 (0.7105) Acc D Fake: 83.380% 
Loss D: 5.134 
Loss G: 0.2296 (0.9117) Acc G: 16.649% 
LR: 2.000e-04 

2023-03-02 02:00:14,527 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3178 (0.3110) Acc D Real: 77.043% 
Loss D Fake: 5.5871 (0.9321) Acc D Fake: 79.590% 
Loss D: 5.905 
Loss G: 0.1703 (0.8780) Acc G: 20.438% 
LR: 2.000e-04 

2023-03-02 02:00:14,534 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.2611 (0.3088) Acc D Real: 77.409% 
Loss D Fake: 5.8057 (1.1440) Acc D Fake: 76.130% 
Loss D: 6.067 
Loss G: 0.1500 (0.8463) Acc G: 23.897% 
LR: 2.000e-04 

2023-03-02 02:00:14,542 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3664 (0.3112) Acc D Real: 77.101% 
Loss D Fake: 5.8920 (1.3419) Acc D Fake: 72.958% 
Loss D: 6.258 
Loss G: 0.1389 (0.8169) Acc G: 27.068% 
LR: 2.000e-04 

2023-03-02 02:00:14,549 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3215 (0.3116) Acc D Real: 76.975% 
Loss D Fake: 5.9159 (1.5248) Acc D Fake: 70.040% 
Loss D: 6.237 
Loss G: 0.1318 (0.7895) Acc G: 29.985% 
LR: 2.000e-04 

2023-03-02 02:00:14,556 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.2411 (0.3089) Acc D Real: 77.192% 
Loss D Fake: 5.9042 (1.6933) Acc D Fake: 67.346% 
Loss D: 6.145 
Loss G: 0.1270 (0.7640) Acc G: 32.678% 
LR: 2.000e-04 

2023-03-02 02:00:14,563 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4254 (0.3132) Acc D Real: 76.757% 
Loss D Fake: 5.8703 (1.8480) Acc D Fake: 64.851% 
Loss D: 6.296 
Loss G: 0.1235 (0.7403) Acc G: 35.172% 
LR: 2.000e-04 

2023-03-02 02:00:14,571 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3232 (0.3136) Acc D Real: 76.745% 
Loss D Fake: 5.8221 (1.9899) Acc D Fake: 62.535% 
Loss D: 6.145 
Loss G: 0.1208 (0.7181) Acc G: 37.487% 
LR: 2.000e-04 

2023-03-02 02:00:14,579 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3408 (0.3145) Acc D Real: 76.613% 
Loss D Fake: 5.7642 (2.1200) Acc D Fake: 60.379% 
Loss D: 6.105 
Loss G: 0.1188 (0.6975) Acc G: 39.643% 
LR: 2.000e-04 

2023-03-02 02:00:14,587 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.2313 (0.3117) Acc D Real: 76.788% 
Loss D Fake: 5.6995 (2.2394) Acc D Fake: 58.366% 
Loss D: 5.931 
Loss G: 0.1173 (0.6781) Acc G: 41.655% 
LR: 2.000e-04 

2023-03-02 02:00:14,595 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2352 (0.3093) Acc D Real: 77.088% 
Loss D Fake: 5.6300 (2.3487) Acc D Fake: 56.484% 
Loss D: 5.865 
Loss G: 0.1161 (0.6600) Acc G: 43.537% 
LR: 2.000e-04 

2023-03-02 02:00:14,602 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3239 (0.3097) Acc D Real: 77.030% 
Loss D Fake: 5.5571 (2.4490) Acc D Fake: 54.718% 
Loss D: 5.881 
Loss G: 0.1151 (0.6430) Acc G: 45.301% 
LR: 2.000e-04 

2023-03-02 02:00:14,610 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2869 (0.3090) Acc D Real: 77.208% 
Loss D Fake: 5.4818 (2.5409) Acc D Fake: 53.060% 
Loss D: 5.769 
Loss G: 0.1144 (0.6270) Acc G: 46.959% 
LR: 2.000e-04 

2023-03-02 02:00:14,617 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3634 (0.3106) Acc D Real: 77.132% 
Loss D Fake: 5.4049 (2.6251) Acc D Fake: 51.500% 
Loss D: 5.768 
Loss G: 0.1138 (0.6119) Acc G: 48.519% 
LR: 2.000e-04 

2023-03-02 02:00:14,625 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2262 (0.3082) Acc D Real: 77.388% 
Loss D Fake: 5.3269 (2.7023) Acc D Fake: 50.028% 
Loss D: 5.553 
Loss G: 0.1134 (0.5976) Acc G: 49.990% 
LR: 2.000e-04 

2023-03-02 02:00:14,632 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3294 (0.3088) Acc D Real: 77.367% 
Loss D Fake: 5.2480 (2.7730) Acc D Fake: 48.639% 
Loss D: 5.577 
Loss G: 0.1131 (0.5842) Acc G: 51.379% 
LR: 2.000e-04 

2023-03-02 02:00:14,640 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3527 (0.3100) Acc D Real: 77.242% 
Loss D Fake: 5.1686 (2.8378) Acc D Fake: 47.324% 
Loss D: 5.521 
Loss G: 0.1129 (0.5714) Acc G: 52.693% 
LR: 2.000e-04 

2023-03-02 02:00:14,649 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3260 (0.3104) Acc D Real: 77.233% 
Loss D Fake: 5.0889 (2.8970) Acc D Fake: 46.079% 
Loss D: 5.415 
Loss G: 0.1128 (0.5594) Acc G: 53.938% 
LR: 2.000e-04 

2023-03-02 02:00:14,656 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2560 (0.3090) Acc D Real: 77.408% 
Loss D Fake: 5.0093 (2.9512) Acc D Fake: 44.897% 
Loss D: 5.265 
Loss G: 0.1127 (0.5479) Acc G: 55.119% 
LR: 2.000e-04 

2023-03-02 02:00:14,665 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3462 (0.3100) Acc D Real: 77.279% 
Loss D Fake: 4.9297 (3.0007) Acc D Fake: 43.775% 
Loss D: 5.276 
Loss G: 0.1127 (0.5370) Acc G: 56.241% 
LR: 2.000e-04 

2023-03-02 02:00:14,674 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3151 (0.3101) Acc D Real: 77.353% 
Loss D Fake: 4.8505 (3.0458) Acc D Fake: 42.707% 
Loss D: 5.166 
Loss G: 0.1128 (0.5267) Acc G: 57.308% 
LR: 2.000e-04 

2023-03-02 02:00:14,682 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3922 (0.3120) Acc D Real: 77.176% 
Loss D Fake: 4.7720 (3.0869) Acc D Fake: 41.690% 
Loss D: 5.164 
Loss G: 0.1129 (0.5168) Acc G: 58.325% 
LR: 2.000e-04 

2023-03-02 02:00:14,689 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2676 (0.3110) Acc D Real: 77.350% 
Loss D Fake: 4.6943 (3.1243) Acc D Fake: 40.721% 
Loss D: 4.962 
Loss G: 0.1131 (0.5074) Acc G: 59.294% 
LR: 2.000e-04 

2023-03-02 02:00:14,698 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3237 (0.3113) Acc D Real: 77.430% 
Loss D Fake: 4.6176 (3.1582) Acc D Fake: 39.795% 
Loss D: 4.941 
Loss G: 0.1133 (0.4985) Acc G: 60.219% 
LR: 2.000e-04 

2023-03-02 02:00:14,705 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2178 (0.3092) Acc D Real: 77.625% 
Loss D Fake: 4.5421 (3.1889) Acc D Fake: 38.911% 
Loss D: 4.760 
Loss G: 0.1136 (0.4899) Acc G: 61.103% 
LR: 2.000e-04 

2023-03-02 02:00:14,713 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2461 (0.3078) Acc D Real: 77.741% 
Loss D Fake: 4.4675 (3.2167) Acc D Fake: 38.065% 
Loss D: 4.714 
Loss G: 0.1139 (0.4817) Acc G: 61.949% 
LR: 2.000e-04 

2023-03-02 02:00:14,720 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2735 (0.3071) Acc D Real: 77.930% 
Loss D Fake: 4.3942 (3.2418) Acc D Fake: 37.255% 
Loss D: 4.668 
Loss G: 0.1142 (0.4739) Acc G: 62.758% 
LR: 2.000e-04 

2023-03-02 02:00:14,728 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3072 (0.3071) Acc D Real: 77.859% 
Loss D Fake: 4.3223 (3.2643) Acc D Fake: 36.479% 
Loss D: 4.629 
Loss G: 0.1146 (0.4664) Acc G: 63.534% 
LR: 2.000e-04 

2023-03-02 02:00:14,737 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3278 (0.3075) Acc D Real: 77.883% 
Loss D Fake: 4.2515 (3.2844) Acc D Fake: 35.734% 
Loss D: 4.579 
Loss G: 0.1150 (0.4593) Acc G: 64.278% 
LR: 2.000e-04 

2023-03-02 02:00:14,744 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4068 (0.3095) Acc D Real: 77.802% 
Loss D Fake: 4.1821 (3.3024) Acc D Fake: 35.020% 
Loss D: 4.589 
Loss G: 0.1154 (0.4524) Acc G: 64.993% 
LR: 2.000e-04 

2023-03-02 02:00:14,752 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2734 (0.3088) Acc D Real: 77.925% 
Loss D Fake: 4.1140 (3.3183) Acc D Fake: 34.333% 
Loss D: 4.387 
Loss G: 0.1159 (0.4458) Acc G: 65.679% 
LR: 2.000e-04 

2023-03-02 02:00:14,759 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3471 (0.3095) Acc D Real: 77.887% 
Loss D Fake: 4.0470 (3.3323) Acc D Fake: 33.673% 
Loss D: 4.394 
Loss G: 0.1165 (0.4395) Acc G: 66.339% 
LR: 2.000e-04 

2023-03-02 02:00:14,766 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2773 (0.3089) Acc D Real: 77.975% 
Loss D Fake: 3.9812 (3.3446) Acc D Fake: 33.038% 
Loss D: 4.258 
Loss G: 0.1170 (0.4334) Acc G: 66.974% 
LR: 2.000e-04 

2023-03-02 02:00:14,774 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2568 (0.3080) Acc D Real: 78.143% 
Loss D Fake: 3.9164 (3.3552) Acc D Fake: 32.426% 
Loss D: 4.173 
Loss G: 0.1176 (0.4275) Acc G: 67.586% 
LR: 2.000e-04 

2023-03-02 02:00:14,781 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3190 (0.3082) Acc D Real: 78.095% 
Loss D Fake: 3.8527 (3.3642) Acc D Fake: 31.836% 
Loss D: 4.172 
Loss G: 0.1183 (0.4219) Acc G: 68.175% 
LR: 2.000e-04 

2023-03-02 02:00:14,789 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3791 (0.3094) Acc D Real: 78.078% 
Loss D Fake: 3.7899 (3.3718) Acc D Fake: 31.268% 
Loss D: 4.169 
Loss G: 0.1190 (0.4165) Acc G: 68.743% 
LR: 2.000e-04 

2023-03-02 02:00:14,796 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2850 (0.3090) Acc D Real: 78.166% 
Loss D Fake: 3.7283 (3.3781) Acc D Fake: 30.719% 
Loss D: 4.013 
Loss G: 0.1198 (0.4113) Acc G: 69.292% 
LR: 2.000e-04 

2023-03-02 02:00:14,804 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3303 (0.3094) Acc D Real: 78.240% 
Loss D Fake: 3.6676 (3.3831) Acc D Fake: 30.189% 
Loss D: 3.998 
Loss G: 0.1205 (0.4063) Acc G: 69.821% 
LR: 2.000e-04 

2023-03-02 02:00:14,811 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.2467 (0.3083) Acc D Real: 78.358% 
Loss D Fake: 3.6080 (3.3869) Acc D Fake: 29.678% 
Loss D: 3.855 
Loss G: 0.1214 (0.4015) Acc G: 70.305% 
LR: 2.000e-04 

2023-03-02 02:00:14,818 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3430 (0.3089) Acc D Real: 78.436% 
Loss D Fake: 3.5492 (3.3896) Acc D Fake: 29.211% 
Loss D: 3.892 
Loss G: 0.1222 (0.3968) Acc G: 70.772% 
LR: 2.000e-04 

2023-03-02 02:00:14,826 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3245 (0.3092) Acc D Real: 78.356% 
Loss D Fake: 3.4912 (3.3912) Acc D Fake: 28.759% 
Loss D: 3.816 
Loss G: 0.1232 (0.3923) Acc G: 71.224% 
LR: 2.000e-04 

2023-03-02 02:00:14,834 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2633 (0.3084) Acc D Real: 78.495% 
Loss D Fake: 3.4340 (3.3919) Acc D Fake: 28.322% 
Loss D: 3.697 
Loss G: 0.1242 (0.3880) Acc G: 71.661% 
LR: 2.000e-04 

2023-03-02 02:00:14,841 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2896 (0.3081) Acc D Real: 78.589% 
Loss D Fake: 3.3777 (3.3917) Acc D Fake: 27.899% 
Loss D: 3.667 
Loss G: 0.1252 (0.3838) Acc G: 72.084% 
LR: 2.000e-04 

2023-03-02 02:00:14,849 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2524 (0.3072) Acc D Real: 78.709% 
Loss D Fake: 3.3221 (3.3906) Acc D Fake: 27.489% 
Loss D: 3.575 
Loss G: 0.1263 (0.3798) Acc G: 72.494% 
LR: 2.000e-04 

2023-03-02 02:00:14,856 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3798 (0.3084) Acc D Real: 78.639% 
Loss D Fake: 3.2673 (3.3887) Acc D Fake: 27.092% 
Loss D: 3.647 
Loss G: 0.1275 (0.3759) Acc G: 72.892% 
LR: 2.000e-04 

2023-03-02 02:00:14,864 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2480 (0.3074) Acc D Real: 78.716% 
Loss D Fake: 3.2132 (3.3861) Acc D Fake: 26.707% 
Loss D: 3.461 
Loss G: 0.1287 (0.3722) Acc G: 73.277% 
LR: 2.000e-04 

2023-03-02 02:00:14,872 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2342 (0.3064) Acc D Real: 78.812% 
Loss D Fake: 3.1598 (3.3827) Acc D Fake: 26.333% 
Loss D: 3.394 
Loss G: 0.1301 (0.3686) Acc G: 73.651% 
LR: 2.000e-04 

2023-03-02 02:00:14,880 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3295 (0.3067) Acc D Real: 78.858% 
Loss D Fake: 3.1070 (3.3786) Acc D Fake: 25.970% 
Loss D: 3.436 
Loss G: 0.1315 (0.3651) Acc G: 74.014% 
LR: 2.000e-04 

2023-03-02 02:00:14,887 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3032 (0.3066) Acc D Real: 78.817% 
Loss D Fake: 3.0551 (3.3739) Acc D Fake: 25.618% 
Loss D: 3.358 
Loss G: 0.1329 (0.3617) Acc G: 74.367% 
LR: 2.000e-04 

2023-03-02 02:00:14,894 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3426 (0.3072) Acc D Real: 78.826% 
Loss D Fake: 3.0039 (3.3687) Acc D Fake: 25.276% 
Loss D: 3.346 
Loss G: 0.1345 (0.3585) Acc G: 74.709% 
LR: 2.000e-04 

2023-03-02 02:00:14,902 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3392 (0.3076) Acc D Real: 78.869% 
Loss D Fake: 2.9535 (3.3628) Acc D Fake: 24.944% 
Loss D: 3.293 
Loss G: 0.1361 (0.3553) Acc G: 75.042% 
LR: 2.000e-04 

2023-03-02 02:00:14,910 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3241 (0.3078) Acc D Real: 78.877% 
Loss D Fake: 2.9039 (3.3564) Acc D Fake: 24.620% 
Loss D: 3.228 
Loss G: 0.1378 (0.3523) Acc G: 75.365% 
LR: 2.000e-04 

2023-03-02 02:00:14,918 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.2917 (0.3076) Acc D Real: 78.901% 
Loss D Fake: 2.8550 (3.3496) Acc D Fake: 24.306% 
Loss D: 3.147 
Loss G: 0.1396 (0.3494) Acc G: 75.657% 
LR: 2.000e-04 

2023-03-02 02:00:14,925 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3691 (0.3084) Acc D Real: 78.765% 
Loss D Fake: 2.8069 (3.3422) Acc D Fake: 24.022% 
Loss D: 3.176 
Loss G: 0.1414 (0.3466) Acc G: 75.941% 
LR: 2.000e-04 

2023-03-02 02:00:14,932 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3941 (0.3096) Acc D Real: 78.689% 
Loss D Fake: 2.7593 (3.3345) Acc D Fake: 23.747% 
Loss D: 3.153 
Loss G: 0.1434 (0.3439) Acc G: 76.217% 
LR: 2.000e-04 

2023-03-02 02:00:14,940 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2889 (0.3093) Acc D Real: 78.732% 
Loss D Fake: 2.7126 (3.3263) Acc D Fake: 23.478% 
Loss D: 3.002 
Loss G: 0.1455 (0.3413) Acc G: 76.486% 
LR: 2.000e-04 

2023-03-02 02:00:14,947 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2935 (0.3091) Acc D Real: 78.825% 
Loss D Fake: 2.6666 (3.3177) Acc D Fake: 23.216% 
Loss D: 2.960 
Loss G: 0.1476 (0.3387) Acc G: 76.749% 
LR: 2.000e-04 

2023-03-02 02:00:14,954 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3111 (0.3091) Acc D Real: 78.858% 
Loss D Fake: 2.6214 (3.3088) Acc D Fake: 22.961% 
Loss D: 2.932 
Loss G: 0.1499 (0.3363) Acc G: 77.004% 
LR: 2.000e-04 

2023-03-02 02:00:14,962 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3057 (0.3091) Acc D Real: 78.905% 
Loss D Fake: 2.5770 (3.2995) Acc D Fake: 22.713% 
Loss D: 2.883 
Loss G: 0.1523 (0.3340) Acc G: 77.253% 
LR: 2.000e-04 

2023-03-02 02:00:14,969 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3213 (0.3092) Acc D Real: 78.954% 
Loss D Fake: 2.5333 (3.2899) Acc D Fake: 22.471% 
Loss D: 2.855 
Loss G: 0.1547 (0.3318) Acc G: 77.495% 
LR: 2.000e-04 

2023-03-02 02:00:14,976 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3468 (0.3097) Acc D Real: 78.946% 
Loss D Fake: 2.4903 (3.2801) Acc D Fake: 22.234% 
Loss D: 2.837 
Loss G: 0.1573 (0.3296) Acc G: 77.732% 
LR: 2.000e-04 

2023-03-02 02:00:14,984 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2754 (0.3093) Acc D Real: 79.025% 
Loss D Fake: 2.4480 (3.2699) Acc D Fake: 22.004% 
Loss D: 2.723 
Loss G: 0.1599 (0.3275) Acc G: 77.963% 
LR: 2.000e-04 

2023-03-02 02:00:14,991 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3017 (0.3092) Acc D Real: 79.110% 
Loss D Fake: 2.4065 (3.2595) Acc D Fake: 21.779% 
Loss D: 2.708 
Loss G: 0.1627 (0.3255) Acc G: 78.188% 
LR: 2.000e-04 

2023-03-02 02:00:14,999 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3530 (0.3097) Acc D Real: 79.084% 
Loss D Fake: 2.3657 (3.2489) Acc D Fake: 21.579% 
Loss D: 2.719 
Loss G: 0.1656 (0.3236) Acc G: 78.389% 
LR: 2.000e-04 

2023-03-02 02:00:15,006 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3317 (0.3100) Acc D Real: 79.133% 
Loss D Fake: 2.3255 (3.2380) Acc D Fake: 21.384% 
Loss D: 2.657 
Loss G: 0.1686 (0.3218) Acc G: 78.584% 
LR: 2.000e-04 

2023-03-02 02:00:15,013 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3640 (0.3106) Acc D Real: 79.139% 
Loss D Fake: 2.2860 (3.2269) Acc D Fake: 21.194% 
Loss D: 2.650 
Loss G: 0.1717 (0.3201) Acc G: 78.775% 
LR: 2.000e-04 

2023-03-02 02:00:15,021 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3520 (0.3111) Acc D Real: 79.125% 
Loss D Fake: 2.2471 (3.2157) Acc D Fake: 21.008% 
Loss D: 2.599 
Loss G: 0.1749 (0.3184) Acc G: 78.961% 
LR: 2.000e-04 

2023-03-02 02:00:15,028 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4386 (0.3125) Acc D Real: 78.986% 
Loss D Fake: 2.2088 (3.2042) Acc D Fake: 20.826% 
Loss D: 2.647 
Loss G: 0.1783 (0.3168) Acc G: 79.144% 
LR: 2.000e-04 

2023-03-02 02:00:15,035 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3226 (0.3126) Acc D Real: 79.029% 
Loss D Fake: 2.1710 (3.1926) Acc D Fake: 20.648% 
Loss D: 2.494 
Loss G: 0.1817 (0.3153) Acc G: 79.322% 
LR: 2.000e-04 

2023-03-02 02:00:15,043 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3601 (0.3132) Acc D Real: 79.040% 
Loss D Fake: 2.1339 (3.1809) Acc D Fake: 20.474% 
Loss D: 2.494 
Loss G: 0.1853 (0.3138) Acc G: 79.496% 
LR: 2.000e-04 

2023-03-02 02:00:15,050 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4093 (0.3142) Acc D Real: 78.977% 
Loss D Fake: 2.0975 (3.1690) Acc D Fake: 20.304% 
Loss D: 2.507 
Loss G: 0.1890 (0.3125) Acc G: 79.666% 
LR: 2.000e-04 

2023-03-02 02:00:15,057 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3098 (0.3142) Acc D Real: 79.057% 
Loss D Fake: 2.0617 (3.1569) Acc D Fake: 20.138% 
Loss D: 2.371 
Loss G: 0.1928 (0.3112) Acc G: 79.833% 
LR: 2.000e-04 

2023-03-02 02:00:15,065 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3177 (0.3142) Acc D Real: 79.075% 
Loss D Fake: 2.0267 (3.1448) Acc D Fake: 19.975% 
Loss D: 2.344 
Loss G: 0.1967 (0.3099) Acc G: 79.996% 
LR: 2.000e-04 

2023-03-02 02:00:15,072 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3515 (0.3146) Acc D Real: 79.091% 
Loss D Fake: 1.9924 (3.1325) Acc D Fake: 19.815% 
Loss D: 2.344 
Loss G: 0.2008 (0.3088) Acc G: 80.156% 
LR: 2.000e-04 

2023-03-02 02:00:15,080 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3579 (0.3151) Acc D Real: 79.131% 
Loss D Fake: 1.9587 (3.1202) Acc D Fake: 19.660% 
Loss D: 2.317 
Loss G: 0.2049 (0.3077) Acc G: 80.312% 
LR: 2.000e-04 

2023-03-02 02:00:15,087 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3327 (0.3153) Acc D Real: 79.158% 
Loss D Fake: 1.9257 (3.1077) Acc D Fake: 19.507% 
Loss D: 2.258 
Loss G: 0.2092 (0.3067) Acc G: 80.465% 
LR: 2.000e-04 

2023-03-02 02:00:15,094 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4079 (0.3162) Acc D Real: 79.115% 
Loss D Fake: 1.8934 (3.0952) Acc D Fake: 19.370% 
Loss D: 2.301 
Loss G: 0.2135 (0.3057) Acc G: 80.598% 
LR: 2.000e-04 

2023-03-02 02:00:15,102 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3637 (0.3167) Acc D Real: 79.144% 
Loss D Fake: 1.8617 (3.0826) Acc D Fake: 19.240% 
Loss D: 2.225 
Loss G: 0.2180 (0.3048) Acc G: 80.728% 
LR: 2.000e-04 

2023-03-02 02:00:15,109 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3413 (0.3169) Acc D Real: 79.192% 
Loss D Fake: 1.8307 (3.0700) Acc D Fake: 19.113% 
Loss D: 2.172 
Loss G: 0.2226 (0.3040) Acc G: 80.855% 
LR: 2.000e-04 

2023-03-02 02:00:15,116 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3734 (0.3175) Acc D Real: 79.236% 
Loss D Fake: 1.8004 (3.0573) Acc D Fake: 18.989% 
Loss D: 2.174 
Loss G: 0.2273 (0.3032) Acc G: 80.980% 
LR: 2.000e-04 

2023-03-02 02:00:15,123 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4040 (0.3184) Acc D Real: 79.169% 
Loss D Fake: 1.7706 (3.0445) Acc D Fake: 18.867% 
Loss D: 2.175 
Loss G: 0.2321 (0.3025) Acc G: 81.102% 
LR: 2.000e-04 

2023-03-02 02:00:15,131 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3835 (0.3190) Acc D Real: 79.160% 
Loss D Fake: 1.7415 (3.0318) Acc D Fake: 18.747% 
Loss D: 2.125 
Loss G: 0.2370 (0.3019) Acc G: 81.222% 
LR: 2.000e-04 

2023-03-02 02:00:15,138 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4537 (0.3203) Acc D Real: 79.144% 
Loss D Fake: 1.7128 (3.0189) Acc D Fake: 18.630% 
Loss D: 2.167 
Loss G: 0.2420 (0.3013) Acc G: 81.340% 
LR: 2.000e-04 

2023-03-02 02:00:15,146 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3362 (0.3205) Acc D Real: 79.204% 
Loss D Fake: 1.6846 (3.0061) Acc D Fake: 18.515% 
Loss D: 2.021 
Loss G: 0.2471 (0.3008) Acc G: 81.455% 
LR: 2.000e-04 

2023-03-02 02:00:15,153 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3857 (0.3211) Acc D Real: 79.250% 
Loss D Fake: 1.6572 (2.9933) Acc D Fake: 18.402% 
Loss D: 2.043 
Loss G: 0.2523 (0.3003) Acc G: 81.568% 
LR: 2.000e-04 

2023-03-02 02:00:15,160 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3739 (0.3216) Acc D Real: 79.303% 
Loss D Fake: 1.6305 (2.9804) Acc D Fake: 18.291% 
Loss D: 2.004 
Loss G: 0.2576 (0.2999) Acc G: 81.679% 
LR: 2.000e-04 

2023-03-02 02:00:15,168 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3600 (0.3219) Acc D Real: 79.354% 
Loss D Fake: 1.6045 (2.9676) Acc D Fake: 18.182% 
Loss D: 1.964 
Loss G: 0.2630 (0.2996) Acc G: 81.788% 
LR: 2.000e-04 

2023-03-02 02:00:15,175 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4027 (0.3227) Acc D Real: 79.338% 
Loss D Fake: 1.5791 (2.9547) Acc D Fake: 18.076% 
Loss D: 1.982 
Loss G: 0.2684 (0.2993) Acc G: 81.895% 
LR: 2.000e-04 

2023-03-02 02:00:15,183 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4564 (0.3239) Acc D Real: 79.232% 
Loss D Fake: 1.5544 (2.9419) Acc D Fake: 17.971% 
Loss D: 2.011 
Loss G: 0.2739 (0.2990) Acc G: 82.000% 
LR: 2.000e-04 

2023-03-02 02:00:15,190 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3897 (0.3245) Acc D Real: 79.253% 
Loss D Fake: 1.5301 (2.9290) Acc D Fake: 17.868% 
Loss D: 1.920 
Loss G: 0.2795 (0.2989) Acc G: 82.103% 
LR: 2.000e-04 

2023-03-02 02:00:15,197 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3986 (0.3252) Acc D Real: 79.280% 
Loss D Fake: 1.5065 (2.9162) Acc D Fake: 17.767% 
Loss D: 1.905 
Loss G: 0.2851 (0.2987) Acc G: 82.204% 
LR: 2.000e-04 

2023-03-02 02:00:15,204 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3995 (0.3258) Acc D Real: 79.286% 
Loss D Fake: 1.4836 (2.9034) Acc D Fake: 17.668% 
Loss D: 1.883 
Loss G: 0.2907 (0.2987) Acc G: 82.303% 
LR: 2.000e-04 

2023-03-02 02:00:15,212 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4131 (0.3266) Acc D Real: 79.306% 
Loss D Fake: 1.4614 (2.8907) Acc D Fake: 17.571% 
Loss D: 1.874 
Loss G: 0.2964 (0.2986) Acc G: 82.401% 
LR: 2.000e-04 

2023-03-02 02:00:15,219 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4137 (0.3274) Acc D Real: 79.305% 
Loss D Fake: 1.4397 (2.8779) Acc D Fake: 17.475% 
Loss D: 1.853 
Loss G: 0.3021 (0.2987) Acc G: 82.497% 
LR: 2.000e-04 

2023-03-02 02:00:15,227 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4169 (0.3282) Acc D Real: 79.300% 
Loss D Fake: 1.4186 (2.8652) Acc D Fake: 17.381% 
Loss D: 1.836 
Loss G: 0.3079 (0.2987) Acc G: 82.591% 
LR: 2.000e-04 

2023-03-02 02:00:15,234 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4283 (0.3290) Acc D Real: 79.282% 
Loss D Fake: 1.3982 (2.8526) Acc D Fake: 17.289% 
Loss D: 1.826 
Loss G: 0.3137 (0.2989) Acc G: 82.684% 
LR: 2.000e-04 

2023-03-02 02:00:15,241 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4320 (0.3299) Acc D Real: 79.271% 
Loss D Fake: 1.3783 (2.8400) Acc D Fake: 17.198% 
Loss D: 1.810 
Loss G: 0.3194 (0.2991) Acc G: 82.775% 
LR: 2.000e-04 

2023-03-02 02:00:15,248 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4138 (0.3306) Acc D Real: 79.297% 
Loss D Fake: 1.3590 (2.8274) Acc D Fake: 17.109% 
Loss D: 1.773 
Loss G: 0.3252 (0.2993) Acc G: 82.864% 
LR: 2.000e-04 

2023-03-02 02:00:15,256 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3793 (0.3310) Acc D Real: 79.368% 
Loss D Fake: 1.3404 (2.8149) Acc D Fake: 17.021% 
Loss D: 1.720 
Loss G: 0.3309 (0.2995) Acc G: 82.952% 
LR: 2.000e-04 

2023-03-02 02:00:15,263 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4306 (0.3318) Acc D Real: 79.405% 
Loss D Fake: 1.3226 (2.8025) Acc D Fake: 16.935% 
Loss D: 1.753 
Loss G: 0.3366 (0.2998) Acc G: 83.039% 
LR: 2.000e-04 

2023-03-02 02:00:15,271 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4419 (0.3328) Acc D Real: 79.401% 
Loss D Fake: 1.3050 (2.7901) Acc D Fake: 16.850% 
Loss D: 1.747 
Loss G: 0.3424 (0.3002) Acc G: 83.124% 
LR: 2.000e-04 

2023-03-02 02:00:15,278 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4154 (0.3334) Acc D Real: 79.409% 
Loss D Fake: 1.2880 (2.7778) Acc D Fake: 16.767% 
Loss D: 1.703 
Loss G: 0.3480 (0.3006) Acc G: 83.207% 
LR: 2.000e-04 

2023-03-02 02:00:15,286 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4554 (0.3344) Acc D Real: 79.387% 
Loss D Fake: 1.2718 (2.7656) Acc D Fake: 16.684% 
Loss D: 1.727 
Loss G: 0.3536 (0.3010) Acc G: 83.290% 
LR: 2.000e-04 

2023-03-02 02:00:15,293 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4138 (0.3351) Acc D Real: 79.444% 
Loss D Fake: 1.2560 (2.7534) Acc D Fake: 16.604% 
Loss D: 1.670 
Loss G: 0.3592 (0.3015) Acc G: 83.371% 
LR: 2.000e-04 

2023-03-02 02:00:15,300 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4384 (0.3359) Acc D Real: 79.457% 
Loss D Fake: 1.2407 (2.7413) Acc D Fake: 16.524% 
Loss D: 1.679 
Loss G: 0.3648 (0.3020) Acc G: 83.450% 
LR: 2.000e-04 

2023-03-02 02:00:15,307 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4343 (0.3367) Acc D Real: 79.496% 
Loss D Fake: 1.2258 (2.7293) Acc D Fake: 16.446% 
Loss D: 1.660 
Loss G: 0.3704 (0.3025) Acc G: 83.529% 
LR: 2.000e-04 

2023-03-02 02:00:15,315 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4164 (0.3373) Acc D Real: 79.548% 
Loss D Fake: 1.2113 (2.7173) Acc D Fake: 16.369% 
Loss D: 1.628 
Loss G: 0.3758 (0.3031) Acc G: 83.606% 
LR: 2.000e-04 

2023-03-02 02:00:15,322 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4531 (0.3382) Acc D Real: 79.567% 
Loss D Fake: 1.1974 (2.7054) Acc D Fake: 16.293% 
Loss D: 1.651 
Loss G: 0.3813 (0.3037) Acc G: 83.682% 
LR: 2.000e-04 

2023-03-02 02:00:15,330 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4834 (0.3393) Acc D Real: 79.523% 
Loss D Fake: 1.1836 (2.6936) Acc D Fake: 16.219% 
Loss D: 1.667 
Loss G: 0.3869 (0.3044) Acc G: 83.757% 
LR: 2.000e-04 

2023-03-02 02:00:15,337 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4827 (0.3404) Acc D Real: 79.485% 
Loss D Fake: 1.1701 (2.6819) Acc D Fake: 16.145% 
Loss D: 1.653 
Loss G: 0.3925 (0.3051) Acc G: 83.831% 
LR: 2.000e-04 

2023-03-02 02:00:15,344 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4469 (0.3412) Acc D Real: 79.513% 
Loss D Fake: 1.1568 (2.6703) Acc D Fake: 16.073% 
Loss D: 1.604 
Loss G: 0.3981 (0.3058) Acc G: 83.903% 
LR: 2.000e-04 

2023-03-02 02:00:15,352 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4707 (0.3422) Acc D Real: 79.512% 
Loss D Fake: 1.1438 (2.6587) Acc D Fake: 16.001% 
Loss D: 1.615 
Loss G: 0.4038 (0.3065) Acc G: 83.975% 
LR: 2.000e-04 

2023-03-02 02:00:15,359 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4433 (0.3430) Acc D Real: 79.547% 
Loss D Fake: 1.1311 (2.6472) Acc D Fake: 15.931% 
Loss D: 1.574 
Loss G: 0.4091 (0.3073) Acc G: 84.045% 
LR: 2.000e-04 

2023-03-02 02:00:15,367 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4844 (0.3440) Acc D Real: 79.522% 
Loss D Fake: 1.1195 (2.6358) Acc D Fake: 15.862% 
Loss D: 1.604 
Loss G: 0.4143 (0.3081) Acc G: 84.114% 
LR: 2.000e-04 

2023-03-02 02:00:15,374 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4761 (0.3450) Acc D Real: 79.519% 
Loss D Fake: 1.1083 (2.6245) Acc D Fake: 15.794% 
Loss D: 1.584 
Loss G: 0.4194 (0.3089) Acc G: 84.182% 
LR: 2.000e-04 

2023-03-02 02:00:15,381 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4806 (0.3460) Acc D Real: 79.512% 
Loss D Fake: 1.0976 (2.6133) Acc D Fake: 15.727% 
Loss D: 1.578 
Loss G: 0.4244 (0.3097) Acc G: 84.250% 
LR: 2.000e-04 

2023-03-02 02:00:15,389 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4693 (0.3469) Acc D Real: 79.527% 
Loss D Fake: 1.0873 (2.6022) Acc D Fake: 15.661% 
Loss D: 1.557 
Loss G: 0.4292 (0.3106) Acc G: 84.316% 
LR: 2.000e-04 

2023-03-02 02:00:15,397 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4836 (0.3479) Acc D Real: 79.523% 
Loss D Fake: 1.0773 (2.5911) Acc D Fake: 15.596% 
Loss D: 1.561 
Loss G: 0.4341 (0.3115) Acc G: 84.381% 
LR: 2.000e-04 

2023-03-02 02:00:15,405 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4885 (0.3489) Acc D Real: 79.515% 
Loss D Fake: 1.0674 (2.5801) Acc D Fake: 15.531% 
Loss D: 1.556 
Loss G: 0.4390 (0.3124) Acc G: 84.446% 
LR: 2.000e-04 

2023-03-02 02:00:15,412 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4884 (0.3499) Acc D Real: 79.514% 
Loss D Fake: 1.0578 (2.5693) Acc D Fake: 15.468% 
Loss D: 1.546 
Loss G: 0.4438 (0.3134) Acc G: 84.509% 
LR: 2.000e-04 

2023-03-02 02:00:15,420 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4827 (0.3509) Acc D Real: 79.526% 
Loss D Fake: 1.0486 (2.5585) Acc D Fake: 15.406% 
Loss D: 1.531 
Loss G: 0.4483 (0.3143) Acc G: 84.572% 
LR: 2.000e-04 

2023-03-02 02:00:15,427 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4974 (0.3519) Acc D Real: 79.514% 
Loss D Fake: 1.0399 (2.5478) Acc D Fake: 15.344% 
Loss D: 1.537 
Loss G: 0.4529 (0.3153) Acc G: 84.634% 
LR: 2.000e-04 

2023-03-02 02:00:15,435 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4660 (0.3527) Acc D Real: 79.552% 
Loss D Fake: 1.0313 (2.5372) Acc D Fake: 15.283% 
Loss D: 1.497 
Loss G: 0.4574 (0.3163) Acc G: 84.694% 
LR: 2.000e-04 

2023-03-02 02:00:15,442 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4749 (0.3535) Acc D Real: 79.580% 
Loss D Fake: 1.0229 (2.5267) Acc D Fake: 15.224% 
Loss D: 1.498 
Loss G: 0.4620 (0.3173) Acc G: 84.754% 
LR: 2.000e-04 

2023-03-02 02:00:15,450 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.5107 (0.3546) Acc D Real: 79.564% 
Loss D Fake: 1.0147 (2.5162) Acc D Fake: 15.165% 
Loss D: 1.525 
Loss G: 0.4664 (0.3183) Acc G: 84.814% 
LR: 2.000e-04 

2023-03-02 02:00:15,457 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.5111 (0.3557) Acc D Real: 79.541% 
Loss D Fake: 1.0067 (2.5059) Acc D Fake: 15.106% 
Loss D: 1.518 
Loss G: 0.4708 (0.3194) Acc G: 84.872% 
LR: 2.000e-04 

2023-03-02 02:00:15,464 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.5048 (0.3567) Acc D Real: 79.537% 
Loss D Fake: 0.9989 (2.4956) Acc D Fake: 15.049% 
Loss D: 1.504 
Loss G: 0.4752 (0.3204) Acc G: 84.929% 
LR: 2.000e-04 

2023-03-02 02:00:15,472 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4945 (0.3576) Acc D Real: 79.552% 
Loss D Fake: 0.9914 (2.4855) Acc D Fake: 14.992% 
Loss D: 1.486 
Loss G: 0.4794 (0.3215) Acc G: 84.986% 
LR: 2.000e-04 

2023-03-02 02:00:15,479 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4880 (0.3585) Acc D Real: 79.572% 
Loss D Fake: 0.9842 (2.4754) Acc D Fake: 14.936% 
Loss D: 1.472 
Loss G: 0.4835 (0.3226) Acc G: 85.042% 
LR: 2.000e-04 

2023-03-02 02:00:15,487 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4676 (0.3592) Acc D Real: 79.612% 
Loss D Fake: 0.9771 (2.4654) Acc D Fake: 14.881% 
Loss D: 1.445 
Loss G: 0.4877 (0.3237) Acc G: 85.098% 
LR: 2.000e-04 

2023-03-02 02:00:15,494 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.5250 (0.3603) Acc D Real: 79.581% 
Loss D Fake: 0.9700 (2.4555) Acc D Fake: 14.827% 
Loss D: 1.495 
Loss G: 0.4920 (0.3248) Acc G: 85.152% 
LR: 2.000e-04 

2023-03-02 02:00:15,501 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.5367 (0.3615) Acc D Real: 79.516% 
Loss D Fake: 0.9627 (2.4457) Acc D Fake: 14.773% 
Loss D: 1.499 
Loss G: 0.4965 (0.3259) Acc G: 85.206% 
LR: 2.000e-04 

2023-03-02 02:00:15,509 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4857 (0.3623) Acc D Real: 79.541% 
Loss D Fake: 0.9552 (2.4360) Acc D Fake: 14.720% 
Loss D: 1.441 
Loss G: 0.5011 (0.3271) Acc G: 85.259% 
LR: 2.000e-04 

2023-03-02 02:00:15,516 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4825 (0.3631) Acc D Real: 79.573% 
Loss D Fake: 0.9478 (2.4263) Acc D Fake: 14.668% 
Loss D: 1.430 
Loss G: 0.5056 (0.3283) Acc G: 85.311% 
LR: 2.000e-04 

2023-03-02 02:00:15,523 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4949 (0.3639) Acc D Real: 79.623% 
Loss D Fake: 0.9409 (2.4167) Acc D Fake: 14.616% 
Loss D: 1.436 
Loss G: 0.5097 (0.3294) Acc G: 85.363% 
LR: 2.000e-04 

2023-03-02 02:00:15,530 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4693 (0.3646) Acc D Real: 79.674% 
Loss D Fake: 0.9347 (2.4072) Acc D Fake: 14.565% 
Loss D: 1.404 
Loss G: 0.5135 (0.3306) Acc G: 85.414% 
LR: 2.000e-04 

2023-03-02 02:00:15,538 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4987 (0.3655) Acc D Real: 79.740% 
Loss D Fake: 0.9290 (2.3978) Acc D Fake: 14.515% 
Loss D: 1.428 
Loss G: 0.5169 (0.3318) Acc G: 85.465% 
LR: 2.000e-04 

2023-03-02 02:00:15,545 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5354 (0.3665) Acc D Real: 79.743% 
Loss D Fake: 0.9246 (2.3885) Acc D Fake: 14.510% 
Loss D: 1.460 
Loss G: 0.5192 (0.3330) Acc G: 85.469% 
LR: 2.000e-04 

2023-03-02 02:00:15,771 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.430 | Generator Loss: 0.519 | Avg: 1.949 
2023-03-02 02:00:15,795 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.457 | Generator Loss: 0.519 | Avg: 1.976 
2023-03-02 02:00:15,820 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.450 | Generator Loss: 0.519 | Avg: 1.969 
2023-03-02 02:00:15,847 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.442 | Generator Loss: 0.519 | Avg: 1.961 
2023-03-02 02:00:15,873 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.442 | Generator Loss: 0.519 | Avg: 1.961 
2023-03-02 02:00:15,900 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.431 | Generator Loss: 0.519 | Avg: 1.950 
2023-03-02 02:00:15,927 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.440 | Generator Loss: 0.519 | Avg: 1.959 
2023-03-02 02:00:15,954 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.438 | Generator Loss: 0.519 | Avg: 1.957 
2023-03-02 02:00:15,984 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.444 | Generator Loss: 0.519 | Avg: 1.963 
2023-03-02 02:00:16,011 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.441 | Generator Loss: 0.519 | Avg: 1.960 
2023-03-02 02:00:16,039 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.437 | Generator Loss: 0.519 | Avg: 1.956 
2023-03-02 02:00:16,064 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.425 | Generator Loss: 0.519 | Avg: 1.944 
2023-03-02 02:00:16,090 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.426 | Generator Loss: 0.519 | Avg: 1.945 
2023-03-02 02:00:16,116 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.430 | Generator Loss: 0.519 | Avg: 1.949 
2023-03-02 02:00:16,147 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.433 | Generator Loss: 0.519 | Avg: 1.952 
2023-03-02 02:00:16,179 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.434 | Generator Loss: 0.519 | Avg: 1.953 
2023-03-02 02:00:16,207 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.434 | Generator Loss: 0.519 | Avg: 1.953 
2023-03-02 02:00:16,245 -                train: [    INFO] - 
Epoch: 8/20
2023-03-02 02:00:16,457 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4958 (0.5029) Acc D Real: 82.682% 
Loss D Fake: 0.9178 (0.9196) Acc D Fake: 6.667% 
Loss D: 1.414 
Loss G: 0.5238 (0.5226) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,465 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5056 (0.5038) Acc D Real: 83.663% 
Loss D Fake: 0.9141 (0.9178) Acc D Fake: 6.667% 
Loss D: 1.420 
Loss G: 0.5262 (0.5238) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,474 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.5144 (0.5064) Acc D Real: 84.505% 
Loss D Fake: 0.9107 (0.9160) Acc D Fake: 6.667% 
Loss D: 1.425 
Loss G: 0.5282 (0.5249) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,496 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.5174 (0.5086) Acc D Real: 84.562% 
Loss D Fake: 0.9080 (0.9144) Acc D Fake: 6.667% 
Loss D: 1.425 
Loss G: 0.5299 (0.5259) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,504 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.5340 (0.5128) Acc D Real: 83.438% 
Loss D Fake: 0.9054 (0.9129) Acc D Fake: 6.667% 
Loss D: 1.439 
Loss G: 0.5317 (0.5269) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,512 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4994 (0.5109) Acc D Real: 83.824% 
Loss D Fake: 0.9027 (0.9114) Acc D Fake: 6.667% 
Loss D: 1.402 
Loss G: 0.5335 (0.5278) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,519 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.5159 (0.5115) Acc D Real: 83.405% 
Loss D Fake: 0.9000 (0.9100) Acc D Fake: 6.667% 
Loss D: 1.416 
Loss G: 0.5355 (0.5288) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,526 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.5371 (0.5144) Acc D Real: 82.888% 
Loss D Fake: 0.8971 (0.9086) Acc D Fake: 6.667% 
Loss D: 1.434 
Loss G: 0.5374 (0.5297) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,534 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.5261 (0.5156) Acc D Real: 82.807% 
Loss D Fake: 0.8944 (0.9072) Acc D Fake: 6.667% 
Loss D: 1.420 
Loss G: 0.5392 (0.5307) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,541 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.5243 (0.5163) Acc D Real: 82.566% 
Loss D Fake: 0.8918 (0.9058) Acc D Fake: 6.667% 
Loss D: 1.416 
Loss G: 0.5410 (0.5316) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,549 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.5055 (0.5154) Acc D Real: 82.326% 
Loss D Fake: 0.8889 (0.9044) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5433 (0.5326) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,558 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.5070 (0.5148) Acc D Real: 82.007% 
Loss D Fake: 0.8853 (0.9029) Acc D Fake: 6.667% 
Loss D: 1.392 
Loss G: 0.5460 (0.5336) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,566 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4886 (0.5129) Acc D Real: 82.165% 
Loss D Fake: 0.8812 (0.9014) Acc D Fake: 6.667% 
Loss D: 1.370 
Loss G: 0.5490 (0.5347) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,574 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.5095 (0.5127) Acc D Real: 82.257% 
Loss D Fake: 0.8770 (0.8997) Acc D Fake: 6.667% 
Loss D: 1.387 
Loss G: 0.5518 (0.5359) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,583 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.5083 (0.5124) Acc D Real: 82.116% 
Loss D Fake: 0.8730 (0.8981) Acc D Fake: 6.667% 
Loss D: 1.381 
Loss G: 0.5549 (0.5371) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,593 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.5248 (0.5131) Acc D Real: 82.037% 
Loss D Fake: 0.8689 (0.8963) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5577 (0.5383) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,602 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.5038 (0.5126) Acc D Real: 82.219% 
Loss D Fake: 0.8650 (0.8946) Acc D Fake: 6.667% 
Loss D: 1.369 
Loss G: 0.5605 (0.5395) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,610 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.5328 (0.5137) Acc D Real: 81.804% 
Loss D Fake: 0.8612 (0.8928) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5634 (0.5408) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,619 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.5347 (0.5147) Acc D Real: 81.714% 
Loss D Fake: 0.8573 (0.8911) Acc D Fake: 6.667% 
Loss D: 1.392 
Loss G: 0.5661 (0.5420) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,627 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.5343 (0.5157) Acc D Real: 81.845% 
Loss D Fake: 0.8539 (0.8893) Acc D Fake: 6.667% 
Loss D: 1.388 
Loss G: 0.5683 (0.5433) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,635 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.5086 (0.5153) Acc D Real: 81.856% 
Loss D Fake: 0.8511 (0.8876) Acc D Fake: 6.667% 
Loss D: 1.360 
Loss G: 0.5705 (0.5445) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,642 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.5339 (0.5162) Acc D Real: 81.927% 
Loss D Fake: 0.8483 (0.8859) Acc D Fake: 6.667% 
Loss D: 1.382 
Loss G: 0.5724 (0.5457) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,649 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.5209 (0.5163) Acc D Real: 81.858% 
Loss D Fake: 0.8458 (0.8842) Acc D Fake: 6.667% 
Loss D: 1.367 
Loss G: 0.5744 (0.5469) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,656 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.5234 (0.5166) Acc D Real: 82.096% 
Loss D Fake: 0.8434 (0.8826) Acc D Fake: 6.667% 
Loss D: 1.367 
Loss G: 0.5759 (0.5481) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,664 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.5264 (0.5170) Acc D Real: 82.488% 
Loss D Fake: 0.8418 (0.8810) Acc D Fake: 6.667% 
Loss D: 1.368 
Loss G: 0.5767 (0.5492) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,672 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.5118 (0.5168) Acc D Real: 82.296% 
Loss D Fake: 0.8408 (0.8795) Acc D Fake: 6.667% 
Loss D: 1.353 
Loss G: 0.5779 (0.5503) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,680 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.5690 (0.5187) Acc D Real: 81.955% 
Loss D Fake: 0.8390 (0.8780) Acc D Fake: 6.667% 
Loss D: 1.408 
Loss G: 0.5793 (0.5513) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,688 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.5572 (0.5200) Acc D Real: 81.909% 
Loss D Fake: 0.8372 (0.8766) Acc D Fake: 6.667% 
Loss D: 1.394 
Loss G: 0.5804 (0.5523) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,696 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.5184 (0.5200) Acc D Real: 81.892% 
Loss D Fake: 0.8358 (0.8753) Acc D Fake: 6.667% 
Loss D: 1.354 
Loss G: 0.5816 (0.5533) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,704 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.5283 (0.5202) Acc D Real: 81.976% 
Loss D Fake: 0.8343 (0.8740) Acc D Fake: 6.667% 
Loss D: 1.363 
Loss G: 0.5826 (0.5542) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,712 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.5238 (0.5203) Acc D Real: 82.077% 
Loss D Fake: 0.8330 (0.8727) Acc D Fake: 6.667% 
Loss D: 1.357 
Loss G: 0.5835 (0.5551) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,719 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4693 (0.5188) Acc D Real: 82.303% 
Loss D Fake: 0.8319 (0.8714) Acc D Fake: 6.667% 
Loss D: 1.301 
Loss G: 0.5845 (0.5560) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,727 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5326 (0.5192) Acc D Real: 82.587% 
Loss D Fake: 0.8307 (0.8702) Acc D Fake: 6.667% 
Loss D: 1.363 
Loss G: 0.5849 (0.5569) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,735 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5440 (0.5199) Acc D Real: 82.506% 
Loss D Fake: 0.8304 (0.8691) Acc D Fake: 6.667% 
Loss D: 1.374 
Loss G: 0.5852 (0.5577) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,743 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4593 (0.5182) Acc D Real: 82.577% 
Loss D Fake: 0.8297 (0.8680) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.5861 (0.5585) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,751 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4690 (0.5169) Acc D Real: 82.672% 
Loss D Fake: 0.8281 (0.8669) Acc D Fake: 6.667% 
Loss D: 1.297 
Loss G: 0.5877 (0.5593) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,761 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5020 (0.5165) Acc D Real: 82.797% 
Loss D Fake: 0.8260 (0.8659) Acc D Fake: 6.667% 
Loss D: 1.328 
Loss G: 0.5892 (0.5601) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,770 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4961 (0.5160) Acc D Real: 82.985% 
Loss D Fake: 0.8242 (0.8648) Acc D Fake: 6.667% 
Loss D: 1.320 
Loss G: 0.5904 (0.5608) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,779 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5503 (0.5168) Acc D Real: 82.999% 
Loss D Fake: 0.8229 (0.8637) Acc D Fake: 6.667% 
Loss D: 1.373 
Loss G: 0.5912 (0.5616) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,788 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5102 (0.5167) Acc D Real: 82.942% 
Loss D Fake: 0.8219 (0.8627) Acc D Fake: 6.667% 
Loss D: 1.332 
Loss G: 0.5922 (0.5623) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,797 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4951 (0.5162) Acc D Real: 83.047% 
Loss D Fake: 0.8205 (0.8617) Acc D Fake: 6.667% 
Loss D: 1.316 
Loss G: 0.5933 (0.5631) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,806 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.5174 (0.5162) Acc D Real: 83.161% 
Loss D Fake: 0.8193 (0.8607) Acc D Fake: 6.667% 
Loss D: 1.337 
Loss G: 0.5941 (0.5638) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,814 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.5368 (0.5167) Acc D Real: 83.252% 
Loss D Fake: 0.8185 (0.8598) Acc D Fake: 6.667% 
Loss D: 1.355 
Loss G: 0.5944 (0.5645) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,823 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.5028 (0.5163) Acc D Real: 83.229% 
Loss D Fake: 0.8180 (0.8588) Acc D Fake: 6.667% 
Loss D: 1.321 
Loss G: 0.5950 (0.5652) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,832 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4950 (0.5159) Acc D Real: 83.293% 
Loss D Fake: 0.8171 (0.8579) Acc D Fake: 6.667% 
Loss D: 1.312 
Loss G: 0.5958 (0.5658) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,841 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.5235 (0.5160) Acc D Real: 83.317% 
Loss D Fake: 0.8161 (0.8570) Acc D Fake: 6.667% 
Loss D: 1.340 
Loss G: 0.5965 (0.5665) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,850 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.5445 (0.5166) Acc D Real: 83.403% 
Loss D Fake: 0.8154 (0.8562) Acc D Fake: 6.667% 
Loss D: 1.360 
Loss G: 0.5967 (0.5671) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,860 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.5269 (0.5168) Acc D Real: 83.440% 
Loss D Fake: 0.8154 (0.8553) Acc D Fake: 6.667% 
Loss D: 1.342 
Loss G: 0.5966 (0.5677) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,870 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4793 (0.5161) Acc D Real: 83.477% 
Loss D Fake: 0.8153 (0.8545) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5969 (0.5683) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,878 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.5041 (0.5159) Acc D Real: 83.599% 
Loss D Fake: 0.8148 (0.8538) Acc D Fake: 6.667% 
Loss D: 1.319 
Loss G: 0.5972 (0.5689) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,886 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.5050 (0.5157) Acc D Real: 83.662% 
Loss D Fake: 0.8145 (0.8530) Acc D Fake: 6.667% 
Loss D: 1.320 
Loss G: 0.5974 (0.5694) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,896 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.5153 (0.5156) Acc D Real: 83.648% 
Loss D Fake: 0.8141 (0.8523) Acc D Fake: 6.667% 
Loss D: 1.329 
Loss G: 0.5978 (0.5700) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,905 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.5489 (0.5163) Acc D Real: 83.597% 
Loss D Fake: 0.8136 (0.8516) Acc D Fake: 6.667% 
Loss D: 1.362 
Loss G: 0.5981 (0.5705) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,914 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4893 (0.5158) Acc D Real: 83.680% 
Loss D Fake: 0.8132 (0.8509) Acc D Fake: 6.667% 
Loss D: 1.303 
Loss G: 0.5984 (0.5710) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,923 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4667 (0.5149) Acc D Real: 83.787% 
Loss D Fake: 0.8127 (0.8502) Acc D Fake: 6.667% 
Loss D: 1.279 
Loss G: 0.5989 (0.5715) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,932 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.5330 (0.5152) Acc D Real: 83.912% 
Loss D Fake: 0.8122 (0.8495) Acc D Fake: 6.667% 
Loss D: 1.345 
Loss G: 0.5989 (0.5720) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,940 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.5350 (0.5156) Acc D Real: 83.878% 
Loss D Fake: 0.8124 (0.8489) Acc D Fake: 6.667% 
Loss D: 1.347 
Loss G: 0.5987 (0.5724) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,949 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.5008 (0.5153) Acc D Real: 83.955% 
Loss D Fake: 0.8126 (0.8483) Acc D Fake: 6.667% 
Loss D: 1.313 
Loss G: 0.5985 (0.5729) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,957 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4575 (0.5143) Acc D Real: 84.022% 
Loss D Fake: 0.8126 (0.8477) Acc D Fake: 6.667% 
Loss D: 1.270 
Loss G: 0.5988 (0.5733) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,965 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4951 (0.5140) Acc D Real: 84.097% 
Loss D Fake: 0.8121 (0.8471) Acc D Fake: 6.667% 
Loss D: 1.307 
Loss G: 0.5991 (0.5737) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,973 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.5327 (0.5143) Acc D Real: 84.127% 
Loss D Fake: 0.8118 (0.8465) Acc D Fake: 6.667% 
Loss D: 1.344 
Loss G: 0.5991 (0.5741) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,981 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4841 (0.5138) Acc D Real: 84.238% 
Loss D Fake: 0.8119 (0.8460) Acc D Fake: 6.667% 
Loss D: 1.296 
Loss G: 0.5990 (0.5745) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,990 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4613 (0.5130) Acc D Real: 84.351% 
Loss D Fake: 0.8119 (0.8454) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.5990 (0.5749) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:16,997 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4800 (0.5125) Acc D Real: 84.405% 
Loss D Fake: 0.8117 (0.8449) Acc D Fake: 6.667% 
Loss D: 1.292 
Loss G: 0.5993 (0.5753) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,005 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.5025 (0.5124) Acc D Real: 84.408% 
Loss D Fake: 0.8112 (0.8444) Acc D Fake: 6.667% 
Loss D: 1.314 
Loss G: 0.5997 (0.5756) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,013 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4839 (0.5119) Acc D Real: 84.537% 
Loss D Fake: 0.8108 (0.8439) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.5999 (0.5760) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,021 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4740 (0.5114) Acc D Real: 84.611% 
Loss D Fake: 0.8105 (0.8434) Acc D Fake: 6.667% 
Loss D: 1.284 
Loss G: 0.6002 (0.5764) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,028 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4482 (0.5105) Acc D Real: 84.682% 
Loss D Fake: 0.8099 (0.8429) Acc D Fake: 6.667% 
Loss D: 1.258 
Loss G: 0.6008 (0.5767) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,036 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4844 (0.5101) Acc D Real: 84.648% 
Loss D Fake: 0.8088 (0.8424) Acc D Fake: 6.667% 
Loss D: 1.293 
Loss G: 0.6019 (0.5771) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,044 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.5279 (0.5103) Acc D Real: 84.711% 
Loss D Fake: 0.8076 (0.8419) Acc D Fake: 6.667% 
Loss D: 1.336 
Loss G: 0.6025 (0.5774) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,051 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4841 (0.5100) Acc D Real: 84.795% 
Loss D Fake: 0.8071 (0.8415) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6029 (0.5778) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,059 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4906 (0.5097) Acc D Real: 84.871% 
Loss D Fake: 0.8067 (0.8410) Acc D Fake: 6.667% 
Loss D: 1.297 
Loss G: 0.6030 (0.5781) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,066 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4816 (0.5093) Acc D Real: 84.988% 
Loss D Fake: 0.8067 (0.8405) Acc D Fake: 6.667% 
Loss D: 1.288 
Loss G: 0.6029 (0.5785) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,074 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4660 (0.5088) Acc D Real: 85.060% 
Loss D Fake: 0.8068 (0.8401) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.6029 (0.5788) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,081 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4843 (0.5084) Acc D Real: 85.036% 
Loss D Fake: 0.8065 (0.8396) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6034 (0.5791) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,089 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.5580 (0.5091) Acc D Real: 84.948% 
Loss D Fake: 0.8058 (0.8392) Acc D Fake: 6.667% 
Loss D: 1.364 
Loss G: 0.6038 (0.5794) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,096 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4852 (0.5088) Acc D Real: 85.015% 
Loss D Fake: 0.8055 (0.8388) Acc D Fake: 6.667% 
Loss D: 1.291 
Loss G: 0.6040 (0.5798) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,103 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4833 (0.5084) Acc D Real: 85.067% 
Loss D Fake: 0.8052 (0.8383) Acc D Fake: 6.667% 
Loss D: 1.289 
Loss G: 0.6042 (0.5801) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,111 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4808 (0.5081) Acc D Real: 85.124% 
Loss D Fake: 0.8049 (0.8379) Acc D Fake: 6.667% 
Loss D: 1.286 
Loss G: 0.6044 (0.5804) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,120 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4303 (0.5071) Acc D Real: 85.199% 
Loss D Fake: 0.8044 (0.8375) Acc D Fake: 6.667% 
Loss D: 1.235 
Loss G: 0.6050 (0.5807) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,127 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4913 (0.5070) Acc D Real: 85.248% 
Loss D Fake: 0.8036 (0.8371) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.6056 (0.5810) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,135 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4666 (0.5065) Acc D Real: 85.297% 
Loss D Fake: 0.8028 (0.8367) Acc D Fake: 6.667% 
Loss D: 1.269 
Loss G: 0.6063 (0.5813) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,143 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4367 (0.5056) Acc D Real: 85.368% 
Loss D Fake: 0.8019 (0.8363) Acc D Fake: 6.667% 
Loss D: 1.239 
Loss G: 0.6072 (0.5816) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,151 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4789 (0.5053) Acc D Real: 85.394% 
Loss D Fake: 0.8006 (0.8358) Acc D Fake: 6.667% 
Loss D: 1.279 
Loss G: 0.6082 (0.5819) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,158 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4658 (0.5049) Acc D Real: 85.399% 
Loss D Fake: 0.7992 (0.8354) Acc D Fake: 6.667% 
Loss D: 1.265 
Loss G: 0.6096 (0.5822) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,166 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4658 (0.5044) Acc D Real: 85.457% 
Loss D Fake: 0.7975 (0.8350) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6110 (0.5826) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,173 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4851 (0.5042) Acc D Real: 85.509% 
Loss D Fake: 0.7959 (0.8345) Acc D Fake: 6.667% 
Loss D: 1.281 
Loss G: 0.6122 (0.5829) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,181 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4928 (0.5041) Acc D Real: 85.546% 
Loss D Fake: 0.7947 (0.8341) Acc D Fake: 6.667% 
Loss D: 1.287 
Loss G: 0.6130 (0.5832) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,189 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4603 (0.5036) Acc D Real: 85.580% 
Loss D Fake: 0.7937 (0.8336) Acc D Fake: 6.667% 
Loss D: 1.254 
Loss G: 0.6139 (0.5836) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,196 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4635 (0.5031) Acc D Real: 85.610% 
Loss D Fake: 0.7924 (0.8332) Acc D Fake: 6.667% 
Loss D: 1.256 
Loss G: 0.6151 (0.5839) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,204 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4856 (0.5029) Acc D Real: 85.688% 
Loss D Fake: 0.7912 (0.8327) Acc D Fake: 6.667% 
Loss D: 1.277 
Loss G: 0.6158 (0.5843) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,212 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.5073 (0.5030) Acc D Real: 85.720% 
Loss D Fake: 0.7905 (0.8323) Acc D Fake: 6.667% 
Loss D: 1.298 
Loss G: 0.6161 (0.5846) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,219 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4738 (0.5027) Acc D Real: 85.750% 
Loss D Fake: 0.7902 (0.8318) Acc D Fake: 6.667% 
Loss D: 1.264 
Loss G: 0.6164 (0.5849) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,227 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4349 (0.5020) Acc D Real: 85.747% 
Loss D Fake: 0.7896 (0.8314) Acc D Fake: 6.667% 
Loss D: 1.224 
Loss G: 0.6174 (0.5853) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,235 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4840 (0.5018) Acc D Real: 85.782% 
Loss D Fake: 0.7882 (0.8309) Acc D Fake: 6.667% 
Loss D: 1.272 
Loss G: 0.6184 (0.5856) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,242 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4950 (0.5017) Acc D Real: 85.827% 
Loss D Fake: 0.7873 (0.8305) Acc D Fake: 6.667% 
Loss D: 1.282 
Loss G: 0.6189 (0.5860) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,250 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4589 (0.5013) Acc D Real: 85.891% 
Loss D Fake: 0.7868 (0.8300) Acc D Fake: 6.667% 
Loss D: 1.246 
Loss G: 0.6194 (0.5863) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,257 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.5362 (0.5016) Acc D Real: 85.897% 
Loss D Fake: 0.7864 (0.8296) Acc D Fake: 6.667% 
Loss D: 1.323 
Loss G: 0.6193 (0.5867) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,265 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4113 (0.5007) Acc D Real: 85.954% 
Loss D Fake: 0.7864 (0.8292) Acc D Fake: 6.667% 
Loss D: 1.198 
Loss G: 0.6197 (0.5870) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,272 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4491 (0.5002) Acc D Real: 86.029% 
Loss D Fake: 0.7857 (0.8287) Acc D Fake: 6.667% 
Loss D: 1.235 
Loss G: 0.6203 (0.5873) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,280 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4707 (0.4999) Acc D Real: 86.071% 
Loss D Fake: 0.7850 (0.8283) Acc D Fake: 6.667% 
Loss D: 1.256 
Loss G: 0.6208 (0.5876) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,288 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4868 (0.4998) Acc D Real: 86.088% 
Loss D Fake: 0.7845 (0.8279) Acc D Fake: 6.667% 
Loss D: 1.271 
Loss G: 0.6212 (0.5880) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,296 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4528 (0.4993) Acc D Real: 86.127% 
Loss D Fake: 0.7839 (0.8275) Acc D Fake: 6.667% 
Loss D: 1.237 
Loss G: 0.6218 (0.5883) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,303 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4490 (0.4989) Acc D Real: 86.195% 
Loss D Fake: 0.7831 (0.8270) Acc D Fake: 6.667% 
Loss D: 1.232 
Loss G: 0.6225 (0.5886) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,311 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4906 (0.4988) Acc D Real: 86.222% 
Loss D Fake: 0.7824 (0.8266) Acc D Fake: 6.667% 
Loss D: 1.273 
Loss G: 0.6229 (0.5889) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,318 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.5259 (0.4990) Acc D Real: 86.249% 
Loss D Fake: 0.7822 (0.8262) Acc D Fake: 6.667% 
Loss D: 1.308 
Loss G: 0.6227 (0.5893) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,326 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4870 (0.4989) Acc D Real: 86.305% 
Loss D Fake: 0.7827 (0.8258) Acc D Fake: 6.667% 
Loss D: 1.270 
Loss G: 0.6221 (0.5896) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,334 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4044 (0.4981) Acc D Real: 86.347% 
Loss D Fake: 0.7831 (0.8254) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6221 (0.5899) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,341 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4633 (0.4977) Acc D Real: 86.405% 
Loss D Fake: 0.7829 (0.8250) Acc D Fake: 6.667% 
Loss D: 1.246 
Loss G: 0.6222 (0.5902) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,349 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4328 (0.4972) Acc D Real: 86.397% 
Loss D Fake: 0.7825 (0.8246) Acc D Fake: 6.667% 
Loss D: 1.215 
Loss G: 0.6229 (0.5904) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,357 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4712 (0.4969) Acc D Real: 86.435% 
Loss D Fake: 0.7815 (0.8243) Acc D Fake: 6.667% 
Loss D: 1.253 
Loss G: 0.6237 (0.5907) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,365 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4058 (0.4961) Acc D Real: 86.488% 
Loss D Fake: 0.7804 (0.8239) Acc D Fake: 6.667% 
Loss D: 1.186 
Loss G: 0.6249 (0.5910) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,372 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4633 (0.4958) Acc D Real: 86.512% 
Loss D Fake: 0.7789 (0.8235) Acc D Fake: 6.667% 
Loss D: 1.242 
Loss G: 0.6261 (0.5914) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,380 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3753 (0.4948) Acc D Real: 86.529% 
Loss D Fake: 0.7772 (0.8231) Acc D Fake: 6.667% 
Loss D: 1.152 
Loss G: 0.6281 (0.5917) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,388 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4846 (0.4947) Acc D Real: 86.541% 
Loss D Fake: 0.7747 (0.8226) Acc D Fake: 6.667% 
Loss D: 1.259 
Loss G: 0.6300 (0.5920) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,395 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4375 (0.4942) Acc D Real: 86.583% 
Loss D Fake: 0.7726 (0.8222) Acc D Fake: 6.667% 
Loss D: 1.210 
Loss G: 0.6318 (0.5923) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,402 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4568 (0.4939) Acc D Real: 86.599% 
Loss D Fake: 0.7706 (0.8218) Acc D Fake: 6.667% 
Loss D: 1.227 
Loss G: 0.6335 (0.5927) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,410 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4497 (0.4935) Acc D Real: 86.643% 
Loss D Fake: 0.7687 (0.8213) Acc D Fake: 6.667% 
Loss D: 1.218 
Loss G: 0.6351 (0.5930) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,417 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4613 (0.4933) Acc D Real: 86.673% 
Loss D Fake: 0.7670 (0.8209) Acc D Fake: 6.667% 
Loss D: 1.228 
Loss G: 0.6364 (0.5934) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,425 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4475 (0.4929) Acc D Real: 86.730% 
Loss D Fake: 0.7657 (0.8204) Acc D Fake: 6.667% 
Loss D: 1.213 
Loss G: 0.6374 (0.5938) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,432 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4399 (0.4924) Acc D Real: 86.765% 
Loss D Fake: 0.7645 (0.8200) Acc D Fake: 6.667% 
Loss D: 1.204 
Loss G: 0.6386 (0.5941) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,440 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4625 (0.4922) Acc D Real: 86.821% 
Loss D Fake: 0.7633 (0.8195) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6394 (0.5945) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,447 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4493 (0.4918) Acc D Real: 86.876% 
Loss D Fake: 0.7625 (0.8190) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.6399 (0.5949) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,455 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3911 (0.4910) Acc D Real: 86.912% 
Loss D Fake: 0.7617 (0.8186) Acc D Fake: 6.667% 
Loss D: 1.153 
Loss G: 0.6410 (0.5952) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,463 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4838 (0.4910) Acc D Real: 86.951% 
Loss D Fake: 0.7604 (0.8181) Acc D Fake: 6.667% 
Loss D: 1.244 
Loss G: 0.6418 (0.5956) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,471 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4971 (0.4910) Acc D Real: 86.975% 
Loss D Fake: 0.7598 (0.8177) Acc D Fake: 6.667% 
Loss D: 1.257 
Loss G: 0.6421 (0.5960) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,478 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4646 (0.4908) Acc D Real: 86.992% 
Loss D Fake: 0.7596 (0.8172) Acc D Fake: 6.667% 
Loss D: 1.224 
Loss G: 0.6424 (0.5963) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,486 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4614 (0.4906) Acc D Real: 87.034% 
Loss D Fake: 0.7593 (0.8168) Acc D Fake: 6.667% 
Loss D: 1.221 
Loss G: 0.6425 (0.5967) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,493 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4815 (0.4905) Acc D Real: 87.080% 
Loss D Fake: 0.7594 (0.8163) Acc D Fake: 6.667% 
Loss D: 1.241 
Loss G: 0.6420 (0.5970) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,501 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4540 (0.4903) Acc D Real: 87.094% 
Loss D Fake: 0.7599 (0.8159) Acc D Fake: 6.667% 
Loss D: 1.214 
Loss G: 0.6418 (0.5974) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,508 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.5036 (0.4904) Acc D Real: 87.138% 
Loss D Fake: 0.7603 (0.8155) Acc D Fake: 6.667% 
Loss D: 1.264 
Loss G: 0.6410 (0.5977) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,516 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.5013 (0.4904) Acc D Real: 87.150% 
Loss D Fake: 0.7613 (0.8151) Acc D Fake: 6.667% 
Loss D: 1.263 
Loss G: 0.6399 (0.5980) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,523 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4166 (0.4899) Acc D Real: 87.207% 
Loss D Fake: 0.7625 (0.8147) Acc D Fake: 6.667% 
Loss D: 1.179 
Loss G: 0.6391 (0.5983) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,531 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4706 (0.4897) Acc D Real: 87.231% 
Loss D Fake: 0.7633 (0.8143) Acc D Fake: 6.667% 
Loss D: 1.234 
Loss G: 0.6383 (0.5986) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,538 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4616 (0.4895) Acc D Real: 87.235% 
Loss D Fake: 0.7640 (0.8139) Acc D Fake: 6.667% 
Loss D: 1.226 
Loss G: 0.6378 (0.5989) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,546 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4611 (0.4893) Acc D Real: 87.255% 
Loss D Fake: 0.7644 (0.8136) Acc D Fake: 6.667% 
Loss D: 1.225 
Loss G: 0.6375 (0.5992) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,553 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4237 (0.4888) Acc D Real: 87.295% 
Loss D Fake: 0.7645 (0.8132) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6376 (0.5995) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,561 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4364 (0.4885) Acc D Real: 87.315% 
Loss D Fake: 0.7642 (0.8129) Acc D Fake: 6.667% 
Loss D: 1.201 
Loss G: 0.6380 (0.5998) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,568 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.5076 (0.4886) Acc D Real: 87.337% 
Loss D Fake: 0.7639 (0.8125) Acc D Fake: 6.667% 
Loss D: 1.271 
Loss G: 0.6378 (0.6000) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,575 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4452 (0.4883) Acc D Real: 87.359% 
Loss D Fake: 0.7642 (0.8122) Acc D Fake: 6.667% 
Loss D: 1.209 
Loss G: 0.6376 (0.6003) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,583 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.5307 (0.4886) Acc D Real: 87.393% 
Loss D Fake: 0.7647 (0.8118) Acc D Fake: 6.667% 
Loss D: 1.295 
Loss G: 0.6365 (0.6006) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,590 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4836 (0.4886) Acc D Real: 87.410% 
Loss D Fake: 0.7663 (0.8115) Acc D Fake: 6.667% 
Loss D: 1.250 
Loss G: 0.6350 (0.6008) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,597 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4845 (0.4885) Acc D Real: 87.438% 
Loss D Fake: 0.7680 (0.8112) Acc D Fake: 6.667% 
Loss D: 1.252 
Loss G: 0.6333 (0.6010) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,605 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4379 (0.4882) Acc D Real: 87.461% 
Loss D Fake: 0.7698 (0.8109) Acc D Fake: 6.667% 
Loss D: 1.208 
Loss G: 0.6320 (0.6012) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,612 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4079 (0.4876) Acc D Real: 87.503% 
Loss D Fake: 0.7709 (0.8106) Acc D Fake: 6.667% 
Loss D: 1.179 
Loss G: 0.6313 (0.6014) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,620 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4624 (0.4875) Acc D Real: 87.538% 
Loss D Fake: 0.7715 (0.8104) Acc D Fake: 6.667% 
Loss D: 1.234 
Loss G: 0.6306 (0.6016) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,627 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.5170 (0.4877) Acc D Real: 87.564% 
Loss D Fake: 0.7726 (0.8101) Acc D Fake: 6.667% 
Loss D: 1.290 
Loss G: 0.6292 (0.6018) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,635 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5060 (0.4878) Acc D Real: 87.574% 
Loss D Fake: 0.7745 (0.8099) Acc D Fake: 6.667% 
Loss D: 1.280 
Loss G: 0.6273 (0.6020) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,642 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4358 (0.4874) Acc D Real: 87.597% 
Loss D Fake: 0.7766 (0.8097) Acc D Fake: 6.667% 
Loss D: 1.212 
Loss G: 0.6257 (0.6022) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,650 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4367 (0.4871) Acc D Real: 87.630% 
Loss D Fake: 0.7781 (0.8095) Acc D Fake: 6.667% 
Loss D: 1.215 
Loss G: 0.6244 (0.6023) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,657 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4133 (0.4866) Acc D Real: 87.639% 
Loss D Fake: 0.7792 (0.8093) Acc D Fake: 6.667% 
Loss D: 1.192 
Loss G: 0.6239 (0.6024) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,664 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4949 (0.4867) Acc D Real: 87.669% 
Loss D Fake: 0.7797 (0.8091) Acc D Fake: 6.667% 
Loss D: 1.275 
Loss G: 0.6230 (0.6026) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,672 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4040 (0.4861) Acc D Real: 87.692% 
Loss D Fake: 0.7807 (0.8089) Acc D Fake: 6.667% 
Loss D: 1.185 
Loss G: 0.6226 (0.6027) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,679 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4519 (0.4859) Acc D Real: 87.705% 
Loss D Fake: 0.7809 (0.8087) Acc D Fake: 6.667% 
Loss D: 1.233 
Loss G: 0.6225 (0.6028) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,686 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4346 (0.4856) Acc D Real: 87.745% 
Loss D Fake: 0.7809 (0.8085) Acc D Fake: 6.667% 
Loss D: 1.216 
Loss G: 0.6224 (0.6030) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,694 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4878 (0.4856) Acc D Real: 87.777% 
Loss D Fake: 0.7813 (0.8083) Acc D Fake: 6.667% 
Loss D: 1.269 
Loss G: 0.6215 (0.6031) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,702 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4614 (0.4854) Acc D Real: 87.782% 
Loss D Fake: 0.7827 (0.8082) Acc D Fake: 6.667% 
Loss D: 1.244 
Loss G: 0.6199 (0.6032) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:17,947 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.374 | Generator Loss: 0.620 | Avg: 1.994 
2023-03-02 02:00:17,969 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.356 | Generator Loss: 0.620 | Avg: 1.976 
2023-03-02 02:00:17,992 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.361 | Generator Loss: 0.620 | Avg: 1.980 
2023-03-02 02:00:18,017 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.364 | Generator Loss: 0.620 | Avg: 1.984 
2023-03-02 02:00:18,043 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.369 | Generator Loss: 0.620 | Avg: 1.989 
2023-03-02 02:00:18,069 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.331 | Generator Loss: 0.620 | Avg: 1.950 
2023-03-02 02:00:18,095 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.333 | Generator Loss: 0.620 | Avg: 1.952 
2023-03-02 02:00:18,122 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.308 | Generator Loss: 0.620 | Avg: 1.928 
2023-03-02 02:00:18,148 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.300 | Generator Loss: 0.620 | Avg: 1.920 
2023-03-02 02:00:18,174 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.268 | Generator Loss: 0.620 | Avg: 1.888 
2023-03-02 02:00:18,200 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.248 | Generator Loss: 0.620 | Avg: 1.868 
2023-03-02 02:00:18,226 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.223 | Generator Loss: 0.620 | Avg: 1.842 
2023-03-02 02:00:18,252 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.206 | Generator Loss: 0.620 | Avg: 1.825 
2023-03-02 02:00:18,278 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.200 | Generator Loss: 0.620 | Avg: 1.820 
2023-03-02 02:00:18,304 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.212 | Generator Loss: 0.620 | Avg: 1.832 
2023-03-02 02:00:18,330 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.223 | Generator Loss: 0.620 | Avg: 1.843 
2023-03-02 02:00:18,355 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.232 | Generator Loss: 0.620 | Avg: 1.852 
2023-03-02 02:00:18,389 -                train: [    INFO] - 
Epoch: 9/20
2023-03-02 02:00:18,579 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4031 (0.4085) Acc D Real: 90.260% 
Loss D Fake: 0.7851 (0.7848) Acc D Fake: 6.667% 
Loss D: 1.188 
Loss G: 0.6186 (0.6187) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,588 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4381 (0.4183) Acc D Real: 91.076% 
Loss D Fake: 0.7852 (0.7849) Acc D Fake: 6.667% 
Loss D: 1.223 
Loss G: 0.6184 (0.6186) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,596 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4191 (0.4185) Acc D Real: 91.016% 
Loss D Fake: 0.7852 (0.7850) Acc D Fake: 6.667% 
Loss D: 1.204 
Loss G: 0.6186 (0.6186) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,613 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4022 (0.4153) Acc D Real: 91.510% 
Loss D Fake: 0.7846 (0.7849) Acc D Fake: 6.667% 
Loss D: 1.187 
Loss G: 0.6192 (0.6187) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,621 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4186 (0.4158) Acc D Real: 91.918% 
Loss D Fake: 0.7838 (0.7847) Acc D Fake: 6.667% 
Loss D: 1.202 
Loss G: 0.6199 (0.6189) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,628 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4688 (0.4234) Acc D Real: 91.153% 
Loss D Fake: 0.7830 (0.7845) Acc D Fake: 6.667% 
Loss D: 1.252 
Loss G: 0.6206 (0.6192) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,635 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3392 (0.4129) Acc D Real: 91.823% 
Loss D Fake: 0.7819 (0.7841) Acc D Fake: 6.667% 
Loss D: 1.121 
Loss G: 0.6221 (0.6195) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,643 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3386 (0.4046) Acc D Real: 92.193% 
Loss D Fake: 0.7795 (0.7836) Acc D Fake: 6.667% 
Loss D: 1.118 
Loss G: 0.6247 (0.6201) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,650 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4533 (0.4095) Acc D Real: 92.198% 
Loss D Fake: 0.7764 (0.7829) Acc D Fake: 6.667% 
Loss D: 1.230 
Loss G: 0.6269 (0.6208) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,658 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4152 (0.4100) Acc D Real: 92.230% 
Loss D Fake: 0.7742 (0.7821) Acc D Fake: 6.667% 
Loss D: 1.189 
Loss G: 0.6287 (0.6215) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,665 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4300 (0.4117) Acc D Real: 92.287% 
Loss D Fake: 0.7724 (0.7813) Acc D Fake: 6.667% 
Loss D: 1.202 
Loss G: 0.6300 (0.6222) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,672 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3993 (0.4107) Acc D Real: 92.464% 
Loss D Fake: 0.7712 (0.7805) Acc D Fake: 6.667% 
Loss D: 1.171 
Loss G: 0.6309 (0.6229) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,680 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4369 (0.4126) Acc D Real: 92.452% 
Loss D Fake: 0.7704 (0.7798) Acc D Fake: 6.667% 
Loss D: 1.207 
Loss G: 0.6314 (0.6235) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,687 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4135 (0.4126) Acc D Real: 92.451% 
Loss D Fake: 0.7698 (0.7791) Acc D Fake: 6.667% 
Loss D: 1.183 
Loss G: 0.6319 (0.6240) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,694 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3983 (0.4117) Acc D Real: 92.500% 
Loss D Fake: 0.7690 (0.7785) Acc D Fake: 6.667% 
Loss D: 1.167 
Loss G: 0.6332 (0.6246) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,702 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4378 (0.4133) Acc D Real: 92.546% 
Loss D Fake: 0.7672 (0.7778) Acc D Fake: 6.667% 
Loss D: 1.205 
Loss G: 0.6349 (0.6252) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,709 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4316 (0.4143) Acc D Real: 92.474% 
Loss D Fake: 0.7653 (0.7771) Acc D Fake: 6.667% 
Loss D: 1.197 
Loss G: 0.6369 (0.6259) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,716 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.4047 (0.4138) Acc D Real: 92.519% 
Loss D Fake: 0.7626 (0.7764) Acc D Fake: 6.667% 
Loss D: 1.167 
Loss G: 0.6400 (0.6266) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,723 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4247 (0.4143) Acc D Real: 92.240% 
Loss D Fake: 0.7585 (0.7755) Acc D Fake: 6.667% 
Loss D: 1.183 
Loss G: 0.6447 (0.6275) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,730 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4624 (0.4166) Acc D Real: 92.205% 
Loss D Fake: 0.7536 (0.7744) Acc D Fake: 6.667% 
Loss D: 1.216 
Loss G: 0.6483 (0.6285) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,737 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4231 (0.4169) Acc D Real: 92.299% 
Loss D Fake: 0.7502 (0.7733) Acc D Fake: 6.667% 
Loss D: 1.173 
Loss G: 0.6516 (0.6296) Acc G: 93.333% 
LR: 2.000e-04 

2023-03-02 02:00:18,744 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4278 (0.4174) Acc D Real: 92.235% 
Loss D Fake: 0.7466 (0.7722) Acc D Fake: 6.667% 
Loss D: 1.174 
Loss G: 0.6553 (0.6307) Acc G: 91.667% 
LR: 2.000e-04 

2023-03-02 02:00:18,751 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4516 (0.4188) Acc D Real: 92.159% 
Loss D Fake: 0.7430 (0.7710) Acc D Fake: 8.403% 
Loss D: 1.195 
Loss G: 0.6579 (0.6318) Acc G: 89.653% 
LR: 2.000e-04 

2023-03-02 02:00:18,758 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4465 (0.4199) Acc D Real: 92.079% 
Loss D Fake: 0.7414 (0.7698) Acc D Fake: 10.267% 
Loss D: 1.188 
Loss G: 0.6583 (0.6329) Acc G: 87.733% 
LR: 2.000e-04 

2023-03-02 02:00:18,766 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.4084 (0.4195) Acc D Real: 91.905% 
Loss D Fake: 0.7414 (0.7687) Acc D Fake: 12.051% 
Loss D: 1.150 
Loss G: 0.6604 (0.6339) Acc G: 85.769% 
LR: 2.000e-04 

2023-03-02 02:00:18,774 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4448 (0.4204) Acc D Real: 91.821% 
Loss D Fake: 0.7377 (0.7675) Acc D Fake: 14.012% 
Loss D: 1.183 
Loss G: 0.6643 (0.6351) Acc G: 83.765% 
LR: 2.000e-04 

2023-03-02 02:00:18,781 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3895 (0.4193) Acc D Real: 91.743% 
Loss D Fake: 0.7336 (0.7663) Acc D Fake: 15.952% 
Loss D: 1.123 
Loss G: 0.6680 (0.6362) Acc G: 81.845% 
LR: 2.000e-04 

2023-03-02 02:00:18,789 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3765 (0.4178) Acc D Real: 91.722% 
Loss D Fake: 0.7300 (0.7651) Acc D Fake: 17.816% 
Loss D: 1.107 
Loss G: 0.6712 (0.6374) Acc G: 80.000% 
LR: 2.000e-04 

2023-03-02 02:00:18,796 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3934 (0.4170) Acc D Real: 91.774% 
Loss D Fake: 0.7267 (0.7638) Acc D Fake: 19.611% 
Loss D: 1.120 
Loss G: 0.6744 (0.6387) Acc G: 78.222% 
LR: 2.000e-04 

2023-03-02 02:00:18,805 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3753 (0.4157) Acc D Real: 91.725% 
Loss D Fake: 0.7235 (0.7625) Acc D Fake: 21.344% 
Loss D: 1.099 
Loss G: 0.6773 (0.6399) Acc G: 76.559% 
LR: 2.000e-04 

2023-03-02 02:00:18,813 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3834 (0.4147) Acc D Real: 91.662% 
Loss D Fake: 0.7206 (0.7612) Acc D Fake: 22.969% 
Loss D: 1.104 
Loss G: 0.6805 (0.6412) Acc G: 74.948% 
LR: 2.000e-04 

2023-03-02 02:00:18,820 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4192 (0.4148) Acc D Real: 91.512% 
Loss D Fake: 0.7176 (0.7599) Acc D Fake: 24.545% 
Loss D: 1.137 
Loss G: 0.6828 (0.6424) Acc G: 73.434% 
LR: 2.000e-04 

2023-03-02 02:00:18,828 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3826 (0.4139) Acc D Real: 91.441% 
Loss D Fake: 0.7158 (0.7586) Acc D Fake: 26.029% 
Loss D: 1.098 
Loss G: 0.6848 (0.6437) Acc G: 72.010% 
LR: 2.000e-04 

2023-03-02 02:00:18,835 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4232 (0.4141) Acc D Real: 91.321% 
Loss D Fake: 0.7147 (0.7573) Acc D Fake: 27.429% 
Loss D: 1.138 
Loss G: 0.6841 (0.6448) Acc G: 70.667% 
LR: 2.000e-04 

2023-03-02 02:00:18,842 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3478 (0.4123) Acc D Real: 91.412% 
Loss D Fake: 0.7163 (0.7562) Acc D Fake: 28.750% 
Loss D: 1.064 
Loss G: 0.6860 (0.6460) Acc G: 69.398% 
LR: 2.000e-04 

2023-03-02 02:00:18,850 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3709 (0.4112) Acc D Real: 91.341% 
Loss D Fake: 0.7111 (0.7550) Acc D Fake: 30.000% 
Loss D: 1.082 
Loss G: 0.6921 (0.6472) Acc G: 68.153% 
LR: 2.000e-04 

2023-03-02 02:00:18,857 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3650 (0.4099) Acc D Real: 91.327% 
Loss D Fake: 0.7047 (0.7536) Acc D Fake: 31.228% 
Loss D: 1.070 
Loss G: 0.6976 (0.6486) Acc G: 66.974% 
LR: 2.000e-04 

2023-03-02 02:00:18,865 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4003 (0.4097) Acc D Real: 91.178% 
Loss D Fake: 0.7008 (0.7523) Acc D Fake: 32.436% 
Loss D: 1.101 
Loss G: 0.7002 (0.6499) Acc G: 65.812% 
LR: 2.000e-04 

2023-03-02 02:00:18,873 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4042 (0.4096) Acc D Real: 91.128% 
Loss D Fake: 0.6992 (0.7510) Acc D Fake: 33.583% 
Loss D: 1.103 
Loss G: 0.7014 (0.6512) Acc G: 64.708% 
LR: 2.000e-04 

2023-03-02 02:00:18,880 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4093 (0.4095) Acc D Real: 91.014% 
Loss D Fake: 0.6987 (0.7497) Acc D Fake: 34.675% 
Loss D: 1.108 
Loss G: 0.7019 (0.6524) Acc G: 63.659% 
LR: 2.000e-04 

2023-03-02 02:00:18,888 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3360 (0.4078) Acc D Real: 91.026% 
Loss D Fake: 0.6976 (0.7484) Acc D Fake: 35.714% 
Loss D: 1.034 
Loss G: 0.7052 (0.6537) Acc G: 62.659% 
LR: 2.000e-04 

2023-03-02 02:00:18,895 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4696 (0.4092) Acc D Real: 90.742% 
Loss D Fake: 0.6938 (0.7472) Acc D Fake: 36.705% 
Loss D: 1.163 
Loss G: 0.7071 (0.6549) Acc G: 61.705% 
LR: 2.000e-04 

2023-03-02 02:00:18,904 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3455 (0.4078) Acc D Real: 90.627% 
Loss D Fake: 0.6931 (0.7459) Acc D Fake: 37.652% 
Loss D: 1.039 
Loss G: 0.7080 (0.6561) Acc G: 60.795% 
LR: 2.000e-04 

2023-03-02 02:00:18,911 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3567 (0.4067) Acc D Real: 90.499% 
Loss D Fake: 0.6940 (0.7448) Acc D Fake: 38.556% 
Loss D: 1.051 
Loss G: 0.7005 (0.6571) Acc G: 59.926% 
LR: 2.000e-04 

2023-03-02 02:00:18,918 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3691 (0.4058) Acc D Real: 90.437% 
Loss D Fake: 0.8413 (0.7469) Acc D Fake: 38.188% 
Loss D: 1.210 
Loss G: 0.7070 (0.6582) Acc G: 59.058% 
LR: 2.000e-04 

2023-03-02 02:00:18,926 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3778 (0.4052) Acc D Real: 90.376% 
Loss D Fake: 0.7184 (0.7463) Acc D Fake: 38.617% 
Loss D: 1.096 
Loss G: 0.6852 (0.6588) Acc G: 58.582% 
LR: 2.000e-04 

2023-03-02 02:00:18,933 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3543 (0.4042) Acc D Real: 90.237% 
Loss D Fake: 0.7056 (0.7454) Acc D Fake: 39.236% 
Loss D: 1.060 
Loss G: 0.7301 (0.6603) Acc G: 57.743% 
LR: 2.000e-04 

2023-03-02 02:00:18,941 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3858 (0.4038) Acc D Real: 90.134% 
Loss D Fake: 0.6571 (0.7436) Acc D Fake: 40.102% 
Loss D: 1.043 
Loss G: 0.7548 (0.6622) Acc G: 56.905% 
LR: 2.000e-04 

2023-03-02 02:00:18,948 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3812 (0.4034) Acc D Real: 89.857% 
Loss D Fake: 0.6464 (0.7417) Acc D Fake: 40.969% 
Loss D: 1.028 
Loss G: 0.7626 (0.6642) Acc G: 56.070% 
LR: 2.000e-04 

2023-03-02 02:00:18,956 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4985 (0.4052) Acc D Real: 89.304% 
Loss D Fake: 0.6420 (0.7397) Acc D Fake: 41.832% 
Loss D: 1.140 
Loss G: 0.7653 (0.6662) Acc G: 55.265% 
LR: 2.000e-04 

2023-03-02 02:00:18,963 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4918 (0.4069) Acc D Real: 88.736% 
Loss D Fake: 0.6411 (0.7378) Acc D Fake: 42.662% 
Loss D: 1.133 
Loss G: 0.7650 (0.6681) Acc G: 54.490% 
LR: 2.000e-04 

2023-03-02 02:00:18,970 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3621 (0.4060) Acc D Real: 88.520% 
Loss D Fake: 0.6417 (0.7360) Acc D Fake: 43.461% 
Loss D: 1.004 
Loss G: 0.7642 (0.6699) Acc G: 53.745% 
LR: 2.000e-04 

2023-03-02 02:00:18,978 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.5193 (0.4081) Acc D Real: 87.914% 
Loss D Fake: 0.6431 (0.7343) Acc D Fake: 44.230% 
Loss D: 1.162 
Loss G: 0.7603 (0.6716) Acc G: 53.028% 
LR: 2.000e-04 

2023-03-02 02:00:18,986 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3693 (0.4074) Acc D Real: 87.719% 
Loss D Fake: 0.6471 (0.7327) Acc D Fake: 44.972% 
Loss D: 1.016 
Loss G: 0.7553 (0.6731) Acc G: 52.336% 
LR: 2.000e-04 

2023-03-02 02:00:18,993 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3967 (0.4072) Acc D Real: 87.504% 
Loss D Fake: 0.6516 (0.7313) Acc D Fake: 45.686% 
Loss D: 1.048 
Loss G: 0.7487 (0.6744) Acc G: 51.669% 
LR: 2.000e-04 

2023-03-02 02:00:19,001 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3088 (0.4055) Acc D Real: 87.475% 
Loss D Fake: 0.6584 (0.7300) Acc D Fake: 46.376% 
Loss D: 0.967 
Loss G: 0.7384 (0.6756) Acc G: 51.026% 
LR: 2.000e-04 

2023-03-02 02:00:19,008 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3678 (0.4049) Acc D Real: 87.322% 
Loss D Fake: 0.6687 (0.7289) Acc D Fake: 47.042% 
Loss D: 1.036 
Loss G: 0.7261 (0.6764) Acc G: 50.405% 
LR: 2.000e-04 

2023-03-02 02:00:19,016 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3858 (0.4045) Acc D Real: 87.180% 
Loss D Fake: 0.6806 (0.7281) Acc D Fake: 47.657% 
Loss D: 1.066 
Loss G: 0.7117 (0.6770) Acc G: 49.833% 
LR: 2.000e-04 

2023-03-02 02:00:19,024 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3750 (0.4040) Acc D Real: 87.072% 
Loss D Fake: 0.6951 (0.7276) Acc D Fake: 48.252% 
Loss D: 1.070 
Loss G: 0.6970 (0.6774) Acc G: 49.280% 
LR: 2.000e-04 

2023-03-02 02:00:19,031 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3207 (0.4027) Acc D Real: 87.007% 
Loss D Fake: 0.7081 (0.7272) Acc D Fake: 48.827% 
Loss D: 1.029 
Loss G: 0.6991 (0.6777) Acc G: 48.746% 
LR: 2.000e-04 

2023-03-02 02:00:19,039 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3586 (0.4020) Acc D Real: 86.936% 
Loss D Fake: 0.6952 (0.7267) Acc D Fake: 49.383% 
Loss D: 1.054 
Loss G: 0.7165 (0.6783) Acc G: 48.228% 
LR: 2.000e-04 

2023-03-02 02:00:19,046 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3053 (0.4004) Acc D Real: 87.030% 
Loss D Fake: 0.6755 (0.7259) Acc D Fake: 49.922% 
Loss D: 0.981 
Loss G: 0.7342 (0.6792) Acc G: 47.701% 
LR: 2.000e-04 

2023-03-02 02:00:19,053 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3151 (0.3991) Acc D Real: 87.060% 
Loss D Fake: 0.6603 (0.7249) Acc D Fake: 50.470% 
Loss D: 0.975 
Loss G: 0.7506 (0.6803) Acc G: 47.190% 
LR: 2.000e-04 

2023-03-02 02:00:19,061 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3948 (0.3990) Acc D Real: 86.935% 
Loss D Fake: 0.6480 (0.7237) Acc D Fake: 51.002% 
Loss D: 1.043 
Loss G: 0.7618 (0.6816) Acc G: 46.695% 
LR: 2.000e-04 

2023-03-02 02:00:19,068 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4419 (0.3997) Acc D Real: 86.701% 
Loss D Fake: 0.6407 (0.7224) Acc D Fake: 51.517% 
Loss D: 1.083 
Loss G: 0.7693 (0.6829) Acc G: 46.214% 
LR: 2.000e-04 

2023-03-02 02:00:19,076 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3123 (0.3984) Acc D Real: 86.699% 
Loss D Fake: 0.6347 (0.7211) Acc D Fake: 52.016% 
Loss D: 0.947 
Loss G: 0.7778 (0.6843) Acc G: 45.749% 
LR: 2.000e-04 

2023-03-02 02:00:19,083 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3889 (0.3982) Acc D Real: 86.620% 
Loss D Fake: 0.6471 (0.7200) Acc D Fake: 52.502% 
Loss D: 1.036 
Loss G: 0.4741 (0.6813) Acc G: 46.252% 
LR: 2.000e-04 

2023-03-02 02:00:19,092 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3913 (0.3981) Acc D Real: 86.470% 
Loss D Fake: 1.1128 (0.7257) Acc D Fake: 51.861% 
Loss D: 1.504 
Loss G: 0.4189 (0.6774) Acc G: 46.910% 
LR: 2.000e-04 

2023-03-02 02:00:19,099 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2888 (0.3966) Acc D Real: 86.516% 
Loss D Fake: 1.1566 (0.7319) Acc D Fake: 51.240% 
Loss D: 1.445 
Loss G: 0.4052 (0.6736) Acc G: 47.550% 
LR: 2.000e-04 

2023-03-02 02:00:19,107 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3330 (0.3957) Acc D Real: 86.545% 
Loss D Fake: 1.1668 (0.7380) Acc D Fake: 50.635% 
Loss D: 1.500 
Loss G: 0.4026 (0.6697) Acc G: 48.171% 
LR: 2.000e-04 

2023-03-02 02:00:19,115 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3182 (0.3946) Acc D Real: 86.611% 
Loss D Fake: 1.1588 (0.7439) Acc D Fake: 50.048% 
Loss D: 1.477 
Loss G: 0.4073 (0.6661) Acc G: 48.775% 
LR: 2.000e-04 

2023-03-02 02:00:19,122 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3384 (0.3938) Acc D Real: 86.654% 
Loss D Fake: 1.1365 (0.7492) Acc D Fake: 49.476% 
Loss D: 1.475 
Loss G: 0.4181 (0.6627) Acc G: 49.363% 
LR: 2.000e-04 

2023-03-02 02:00:19,131 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2861 (0.3924) Acc D Real: 86.765% 
Loss D Fake: 1.1030 (0.7540) Acc D Fake: 48.920% 
Loss D: 1.389 
Loss G: 0.4340 (0.6596) Acc G: 49.935% 
LR: 2.000e-04 

2023-03-02 02:00:19,138 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3123 (0.3913) Acc D Real: 86.838% 
Loss D Fake: 1.0621 (0.7581) Acc D Fake: 48.379% 
Loss D: 1.374 
Loss G: 0.4537 (0.6569) Acc G: 50.491% 
LR: 2.000e-04 

2023-03-02 02:00:19,146 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3218 (0.3904) Acc D Real: 86.913% 
Loss D Fake: 1.0183 (0.7615) Acc D Fake: 47.852% 
Loss D: 1.340 
Loss G: 0.4759 (0.6545) Acc G: 51.033% 
LR: 2.000e-04 

2023-03-02 02:00:19,154 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3485 (0.3899) Acc D Real: 86.974% 
Loss D Fake: 0.9748 (0.7643) Acc D Fake: 47.339% 
Loss D: 1.323 
Loss G: 0.4995 (0.6525) Acc G: 51.560% 
LR: 2.000e-04 

2023-03-02 02:00:19,162 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3304 (0.3891) Acc D Real: 87.055% 
Loss D Fake: 0.9320 (0.7665) Acc D Fake: 46.839% 
Loss D: 1.262 
Loss G: 0.5264 (0.6509) Acc G: 52.075% 
LR: 2.000e-04 

2023-03-02 02:00:19,169 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4053 (0.3893) Acc D Real: 87.154% 
Loss D Fake: 0.8863 (0.7680) Acc D Fake: 46.352% 
Loss D: 1.292 
Loss G: 0.5585 (0.6497) Acc G: 52.576% 
LR: 2.000e-04 

2023-03-02 02:00:19,177 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3957 (0.3894) Acc D Real: 87.204% 
Loss D Fake: 0.8358 (0.7688) Acc D Fake: 45.876% 
Loss D: 1.231 
Loss G: 0.5905 (0.6489) Acc G: 53.064% 
LR: 2.000e-04 

2023-03-02 02:00:19,184 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3864 (0.3893) Acc D Real: 87.274% 
Loss D Fake: 0.8009 (0.7692) Acc D Fake: 45.413% 
Loss D: 1.187 
Loss G: 0.6097 (0.6485) Acc G: 53.500% 
LR: 2.000e-04 

2023-03-02 02:00:19,192 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4006 (0.3895) Acc D Real: 87.271% 
Loss D Fake: 0.7821 (0.7694) Acc D Fake: 45.042% 
Loss D: 1.183 
Loss G: 0.6228 (0.6481) Acc G: 53.864% 
LR: 2.000e-04 

2023-03-02 02:00:19,199 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4296 (0.3900) Acc D Real: 87.208% 
Loss D Fake: 0.7686 (0.7694) Acc D Fake: 44.700% 
Loss D: 1.198 
Loss G: 0.6331 (0.6480) Acc G: 54.199% 
LR: 2.000e-04 

2023-03-02 02:00:19,207 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3537 (0.3895) Acc D Real: 87.212% 
Loss D Fake: 0.7577 (0.7692) Acc D Fake: 44.406% 
Loss D: 1.111 
Loss G: 0.6425 (0.6479) Acc G: 54.486% 
LR: 2.000e-04 

2023-03-02 02:00:19,215 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3376 (0.3889) Acc D Real: 87.240% 
Loss D Fake: 0.7475 (0.7690) Acc D Fake: 44.156% 
Loss D: 1.085 
Loss G: 0.6514 (0.6479) Acc G: 54.727% 
LR: 2.000e-04 

2023-03-02 02:00:19,222 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3526 (0.3885) Acc D Real: 87.266% 
Loss D Fake: 0.7382 (0.7686) Acc D Fake: 43.953% 
Loss D: 1.091 
Loss G: 0.6598 (0.6481) Acc G: 54.924% 
LR: 2.000e-04 

2023-03-02 02:00:19,230 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4109 (0.3888) Acc D Real: 87.228% 
Loss D Fake: 0.7297 (0.7682) Acc D Fake: 43.793% 
Loss D: 1.141 
Loss G: 0.6673 (0.6483) Acc G: 55.078% 
LR: 2.000e-04 

2023-03-02 02:00:19,237 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4275 (0.3892) Acc D Real: 87.156% 
Loss D Fake: 0.7225 (0.7677) Acc D Fake: 43.674% 
Loss D: 1.150 
Loss G: 0.6735 (0.6486) Acc G: 55.172% 
LR: 2.000e-04 

2023-03-02 02:00:19,245 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4556 (0.3899) Acc D Real: 87.096% 
Loss D Fake: 0.7168 (0.7671) Acc D Fake: 43.595% 
Loss D: 1.172 
Loss G: 0.6785 (0.6489) Acc G: 55.226% 
LR: 2.000e-04 

2023-03-02 02:00:19,253 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4886 (0.3910) Acc D Real: 86.930% 
Loss D Fake: 0.7125 (0.7665) Acc D Fake: 43.573% 
Loss D: 1.201 
Loss G: 0.6820 (0.6493) Acc G: 55.242% 
LR: 2.000e-04 

2023-03-02 02:00:19,260 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4457 (0.3916) Acc D Real: 86.846% 
Loss D Fake: 0.7098 (0.7659) Acc D Fake: 43.589% 
Loss D: 1.156 
Loss G: 0.6839 (0.6497) Acc G: 55.221% 
LR: 2.000e-04 

2023-03-02 02:00:19,268 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4502 (0.3923) Acc D Real: 86.785% 
Loss D Fake: 0.7087 (0.7652) Acc D Fake: 43.623% 
Loss D: 1.159 
Loss G: 0.6845 (0.6501) Acc G: 55.183% 
LR: 2.000e-04 

2023-03-02 02:00:19,275 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3951 (0.3923) Acc D Real: 86.750% 
Loss D Fake: 0.7084 (0.7646) Acc D Fake: 43.655% 
Loss D: 1.104 
Loss G: 0.6848 (0.6504) Acc G: 55.145% 
LR: 2.000e-04 

2023-03-02 02:00:19,283 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3858 (0.3922) Acc D Real: 86.735% 
Loss D Fake: 0.7085 (0.7640) Acc D Fake: 43.697% 
Loss D: 1.094 
Loss G: 0.6843 (0.6508) Acc G: 55.108% 
LR: 2.000e-04 

2023-03-02 02:00:19,291 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4391 (0.3927) Acc D Real: 86.637% 
Loss D Fake: 0.7096 (0.7635) Acc D Fake: 43.729% 
Loss D: 1.149 
Loss G: 0.6832 (0.6511) Acc G: 55.072% 
LR: 2.000e-04 

2023-03-02 02:00:19,298 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4679 (0.3935) Acc D Real: 86.540% 
Loss D Fake: 0.7112 (0.7629) Acc D Fake: 43.759% 
Loss D: 1.179 
Loss G: 0.6825 (0.6515) Acc G: 55.019% 
LR: 2.000e-04 

2023-03-02 02:00:19,306 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3616 (0.3932) Acc D Real: 86.512% 
Loss D Fake: 0.7108 (0.7624) Acc D Fake: 43.824% 
Loss D: 1.072 
Loss G: 0.6855 (0.6518) Acc G: 54.933% 
LR: 2.000e-04 

2023-03-02 02:00:19,313 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4291 (0.3935) Acc D Real: 86.482% 
Loss D Fake: 0.7059 (0.7618) Acc D Fake: 43.938% 
Loss D: 1.135 
Loss G: 0.6917 (0.6522) Acc G: 54.763% 
LR: 2.000e-04 

2023-03-02 02:00:19,321 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3736 (0.3933) Acc D Real: 86.388% 
Loss D Fake: 0.6978 (0.7611) Acc D Fake: 44.167% 
Loss D: 1.071 
Loss G: 0.7020 (0.6527) Acc G: 54.412% 
LR: 2.000e-04 

2023-03-02 02:00:19,329 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4697 (0.3941) Acc D Real: 86.330% 
Loss D Fake: 0.6875 (0.7604) Acc D Fake: 44.592% 
Loss D: 1.157 
Loss G: 0.7110 (0.6533) Acc G: 54.002% 
LR: 2.000e-04 

2023-03-02 02:00:19,336 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3627 (0.3938) Acc D Real: 86.280% 
Loss D Fake: 0.6791 (0.7596) Acc D Fake: 45.009% 
Loss D: 1.042 
Loss G: 0.7205 (0.6540) Acc G: 53.599% 
LR: 2.000e-04 

2023-03-02 02:00:19,346 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4173 (0.3940) Acc D Real: 86.262% 
Loss D Fake: 0.6707 (0.7587) Acc D Fake: 45.417% 
Loss D: 1.088 
Loss G: 0.7271 (0.6547) Acc G: 53.204% 
LR: 2.000e-04 

2023-03-02 02:00:19,353 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3327 (0.3934) Acc D Real: 86.289% 
Loss D Fake: 0.6699 (0.7579) Acc D Fake: 45.818% 
Loss D: 1.003 
Loss G: 0.7304 (0.6554) Acc G: 52.817% 
LR: 2.000e-04 

2023-03-02 02:00:19,361 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3949 (0.3934) Acc D Real: 86.287% 
Loss D Fake: 0.6630 (0.7570) Acc D Fake: 46.210% 
Loss D: 1.058 
Loss G: 0.7347 (0.6562) Acc G: 52.437% 
LR: 2.000e-04 

2023-03-02 02:00:19,369 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3472 (0.3930) Acc D Real: 86.309% 
Loss D Fake: 0.6608 (0.7560) Acc D Fake: 46.596% 
Loss D: 1.008 
Loss G: 0.7372 (0.6569) Acc G: 52.065% 
LR: 2.000e-04 

2023-03-02 02:00:19,376 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4047 (0.3931) Acc D Real: 86.261% 
Loss D Fake: 0.6590 (0.7551) Acc D Fake: 46.974% 
Loss D: 1.064 
Loss G: 0.7385 (0.6577) Acc G: 51.700% 
LR: 2.000e-04 

2023-03-02 02:00:19,384 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4225 (0.3934) Acc D Real: 86.173% 
Loss D Fake: 0.6582 (0.7542) Acc D Fake: 47.345% 
Loss D: 1.081 
Loss G: 0.7408 (0.6585) Acc G: 51.341% 
LR: 2.000e-04 

2023-03-02 02:00:19,392 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3465 (0.3930) Acc D Real: 86.208% 
Loss D Fake: 0.6557 (0.7533) Acc D Fake: 47.709% 
Loss D: 1.002 
Loss G: 0.7439 (0.6593) Acc G: 50.989% 
LR: 2.000e-04 

2023-03-02 02:00:19,400 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4135 (0.3931) Acc D Real: 86.139% 
Loss D Fake: 0.6526 (0.7524) Acc D Fake: 48.066% 
Loss D: 1.066 
Loss G: 0.7486 (0.6601) Acc G: 50.644% 
LR: 2.000e-04 

2023-03-02 02:00:19,409 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4169 (0.3934) Acc D Real: 86.057% 
Loss D Fake: 0.6481 (0.7514) Acc D Fake: 48.417% 
Loss D: 1.065 
Loss G: 0.7525 (0.6609) Acc G: 50.304% 
LR: 2.000e-04 

2023-03-02 02:00:19,417 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3656 (0.3931) Acc D Real: 86.065% 
Loss D Fake: 0.6458 (0.7505) Acc D Fake: 48.762% 
Loss D: 1.011 
Loss G: 0.7543 (0.6618) Acc G: 49.971% 
LR: 2.000e-04 

2023-03-02 02:00:19,425 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3857 (0.3930) Acc D Real: 86.019% 
Loss D Fake: 0.6446 (0.7495) Acc D Fake: 49.100% 
Loss D: 1.030 
Loss G: 0.7545 (0.6626) Acc G: 49.644% 
LR: 2.000e-04 

2023-03-02 02:00:19,432 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3595 (0.3928) Acc D Real: 85.973% 
Loss D Fake: 0.6464 (0.7486) Acc D Fake: 49.433% 
Loss D: 1.006 
Loss G: 0.7559 (0.6634) Acc G: 49.323% 
LR: 2.000e-04 

2023-03-02 02:00:19,439 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3684 (0.3925) Acc D Real: 85.937% 
Loss D Fake: 0.6422 (0.7477) Acc D Fake: 49.759% 
Loss D: 1.011 
Loss G: 0.7587 (0.6643) Acc G: 49.007% 
LR: 2.000e-04 

2023-03-02 02:00:19,447 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3508 (0.3922) Acc D Real: 85.901% 
Loss D Fake: 0.6425 (0.7468) Acc D Fake: 50.080% 
Loss D: 0.993 
Loss G: 0.7480 (0.6650) Acc G: 48.697% 
LR: 2.000e-04 

2023-03-02 02:00:19,454 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3068 (0.3914) Acc D Real: 85.904% 
Loss D Fake: 0.7061 (0.7464) Acc D Fake: 50.194% 
Loss D: 1.013 
Loss G: 0.7698 (0.6659) Acc G: 48.392% 
LR: 2.000e-04 

2023-03-02 02:00:19,461 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3676 (0.3912) Acc D Real: 85.822% 
Loss D Fake: 0.6421 (0.7455) Acc D Fake: 50.506% 
Loss D: 1.010 
Loss G: 0.7871 (0.6669) Acc G: 48.093% 
LR: 2.000e-04 

2023-03-02 02:00:19,470 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3520 (0.3909) Acc D Real: 85.757% 
Loss D Fake: 0.6129 (0.7444) Acc D Fake: 50.813% 
Loss D: 0.965 
Loss G: 0.7967 (0.6680) Acc G: 47.784% 
LR: 2.000e-04 

2023-03-02 02:00:19,477 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3349 (0.3904) Acc D Real: 85.709% 
Loss D Fake: 0.6093 (0.7433) Acc D Fake: 51.128% 
Loss D: 0.944 
Loss G: 0.8015 (0.6692) Acc G: 47.480% 
LR: 2.000e-04 

2023-03-02 02:00:19,485 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3405 (0.3900) Acc D Real: 85.647% 
Loss D Fake: 0.6051 (0.7421) Acc D Fake: 51.438% 
Loss D: 0.946 
Loss G: 0.8077 (0.6703) Acc G: 47.182% 
LR: 2.000e-04 

2023-03-02 02:00:19,493 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2938 (0.3892) Acc D Real: 85.648% 
Loss D Fake: 0.5996 (0.7409) Acc D Fake: 51.743% 
Loss D: 0.893 
Loss G: 0.8157 (0.6715) Acc G: 46.888% 
LR: 2.000e-04 

2023-03-02 02:00:19,500 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3593 (0.3890) Acc D Real: 85.587% 
Loss D Fake: 0.5938 (0.7397) Acc D Fake: 52.043% 
Loss D: 0.953 
Loss G: 0.8223 (0.6728) Acc G: 46.600% 
LR: 2.000e-04 

2023-03-02 02:00:19,507 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3449 (0.3886) Acc D Real: 85.528% 
Loss D Fake: 0.5897 (0.7385) Acc D Fake: 52.338% 
Loss D: 0.935 
Loss G: 0.8268 (0.6740) Acc G: 46.316% 
LR: 2.000e-04 

2023-03-02 02:00:19,515 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3388 (0.3882) Acc D Real: 85.465% 
Loss D Fake: 0.5863 (0.7373) Acc D Fake: 52.628% 
Loss D: 0.925 
Loss G: 0.8330 (0.6753) Acc G: 46.036% 
LR: 2.000e-04 

2023-03-02 02:00:19,522 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2995 (0.3875) Acc D Real: 85.435% 
Loss D Fake: 0.5817 (0.7361) Acc D Fake: 52.914% 
Loss D: 0.881 
Loss G: 0.8387 (0.6766) Acc G: 45.761% 
LR: 2.000e-04 

2023-03-02 02:00:19,531 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3638 (0.3873) Acc D Real: 85.381% 
Loss D Fake: 0.5786 (0.7348) Acc D Fake: 53.195% 
Loss D: 0.942 
Loss G: 0.8411 (0.6779) Acc G: 45.491% 
LR: 2.000e-04 

2023-03-02 02:00:19,539 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2794 (0.3865) Acc D Real: 85.374% 
Loss D Fake: 0.5770 (0.7336) Acc D Fake: 53.472% 
Loss D: 0.856 
Loss G: 0.8455 (0.6792) Acc G: 45.224% 
LR: 2.000e-04 

2023-03-02 02:00:19,547 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3750 (0.3864) Acc D Real: 85.298% 
Loss D Fake: 0.5737 (0.7323) Acc D Fake: 53.744% 
Loss D: 0.949 
Loss G: 0.8492 (0.6806) Acc G: 44.962% 
LR: 2.000e-04 

2023-03-02 02:00:19,555 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3794 (0.3863) Acc D Real: 85.211% 
Loss D Fake: 0.5728 (0.7311) Acc D Fake: 54.012% 
Loss D: 0.952 
Loss G: 0.8476 (0.6818) Acc G: 44.704% 
LR: 2.000e-04 

2023-03-02 02:00:19,562 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3716 (0.3862) Acc D Real: 85.151% 
Loss D Fake: 0.5769 (0.7299) Acc D Fake: 54.276% 
Loss D: 0.948 
Loss G: 0.8417 (0.6831) Acc G: 44.450% 
LR: 2.000e-04 

2023-03-02 02:00:19,570 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2621 (0.3853) Acc D Real: 85.145% 
Loss D Fake: 0.5775 (0.7287) Acc D Fake: 54.536% 
Loss D: 0.840 
Loss G: 0.8555 (0.6844) Acc G: 44.200% 
LR: 2.000e-04 

2023-03-02 02:00:19,578 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3208 (0.3848) Acc D Real: 85.107% 
Loss D Fake: 0.5617 (0.7275) Acc D Fake: 54.792% 
Loss D: 0.883 
Loss G: 0.8738 (0.6858) Acc G: 43.953% 
LR: 2.000e-04 

2023-03-02 02:00:19,585 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.2919 (0.3841) Acc D Real: 85.099% 
Loss D Fake: 0.5511 (0.7261) Acc D Fake: 55.044% 
Loss D: 0.843 
Loss G: 0.8873 (0.6873) Acc G: 43.710% 
LR: 2.000e-04 

2023-03-02 02:00:19,593 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3534 (0.3839) Acc D Real: 85.010% 
Loss D Fake: 0.5434 (0.7248) Acc D Fake: 55.293% 
Loss D: 0.897 
Loss G: 0.8978 (0.6889) Acc G: 43.471% 
LR: 2.000e-04 

2023-03-02 02:00:19,600 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4057 (0.3840) Acc D Real: 84.900% 
Loss D Fake: 0.5380 (0.7234) Acc D Fake: 55.537% 
Loss D: 0.944 
Loss G: 0.9038 (0.6905) Acc G: 43.236% 
LR: 2.000e-04 

2023-03-02 02:00:19,608 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3494 (0.3838) Acc D Real: 84.829% 
Loss D Fake: 0.5351 (0.7220) Acc D Fake: 55.779% 
Loss D: 0.885 
Loss G: 0.9083 (0.6921) Acc G: 43.004% 
LR: 2.000e-04 

2023-03-02 02:00:19,615 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2670 (0.3829) Acc D Real: 84.824% 
Loss D Fake: 0.5326 (0.7206) Acc D Fake: 56.016% 
Loss D: 0.800 
Loss G: 0.9121 (0.6937) Acc G: 42.775% 
LR: 2.000e-04 

2023-03-02 02:00:19,623 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3781 (0.3829) Acc D Real: 84.735% 
Loss D Fake: 0.5306 (0.7192) Acc D Fake: 56.250% 
Loss D: 0.909 
Loss G: 0.9145 (0.6953) Acc G: 42.549% 
LR: 2.000e-04 

2023-03-02 02:00:19,631 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3463 (0.3826) Acc D Real: 84.674% 
Loss D Fake: 0.5293 (0.7179) Acc D Fake: 56.481% 
Loss D: 0.876 
Loss G: 0.9171 (0.6969) Acc G: 42.327% 
LR: 2.000e-04 

2023-03-02 02:00:19,639 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3396 (0.3823) Acc D Real: 84.615% 
Loss D Fake: 0.5277 (0.7165) Acc D Fake: 56.709% 
Loss D: 0.867 
Loss G: 0.9193 (0.6985) Acc G: 42.108% 
LR: 2.000e-04 

2023-03-02 02:00:19,647 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3063 (0.3818) Acc D Real: 84.580% 
Loss D Fake: 0.5266 (0.7152) Acc D Fake: 56.933% 
Loss D: 0.833 
Loss G: 0.9212 (0.7001) Acc G: 41.892% 
LR: 2.000e-04 

2023-03-02 02:00:19,655 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2520 (0.3808) Acc D Real: 84.595% 
Loss D Fake: 0.5255 (0.7138) Acc D Fake: 57.154% 
Loss D: 0.777 
Loss G: 0.9232 (0.7016) Acc G: 41.680% 
LR: 2.000e-04 

2023-03-02 02:00:19,663 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3453 (0.3806) Acc D Real: 84.540% 
Loss D Fake: 0.5243 (0.7125) Acc D Fake: 57.372% 
Loss D: 0.870 
Loss G: 0.9255 (0.7032) Acc G: 41.470% 
LR: 2.000e-04 

2023-03-02 02:00:19,671 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4166 (0.3808) Acc D Real: 84.421% 
Loss D Fake: 0.5243 (0.7112) Acc D Fake: 57.587% 
Loss D: 0.941 
Loss G: 0.9206 (0.7047) Acc G: 41.263% 
LR: 2.000e-04 

2023-03-02 02:00:19,678 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3695 (0.3808) Acc D Real: 84.349% 
Loss D Fake: 0.5312 (0.7100) Acc D Fake: 57.799% 
Loss D: 0.901 
Loss G: 0.9088 (0.7061) Acc G: 41.059% 
LR: 2.000e-04 

2023-03-02 02:00:19,685 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3611 (0.3806) Acc D Real: 84.281% 
Loss D Fake: 0.5447 (0.7088) Acc D Fake: 58.008% 
Loss D: 0.906 
Loss G: 0.9118 (0.7075) Acc G: 40.857% 
LR: 2.000e-04 

2023-03-02 02:00:19,693 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4042 (0.3808) Acc D Real: 84.172% 
Loss D Fake: 0.5286 (0.7076) Acc D Fake: 58.215% 
Loss D: 0.933 
Loss G: 0.9230 (0.7090) Acc G: 40.659% 
LR: 2.000e-04 

2023-03-02 02:00:19,701 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4076 (0.3810) Acc D Real: 84.068% 
Loss D Fake: 0.5249 (0.7064) Acc D Fake: 58.418% 
Loss D: 0.932 
Loss G: 0.9228 (0.7104) Acc G: 40.463% 
LR: 2.000e-04 

2023-03-02 02:00:19,708 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4233 (0.3813) Acc D Real: 83.945% 
Loss D Fake: 0.5263 (0.7052) Acc D Fake: 58.619% 
Loss D: 0.950 
Loss G: 0.9201 (0.7119) Acc G: 40.270% 
LR: 2.000e-04 

2023-03-02 02:00:19,716 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2947 (0.3807) Acc D Real: 83.936% 
Loss D Fake: 0.5272 (0.7040) Acc D Fake: 58.817% 
Loss D: 0.822 
Loss G: 0.9207 (0.7132) Acc G: 40.079% 
LR: 2.000e-04 

2023-03-02 02:00:19,724 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2943 (0.3801) Acc D Real: 83.916% 
Loss D Fake: 0.5264 (0.7028) Acc D Fake: 59.012% 
Loss D: 0.821 
Loss G: 0.9244 (0.7146) Acc G: 39.891% 
LR: 2.000e-04 

2023-03-02 02:00:19,733 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2909 (0.3795) Acc D Real: 83.899% 
Loss D Fake: 0.5233 (0.7016) Acc D Fake: 59.216% 
Loss D: 0.814 
Loss G: 0.9307 (0.7161) Acc G: 39.694% 
LR: 2.000e-04 

2023-03-02 02:00:19,741 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3306 (0.3792) Acc D Real: 83.857% 
Loss D Fake: 0.5199 (0.7004) Acc D Fake: 59.418% 
Loss D: 0.850 
Loss G: 0.9359 (0.7175) Acc G: 39.500% 
LR: 2.000e-04 

2023-03-02 02:00:19,748 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3517 (0.3790) Acc D Real: 83.789% 
Loss D Fake: 0.5177 (0.6992) Acc D Fake: 59.616% 
Loss D: 0.869 
Loss G: 0.9415 (0.7190) Acc G: 39.308% 
LR: 2.000e-04 

2023-03-02 02:00:19,756 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2808 (0.3784) Acc D Real: 83.772% 
Loss D Fake: 0.5128 (0.6980) Acc D Fake: 59.812% 
Loss D: 0.794 
Loss G: 0.9530 (0.7205) Acc G: 39.119% 
LR: 2.000e-04 

2023-03-02 02:00:19,764 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3481 (0.3782) Acc D Real: 83.704% 
Loss D Fake: 0.5059 (0.6968) Acc D Fake: 60.006% 
Loss D: 0.854 
Loss G: 0.9623 (0.7220) Acc G: 38.933% 
LR: 2.000e-04 

2023-03-02 02:00:19,772 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3414 (0.3780) Acc D Real: 83.655% 
Loss D Fake: 0.5018 (0.6956) Acc D Fake: 60.197% 
Loss D: 0.843 
Loss G: 0.9677 (0.7236) Acc G: 38.748% 
LR: 2.000e-04 

2023-03-02 02:00:19,780 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5217 (0.3789) Acc D Real: 83.639% 
Loss D Fake: 0.5153 (0.6944) Acc D Fake: 60.215% 
Loss D: 1.037 
Loss G: 0.9333 (0.7249) Acc G: 38.731% 
LR: 2.000e-04 

2023-03-02 02:00:20,003 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.144 | Generator Loss: 0.931 | Avg: 2.075 
2023-03-02 02:00:20,027 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.047 | Generator Loss: 0.931 | Avg: 1.978 
2023-03-02 02:00:20,050 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.066 | Generator Loss: 0.931 | Avg: 1.997 
2023-03-02 02:00:20,078 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.123 | Generator Loss: 0.931 | Avg: 2.054 
2023-03-02 02:00:20,105 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.118 | Generator Loss: 0.931 | Avg: 2.049 
2023-03-02 02:00:20,132 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.071 | Generator Loss: 0.931 | Avg: 2.002 
2023-03-02 02:00:20,158 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.034 | Generator Loss: 0.931 | Avg: 1.965 
2023-03-02 02:00:20,185 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.996 | Generator Loss: 0.931 | Avg: 1.926 
2023-03-02 02:00:20,212 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.971 | Generator Loss: 0.931 | Avg: 1.902 
2023-03-02 02:00:20,239 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 0.938 | Generator Loss: 0.931 | Avg: 1.868 
2023-03-02 02:00:20,266 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 0.914 | Generator Loss: 0.931 | Avg: 1.845 
2023-03-02 02:00:20,293 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.889 | Generator Loss: 0.931 | Avg: 1.820 
2023-03-02 02:00:20,320 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.870 | Generator Loss: 0.931 | Avg: 1.801 
2023-03-02 02:00:20,347 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.859 | Generator Loss: 0.931 | Avg: 1.789 
2023-03-02 02:00:20,375 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.860 | Generator Loss: 0.931 | Avg: 1.791 
2023-03-02 02:00:20,401 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.870 | Generator Loss: 0.931 | Avg: 1.800 
2023-03-02 02:00:20,430 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.882 | Generator Loss: 0.931 | Avg: 1.812 
2023-03-02 02:00:20,463 -                train: [    INFO] - 
Epoch: 10/20
2023-03-02 02:00:20,671 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4066 (0.3858) Acc D Real: 69.505% 
Loss D Fake: 0.5019 (0.5122) Acc D Fake: 90.000% 
Loss D: 0.909 
Loss G: 0.9563 (0.9650) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,679 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3511 (0.3743) Acc D Real: 70.955% 
Loss D Fake: 0.5153 (0.5132) Acc D Fake: 90.000% 
Loss D: 0.866 
Loss G: 0.9540 (0.9613) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,686 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2639 (0.3467) Acc D Real: 74.479% 
Loss D Fake: 0.5050 (0.5112) Acc D Fake: 90.000% 
Loss D: 0.769 
Loss G: 0.9738 (0.9644) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,693 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4445 (0.3662) Acc D Real: 72.750% 
Loss D Fake: 0.4961 (0.5082) Acc D Fake: 90.000% 
Loss D: 0.941 
Loss G: 0.9790 (0.9674) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,700 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2741 (0.3509) Acc D Real: 74.731% 
Loss D Fake: 0.4951 (0.5060) Acc D Fake: 90.000% 
Loss D: 0.769 
Loss G: 0.9839 (0.9701) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,708 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3709 (0.3537) Acc D Real: 74.598% 
Loss D Fake: 0.4921 (0.5040) Acc D Fake: 90.000% 
Loss D: 0.863 
Loss G: 0.9888 (0.9728) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,717 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3171 (0.3492) Acc D Real: 75.228% 
Loss D Fake: 0.4898 (0.5022) Acc D Fake: 90.000% 
Loss D: 0.807 
Loss G: 0.9940 (0.9754) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,724 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3671 (0.3512) Acc D Real: 74.925% 
Loss D Fake: 0.4871 (0.5005) Acc D Fake: 90.000% 
Loss D: 0.854 
Loss G: 0.9989 (0.9780) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,732 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3338 (0.3494) Acc D Real: 75.031% 
Loss D Fake: 0.4871 (0.4992) Acc D Fake: 90.000% 
Loss D: 0.821 
Loss G: 0.9907 (0.9793) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,739 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3030 (0.3452) Acc D Real: 75.559% 
Loss D Fake: 0.4949 (0.4988) Acc D Fake: 90.000% 
Loss D: 0.798 
Loss G: 0.9797 (0.9793) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,748 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2862 (0.3403) Acc D Real: 76.029% 
Loss D Fake: 0.4999 (0.4989) Acc D Fake: 90.000% 
Loss D: 0.786 
Loss G: 0.9835 (0.9797) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,755 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.2988 (0.3371) Acc D Real: 76.462% 
Loss D Fake: 0.4922 (0.4984) Acc D Fake: 89.872% 
Loss D: 0.791 
Loss G: 0.9994 (0.9812) Acc G: 10.000% 
LR: 2.000e-04 

2023-03-02 02:00:20,762 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3546 (0.3383) Acc D Real: 76.261% 
Loss D Fake: 0.4846 (0.4974) Acc D Fake: 89.762% 
Loss D: 0.839 
Loss G: 1.0055 (0.9829) Acc G: 10.119% 
LR: 2.000e-04 

2023-03-02 02:00:20,769 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3016 (0.3359) Acc D Real: 76.479% 
Loss D Fake: 0.4857 (0.4966) Acc D Fake: 89.667% 
Loss D: 0.787 
Loss G: 0.9920 (0.9835) Acc G: 10.222% 
LR: 2.000e-04 

2023-03-02 02:00:20,777 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3707 (0.3381) Acc D Real: 76.201% 
Loss D Fake: 0.5165 (0.4979) Acc D Fake: 89.583% 
Loss D: 0.887 
Loss G: 1.0134 (0.9854) Acc G: 10.312% 
LR: 2.000e-04 

2023-03-02 02:00:20,785 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3345 (0.3379) Acc D Real: 76.241% 
Loss D Fake: 0.4987 (0.4979) Acc D Fake: 89.510% 
Loss D: 0.833 
Loss G: 0.9741 (0.9847) Acc G: 10.392% 
LR: 2.000e-04 

2023-03-02 02:00:20,792 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4154 (0.3422) Acc D Real: 75.660% 
Loss D Fake: 0.4917 (0.4976) Acc D Fake: 89.444% 
Loss D: 0.907 
Loss G: 1.0445 (0.9881) Acc G: 10.463% 
LR: 2.000e-04 

2023-03-02 02:00:20,799 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2770 (0.3387) Acc D Real: 76.028% 
Loss D Fake: 0.4595 (0.4956) Acc D Fake: 89.298% 
Loss D: 0.736 
Loss G: 1.0542 (0.9916) Acc G: 10.614% 
LR: 2.000e-04 

2023-03-02 02:00:20,806 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4116 (0.3424) Acc D Real: 75.573% 
Loss D Fake: 0.4606 (0.4938) Acc D Fake: 89.167% 
Loss D: 0.872 
Loss G: 1.0495 (0.9944) Acc G: 10.750% 
LR: 2.000e-04 

2023-03-02 02:00:20,813 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4019 (0.3452) Acc D Real: 75.312% 
Loss D Fake: 0.4647 (0.4924) Acc D Fake: 89.048% 
Loss D: 0.867 
Loss G: 1.0365 (0.9964) Acc G: 10.873% 
LR: 2.000e-04 

2023-03-02 02:00:20,821 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4180 (0.3485) Acc D Real: 74.934% 
Loss D Fake: 0.4741 (0.4916) Acc D Fake: 88.939% 
Loss D: 0.892 
Loss G: 1.0169 (0.9974) Acc G: 10.985% 
LR: 2.000e-04 

2023-03-02 02:00:20,828 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3515 (0.3487) Acc D Real: 74.880% 
Loss D Fake: 0.4893 (0.4915) Acc D Fake: 88.841% 
Loss D: 0.841 
Loss G: 0.9990 (0.9975) Acc G: 11.087% 
LR: 2.000e-04 

2023-03-02 02:00:20,835 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.2168 (0.3432) Acc D Real: 75.510% 
Loss D Fake: 0.4937 (0.4916) Acc D Fake: 88.750% 
Loss D: 0.711 
Loss G: 1.0206 (0.9984) Acc G: 11.181% 
LR: 2.000e-04 

2023-03-02 02:00:20,843 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.2833 (0.3408) Acc D Real: 75.744% 
Loss D Fake: 0.4752 (0.4909) Acc D Fake: 88.667% 
Loss D: 0.758 
Loss G: 1.0322 (0.9998) Acc G: 11.267% 
LR: 2.000e-04 

2023-03-02 02:00:20,851 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3157 (0.3398) Acc D Real: 75.921% 
Loss D Fake: 0.4724 (0.4902) Acc D Fake: 88.590% 
Loss D: 0.788 
Loss G: 1.0382 (1.0012) Acc G: 11.346% 
LR: 2.000e-04 

2023-03-02 02:00:20,858 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4523 (0.3440) Acc D Real: 75.459% 
Loss D Fake: 0.4684 (0.4894) Acc D Fake: 88.519% 
Loss D: 0.921 
Loss G: 1.0477 (1.0030) Acc G: 11.420% 
LR: 2.000e-04 

2023-03-02 02:00:20,866 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3227 (0.3432) Acc D Real: 75.502% 
Loss D Fake: 0.4636 (0.4885) Acc D Fake: 88.512% 
Loss D: 0.786 
Loss G: 1.0497 (1.0046) Acc G: 11.429% 
LR: 2.000e-04 

2023-03-02 02:00:20,873 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3477 (0.3434) Acc D Real: 75.424% 
Loss D Fake: 0.4695 (0.4878) Acc D Fake: 88.506% 
Loss D: 0.817 
Loss G: 0.9951 (1.0043) Acc G: 11.437% 
LR: 2.000e-04 

2023-03-02 02:00:20,881 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3701 (0.3443) Acc D Real: 75.292% 
Loss D Fake: 1.5329 (0.5227) Acc D Fake: 86.444% 
Loss D: 1.903 
Loss G: 0.4612 (0.9862) Acc G: 13.167% 
LR: 2.000e-04 

2023-03-02 02:00:20,888 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3360 (0.3440) Acc D Real: 75.247% 
Loss D Fake: 1.3235 (0.5485) Acc D Fake: 84.839% 
Loss D: 1.659 
Loss G: 0.8414 (0.9815) Acc G: 13.118% 
LR: 2.000e-04 

2023-03-02 02:00:20,896 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3819 (0.3452) Acc D Real: 75.003% 
Loss D Fake: 0.5797 (0.5495) Acc D Fake: 84.948% 
Loss D: 0.962 
Loss G: 0.8374 (0.9770) Acc G: 13.073% 
LR: 2.000e-04 

2023-03-02 02:00:20,903 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.5612 (0.3517) Acc D Real: 74.145% 
Loss D Fake: 0.5741 (0.5502) Acc D Fake: 85.051% 
Loss D: 1.135 
Loss G: 0.8447 (0.9730) Acc G: 13.030% 
LR: 2.000e-04 

2023-03-02 02:00:20,910 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.5163 (0.3566) Acc D Real: 73.511% 
Loss D Fake: 0.5693 (0.5508) Acc D Fake: 85.147% 
Loss D: 1.086 
Loss G: 0.8495 (0.9694) Acc G: 12.990% 
LR: 2.000e-04 

2023-03-02 02:00:20,918 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.5403 (0.3618) Acc D Real: 72.817% 
Loss D Fake: 0.5669 (0.5512) Acc D Fake: 85.284% 
Loss D: 1.107 
Loss G: 0.8513 (0.9660) Acc G: 12.905% 
LR: 2.000e-04 

2023-03-02 02:00:20,925 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.4761 (0.3650) Acc D Real: 72.352% 
Loss D Fake: 0.5665 (0.5517) Acc D Fake: 85.415% 
Loss D: 1.043 
Loss G: 0.8509 (0.9628) Acc G: 12.824% 
LR: 2.000e-04 

2023-03-02 02:00:20,934 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.5092 (0.3689) Acc D Real: 71.857% 
Loss D Fake: 0.5673 (0.5521) Acc D Fake: 85.539% 
Loss D: 1.076 
Loss G: 0.8488 (0.9597) Acc G: 12.748% 
LR: 2.000e-04 

2023-03-02 02:00:20,942 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.5921 (0.3748) Acc D Real: 71.073% 
Loss D Fake: 0.5695 (0.5525) Acc D Fake: 85.657% 
Loss D: 1.162 
Loss G: 0.8442 (0.9567) Acc G: 12.763% 
LR: 2.000e-04 

2023-03-02 02:00:20,950 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5186 (0.3784) Acc D Real: 70.576% 
Loss D Fake: 0.5734 (0.5531) Acc D Fake: 85.682% 
Loss D: 1.092 
Loss G: 0.8381 (0.9537) Acc G: 12.778% 
LR: 2.000e-04 

2023-03-02 02:00:20,958 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5125 (0.3818) Acc D Real: 70.064% 
Loss D Fake: 0.5782 (0.5537) Acc D Fake: 85.707% 
Loss D: 1.091 
Loss G: 0.8310 (0.9506) Acc G: 12.792% 
LR: 2.000e-04 

2023-03-02 02:00:20,965 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.5083 (0.3849) Acc D Real: 69.605% 
Loss D Fake: 0.5835 (0.5544) Acc D Fake: 85.730% 
Loss D: 1.092 
Loss G: 0.8234 (0.9475) Acc G: 12.805% 
LR: 2.000e-04 

2023-03-02 02:00:20,973 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3914 (0.3850) Acc D Real: 69.475% 
Loss D Fake: 0.5890 (0.5553) Acc D Fake: 85.753% 
Loss D: 0.980 
Loss G: 0.8166 (0.9444) Acc G: 12.817% 
LR: 2.000e-04 

2023-03-02 02:00:20,981 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4759 (0.3872) Acc D Real: 69.110% 
Loss D Fake: 0.5940 (0.5562) Acc D Fake: 85.774% 
Loss D: 1.070 
Loss G: 0.8096 (0.9412) Acc G: 12.829% 
LR: 2.000e-04 

2023-03-02 02:00:20,988 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4764 (0.3892) Acc D Real: 68.780% 
Loss D Fake: 0.5993 (0.5571) Acc D Fake: 85.794% 
Loss D: 1.076 
Loss G: 0.8026 (0.9381) Acc G: 12.841% 
LR: 2.000e-04 

2023-03-02 02:00:20,996 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4739 (0.3911) Acc D Real: 68.443% 
Loss D Fake: 0.6047 (0.5582) Acc D Fake: 85.814% 
Loss D: 1.079 
Loss G: 0.7956 (0.9349) Acc G: 12.852% 
LR: 2.000e-04 

2023-03-02 02:00:21,004 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.5212 (0.3939) Acc D Real: 68.014% 
Loss D Fake: 0.6102 (0.5593) Acc D Fake: 85.832% 
Loss D: 1.131 
Loss G: 0.7885 (0.9317) Acc G: 12.862% 
LR: 2.000e-04 

2023-03-02 02:00:21,011 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.5027 (0.3962) Acc D Real: 67.588% 
Loss D Fake: 0.6161 (0.5605) Acc D Fake: 85.814% 
Loss D: 1.119 
Loss G: 0.7807 (0.9285) Acc G: 12.908% 
LR: 2.000e-04 

2023-03-02 02:00:21,019 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4998 (0.3984) Acc D Real: 67.207% 
Loss D Fake: 0.6224 (0.5618) Acc D Fake: 85.798% 
Loss D: 1.122 
Loss G: 0.7728 (0.9253) Acc G: 12.986% 
LR: 2.000e-04 

2023-03-02 02:00:21,027 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3704 (0.3978) Acc D Real: 67.190% 
Loss D Fake: 0.6286 (0.5632) Acc D Fake: 85.747% 
Loss D: 0.999 
Loss G: 0.7663 (0.9220) Acc G: 13.061% 
LR: 2.000e-04 

2023-03-02 02:00:21,035 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.5127 (0.4001) Acc D Real: 66.771% 
Loss D Fake: 0.6338 (0.5646) Acc D Fake: 85.699% 
Loss D: 1.146 
Loss G: 0.7596 (0.9188) Acc G: 13.133% 
LR: 2.000e-04 

2023-03-02 02:00:21,042 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4763 (0.4016) Acc D Real: 66.458% 
Loss D Fake: 0.6395 (0.5661) Acc D Fake: 85.653% 
Loss D: 1.116 
Loss G: 0.7528 (0.9155) Acc G: 13.203% 
LR: 2.000e-04 

2023-03-02 02:00:21,050 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4237 (0.4020) Acc D Real: 66.320% 
Loss D Fake: 0.6452 (0.5676) Acc D Fake: 85.608% 
Loss D: 1.069 
Loss G: 0.7467 (0.9123) Acc G: 13.269% 
LR: 2.000e-04 

2023-03-02 02:00:21,057 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4032 (0.4020) Acc D Real: 66.207% 
Loss D Fake: 0.6502 (0.5691) Acc D Fake: 85.565% 
Loss D: 1.053 
Loss G: 0.7413 (0.9091) Acc G: 13.333% 
LR: 2.000e-04 

2023-03-02 02:00:21,067 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3755 (0.4015) Acc D Real: 66.160% 
Loss D Fake: 0.6545 (0.5707) Acc D Fake: 85.524% 
Loss D: 1.030 
Loss G: 0.7369 (0.9059) Acc G: 13.395% 
LR: 2.000e-04 

2023-03-02 02:00:21,075 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4148 (0.4018) Acc D Real: 66.012% 
Loss D Fake: 0.6581 (0.5723) Acc D Fake: 85.484% 
Loss D: 1.073 
Loss G: 0.7330 (0.9027) Acc G: 13.455% 
LR: 2.000e-04 

2023-03-02 02:00:21,083 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3990 (0.4017) Acc D Real: 65.907% 
Loss D Fake: 0.6615 (0.5739) Acc D Fake: 85.445% 
Loss D: 1.060 
Loss G: 0.7294 (0.8996) Acc G: 13.512% 
LR: 2.000e-04 

2023-03-02 02:00:21,090 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3920 (0.4016) Acc D Real: 65.809% 
Loss D Fake: 0.6644 (0.5755) Acc D Fake: 85.408% 
Loss D: 1.056 
Loss G: 0.7264 (0.8966) Acc G: 13.567% 
LR: 2.000e-04 

2023-03-02 02:00:21,098 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4372 (0.4022) Acc D Real: 65.611% 
Loss D Fake: 0.6672 (0.5771) Acc D Fake: 85.373% 
Loss D: 1.104 
Loss G: 0.7233 (0.8936) Acc G: 13.621% 
LR: 2.000e-04 

2023-03-02 02:00:21,105 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4285 (0.4026) Acc D Real: 65.443% 
Loss D Fake: 0.6701 (0.5786) Acc D Fake: 85.338% 
Loss D: 1.099 
Loss G: 0.7201 (0.8907) Acc G: 13.672% 
LR: 2.000e-04 

2023-03-02 02:00:21,112 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3854 (0.4023) Acc D Real: 65.379% 
Loss D Fake: 0.6729 (0.5802) Acc D Fake: 85.305% 
Loss D: 1.058 
Loss G: 0.7172 (0.8878) Acc G: 13.722% 
LR: 2.000e-04 

2023-03-02 02:00:21,120 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3056 (0.4008) Acc D Real: 65.518% 
Loss D Fake: 0.6751 (0.5818) Acc D Fake: 85.272% 
Loss D: 0.981 
Loss G: 0.7159 (0.8850) Acc G: 13.770% 
LR: 2.000e-04 

2023-03-02 02:00:21,127 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4467 (0.4015) Acc D Real: 65.386% 
Loss D Fake: 0.6761 (0.5833) Acc D Fake: 85.241% 
Loss D: 1.123 
Loss G: 0.7145 (0.8822) Acc G: 13.817% 
LR: 2.000e-04 

2023-03-02 02:00:21,135 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4570 (0.4024) Acc D Real: 65.198% 
Loss D Fake: 0.6777 (0.5848) Acc D Fake: 85.211% 
Loss D: 1.135 
Loss G: 0.7122 (0.8795) Acc G: 13.862% 
LR: 2.000e-04 

2023-03-02 02:00:21,142 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3263 (0.4012) Acc D Real: 65.306% 
Loss D Fake: 0.6797 (0.5863) Acc D Fake: 85.181% 
Loss D: 1.006 
Loss G: 0.7107 (0.8769) Acc G: 13.906% 
LR: 2.000e-04 

2023-03-02 02:00:21,150 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4251 (0.4016) Acc D Real: 65.225% 
Loss D Fake: 0.6810 (0.5877) Acc D Fake: 85.127% 
Loss D: 1.106 
Loss G: 0.7092 (0.8743) Acc G: 13.974% 
LR: 2.000e-04 

2023-03-02 02:00:21,158 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3242 (0.4004) Acc D Real: 65.321% 
Loss D Fake: 0.6823 (0.5892) Acc D Fake: 85.075% 
Loss D: 1.006 
Loss G: 0.7084 (0.8718) Acc G: 14.040% 
LR: 2.000e-04 

2023-03-02 02:00:21,165 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3410 (0.3995) Acc D Real: 65.483% 
Loss D Fake: 0.6826 (0.5906) Acc D Fake: 85.024% 
Loss D: 1.024 
Loss G: 0.7084 (0.8693) Acc G: 14.104% 
LR: 2.000e-04 

2023-03-02 02:00:21,172 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4357 (0.4000) Acc D Real: 65.427% 
Loss D Fake: 0.6826 (0.5919) Acc D Fake: 84.975% 
Loss D: 1.118 
Loss G: 0.7080 (0.8670) Acc G: 14.167% 
LR: 2.000e-04 

2023-03-02 02:00:21,180 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3413 (0.3992) Acc D Real: 65.515% 
Loss D Fake: 0.6830 (0.5932) Acc D Fake: 84.927% 
Loss D: 1.024 
Loss G: 0.7080 (0.8647) Acc G: 14.227% 
LR: 2.000e-04 

2023-03-02 02:00:21,187 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4323 (0.3996) Acc D Real: 65.478% 
Loss D Fake: 0.6831 (0.5945) Acc D Fake: 84.880% 
Loss D: 1.115 
Loss G: 0.7075 (0.8624) Acc G: 14.286% 
LR: 2.000e-04 

2023-03-02 02:00:21,195 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3903 (0.3995) Acc D Real: 65.484% 
Loss D Fake: 0.6838 (0.5958) Acc D Fake: 84.835% 
Loss D: 1.074 
Loss G: 0.7067 (0.8602) Acc G: 14.343% 
LR: 2.000e-04 

2023-03-02 02:00:21,202 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3608 (0.3990) Acc D Real: 65.549% 
Loss D Fake: 0.6845 (0.5970) Acc D Fake: 84.791% 
Loss D: 1.045 
Loss G: 0.7061 (0.8581) Acc G: 14.398% 
LR: 2.000e-04 

2023-03-02 02:00:21,210 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3337 (0.3981) Acc D Real: 65.659% 
Loss D Fake: 0.6847 (0.5982) Acc D Fake: 84.748% 
Loss D: 1.018 
Loss G: 0.7064 (0.8560) Acc G: 14.452% 
LR: 2.000e-04 

2023-03-02 02:00:21,217 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4480 (0.3988) Acc D Real: 65.612% 
Loss D Fake: 0.6846 (0.5994) Acc D Fake: 84.707% 
Loss D: 1.133 
Loss G: 0.7059 (0.8540) Acc G: 14.505% 
LR: 2.000e-04 

2023-03-02 02:00:21,225 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3742 (0.3984) Acc D Real: 65.688% 
Loss D Fake: 0.6852 (0.6005) Acc D Fake: 84.666% 
Loss D: 1.059 
Loss G: 0.7054 (0.8520) Acc G: 14.556% 
LR: 2.000e-04 

2023-03-02 02:00:21,233 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3916 (0.3983) Acc D Real: 65.656% 
Loss D Fake: 0.6857 (0.6016) Acc D Fake: 84.627% 
Loss D: 1.077 
Loss G: 0.7048 (0.8501) Acc G: 14.605% 
LR: 2.000e-04 

2023-03-02 02:00:21,240 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3571 (0.3978) Acc D Real: 65.724% 
Loss D Fake: 0.6862 (0.6027) Acc D Fake: 84.588% 
Loss D: 1.043 
Loss G: 0.7045 (0.8482) Acc G: 14.654% 
LR: 2.000e-04 

2023-03-02 02:00:21,247 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3593 (0.3973) Acc D Real: 65.783% 
Loss D Fake: 0.6863 (0.6038) Acc D Fake: 84.551% 
Loss D: 1.046 
Loss G: 0.7046 (0.8463) Acc G: 14.701% 
LR: 2.000e-04 

2023-03-02 02:00:21,255 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3647 (0.3969) Acc D Real: 65.906% 
Loss D Fake: 0.6861 (0.6049) Acc D Fake: 84.514% 
Loss D: 1.051 
Loss G: 0.7049 (0.8445) Acc G: 14.747% 
LR: 2.000e-04 

2023-03-02 02:00:21,262 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3200 (0.3959) Acc D Real: 66.050% 
Loss D Fake: 0.6856 (0.6059) Acc D Fake: 84.479% 
Loss D: 1.006 
Loss G: 0.7057 (0.8428) Acc G: 14.792% 
LR: 2.000e-04 

2023-03-02 02:00:21,270 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4395 (0.3965) Acc D Real: 65.990% 
Loss D Fake: 0.6848 (0.6068) Acc D Fake: 84.444% 
Loss D: 1.124 
Loss G: 0.7061 (0.8411) Acc G: 14.835% 
LR: 2.000e-04 

2023-03-02 02:00:21,277 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4822 (0.3975) Acc D Real: 65.966% 
Loss D Fake: 0.6851 (0.6078) Acc D Fake: 84.410% 
Loss D: 1.167 
Loss G: 0.7051 (0.8395) Acc G: 14.878% 
LR: 2.000e-04 

2023-03-02 02:00:21,285 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4120 (0.3977) Acc D Real: 65.997% 
Loss D Fake: 0.6864 (0.6087) Acc D Fake: 84.377% 
Loss D: 1.098 
Loss G: 0.7036 (0.8378) Acc G: 14.920% 
LR: 2.000e-04 

2023-03-02 02:00:21,292 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3502 (0.3971) Acc D Real: 66.096% 
Loss D Fake: 0.6878 (0.6097) Acc D Fake: 84.345% 
Loss D: 1.038 
Loss G: 0.7024 (0.8362) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 02:00:21,300 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3629 (0.3967) Acc D Real: 66.176% 
Loss D Fake: 0.6887 (0.6106) Acc D Fake: 84.294% 
Loss D: 1.052 
Loss G: 0.7017 (0.8346) Acc G: 15.036% 
LR: 2.000e-04 

2023-03-02 02:00:21,307 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4690 (0.3976) Acc D Real: 66.126% 
Loss D Fake: 0.6895 (0.6115) Acc D Fake: 84.244% 
Loss D: 1.159 
Loss G: 0.7003 (0.8331) Acc G: 15.094% 
LR: 2.000e-04 

2023-03-02 02:00:21,314 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3993 (0.3976) Acc D Real: 66.162% 
Loss D Fake: 0.6911 (0.6124) Acc D Fake: 84.195% 
Loss D: 1.090 
Loss G: 0.6987 (0.8315) Acc G: 15.150% 
LR: 2.000e-04 

2023-03-02 02:00:21,323 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4700 (0.3984) Acc D Real: 66.161% 
Loss D Fake: 0.6929 (0.6134) Acc D Fake: 84.147% 
Loss D: 1.163 
Loss G: 0.6964 (0.8300) Acc G: 15.205% 
LR: 2.000e-04 

2023-03-02 02:00:21,331 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4107 (0.3986) Acc D Real: 66.222% 
Loss D Fake: 0.6954 (0.6143) Acc D Fake: 84.101% 
Loss D: 1.106 
Loss G: 0.6939 (0.8285) Acc G: 15.259% 
LR: 2.000e-04 

2023-03-02 02:00:21,338 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3361 (0.3979) Acc D Real: 66.339% 
Loss D Fake: 0.6975 (0.6152) Acc D Fake: 84.055% 
Loss D: 1.034 
Loss G: 0.6924 (0.8269) Acc G: 15.312% 
LR: 2.000e-04 

2023-03-02 02:00:21,347 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4095 (0.3980) Acc D Real: 66.304% 
Loss D Fake: 0.6988 (0.6161) Acc D Fake: 84.010% 
Loss D: 1.108 
Loss G: 0.6910 (0.8254) Acc G: 15.363% 
LR: 2.000e-04 

2023-03-02 02:00:21,354 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3838 (0.3978) Acc D Real: 66.402% 
Loss D Fake: 0.7001 (0.6170) Acc D Fake: 83.967% 
Loss D: 1.084 
Loss G: 0.6897 (0.8240) Acc G: 15.414% 
LR: 2.000e-04 

2023-03-02 02:00:21,362 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4224 (0.3981) Acc D Real: 66.495% 
Loss D Fake: 0.7014 (0.6179) Acc D Fake: 83.924% 
Loss D: 1.124 
Loss G: 0.6882 (0.8225) Acc G: 15.463% 
LR: 2.000e-04 

2023-03-02 02:00:21,369 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4242 (0.3984) Acc D Real: 66.565% 
Loss D Fake: 0.7032 (0.6189) Acc D Fake: 83.882% 
Loss D: 1.127 
Loss G: 0.6863 (0.8211) Acc G: 15.511% 
LR: 2.000e-04 

2023-03-02 02:00:21,376 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3953 (0.3983) Acc D Real: 66.692% 
Loss D Fake: 0.7051 (0.6198) Acc D Fake: 83.842% 
Loss D: 1.100 
Loss G: 0.6846 (0.8196) Acc G: 15.559% 
LR: 2.000e-04 

2023-03-02 02:00:21,383 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4033 (0.3984) Acc D Real: 66.763% 
Loss D Fake: 0.7066 (0.6207) Acc D Fake: 83.792% 
Loss D: 1.110 
Loss G: 0.6831 (0.8182) Acc G: 15.619% 
LR: 2.000e-04 

2023-03-02 02:00:21,391 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3491 (0.3979) Acc D Real: 66.909% 
Loss D Fake: 0.7080 (0.6216) Acc D Fake: 83.736% 
Loss D: 1.057 
Loss G: 0.6822 (0.8168) Acc G: 15.681% 
LR: 2.000e-04 

2023-03-02 02:00:21,399 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4761 (0.3987) Acc D Real: 66.951% 
Loss D Fake: 0.7090 (0.6225) Acc D Fake: 83.681% 
Loss D: 1.185 
Loss G: 0.6805 (0.8154) Acc G: 15.742% 
LR: 2.000e-04 

2023-03-02 02:00:21,406 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3876 (0.3986) Acc D Real: 67.049% 
Loss D Fake: 0.7109 (0.6233) Acc D Fake: 83.627% 
Loss D: 1.098 
Loss G: 0.6789 (0.8140) Acc G: 15.802% 
LR: 2.000e-04 

2023-03-02 02:00:21,413 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3878 (0.3985) Acc D Real: 67.193% 
Loss D Fake: 0.7122 (0.6242) Acc D Fake: 83.191% 
Loss D: 1.100 
Loss G: 0.6778 (0.8127) Acc G: 16.170% 
LR: 2.000e-04 

2023-03-02 02:00:21,421 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4209 (0.3987) Acc D Real: 67.223% 
Loss D Fake: 0.7133 (0.6251) Acc D Fake: 82.631% 
Loss D: 1.134 
Loss G: 0.6766 (0.8113) Acc G: 16.736% 
LR: 2.000e-04 

2023-03-02 02:00:21,428 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2983 (0.3977) Acc D Real: 67.399% 
Loss D Fake: 0.7142 (0.6260) Acc D Fake: 82.034% 
Loss D: 1.013 
Loss G: 0.6765 (0.8100) Acc G: 17.291% 
LR: 2.000e-04 

2023-03-02 02:00:21,435 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3169 (0.3969) Acc D Real: 67.566% 
Loss D Fake: 0.7137 (0.6268) Acc D Fake: 81.496% 
Loss D: 1.031 
Loss G: 0.6776 (0.8087) Acc G: 17.703% 
LR: 2.000e-04 

2023-03-02 02:00:21,443 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4124 (0.3971) Acc D Real: 67.683% 
Loss D Fake: 0.7125 (0.6277) Acc D Fake: 81.065% 
Loss D: 1.125 
Loss G: 0.6784 (0.8075) Acc G: 17.741% 
LR: 2.000e-04 

2023-03-02 02:00:21,450 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3940 (0.3970) Acc D Real: 67.818% 
Loss D Fake: 0.7119 (0.6285) Acc D Fake: 81.039% 
Loss D: 1.106 
Loss G: 0.6788 (0.8062) Acc G: 17.778% 
LR: 2.000e-04 

2023-03-02 02:00:21,457 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3635 (0.3967) Acc D Real: 67.954% 
Loss D Fake: 0.7115 (0.6293) Acc D Fake: 81.014% 
Loss D: 1.075 
Loss G: 0.6794 (0.8050) Acc G: 17.815% 
LR: 2.000e-04 

2023-03-02 02:00:21,465 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.4025 (0.3968) Acc D Real: 68.033% 
Loss D Fake: 0.7110 (0.6300) Acc D Fake: 80.989% 
Loss D: 1.113 
Loss G: 0.6797 (0.8039) Acc G: 17.851% 
LR: 2.000e-04 

2023-03-02 02:00:21,472 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3430 (0.3963) Acc D Real: 68.150% 
Loss D Fake: 0.7106 (0.6308) Acc D Fake: 80.964% 
Loss D: 1.054 
Loss G: 0.6803 (0.8027) Acc G: 17.886% 
LR: 2.000e-04 

2023-03-02 02:00:21,480 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4168 (0.3965) Acc D Real: 68.242% 
Loss D Fake: 0.7100 (0.6315) Acc D Fake: 80.940% 
Loss D: 1.127 
Loss G: 0.6807 (0.8016) Acc G: 17.921% 
LR: 2.000e-04 

2023-03-02 02:00:21,487 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3413 (0.3960) Acc D Real: 68.335% 
Loss D Fake: 0.7096 (0.6322) Acc D Fake: 80.916% 
Loss D: 1.051 
Loss G: 0.6813 (0.8005) Acc G: 17.955% 
LR: 2.000e-04 

2023-03-02 02:00:21,495 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3869 (0.3959) Acc D Real: 68.387% 
Loss D Fake: 0.7088 (0.6329) Acc D Fake: 80.893% 
Loss D: 1.096 
Loss G: 0.6821 (0.7994) Acc G: 17.988% 
LR: 2.000e-04 

2023-03-02 02:00:21,502 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3253 (0.3953) Acc D Real: 68.535% 
Loss D Fake: 0.7079 (0.6336) Acc D Fake: 80.870% 
Loss D: 1.033 
Loss G: 0.6834 (0.7984) Acc G: 18.021% 
LR: 2.000e-04 

2023-03-02 02:00:21,510 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3984 (0.3953) Acc D Real: 68.591% 
Loss D Fake: 0.7065 (0.6342) Acc D Fake: 80.848% 
Loss D: 1.105 
Loss G: 0.6846 (0.7974) Acc G: 18.054% 
LR: 2.000e-04 

2023-03-02 02:00:21,518 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2597 (0.3941) Acc D Real: 68.750% 
Loss D Fake: 0.7049 (0.6348) Acc D Fake: 80.826% 
Loss D: 0.965 
Loss G: 0.6871 (0.7964) Acc G: 18.085% 
LR: 2.000e-04 

2023-03-02 02:00:21,526 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.2620 (0.3929) Acc D Real: 68.875% 
Loss D Fake: 0.7017 (0.6354) Acc D Fake: 80.804% 
Loss D: 0.964 
Loss G: 0.6910 (0.7955) Acc G: 18.116% 
LR: 2.000e-04 

2023-03-02 02:00:21,533 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4029 (0.3930) Acc D Real: 68.883% 
Loss D Fake: 0.6979 (0.6359) Acc D Fake: 80.783% 
Loss D: 1.101 
Loss G: 0.6943 (0.7946) Acc G: 18.147% 
LR: 2.000e-04 

2023-03-02 02:00:21,541 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3470 (0.3926) Acc D Real: 68.974% 
Loss D Fake: 0.6949 (0.6364) Acc D Fake: 80.762% 
Loss D: 1.042 
Loss G: 0.6973 (0.7938) Acc G: 18.163% 
LR: 2.000e-04 

2023-03-02 02:00:21,548 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3841 (0.3926) Acc D Real: 69.031% 
Loss D Fake: 0.6922 (0.6369) Acc D Fake: 80.755% 
Loss D: 1.076 
Loss G: 0.6997 (0.7930) Acc G: 18.178% 
LR: 2.000e-04 

2023-03-02 02:00:21,556 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4338 (0.3929) Acc D Real: 69.003% 
Loss D Fake: 0.6903 (0.6374) Acc D Fake: 80.749% 
Loss D: 1.124 
Loss G: 0.7011 (0.7922) Acc G: 18.194% 
LR: 2.000e-04 

2023-03-02 02:00:21,563 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4005 (0.3930) Acc D Real: 69.005% 
Loss D Fake: 0.6893 (0.6378) Acc D Fake: 80.743% 
Loss D: 1.090 
Loss G: 0.7020 (0.7915) Acc G: 18.209% 
LR: 2.000e-04 

2023-03-02 02:00:21,570 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3984 (0.3930) Acc D Real: 68.982% 
Loss D Fake: 0.6887 (0.6382) Acc D Fake: 80.736% 
Loss D: 1.087 
Loss G: 0.7024 (0.7908) Acc G: 18.224% 
LR: 2.000e-04 

2023-03-02 02:00:21,578 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4217 (0.3933) Acc D Real: 68.965% 
Loss D Fake: 0.6886 (0.6386) Acc D Fake: 80.730% 
Loss D: 1.110 
Loss G: 0.7021 (0.7900) Acc G: 18.238% 
LR: 2.000e-04 

2023-03-02 02:00:21,585 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4767 (0.3939) Acc D Real: 68.877% 
Loss D Fake: 0.6894 (0.6390) Acc D Fake: 80.725% 
Loss D: 1.166 
Loss G: 0.7006 (0.7893) Acc G: 18.252% 
LR: 2.000e-04 

2023-03-02 02:00:21,592 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3716 (0.3938) Acc D Real: 68.881% 
Loss D Fake: 0.6910 (0.6395) Acc D Fake: 80.719% 
Loss D: 1.063 
Loss G: 0.6993 (0.7886) Acc G: 18.267% 
LR: 2.000e-04 

2023-03-02 02:00:21,601 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4532 (0.3942) Acc D Real: 68.820% 
Loss D Fake: 0.6924 (0.6399) Acc D Fake: 80.713% 
Loss D: 1.146 
Loss G: 0.6974 (0.7878) Acc G: 18.280% 
LR: 2.000e-04 

2023-03-02 02:00:21,608 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3692 (0.3940) Acc D Real: 68.922% 
Loss D Fake: 0.6942 (0.6403) Acc D Fake: 80.707% 
Loss D: 1.063 
Loss G: 0.6958 (0.7871) Acc G: 18.294% 
LR: 2.000e-04 

2023-03-02 02:00:21,616 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4306 (0.3943) Acc D Real: 68.905% 
Loss D Fake: 0.6957 (0.6408) Acc D Fake: 80.702% 
Loss D: 1.126 
Loss G: 0.6941 (0.7864) Acc G: 18.317% 
LR: 2.000e-04 

2023-03-02 02:00:21,623 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3304 (0.3938) Acc D Real: 68.968% 
Loss D Fake: 0.6972 (0.6412) Acc D Fake: 80.683% 
Loss D: 1.028 
Loss G: 0.6931 (0.7857) Acc G: 18.343% 
LR: 2.000e-04 

2023-03-02 02:00:21,632 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3860 (0.3938) Acc D Real: 69.034% 
Loss D Fake: 0.6979 (0.6416) Acc D Fake: 80.665% 
Loss D: 1.084 
Loss G: 0.6924 (0.7849) Acc G: 18.368% 
LR: 2.000e-04 

2023-03-02 02:00:21,639 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4505 (0.3942) Acc D Real: 69.038% 
Loss D Fake: 0.6988 (0.6421) Acc D Fake: 80.647% 
Loss D: 1.149 
Loss G: 0.6912 (0.7842) Acc G: 18.394% 
LR: 2.000e-04 

2023-03-02 02:00:21,648 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4501 (0.3946) Acc D Real: 69.044% 
Loss D Fake: 0.7003 (0.6425) Acc D Fake: 80.629% 
Loss D: 1.150 
Loss G: 0.6894 (0.7835) Acc G: 18.419% 
LR: 2.000e-04 

2023-03-02 02:00:21,656 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3636 (0.3944) Acc D Real: 69.137% 
Loss D Fake: 0.7021 (0.6430) Acc D Fake: 80.612% 
Loss D: 1.066 
Loss G: 0.6879 (0.7828) Acc G: 18.443% 
LR: 2.000e-04 

2023-03-02 02:00:21,664 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3861 (0.3943) Acc D Real: 69.178% 
Loss D Fake: 0.7034 (0.6434) Acc D Fake: 80.595% 
Loss D: 1.089 
Loss G: 0.6867 (0.7820) Acc G: 18.468% 
LR: 2.000e-04 

2023-03-02 02:00:21,672 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3849 (0.3942) Acc D Real: 69.230% 
Loss D Fake: 0.7044 (0.6439) Acc D Fake: 80.578% 
Loss D: 1.089 
Loss G: 0.6858 (0.7813) Acc G: 18.492% 
LR: 2.000e-04 

2023-03-02 02:00:21,679 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3346 (0.3938) Acc D Real: 69.311% 
Loss D Fake: 0.7051 (0.6443) Acc D Fake: 80.561% 
Loss D: 1.040 
Loss G: 0.6856 (0.7806) Acc G: 18.515% 
LR: 2.000e-04 

2023-03-02 02:00:21,687 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3728 (0.3937) Acc D Real: 69.334% 
Loss D Fake: 0.7050 (0.6448) Acc D Fake: 80.545% 
Loss D: 1.078 
Loss G: 0.6858 (0.7799) Acc G: 18.538% 
LR: 2.000e-04 

2023-03-02 02:00:21,695 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3658 (0.3935) Acc D Real: 69.418% 
Loss D Fake: 0.7047 (0.6452) Acc D Fake: 80.529% 
Loss D: 1.071 
Loss G: 0.6862 (0.7792) Acc G: 18.561% 
LR: 2.000e-04 

2023-03-02 02:00:21,702 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4432 (0.3938) Acc D Real: 69.426% 
Loss D Fake: 0.7046 (0.6457) Acc D Fake: 80.513% 
Loss D: 1.148 
Loss G: 0.6859 (0.7786) Acc G: 18.584% 
LR: 2.000e-04 

2023-03-02 02:00:21,710 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.2908 (0.3931) Acc D Real: 69.522% 
Loss D Fake: 0.7046 (0.6461) Acc D Fake: 80.497% 
Loss D: 0.995 
Loss G: 0.6866 (0.7779) Acc G: 18.606% 
LR: 2.000e-04 

2023-03-02 02:00:21,717 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3316 (0.3926) Acc D Real: 69.556% 
Loss D Fake: 0.7034 (0.6465) Acc D Fake: 80.482% 
Loss D: 1.035 
Loss G: 0.6882 (0.7773) Acc G: 18.628% 
LR: 2.000e-04 

2023-03-02 02:00:21,725 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4230 (0.3928) Acc D Real: 69.562% 
Loss D Fake: 0.7020 (0.6469) Acc D Fake: 80.467% 
Loss D: 1.125 
Loss G: 0.6892 (0.7766) Acc G: 18.649% 
LR: 2.000e-04 

2023-03-02 02:00:21,732 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3822 (0.3928) Acc D Real: 69.580% 
Loss D Fake: 0.7013 (0.6473) Acc D Fake: 80.452% 
Loss D: 1.084 
Loss G: 0.6898 (0.7760) Acc G: 18.670% 
LR: 2.000e-04 

2023-03-02 02:00:21,740 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3664 (0.3926) Acc D Real: 69.628% 
Loss D Fake: 0.7007 (0.6476) Acc D Fake: 80.437% 
Loss D: 1.067 
Loss G: 0.6904 (0.7754) Acc G: 18.691% 
LR: 2.000e-04 

2023-03-02 02:00:21,748 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3048 (0.3920) Acc D Real: 69.718% 
Loss D Fake: 0.6999 (0.6480) Acc D Fake: 80.422% 
Loss D: 1.005 
Loss G: 0.6917 (0.7748) Acc G: 18.700% 
LR: 2.000e-04 

2023-03-02 02:00:21,756 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4160 (0.3921) Acc D Real: 69.704% 
Loss D Fake: 0.6985 (0.6483) Acc D Fake: 80.419% 
Loss D: 1.115 
Loss G: 0.6927 (0.7743) Acc G: 18.709% 
LR: 2.000e-04 

2023-03-02 02:00:21,763 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4356 (0.3924) Acc D Real: 69.714% 
Loss D Fake: 0.6981 (0.6487) Acc D Fake: 80.416% 
Loss D: 1.134 
Loss G: 0.6925 (0.7737) Acc G: 18.718% 
LR: 2.000e-04 

2023-03-02 02:00:21,771 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3107 (0.3919) Acc D Real: 69.802% 
Loss D Fake: 0.6982 (0.6490) Acc D Fake: 80.413% 
Loss D: 1.009 
Loss G: 0.6930 (0.7732) Acc G: 18.727% 
LR: 2.000e-04 

2023-03-02 02:00:21,778 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3636 (0.3917) Acc D Real: 69.885% 
Loss D Fake: 0.6975 (0.6494) Acc D Fake: 80.411% 
Loss D: 1.061 
Loss G: 0.6937 (0.7726) Acc G: 18.736% 
LR: 2.000e-04 

2023-03-02 02:00:21,786 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.2806 (0.3909) Acc D Real: 69.960% 
Loss D Fake: 0.6964 (0.6497) Acc D Fake: 80.408% 
Loss D: 0.977 
Loss G: 0.6955 (0.7721) Acc G: 18.744% 
LR: 2.000e-04 

2023-03-02 02:00:21,793 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4776 (0.3915) Acc D Real: 69.887% 
Loss D Fake: 0.6949 (0.6500) Acc D Fake: 80.405% 
Loss D: 1.172 
Loss G: 0.6960 (0.7716) Acc G: 18.752% 
LR: 2.000e-04 

2023-03-02 02:00:21,801 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3739 (0.3914) Acc D Real: 69.934% 
Loss D Fake: 0.6949 (0.6503) Acc D Fake: 80.403% 
Loss D: 1.069 
Loss G: 0.6961 (0.7711) Acc G: 18.761% 
LR: 2.000e-04 

2023-03-02 02:00:21,808 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3648 (0.3912) Acc D Real: 69.990% 
Loss D Fake: 0.6947 (0.6506) Acc D Fake: 80.400% 
Loss D: 1.060 
Loss G: 0.6964 (0.7706) Acc G: 18.769% 
LR: 2.000e-04 

2023-03-02 02:00:21,815 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3537 (0.3910) Acc D Real: 70.012% 
Loss D Fake: 0.6943 (0.6508) Acc D Fake: 80.397% 
Loss D: 1.048 
Loss G: 0.6970 (0.7701) Acc G: 18.777% 
LR: 2.000e-04 

2023-03-02 02:00:21,823 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4557 (0.3914) Acc D Real: 69.953% 
Loss D Fake: 0.6939 (0.6511) Acc D Fake: 80.395% 
Loss D: 1.150 
Loss G: 0.6969 (0.7697) Acc G: 18.785% 
LR: 2.000e-04 

2023-03-02 02:00:21,830 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3135 (0.3909) Acc D Real: 69.999% 
Loss D Fake: 0.6941 (0.6514) Acc D Fake: 80.392% 
Loss D: 1.008 
Loss G: 0.6972 (0.7692) Acc G: 18.793% 
LR: 2.000e-04 

2023-03-02 02:00:21,838 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4310 (0.3912) Acc D Real: 69.947% 
Loss D Fake: 0.6938 (0.6517) Acc D Fake: 80.390% 
Loss D: 1.125 
Loss G: 0.6971 (0.7687) Acc G: 18.800% 
LR: 2.000e-04 

2023-03-02 02:00:21,845 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4192 (0.3913) Acc D Real: 69.935% 
Loss D Fake: 0.6942 (0.6519) Acc D Fake: 80.387% 
Loss D: 1.113 
Loss G: 0.6963 (0.7683) Acc G: 18.808% 
LR: 2.000e-04 

2023-03-02 02:00:21,852 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4938 (0.3920) Acc D Real: 69.927% 
Loss D Fake: 0.6955 (0.6522) Acc D Fake: 80.387% 
Loss D: 1.189 
Loss G: 0.6941 (0.7678) Acc G: 18.809% 
LR: 2.000e-04 

2023-03-02 02:00:22,112 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.346 | Generator Loss: 0.694 | Avg: 2.040 
2023-03-02 02:00:22,137 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.267 | Generator Loss: 0.694 | Avg: 1.961 
2023-03-02 02:00:22,162 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.290 | Generator Loss: 0.694 | Avg: 1.984 
2023-03-02 02:00:22,189 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.304 | Generator Loss: 0.694 | Avg: 1.998 
2023-03-02 02:00:22,215 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.308 | Generator Loss: 0.694 | Avg: 2.002 
2023-03-02 02:00:22,241 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.263 | Generator Loss: 0.694 | Avg: 1.957 
2023-03-02 02:00:22,267 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.244 | Generator Loss: 0.694 | Avg: 1.938 
2023-03-02 02:00:22,293 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.206 | Generator Loss: 0.694 | Avg: 1.900 
2023-03-02 02:00:22,319 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.185 | Generator Loss: 0.694 | Avg: 1.879 
2023-03-02 02:00:22,345 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.145 | Generator Loss: 0.694 | Avg: 1.839 
2023-03-02 02:00:22,371 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.116 | Generator Loss: 0.694 | Avg: 1.810 
2023-03-02 02:00:22,397 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.087 | Generator Loss: 0.694 | Avg: 1.781 
2023-03-02 02:00:22,423 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.064 | Generator Loss: 0.694 | Avg: 1.758 
2023-03-02 02:00:22,449 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.047 | Generator Loss: 0.694 | Avg: 1.741 
2023-03-02 02:00:22,475 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.063 | Generator Loss: 0.694 | Avg: 1.757 
2023-03-02 02:00:22,506 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.079 | Generator Loss: 0.694 | Avg: 1.773 
2023-03-02 02:00:22,536 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.092 | Generator Loss: 0.694 | Avg: 1.786 
2023-03-02 02:00:22,573 -                train: [    INFO] - 
Epoch: 11/20
2023-03-02 02:00:22,757 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3544 (0.3931) Acc D Real: 71.771% 
Loss D Fake: 0.7008 (0.6995) Acc D Fake: 79.167% 
Loss D: 1.055 
Loss G: 0.6892 (0.6902) Acc G: 20.833% 
LR: 2.000e-04 

2023-03-02 02:00:22,765 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.2935 (0.3599) Acc D Real: 74.705% 
Loss D Fake: 0.7021 (0.7004) Acc D Fake: 78.889% 
Loss D: 0.996 
Loss G: 0.6887 (0.6897) Acc G: 21.111% 
LR: 2.000e-04 

2023-03-02 02:00:22,773 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3881 (0.3669) Acc D Real: 73.958% 
Loss D Fake: 0.7021 (0.7008) Acc D Fake: 78.750% 
Loss D: 1.090 
Loss G: 0.6887 (0.6894) Acc G: 21.250% 
LR: 2.000e-04 

2023-03-02 02:00:22,789 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3713 (0.3678) Acc D Real: 73.708% 
Loss D Fake: 0.7022 (0.7011) Acc D Fake: 78.667% 
Loss D: 1.073 
Loss G: 0.6887 (0.6893) Acc G: 21.333% 
LR: 2.000e-04 

2023-03-02 02:00:22,796 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4085 (0.3746) Acc D Real: 73.550% 
Loss D Fake: 0.7022 (0.7013) Acc D Fake: 78.333% 
Loss D: 1.111 
Loss G: 0.6885 (0.6892) Acc G: 21.667% 
LR: 2.000e-04 

2023-03-02 02:00:22,803 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3820 (0.3757) Acc D Real: 73.021% 
Loss D Fake: 0.7024 (0.7014) Acc D Fake: 78.095% 
Loss D: 1.084 
Loss G: 0.6885 (0.6891) Acc G: 21.905% 
LR: 2.000e-04 

2023-03-02 02:00:22,810 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3597 (0.3737) Acc D Real: 73.939% 
Loss D Fake: 0.7023 (0.7015) Acc D Fake: 77.917% 
Loss D: 1.062 
Loss G: 0.6886 (0.6890) Acc G: 22.083% 
LR: 2.000e-04 

2023-03-02 02:00:22,817 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3624 (0.3724) Acc D Real: 74.057% 
Loss D Fake: 0.7021 (0.7016) Acc D Fake: 77.778% 
Loss D: 1.065 
Loss G: 0.6890 (0.6890) Acc G: 22.222% 
LR: 2.000e-04 

2023-03-02 02:00:22,824 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4050 (0.3757) Acc D Real: 73.932% 
Loss D Fake: 0.7018 (0.7016) Acc D Fake: 77.667% 
Loss D: 1.107 
Loss G: 0.6891 (0.6890) Acc G: 22.333% 
LR: 2.000e-04 

2023-03-02 02:00:22,831 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3820 (0.3762) Acc D Real: 73.887% 
Loss D Fake: 0.7017 (0.7016) Acc D Fake: 77.576% 
Loss D: 1.084 
Loss G: 0.6892 (0.6890) Acc G: 22.287% 
LR: 2.000e-04 

2023-03-02 02:00:22,838 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4086 (0.3789) Acc D Real: 73.303% 
Loss D Fake: 0.7018 (0.7016) Acc D Fake: 77.500% 
Loss D: 1.110 
Loss G: 0.6889 (0.6890) Acc G: 22.374% 
LR: 2.000e-04 

2023-03-02 02:00:22,845 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4490 (0.3843) Acc D Real: 72.696% 
Loss D Fake: 0.7024 (0.7017) Acc D Fake: 77.436% 
Loss D: 1.151 
Loss G: 0.6878 (0.6889) Acc G: 22.448% 
LR: 2.000e-04 

2023-03-02 02:00:22,852 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3446 (0.3815) Acc D Real: 72.857% 
Loss D Fake: 0.7035 (0.7018) Acc D Fake: 77.381% 
Loss D: 1.048 
Loss G: 0.6871 (0.6888) Acc G: 22.511% 
LR: 2.000e-04 

2023-03-02 02:00:22,859 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3920 (0.3822) Acc D Real: 72.688% 
Loss D Fake: 0.7041 (0.7020) Acc D Fake: 77.333% 
Loss D: 1.096 
Loss G: 0.6864 (0.6886) Acc G: 22.566% 
LR: 2.000e-04 

2023-03-02 02:00:22,866 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4389 (0.3857) Acc D Real: 72.490% 
Loss D Fake: 0.7051 (0.7022) Acc D Fake: 77.292% 
Loss D: 1.144 
Loss G: 0.6850 (0.6884) Acc G: 22.614% 
LR: 2.000e-04 

2023-03-02 02:00:22,874 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.2769 (0.3793) Acc D Real: 73.560% 
Loss D Fake: 0.7061 (0.7024) Acc D Fake: 77.255% 
Loss D: 0.983 
Loss G: 0.6850 (0.6882) Acc G: 22.656% 
LR: 2.000e-04 

2023-03-02 02:00:22,881 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3622 (0.3784) Acc D Real: 73.507% 
Loss D Fake: 0.7057 (0.7026) Acc D Fake: 77.222% 
Loss D: 1.068 
Loss G: 0.6855 (0.6881) Acc G: 22.694% 
LR: 2.000e-04 

2023-03-02 02:00:22,888 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3424 (0.3765) Acc D Real: 73.799% 
Loss D Fake: 0.7050 (0.7027) Acc D Fake: 77.193% 
Loss D: 1.047 
Loss G: 0.6865 (0.6880) Acc G: 22.728% 
LR: 2.000e-04 

2023-03-02 02:00:22,895 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3682 (0.3761) Acc D Real: 73.833% 
Loss D Fake: 0.7039 (0.7028) Acc D Fake: 77.167% 
Loss D: 1.072 
Loss G: 0.6875 (0.6880) Acc G: 22.758% 
LR: 2.000e-04 

2023-03-02 02:00:22,903 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3532 (0.3750) Acc D Real: 73.874% 
Loss D Fake: 0.7027 (0.7028) Acc D Fake: 77.143% 
Loss D: 1.056 
Loss G: 0.6889 (0.6880) Acc G: 22.706% 
LR: 2.000e-04 

2023-03-02 02:00:22,910 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3704 (0.3748) Acc D Real: 73.726% 
Loss D Fake: 0.7013 (0.7027) Acc D Fake: 77.197% 
Loss D: 1.072 
Loss G: 0.6903 (0.6881) Acc G: 22.659% 
LR: 2.000e-04 

2023-03-02 02:00:22,917 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4535 (0.3782) Acc D Real: 73.727% 
Loss D Fake: 0.7003 (0.7026) Acc D Fake: 77.246% 
Loss D: 1.154 
Loss G: 0.6907 (0.6882) Acc G: 22.615% 
LR: 2.000e-04 

2023-03-02 02:00:22,924 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3128 (0.3755) Acc D Real: 73.976% 
Loss D Fake: 0.7000 (0.7025) Acc D Fake: 77.292% 
Loss D: 1.013 
Loss G: 0.6915 (0.6884) Acc G: 22.576% 
LR: 2.000e-04 

2023-03-02 02:00:22,931 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3756 (0.3755) Acc D Real: 74.069% 
Loss D Fake: 0.6990 (0.7023) Acc D Fake: 77.333% 
Loss D: 1.075 
Loss G: 0.6926 (0.6885) Acc G: 22.540% 
LR: 2.000e-04 

2023-03-02 02:00:22,939 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3247 (0.3735) Acc D Real: 74.259% 
Loss D Fake: 0.6976 (0.7022) Acc D Fake: 77.372% 
Loss D: 1.022 
Loss G: 0.6943 (0.6887) Acc G: 22.506% 
LR: 2.000e-04 

2023-03-02 02:00:22,946 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3409 (0.3723) Acc D Real: 74.419% 
Loss D Fake: 0.6958 (0.7019) Acc D Fake: 77.407% 
Loss D: 1.037 
Loss G: 0.6962 (0.6890) Acc G: 22.475% 
LR: 2.000e-04 

2023-03-02 02:00:22,954 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3570 (0.3718) Acc D Real: 74.423% 
Loss D Fake: 0.6940 (0.7016) Acc D Fake: 77.440% 
Loss D: 1.051 
Loss G: 0.6982 (0.6894) Acc G: 22.446% 
LR: 2.000e-04 

2023-03-02 02:00:22,962 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3486 (0.3710) Acc D Real: 74.551% 
Loss D Fake: 0.6920 (0.7013) Acc D Fake: 77.471% 
Loss D: 1.041 
Loss G: 0.7002 (0.6897) Acc G: 22.419% 
LR: 2.000e-04 

2023-03-02 02:00:22,969 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3846 (0.3714) Acc D Real: 74.460% 
Loss D Fake: 0.6902 (0.7009) Acc D Fake: 77.500% 
Loss D: 1.075 
Loss G: 0.7018 (0.6901) Acc G: 22.394% 
LR: 2.000e-04 

2023-03-02 02:00:22,977 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4077 (0.3726) Acc D Real: 74.309% 
Loss D Fake: 0.6891 (0.7006) Acc D Fake: 77.527% 
Loss D: 1.097 
Loss G: 0.7025 (0.6905) Acc G: 22.371% 
LR: 2.000e-04 

2023-03-02 02:00:22,984 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3546 (0.3720) Acc D Real: 74.251% 
Loss D Fake: 0.6886 (0.7002) Acc D Fake: 77.552% 
Loss D: 1.043 
Loss G: 0.7032 (0.6909) Acc G: 22.349% 
LR: 2.000e-04 

2023-03-02 02:00:22,992 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4519 (0.3745) Acc D Real: 73.857% 
Loss D Fake: 0.6881 (0.6998) Acc D Fake: 77.576% 
Loss D: 1.140 
Loss G: 0.7029 (0.6913) Acc G: 22.328% 
LR: 2.000e-04 

2023-03-02 02:00:22,999 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3643 (0.3742) Acc D Real: 73.822% 
Loss D Fake: 0.6887 (0.6995) Acc D Fake: 77.598% 
Loss D: 1.053 
Loss G: 0.7025 (0.6916) Acc G: 22.309% 
LR: 2.000e-04 

2023-03-02 02:00:23,006 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3774 (0.3743) Acc D Real: 73.677% 
Loss D Fake: 0.6890 (0.6992) Acc D Fake: 77.619% 
Loss D: 1.066 
Loss G: 0.7023 (0.6919) Acc G: 22.290% 
LR: 2.000e-04 

2023-03-02 02:00:23,015 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3835 (0.3745) Acc D Real: 73.488% 
Loss D Fake: 0.6892 (0.6989) Acc D Fake: 77.639% 
Loss D: 1.073 
Loss G: 0.7021 (0.6922) Acc G: 22.273% 
LR: 2.000e-04 

2023-03-02 02:00:23,024 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4223 (0.3758) Acc D Real: 73.177% 
Loss D Fake: 0.6894 (0.6987) Acc D Fake: 77.658% 
Loss D: 1.112 
Loss G: 0.7016 (0.6925) Acc G: 22.256% 
LR: 2.000e-04 

2023-03-02 02:00:23,032 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3651 (0.3755) Acc D Real: 73.209% 
Loss D Fake: 0.6900 (0.6984) Acc D Fake: 77.675% 
Loss D: 1.055 
Loss G: 0.7011 (0.6927) Acc G: 22.241% 
LR: 2.000e-04 

2023-03-02 02:00:23,041 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3861 (0.3758) Acc D Real: 73.094% 
Loss D Fake: 0.6904 (0.6982) Acc D Fake: 77.692% 
Loss D: 1.076 
Loss G: 0.7008 (0.6929) Acc G: 22.226% 
LR: 2.000e-04 

2023-03-02 02:00:23,048 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3326 (0.3747) Acc D Real: 73.185% 
Loss D Fake: 0.6905 (0.6980) Acc D Fake: 77.708% 
Loss D: 1.023 
Loss G: 0.7010 (0.6931) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 02:00:23,056 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3579 (0.3743) Acc D Real: 73.121% 
Loss D Fake: 0.6900 (0.6978) Acc D Fake: 77.724% 
Loss D: 1.048 
Loss G: 0.7017 (0.6933) Acc G: 22.199% 
LR: 2.000e-04 

2023-03-02 02:00:23,063 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3473 (0.3737) Acc D Real: 73.065% 
Loss D Fake: 0.6892 (0.6976) Acc D Fake: 77.738% 
Loss D: 1.037 
Loss G: 0.7027 (0.6935) Acc G: 22.186% 
LR: 2.000e-04 

2023-03-02 02:00:23,070 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.2688 (0.3712) Acc D Real: 73.289% 
Loss D Fake: 0.6877 (0.6974) Acc D Fake: 77.752% 
Loss D: 0.956 
Loss G: 0.7052 (0.6938) Acc G: 22.174% 
LR: 2.000e-04 

2023-03-02 02:00:23,077 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3633 (0.3710) Acc D Real: 73.258% 
Loss D Fake: 0.6849 (0.6971) Acc D Fake: 77.765% 
Loss D: 1.048 
Loss G: 0.7080 (0.6941) Acc G: 22.163% 
LR: 2.000e-04 

2023-03-02 02:00:23,085 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4473 (0.3727) Acc D Real: 73.080% 
Loss D Fake: 0.6828 (0.6968) Acc D Fake: 77.778% 
Loss D: 1.130 
Loss G: 0.7095 (0.6945) Acc G: 22.152% 
LR: 2.000e-04 

2023-03-02 02:00:23,092 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4384 (0.3742) Acc D Real: 72.806% 
Loss D Fake: 0.6820 (0.6965) Acc D Fake: 77.790% 
Loss D: 1.120 
Loss G: 0.7096 (0.6948) Acc G: 22.141% 
LR: 2.000e-04 

2023-03-02 02:00:23,099 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3725 (0.3741) Acc D Real: 72.708% 
Loss D Fake: 0.6822 (0.6962) Acc D Fake: 77.801% 
Loss D: 1.055 
Loss G: 0.7094 (0.6951) Acc G: 22.131% 
LR: 2.000e-04 

2023-03-02 02:00:23,107 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3451 (0.3735) Acc D Real: 72.696% 
Loss D Fake: 0.6823 (0.6959) Acc D Fake: 77.812% 
Loss D: 1.027 
Loss G: 0.7097 (0.6954) Acc G: 22.121% 
LR: 2.000e-04 

2023-03-02 02:00:23,114 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3367 (0.3728) Acc D Real: 72.705% 
Loss D Fake: 0.6818 (0.6956) Acc D Fake: 77.823% 
Loss D: 1.018 
Loss G: 0.7105 (0.6957) Acc G: 22.078% 
LR: 2.000e-04 

2023-03-02 02:00:23,121 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4007 (0.3733) Acc D Real: 72.539% 
Loss D Fake: 0.6810 (0.6953) Acc D Fake: 77.867% 
Loss D: 1.082 
Loss G: 0.7111 (0.6960) Acc G: 22.036% 
LR: 2.000e-04 

2023-03-02 02:00:23,129 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3963 (0.3738) Acc D Real: 72.379% 
Loss D Fake: 0.6808 (0.6950) Acc D Fake: 77.908% 
Loss D: 1.077 
Loss G: 0.7110 (0.6963) Acc G: 21.997% 
LR: 2.000e-04 

2023-03-02 02:00:23,136 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3432 (0.3732) Acc D Real: 72.397% 
Loss D Fake: 0.6808 (0.6947) Acc D Fake: 77.949% 
Loss D: 1.024 
Loss G: 0.7112 (0.6966) Acc G: 21.958% 
LR: 2.000e-04 

2023-03-02 02:00:23,144 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3802 (0.3733) Acc D Real: 72.191% 
Loss D Fake: 0.6805 (0.6945) Acc D Fake: 77.987% 
Loss D: 1.061 
Loss G: 0.7114 (0.6969) Acc G: 21.921% 
LR: 2.000e-04 

2023-03-02 02:00:23,151 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3970 (0.3738) Acc D Real: 72.121% 
Loss D Fake: 0.6805 (0.6942) Acc D Fake: 78.025% 
Loss D: 1.078 
Loss G: 0.7112 (0.6971) Acc G: 21.886% 
LR: 2.000e-04 

2023-03-02 02:00:23,159 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4326 (0.3748) Acc D Real: 72.010% 
Loss D Fake: 0.6811 (0.6940) Acc D Fake: 78.061% 
Loss D: 1.114 
Loss G: 0.7101 (0.6974) Acc G: 21.851% 
LR: 2.000e-04 

2023-03-02 02:00:23,167 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3166 (0.3738) Acc D Real: 72.064% 
Loss D Fake: 0.6819 (0.6938) Acc D Fake: 78.095% 
Loss D: 0.999 
Loss G: 0.7099 (0.6976) Acc G: 21.818% 
LR: 2.000e-04 

2023-03-02 02:00:23,175 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4003 (0.3743) Acc D Real: 71.974% 
Loss D Fake: 0.6820 (0.6936) Acc D Fake: 78.129% 
Loss D: 1.082 
Loss G: 0.7096 (0.6978) Acc G: 21.786% 
LR: 2.000e-04 

2023-03-02 02:00:23,182 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.4472 (0.3755) Acc D Real: 71.787% 
Loss D Fake: 0.6826 (0.6934) Acc D Fake: 78.132% 
Loss D: 1.130 
Loss G: 0.7084 (0.6980) Acc G: 21.784% 
LR: 2.000e-04 

2023-03-02 02:00:23,190 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3466 (0.3750) Acc D Real: 71.763% 
Loss D Fake: 0.6839 (0.6932) Acc D Fake: 78.136% 
Loss D: 1.030 
Loss G: 0.7075 (0.6982) Acc G: 21.782% 
LR: 2.000e-04 

2023-03-02 02:00:23,197 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3407 (0.3745) Acc D Real: 71.805% 
Loss D Fake: 0.6843 (0.6931) Acc D Fake: 78.139% 
Loss D: 1.025 
Loss G: 0.7074 (0.6983) Acc G: 21.780% 
LR: 2.000e-04 

2023-03-02 02:00:23,204 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3108 (0.3734) Acc D Real: 71.849% 
Loss D Fake: 0.6840 (0.6929) Acc D Fake: 78.142% 
Loss D: 0.995 
Loss G: 0.7082 (0.6985) Acc G: 21.779% 
LR: 2.000e-04 

2023-03-02 02:00:23,212 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3742 (0.3734) Acc D Real: 71.794% 
Loss D Fake: 0.6831 (0.6928) Acc D Fake: 78.145% 
Loss D: 1.057 
Loss G: 0.7090 (0.6986) Acc G: 21.750% 
LR: 2.000e-04 

2023-03-02 02:00:23,219 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3658 (0.3733) Acc D Real: 71.724% 
Loss D Fake: 0.6824 (0.6926) Acc D Fake: 78.175% 
Loss D: 1.048 
Loss G: 0.7098 (0.6988) Acc G: 21.722% 
LR: 2.000e-04 

2023-03-02 02:00:23,226 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3578 (0.3731) Acc D Real: 71.737% 
Loss D Fake: 0.6817 (0.6924) Acc D Fake: 78.203% 
Loss D: 1.040 
Loss G: 0.7102 (0.6990) Acc G: 21.695% 
LR: 2.000e-04 

2023-03-02 02:00:23,234 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4114 (0.3736) Acc D Real: 71.640% 
Loss D Fake: 0.6817 (0.6923) Acc D Fake: 78.231% 
Loss D: 1.093 
Loss G: 0.7099 (0.6992) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 02:00:23,241 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2748 (0.3722) Acc D Real: 71.766% 
Loss D Fake: 0.6817 (0.6921) Acc D Fake: 78.232% 
Loss D: 0.957 
Loss G: 0.7108 (0.6993) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 02:00:23,248 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3604 (0.3720) Acc D Real: 71.810% 
Loss D Fake: 0.6806 (0.6919) Acc D Fake: 78.234% 
Loss D: 1.041 
Loss G: 0.7120 (0.6995) Acc G: 21.673% 
LR: 2.000e-04 

2023-03-02 02:00:23,256 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3103 (0.3711) Acc D Real: 71.902% 
Loss D Fake: 0.6792 (0.6917) Acc D Fake: 78.235% 
Loss D: 0.990 
Loss G: 0.7138 (0.6997) Acc G: 21.697% 
LR: 2.000e-04 

2023-03-02 02:00:23,263 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3527 (0.3708) Acc D Real: 71.880% 
Loss D Fake: 0.6775 (0.6915) Acc D Fake: 78.213% 
Loss D: 1.030 
Loss G: 0.7155 (0.7000) Acc G: 21.721% 
LR: 2.000e-04 

2023-03-02 02:00:23,271 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2995 (0.3698) Acc D Real: 71.971% 
Loss D Fake: 0.6759 (0.6913) Acc D Fake: 78.190% 
Loss D: 0.975 
Loss G: 0.7177 (0.7002) Acc G: 21.744% 
LR: 2.000e-04 

2023-03-02 02:00:23,279 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3658 (0.3697) Acc D Real: 71.923% 
Loss D Fake: 0.6737 (0.6911) Acc D Fake: 78.169% 
Loss D: 1.040 
Loss G: 0.7199 (0.7005) Acc G: 21.766% 
LR: 2.000e-04 

2023-03-02 02:00:23,287 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4097 (0.3703) Acc D Real: 71.759% 
Loss D Fake: 0.6722 (0.6908) Acc D Fake: 78.148% 
Loss D: 1.082 
Loss G: 0.7210 (0.7008) Acc G: 21.788% 
LR: 2.000e-04 

2023-03-02 02:00:23,294 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3150 (0.3695) Acc D Real: 71.790% 
Loss D Fake: 0.6712 (0.6905) Acc D Fake: 78.128% 
Loss D: 0.986 
Loss G: 0.7224 (0.7011) Acc G: 21.809% 
LR: 2.000e-04 

2023-03-02 02:00:23,302 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3432 (0.3692) Acc D Real: 71.772% 
Loss D Fake: 0.6698 (0.6902) Acc D Fake: 78.108% 
Loss D: 1.013 
Loss G: 0.7241 (0.7014) Acc G: 21.830% 
LR: 2.000e-04 

2023-03-02 02:00:23,309 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3821 (0.3693) Acc D Real: 71.713% 
Loss D Fake: 0.6690 (0.6900) Acc D Fake: 78.089% 
Loss D: 1.051 
Loss G: 0.7233 (0.7017) Acc G: 21.850% 
LR: 2.000e-04 

2023-03-02 02:00:23,317 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4192 (0.3700) Acc D Real: 71.580% 
Loss D Fake: 0.6710 (0.6897) Acc D Fake: 78.070% 
Loss D: 1.090 
Loss G: 0.7209 (0.7019) Acc G: 21.870% 
LR: 2.000e-04 

2023-03-02 02:00:23,325 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3337 (0.3695) Acc D Real: 71.581% 
Loss D Fake: 0.6733 (0.6895) Acc D Fake: 78.052% 
Loss D: 1.007 
Loss G: 0.7188 (0.7022) Acc G: 21.889% 
LR: 2.000e-04 

2023-03-02 02:00:23,332 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3185 (0.3689) Acc D Real: 71.579% 
Loss D Fake: 0.6749 (0.6893) Acc D Fake: 78.034% 
Loss D: 0.993 
Loss G: 0.7177 (0.7024) Acc G: 21.907% 
LR: 2.000e-04 

2023-03-02 02:00:23,340 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.2742 (0.3677) Acc D Real: 71.644% 
Loss D Fake: 0.6755 (0.6891) Acc D Fake: 78.017% 
Loss D: 0.950 
Loss G: 0.7180 (0.7026) Acc G: 21.925% 
LR: 2.000e-04 

2023-03-02 02:00:23,349 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4196 (0.3683) Acc D Real: 71.521% 
Loss D Fake: 0.6753 (0.6890) Acc D Fake: 78.000% 
Loss D: 1.095 
Loss G: 0.7178 (0.7027) Acc G: 21.943% 
LR: 2.000e-04 

2023-03-02 02:00:23,356 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2870 (0.3673) Acc D Real: 71.584% 
Loss D Fake: 0.6754 (0.6888) Acc D Fake: 77.984% 
Loss D: 0.962 
Loss G: 0.7184 (0.7029) Acc G: 21.960% 
LR: 2.000e-04 

2023-03-02 02:00:23,365 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3009 (0.3665) Acc D Real: 71.638% 
Loss D Fake: 0.6744 (0.6886) Acc D Fake: 77.967% 
Loss D: 0.975 
Loss G: 0.7200 (0.7031) Acc G: 21.977% 
LR: 2.000e-04 

2023-03-02 02:00:23,374 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2588 (0.3652) Acc D Real: 71.724% 
Loss D Fake: 0.6724 (0.6884) Acc D Fake: 77.952% 
Loss D: 0.931 
Loss G: 0.7230 (0.7034) Acc G: 21.993% 
LR: 2.000e-04 

2023-03-02 02:00:23,381 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2997 (0.3644) Acc D Real: 71.752% 
Loss D Fake: 0.6693 (0.6882) Acc D Fake: 77.937% 
Loss D: 0.969 
Loss G: 0.7267 (0.7037) Acc G: 22.009% 
LR: 2.000e-04 

2023-03-02 02:00:23,388 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.2676 (0.3633) Acc D Real: 71.829% 
Loss D Fake: 0.6658 (0.6879) Acc D Fake: 77.922% 
Loss D: 0.933 
Loss G: 0.7311 (0.7040) Acc G: 22.025% 
LR: 2.000e-04 

2023-03-02 02:00:23,396 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.2806 (0.3623) Acc D Real: 71.870% 
Loss D Fake: 0.6616 (0.6876) Acc D Fake: 77.907% 
Loss D: 0.942 
Loss G: 0.7361 (0.7044) Acc G: 22.040% 
LR: 2.000e-04 

2023-03-02 02:00:23,404 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2652 (0.3612) Acc D Real: 71.955% 
Loss D Fake: 0.6572 (0.6873) Acc D Fake: 77.893% 
Loss D: 0.922 
Loss G: 0.7412 (0.7048) Acc G: 22.055% 
LR: 2.000e-04 

2023-03-02 02:00:23,411 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3724 (0.3613) Acc D Real: 71.867% 
Loss D Fake: 0.6530 (0.6869) Acc D Fake: 77.879% 
Loss D: 1.025 
Loss G: 0.7455 (0.7052) Acc G: 22.069% 
LR: 2.000e-04 

2023-03-02 02:00:23,419 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3149 (0.3608) Acc D Real: 71.883% 
Loss D Fake: 0.6496 (0.6865) Acc D Fake: 77.865% 
Loss D: 0.965 
Loss G: 0.7495 (0.7057) Acc G: 22.083% 
LR: 2.000e-04 

2023-03-02 02:00:23,426 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3602 (0.3608) Acc D Real: 71.833% 
Loss D Fake: 0.6465 (0.6860) Acc D Fake: 77.852% 
Loss D: 1.007 
Loss G: 0.7527 (0.7063) Acc G: 22.097% 
LR: 2.000e-04 

2023-03-02 02:00:23,433 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3433 (0.3606) Acc D Real: 71.802% 
Loss D Fake: 0.6440 (0.6856) Acc D Fake: 77.839% 
Loss D: 0.987 
Loss G: 0.7554 (0.7068) Acc G: 22.111% 
LR: 2.000e-04 

2023-03-02 02:00:23,441 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.2694 (0.3596) Acc D Real: 71.851% 
Loss D Fake: 0.6417 (0.6851) Acc D Fake: 77.826% 
Loss D: 0.911 
Loss G: 0.7587 (0.7074) Acc G: 22.124% 
LR: 2.000e-04 

2023-03-02 02:00:23,448 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.2970 (0.3590) Acc D Real: 71.876% 
Loss D Fake: 0.6387 (0.6846) Acc D Fake: 77.814% 
Loss D: 0.936 
Loss G: 0.7624 (0.7080) Acc G: 22.137% 
LR: 2.000e-04 

2023-03-02 02:00:23,457 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3104 (0.3584) Acc D Real: 71.869% 
Loss D Fake: 0.6356 (0.6841) Acc D Fake: 77.801% 
Loss D: 0.946 
Loss G: 0.7661 (0.7086) Acc G: 22.150% 
LR: 2.000e-04 

2023-03-02 02:00:23,464 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3273 (0.3581) Acc D Real: 71.865% 
Loss D Fake: 0.6327 (0.6835) Acc D Fake: 77.789% 
Loss D: 0.960 
Loss G: 0.7695 (0.7092) Acc G: 22.162% 
LR: 2.000e-04 

2023-03-02 02:00:23,472 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4643 (0.3592) Acc D Real: 71.673% 
Loss D Fake: 0.6305 (0.6830) Acc D Fake: 77.778% 
Loss D: 1.095 
Loss G: 0.7708 (0.7099) Acc G: 22.174% 
LR: 2.000e-04 

2023-03-02 02:00:23,480 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2920 (0.3585) Acc D Real: 71.691% 
Loss D Fake: 0.6299 (0.6824) Acc D Fake: 77.766% 
Loss D: 0.922 
Loss G: 0.7721 (0.7105) Acc G: 22.186% 
LR: 2.000e-04 

2023-03-02 02:00:23,487 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2495 (0.3574) Acc D Real: 71.801% 
Loss D Fake: 0.6283 (0.6819) Acc D Fake: 77.755% 
Loss D: 0.878 
Loss G: 0.7748 (0.7112) Acc G: 22.198% 
LR: 2.000e-04 

2023-03-02 02:00:23,494 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3244 (0.3571) Acc D Real: 71.802% 
Loss D Fake: 0.6259 (0.6813) Acc D Fake: 77.744% 
Loss D: 0.950 
Loss G: 0.7776 (0.7118) Acc G: 22.210% 
LR: 2.000e-04 

2023-03-02 02:00:23,502 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3542 (0.3570) Acc D Real: 71.770% 
Loss D Fake: 0.6239 (0.6807) Acc D Fake: 77.733% 
Loss D: 0.978 
Loss G: 0.7798 (0.7125) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 02:00:23,510 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2983 (0.3565) Acc D Real: 71.796% 
Loss D Fake: 0.6223 (0.6802) Acc D Fake: 77.723% 
Loss D: 0.921 
Loss G: 0.7821 (0.7132) Acc G: 22.232% 
LR: 2.000e-04 

2023-03-02 02:00:23,517 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2789 (0.3557) Acc D Real: 71.835% 
Loss D Fake: 0.6203 (0.6796) Acc D Fake: 77.712% 
Loss D: 0.899 
Loss G: 0.7850 (0.7139) Acc G: 22.243% 
LR: 2.000e-04 

2023-03-02 02:00:23,524 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3219 (0.3554) Acc D Real: 71.831% 
Loss D Fake: 0.6178 (0.6790) Acc D Fake: 77.702% 
Loss D: 0.940 
Loss G: 0.7881 (0.7146) Acc G: 22.237% 
LR: 2.000e-04 

2023-03-02 02:00:23,532 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2185 (0.3541) Acc D Real: 71.961% 
Loss D Fake: 0.6154 (0.6784) Acc D Fake: 77.708% 
Loss D: 0.834 
Loss G: 0.7912 (0.7154) Acc G: 22.232% 
LR: 2.000e-04 

2023-03-02 02:00:23,539 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2861 (0.3534) Acc D Real: 72.004% 
Loss D Fake: 0.6130 (0.6777) Acc D Fake: 77.714% 
Loss D: 0.899 
Loss G: 0.7947 (0.7161) Acc G: 22.226% 
LR: 2.000e-04 

2023-03-02 02:00:23,547 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2775 (0.3527) Acc D Real: 72.036% 
Loss D Fake: 0.6101 (0.6771) Acc D Fake: 77.720% 
Loss D: 0.888 
Loss G: 0.7987 (0.7169) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 02:00:23,555 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3063 (0.3523) Acc D Real: 72.065% 
Loss D Fake: 0.6071 (0.6764) Acc D Fake: 77.726% 
Loss D: 0.913 
Loss G: 0.8024 (0.7177) Acc G: 22.216% 
LR: 2.000e-04 

2023-03-02 02:00:23,564 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3706 (0.3524) Acc D Real: 72.003% 
Loss D Fake: 0.6047 (0.6758) Acc D Fake: 77.731% 
Loss D: 0.975 
Loss G: 0.8048 (0.7185) Acc G: 22.211% 
LR: 2.000e-04 

2023-03-02 02:00:23,572 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2026 (0.3511) Acc D Real: 72.136% 
Loss D Fake: 0.6029 (0.6751) Acc D Fake: 77.737% 
Loss D: 0.805 
Loss G: 0.8084 (0.7193) Acc G: 22.206% 
LR: 2.000e-04 

2023-03-02 02:00:23,579 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2499 (0.3501) Acc D Real: 72.216% 
Loss D Fake: 0.5998 (0.6744) Acc D Fake: 77.742% 
Loss D: 0.850 
Loss G: 0.8131 (0.7202) Acc G: 22.201% 
LR: 2.000e-04 

2023-03-02 02:00:23,587 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.1987 (0.3488) Acc D Real: 72.345% 
Loss D Fake: 0.5957 (0.6737) Acc D Fake: 77.748% 
Loss D: 0.794 
Loss G: 0.8195 (0.7211) Acc G: 22.196% 
LR: 2.000e-04 

2023-03-02 02:00:23,594 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3066 (0.3484) Acc D Real: 72.350% 
Loss D Fake: 0.5910 (0.6730) Acc D Fake: 77.753% 
Loss D: 0.898 
Loss G: 0.8255 (0.7220) Acc G: 22.191% 
LR: 2.000e-04 

2023-03-02 02:00:23,602 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2349 (0.3474) Acc D Real: 72.457% 
Loss D Fake: 0.5867 (0.6722) Acc D Fake: 77.758% 
Loss D: 0.822 
Loss G: 0.8317 (0.7230) Acc G: 22.187% 
LR: 2.000e-04 

2023-03-02 02:00:23,609 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2524 (0.3466) Acc D Real: 72.535% 
Loss D Fake: 0.5821 (0.6714) Acc D Fake: 77.763% 
Loss D: 0.835 
Loss G: 0.8384 (0.7240) Acc G: 22.182% 
LR: 2.000e-04 

2023-03-02 02:00:23,617 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3239 (0.3464) Acc D Real: 72.550% 
Loss D Fake: 0.5778 (0.6706) Acc D Fake: 77.768% 
Loss D: 0.902 
Loss G: 0.8438 (0.7250) Acc G: 22.178% 
LR: 2.000e-04 

2023-03-02 02:00:23,624 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2865 (0.3458) Acc D Real: 72.607% 
Loss D Fake: 0.5761 (0.6698) Acc D Fake: 77.773% 
Loss D: 0.863 
Loss G: 0.8415 (0.7260) Acc G: 22.173% 
LR: 2.000e-04 

2023-03-02 02:00:23,632 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2228 (0.3448) Acc D Real: 72.725% 
Loss D Fake: 0.5804 (0.6690) Acc D Fake: 77.764% 
Loss D: 0.803 
Loss G: 0.8352 (0.7270) Acc G: 22.183% 
LR: 2.000e-04 

2023-03-02 02:00:23,639 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.2017 (0.3436) Acc D Real: 72.835% 
Loss D Fake: 0.5868 (0.6683) Acc D Fake: 77.754% 
Loss D: 0.788 
Loss G: 0.8270 (0.7278) Acc G: 22.193% 
LR: 2.000e-04 

2023-03-02 02:00:23,647 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2817 (0.3431) Acc D Real: 72.897% 
Loss D Fake: 0.5968 (0.6677) Acc D Fake: 77.745% 
Loss D: 0.879 
Loss G: 0.8212 (0.7286) Acc G: 22.202% 
LR: 2.000e-04 

2023-03-02 02:00:23,655 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2551 (0.3423) Acc D Real: 72.962% 
Loss D Fake: 0.5977 (0.6672) Acc D Fake: 77.736% 
Loss D: 0.853 
Loss G: 0.8405 (0.7295) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 02:00:23,663 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3274 (0.3422) Acc D Real: 72.955% 
Loss D Fake: 0.5742 (0.6664) Acc D Fake: 77.727% 
Loss D: 0.902 
Loss G: 0.8618 (0.7306) Acc G: 22.221% 
LR: 2.000e-04 

2023-03-02 02:00:23,670 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3296 (0.3421) Acc D Real: 72.947% 
Loss D Fake: 0.5607 (0.6655) Acc D Fake: 77.719% 
Loss D: 0.890 
Loss G: 0.8778 (0.7318) Acc G: 22.217% 
LR: 2.000e-04 

2023-03-02 02:00:23,678 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3067 (0.3418) Acc D Real: 72.967% 
Loss D Fake: 0.5509 (0.6646) Acc D Fake: 77.724% 
Loss D: 0.858 
Loss G: 0.8899 (0.7331) Acc G: 22.212% 
LR: 2.000e-04 

2023-03-02 02:00:23,685 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2732 (0.3413) Acc D Real: 73.025% 
Loss D Fake: 0.5436 (0.6636) Acc D Fake: 77.728% 
Loss D: 0.817 
Loss G: 0.9001 (0.7345) Acc G: 22.208% 
LR: 2.000e-04 

2023-03-02 02:00:23,693 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2804 (0.3408) Acc D Real: 73.081% 
Loss D Fake: 0.5375 (0.6626) Acc D Fake: 77.733% 
Loss D: 0.818 
Loss G: 0.9089 (0.7359) Acc G: 22.203% 
LR: 2.000e-04 

2023-03-02 02:00:23,700 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3904 (0.3412) Acc D Real: 73.037% 
Loss D Fake: 0.5332 (0.6616) Acc D Fake: 77.738% 
Loss D: 0.924 
Loss G: 0.9129 (0.7373) Acc G: 22.199% 
LR: 2.000e-04 

2023-03-02 02:00:23,708 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2339 (0.3403) Acc D Real: 73.130% 
Loss D Fake: 0.5320 (0.6606) Acc D Fake: 77.743% 
Loss D: 0.766 
Loss G: 0.9148 (0.7387) Acc G: 22.195% 
LR: 2.000e-04 

2023-03-02 02:00:23,716 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3385 (0.3403) Acc D Real: 73.138% 
Loss D Fake: 0.5318 (0.6595) Acc D Fake: 77.747% 
Loss D: 0.870 
Loss G: 0.9147 (0.7400) Acc G: 22.191% 
LR: 2.000e-04 

2023-03-02 02:00:23,723 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2255 (0.3394) Acc D Real: 73.234% 
Loss D Fake: 0.5325 (0.6586) Acc D Fake: 77.739% 
Loss D: 0.758 
Loss G: 0.9154 (0.7414) Acc G: 22.200% 
LR: 2.000e-04 

2023-03-02 02:00:23,731 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3162 (0.3392) Acc D Real: 73.255% 
Loss D Fake: 0.5349 (0.6576) Acc D Fake: 77.731% 
Loss D: 0.851 
Loss G: 0.9043 (0.7427) Acc G: 22.208% 
LR: 2.000e-04 

2023-03-02 02:00:23,739 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3109 (0.3390) Acc D Real: 73.290% 
Loss D Fake: 0.8571 (0.6591) Acc D Fake: 77.432% 
Loss D: 1.168 
Loss G: 0.9510 (0.7442) Acc G: 22.204% 
LR: 2.000e-04 

2023-03-02 02:00:23,746 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4288 (0.3397) Acc D Real: 73.224% 
Loss D Fake: 0.4966 (0.6579) Acc D Fake: 77.439% 
Loss D: 0.925 
Loss G: 0.9777 (0.7460) Acc G: 22.188% 
LR: 2.000e-04 

2023-03-02 02:00:23,754 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3182 (0.3395) Acc D Real: 73.258% 
Loss D Fake: 0.4854 (0.6566) Acc D Fake: 77.484% 
Loss D: 0.804 
Loss G: 0.9887 (0.7478) Acc G: 22.133% 
LR: 2.000e-04 

2023-03-02 02:00:23,762 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3601 (0.3397) Acc D Real: 73.258% 
Loss D Fake: 0.4797 (0.6553) Acc D Fake: 77.540% 
Loss D: 0.840 
Loss G: 0.9952 (0.7497) Acc G: 22.068% 
LR: 2.000e-04 

2023-03-02 02:00:23,769 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.5023 (0.3409) Acc D Real: 73.136% 
Loss D Fake: 0.4759 (0.6540) Acc D Fake: 77.607% 
Loss D: 0.978 
Loss G: 0.9994 (0.7515) Acc G: 21.991% 
LR: 2.000e-04 

2023-03-02 02:00:23,777 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4477 (0.3417) Acc D Real: 73.059% 
Loss D Fake: 0.4735 (0.6526) Acc D Fake: 77.686% 
Loss D: 0.921 
Loss G: 1.0025 (0.7534) Acc G: 21.915% 
LR: 2.000e-04 

2023-03-02 02:00:23,784 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3438 (0.3417) Acc D Real: 73.065% 
Loss D Fake: 0.4715 (0.6513) Acc D Fake: 77.764% 
Loss D: 0.815 
Loss G: 1.0056 (0.7552) Acc G: 21.828% 
LR: 2.000e-04 

2023-03-02 02:00:23,791 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4271 (0.3423) Acc D Real: 73.012% 
Loss D Fake: 0.4698 (0.6500) Acc D Fake: 77.853% 
Loss D: 0.897 
Loss G: 1.0074 (0.7570) Acc G: 21.742% 
LR: 2.000e-04 

2023-03-02 02:00:23,799 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.5115 (0.3435) Acc D Real: 72.892% 
Loss D Fake: 0.4693 (0.6487) Acc D Fake: 77.940% 
Loss D: 0.981 
Loss G: 1.0064 (0.7588) Acc G: 21.658% 
LR: 2.000e-04 

2023-03-02 02:00:23,806 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.5403 (0.3449) Acc D Real: 72.756% 
Loss D Fake: 0.4705 (0.6474) Acc D Fake: 78.026% 
Loss D: 1.011 
Loss G: 1.0025 (0.7606) Acc G: 21.574% 
LR: 2.000e-04 

2023-03-02 02:00:23,814 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.4024 (0.3454) Acc D Real: 72.727% 
Loss D Fake: 0.4728 (0.6462) Acc D Fake: 78.111% 
Loss D: 0.875 
Loss G: 0.9976 (0.7623) Acc G: 21.492% 
LR: 2.000e-04 

2023-03-02 02:00:23,821 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.5580 (0.3468) Acc D Real: 72.576% 
Loss D Fake: 0.4756 (0.6450) Acc D Fake: 78.195% 
Loss D: 1.034 
Loss G: 0.9911 (0.7639) Acc G: 21.411% 
LR: 2.000e-04 

2023-03-02 02:00:23,829 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3920 (0.3472) Acc D Real: 72.546% 
Loss D Fake: 0.4793 (0.6438) Acc D Fake: 78.277% 
Loss D: 0.871 
Loss G: 0.9845 (0.7654) Acc G: 21.332% 
LR: 2.000e-04 

2023-03-02 02:00:23,836 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.5498 (0.3486) Acc D Real: 72.397% 
Loss D Fake: 0.4830 (0.6427) Acc D Fake: 78.359% 
Loss D: 1.033 
Loss G: 0.9768 (0.7669) Acc G: 21.253% 
LR: 2.000e-04 

2023-03-02 02:00:23,843 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4627 (0.3494) Acc D Real: 72.306% 
Loss D Fake: 0.4875 (0.6416) Acc D Fake: 78.439% 
Loss D: 0.950 
Loss G: 0.9684 (0.7683) Acc G: 21.175% 
LR: 2.000e-04 

2023-03-02 02:00:23,851 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.5712 (0.3509) Acc D Real: 72.136% 
Loss D Fake: 0.4924 (0.6406) Acc D Fake: 78.518% 
Loss D: 1.064 
Loss G: 0.9586 (0.7696) Acc G: 21.099% 
LR: 2.000e-04 

2023-03-02 02:00:23,858 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.5915 (0.3525) Acc D Real: 71.961% 
Loss D Fake: 0.4984 (0.6396) Acc D Fake: 78.596% 
Loss D: 1.090 
Loss G: 0.9474 (0.7708) Acc G: 21.023% 
LR: 2.000e-04 

2023-03-02 02:00:23,866 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4009 (0.3528) Acc D Real: 71.915% 
Loss D Fake: 0.5049 (0.6387) Acc D Fake: 78.673% 
Loss D: 0.906 
Loss G: 0.9370 (0.7719) Acc G: 20.949% 
LR: 2.000e-04 

2023-03-02 02:00:23,873 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.5088 (0.3539) Acc D Real: 71.796% 
Loss D Fake: 0.5110 (0.6379) Acc D Fake: 78.749% 
Loss D: 1.020 
Loss G: 0.9267 (0.7729) Acc G: 20.875% 
LR: 2.000e-04 

2023-03-02 02:00:23,881 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.4425 (0.3545) Acc D Real: 71.719% 
Loss D Fake: 0.5172 (0.6371) Acc D Fake: 78.824% 
Loss D: 0.960 
Loss G: 0.9166 (0.7739) Acc G: 20.803% 
LR: 2.000e-04 

2023-03-02 02:00:23,888 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4144 (0.3549) Acc D Real: 71.656% 
Loss D Fake: 0.5233 (0.6363) Acc D Fake: 78.898% 
Loss D: 0.938 
Loss G: 0.9072 (0.7748) Acc G: 20.731% 
LR: 2.000e-04 

2023-03-02 02:00:23,895 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4381 (0.3554) Acc D Real: 71.582% 
Loss D Fake: 0.5290 (0.6356) Acc D Fake: 78.971% 
Loss D: 0.967 
Loss G: 0.8984 (0.7756) Acc G: 20.661% 
LR: 2.000e-04 

2023-03-02 02:00:23,903 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.5291 (0.3566) Acc D Real: 71.435% 
Loss D Fake: 0.5348 (0.6350) Acc D Fake: 79.043% 
Loss D: 1.064 
Loss G: 0.8890 (0.7763) Acc G: 20.591% 
LR: 2.000e-04 

2023-03-02 02:00:23,910 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.5269 (0.3577) Acc D Real: 71.285% 
Loss D Fake: 0.5411 (0.6343) Acc D Fake: 79.115% 
Loss D: 1.068 
Loss G: 0.8788 (0.7770) Acc G: 20.533% 
LR: 2.000e-04 

2023-03-02 02:00:23,918 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4486 (0.3583) Acc D Real: 71.200% 
Loss D Fake: 0.5480 (0.6338) Acc D Fake: 79.174% 
Loss D: 0.997 
Loss G: 0.8689 (0.7776) Acc G: 20.476% 
LR: 2.000e-04 

2023-03-02 02:00:23,925 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4195 (0.3586) Acc D Real: 71.136% 
Loss D Fake: 0.5544 (0.6333) Acc D Fake: 79.233% 
Loss D: 0.974 
Loss G: 0.8599 (0.7781) Acc G: 20.419% 
LR: 2.000e-04 

2023-03-02 02:00:23,933 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3093 (0.3583) Acc D Real: 71.142% 
Loss D Fake: 0.5601 (0.6328) Acc D Fake: 79.291% 
Loss D: 0.869 
Loss G: 0.8527 (0.7786) Acc G: 20.364% 
LR: 2.000e-04 

2023-03-02 02:00:23,940 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5689 (0.3597) Acc D Real: 71.126% 
Loss D Fake: 0.5651 (0.6324) Acc D Fake: 79.296% 
Loss D: 1.134 
Loss G: 0.8448 (0.7790) Acc G: 20.358% 
LR: 2.000e-04 

2023-03-02 02:00:24,148 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.360 | Generator Loss: 0.845 | Avg: 2.205 
2023-03-02 02:00:24,169 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.257 | Generator Loss: 0.845 | Avg: 2.102 
2023-03-02 02:00:24,192 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.289 | Generator Loss: 0.845 | Avg: 2.133 
2023-03-02 02:00:24,218 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.308 | Generator Loss: 0.845 | Avg: 2.152 
2023-03-02 02:00:24,243 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.313 | Generator Loss: 0.845 | Avg: 2.158 
2023-03-02 02:00:24,269 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.256 | Generator Loss: 0.845 | Avg: 2.100 
2023-03-02 02:00:24,295 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.228 | Generator Loss: 0.845 | Avg: 2.073 
2023-03-02 02:00:24,320 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.177 | Generator Loss: 0.845 | Avg: 2.022 
2023-03-02 02:00:24,346 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.149 | Generator Loss: 0.845 | Avg: 1.994 
2023-03-02 02:00:24,372 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.100 | Generator Loss: 0.845 | Avg: 1.944 
2023-03-02 02:00:24,397 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.062 | Generator Loss: 0.845 | Avg: 1.907 
2023-03-02 02:00:24,426 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.026 | Generator Loss: 0.845 | Avg: 1.870 
2023-03-02 02:00:24,452 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.997 | Generator Loss: 0.845 | Avg: 1.842 
2023-03-02 02:00:24,479 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.976 | Generator Loss: 0.845 | Avg: 1.820 
2023-03-02 02:00:24,506 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.995 | Generator Loss: 0.845 | Avg: 1.839 
2023-03-02 02:00:24,534 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.014 | Generator Loss: 0.845 | Avg: 1.859 
2023-03-02 02:00:24,559 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.031 | Generator Loss: 0.845 | Avg: 1.876 
2023-03-02 02:00:24,596 -                train: [    INFO] - 
Epoch: 12/20
2023-03-02 02:00:24,792 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4105 (0.4463) Acc D Real: 56.536% 
Loss D Fake: 0.5770 (0.5740) Acc D Fake: 88.333% 
Loss D: 0.987 
Loss G: 0.8286 (0.8325) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,799 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4534 (0.4486) Acc D Real: 56.458% 
Loss D Fake: 0.5824 (0.5768) Acc D Fake: 88.333% 
Loss D: 1.036 
Loss G: 0.8214 (0.8288) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,808 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4921 (0.4595) Acc D Real: 54.622% 
Loss D Fake: 0.5879 (0.5796) Acc D Fake: 88.333% 
Loss D: 1.080 
Loss G: 0.8139 (0.8251) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,825 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4468 (0.4570) Acc D Real: 54.781% 
Loss D Fake: 0.5936 (0.5824) Acc D Fake: 88.333% 
Loss D: 1.040 
Loss G: 0.8065 (0.8214) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,832 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3283 (0.4355) Acc D Real: 57.274% 
Loss D Fake: 0.5988 (0.5851) Acc D Fake: 88.333% 
Loss D: 0.927 
Loss G: 0.8007 (0.8179) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,839 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.4122 (0.4322) Acc D Real: 57.307% 
Loss D Fake: 0.6030 (0.5877) Acc D Fake: 88.333% 
Loss D: 1.015 
Loss G: 0.7955 (0.8147) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,846 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3583 (0.4230) Acc D Real: 58.118% 
Loss D Fake: 0.6069 (0.5901) Acc D Fake: 88.333% 
Loss D: 0.965 
Loss G: 0.7911 (0.8118) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,854 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4300 (0.4237) Acc D Real: 57.946% 
Loss D Fake: 0.6103 (0.5923) Acc D Fake: 88.333% 
Loss D: 1.040 
Loss G: 0.7868 (0.8090) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,861 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4077 (0.4221) Acc D Real: 58.099% 
Loss D Fake: 0.6137 (0.5945) Acc D Fake: 88.333% 
Loss D: 1.021 
Loss G: 0.7826 (0.8064) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,868 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3814 (0.4184) Acc D Real: 58.362% 
Loss D Fake: 0.6169 (0.5965) Acc D Fake: 88.333% 
Loss D: 0.998 
Loss G: 0.7789 (0.8039) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,875 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4690 (0.4226) Acc D Real: 57.643% 
Loss D Fake: 0.6201 (0.5985) Acc D Fake: 88.333% 
Loss D: 1.089 
Loss G: 0.7747 (0.8014) Acc G: 11.667% 
LR: 2.000e-04 

2023-03-02 02:00:24,882 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4085 (0.4216) Acc D Real: 57.736% 
Loss D Fake: 0.6236 (0.6004) Acc D Fake: 88.217% 
Loss D: 1.032 
Loss G: 0.7706 (0.7990) Acc G: 11.795% 
LR: 2.000e-04 

2023-03-02 02:00:24,890 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4828 (0.4259) Acc D Real: 57.083% 
Loss D Fake: 0.6271 (0.6023) Acc D Fake: 88.103% 
Loss D: 1.110 
Loss G: 0.7659 (0.7967) Acc G: 12.020% 
LR: 2.000e-04 

2023-03-02 02:00:24,897 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3737 (0.4225) Acc D Real: 57.479% 
Loss D Fake: 0.6311 (0.6042) Acc D Fake: 87.896% 
Loss D: 1.005 
Loss G: 0.7615 (0.7943) Acc G: 12.219% 
LR: 2.000e-04 

2023-03-02 02:00:24,904 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3318 (0.4168) Acc D Real: 58.242% 
Loss D Fake: 0.6344 (0.6061) Acc D Fake: 87.715% 
Loss D: 0.966 
Loss G: 0.7582 (0.7921) Acc G: 12.393% 
LR: 2.000e-04 

2023-03-02 02:00:24,911 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3319 (0.4118) Acc D Real: 58.802% 
Loss D Fake: 0.6368 (0.6079) Acc D Fake: 87.555% 
Loss D: 0.969 
Loss G: 0.7560 (0.7900) Acc G: 12.546% 
LR: 2.000e-04 

2023-03-02 02:00:24,919 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.4747 (0.4153) Acc D Real: 58.267% 
Loss D Fake: 0.6387 (0.6096) Acc D Fake: 87.413% 
Loss D: 1.113 
Loss G: 0.7532 (0.7879) Acc G: 12.682% 
LR: 2.000e-04 

2023-03-02 02:00:24,926 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3895 (0.4139) Acc D Real: 58.391% 
Loss D Fake: 0.6413 (0.6113) Acc D Fake: 87.286% 
Loss D: 1.031 
Loss G: 0.7503 (0.7859) Acc G: 12.892% 
LR: 2.000e-04 

2023-03-02 02:00:24,933 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3481 (0.4106) Acc D Real: 58.685% 
Loss D Fake: 0.6436 (0.6129) Acc D Fake: 87.089% 
Loss D: 0.992 
Loss G: 0.7479 (0.7840) Acc G: 13.081% 
LR: 2.000e-04 

2023-03-02 02:00:24,940 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3263 (0.4066) Acc D Real: 59.149% 
Loss D Fake: 0.6454 (0.6145) Acc D Fake: 86.910% 
Loss D: 0.972 
Loss G: 0.7465 (0.7822) Acc G: 13.251% 
LR: 2.000e-04 

2023-03-02 02:00:24,947 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3546 (0.4043) Acc D Real: 59.384% 
Loss D Fake: 0.6463 (0.6159) Acc D Fake: 86.747% 
Loss D: 1.001 
Loss G: 0.7456 (0.7806) Acc G: 13.407% 
LR: 2.000e-04 

2023-03-02 02:00:24,954 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3616 (0.4024) Acc D Real: 59.667% 
Loss D Fake: 0.6470 (0.6173) Acc D Fake: 86.599% 
Loss D: 1.009 
Loss G: 0.7449 (0.7790) Acc G: 13.548% 
LR: 2.000e-04 

2023-03-02 02:00:24,961 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3974 (0.4022) Acc D Real: 59.664% 
Loss D Fake: 0.6476 (0.6185) Acc D Fake: 86.463% 
Loss D: 1.045 
Loss G: 0.7441 (0.7776) Acc G: 13.678% 
LR: 2.000e-04 

2023-03-02 02:00:24,968 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3275 (0.3992) Acc D Real: 60.048% 
Loss D Fake: 0.6481 (0.6197) Acc D Fake: 86.338% 
Loss D: 0.976 
Loss G: 0.7440 (0.7762) Acc G: 13.798% 
LR: 2.000e-04 

2023-03-02 02:00:24,975 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3902 (0.3989) Acc D Real: 60.260% 
Loss D Fake: 0.6482 (0.6208) Acc D Fake: 86.222% 
Loss D: 1.038 
Loss G: 0.7435 (0.7750) Acc G: 13.908% 
LR: 2.000e-04 

2023-03-02 02:00:24,982 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3943 (0.3987) Acc D Real: 60.258% 
Loss D Fake: 0.6488 (0.6218) Acc D Fake: 86.115% 
Loss D: 1.043 
Loss G: 0.7428 (0.7738) Acc G: 14.010% 
LR: 2.000e-04 

2023-03-02 02:00:24,991 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4516 (0.4006) Acc D Real: 59.961% 
Loss D Fake: 0.6497 (0.6228) Acc D Fake: 86.016% 
Loss D: 1.101 
Loss G: 0.7412 (0.7726) Acc G: 14.105% 
LR: 2.000e-04 

2023-03-02 02:00:24,999 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3482 (0.3988) Acc D Real: 60.167% 
Loss D Fake: 0.6512 (0.6238) Acc D Fake: 85.923% 
Loss D: 0.999 
Loss G: 0.7399 (0.7715) Acc G: 14.194% 
LR: 2.000e-04 

2023-03-02 02:00:25,008 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3816 (0.3982) Acc D Real: 60.299% 
Loss D Fake: 0.6523 (0.6248) Acc D Fake: 85.837% 
Loss D: 1.034 
Loss G: 0.7387 (0.7704) Acc G: 14.276% 
LR: 2.000e-04 

2023-03-02 02:00:25,015 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4835 (0.4010) Acc D Real: 59.976% 
Loss D Fake: 0.6537 (0.6257) Acc D Fake: 85.756% 
Loss D: 1.137 
Loss G: 0.7364 (0.7693) Acc G: 14.353% 
LR: 2.000e-04 

2023-03-02 02:00:25,025 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3658 (0.3999) Acc D Real: 60.133% 
Loss D Fake: 0.6559 (0.6266) Acc D Fake: 85.680% 
Loss D: 1.022 
Loss G: 0.7342 (0.7682) Acc G: 14.425% 
LR: 2.000e-04 

2023-03-02 02:00:25,032 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3851 (0.3994) Acc D Real: 60.180% 
Loss D Fake: 0.6577 (0.6276) Acc D Fake: 85.609% 
Loss D: 1.043 
Loss G: 0.7323 (0.7671) Acc G: 14.493% 
LR: 2.000e-04 

2023-03-02 02:00:25,040 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.2757 (0.3958) Acc D Real: 60.625% 
Loss D Fake: 0.6590 (0.6285) Acc D Fake: 85.542% 
Loss D: 0.935 
Loss G: 0.7315 (0.7661) Acc G: 14.557% 
LR: 2.000e-04 

2023-03-02 02:00:25,048 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2599 (0.3919) Acc D Real: 61.058% 
Loss D Fake: 0.6592 (0.6294) Acc D Fake: 85.479% 
Loss D: 0.919 
Loss G: 0.7320 (0.7651) Acc G: 14.618% 
LR: 2.000e-04 

2023-03-02 02:00:25,055 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2847 (0.3889) Acc D Real: 61.413% 
Loss D Fake: 0.6582 (0.6302) Acc D Fake: 85.420% 
Loss D: 0.943 
Loss G: 0.7336 (0.7642) Acc G: 14.674% 
LR: 2.000e-04 

2023-03-02 02:00:25,063 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3032 (0.3866) Acc D Real: 61.653% 
Loss D Fake: 0.6565 (0.6309) Acc D Fake: 85.363% 
Loss D: 0.960 
Loss G: 0.7357 (0.7634) Acc G: 14.728% 
LR: 2.000e-04 

2023-03-02 02:00:25,071 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4903 (0.3893) Acc D Real: 61.368% 
Loss D Fake: 0.6550 (0.6315) Acc D Fake: 85.311% 
Loss D: 1.145 
Loss G: 0.7364 (0.7627) Acc G: 14.777% 
LR: 2.000e-04 

2023-03-02 02:00:25,079 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.5113 (0.3925) Acc D Real: 61.038% 
Loss D Fake: 0.6552 (0.6321) Acc D Fake: 85.274% 
Loss D: 1.167 
Loss G: 0.7352 (0.7620) Acc G: 14.825% 
LR: 2.000e-04 

2023-03-02 02:00:25,086 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3285 (0.3909) Acc D Real: 61.255% 
Loss D Fake: 0.6564 (0.6327) Acc D Fake: 85.232% 
Loss D: 0.985 
Loss G: 0.7343 (0.7613) Acc G: 14.867% 
LR: 2.000e-04 

2023-03-02 02:00:25,094 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4846 (0.3931) Acc D Real: 61.005% 
Loss D Fake: 0.6573 (0.6333) Acc D Fake: 85.202% 
Loss D: 1.142 
Loss G: 0.7326 (0.7606) Acc G: 14.910% 
LR: 2.000e-04 

2023-03-02 02:00:25,101 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3055 (0.3911) Acc D Real: 61.298% 
Loss D Fake: 0.6589 (0.6340) Acc D Fake: 85.164% 
Loss D: 0.964 
Loss G: 0.7314 (0.7599) Acc G: 14.952% 
LR: 2.000e-04 

2023-03-02 02:00:25,109 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4792 (0.3931) Acc D Real: 61.042% 
Loss D Fake: 0.6600 (0.6346) Acc D Fake: 85.134% 
Loss D: 1.139 
Loss G: 0.7296 (0.7592) Acc G: 14.992% 
LR: 2.000e-04 

2023-03-02 02:00:25,117 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3780 (0.3928) Acc D Real: 61.123% 
Loss D Fake: 0.6619 (0.6352) Acc D Fake: 85.098% 
Loss D: 1.040 
Loss G: 0.7276 (0.7585) Acc G: 15.030% 
LR: 2.000e-04 

2023-03-02 02:00:25,125 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4094 (0.3931) Acc D Real: 61.078% 
Loss D Fake: 0.6637 (0.6358) Acc D Fake: 85.059% 
Loss D: 1.073 
Loss G: 0.7256 (0.7578) Acc G: 15.066% 
LR: 2.000e-04 

2023-03-02 02:00:25,132 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4383 (0.3941) Acc D Real: 60.982% 
Loss D Fake: 0.6657 (0.6365) Acc D Fake: 85.022% 
Loss D: 1.104 
Loss G: 0.7232 (0.7570) Acc G: 15.101% 
LR: 2.000e-04 

2023-03-02 02:00:25,140 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4508 (0.3953) Acc D Real: 60.995% 
Loss D Fake: 0.6682 (0.6371) Acc D Fake: 84.986% 
Loss D: 1.119 
Loss G: 0.7203 (0.7562) Acc G: 15.134% 
LR: 2.000e-04 

2023-03-02 02:00:25,148 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3738 (0.3949) Acc D Real: 61.125% 
Loss D Fake: 0.6709 (0.6378) Acc D Fake: 84.951% 
Loss D: 1.045 
Loss G: 0.7175 (0.7554) Acc G: 15.166% 
LR: 2.000e-04 

2023-03-02 02:00:25,156 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2758 (0.3924) Acc D Real: 61.425% 
Loss D Fake: 0.6730 (0.6386) Acc D Fake: 84.918% 
Loss D: 0.949 
Loss G: 0.7162 (0.7546) Acc G: 15.197% 
LR: 2.000e-04 

2023-03-02 02:00:25,164 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4206 (0.3930) Acc D Real: 61.400% 
Loss D Fake: 0.6739 (0.6393) Acc D Fake: 84.886% 
Loss D: 1.095 
Loss G: 0.7150 (0.7538) Acc G: 15.226% 
LR: 2.000e-04 

2023-03-02 02:00:25,171 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4362 (0.3938) Acc D Real: 61.307% 
Loss D Fake: 0.6754 (0.6400) Acc D Fake: 84.856% 
Loss D: 1.112 
Loss G: 0.7131 (0.7531) Acc G: 15.254% 
LR: 2.000e-04 

2023-03-02 02:00:25,179 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3877 (0.3937) Acc D Real: 61.356% 
Loss D Fake: 0.6772 (0.6407) Acc D Fake: 84.827% 
Loss D: 1.065 
Loss G: 0.7112 (0.7522) Acc G: 15.281% 
LR: 2.000e-04 

2023-03-02 02:00:25,187 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3534 (0.3930) Acc D Real: 61.454% 
Loss D Fake: 0.6789 (0.6414) Acc D Fake: 84.799% 
Loss D: 1.032 
Loss G: 0.7097 (0.7514) Acc G: 15.308% 
LR: 2.000e-04 

2023-03-02 02:00:25,194 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2670 (0.3906) Acc D Real: 61.831% 
Loss D Fake: 0.6799 (0.6421) Acc D Fake: 84.771% 
Loss D: 0.947 
Loss G: 0.7095 (0.7507) Acc G: 15.333% 
LR: 2.000e-04 

2023-03-02 02:00:25,202 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3361 (0.3896) Acc D Real: 61.960% 
Loss D Fake: 0.6796 (0.6428) Acc D Fake: 84.745% 
Loss D: 1.016 
Loss G: 0.7099 (0.7499) Acc G: 15.357% 
LR: 2.000e-04 

2023-03-02 02:00:25,210 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3698 (0.3893) Acc D Real: 62.050% 
Loss D Fake: 0.6791 (0.6435) Acc D Fake: 84.730% 
Loss D: 1.049 
Loss G: 0.7104 (0.7492) Acc G: 15.359% 
LR: 2.000e-04 

2023-03-02 02:00:25,217 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3545 (0.3887) Acc D Real: 62.202% 
Loss D Fake: 0.6787 (0.6441) Acc D Fake: 84.735% 
Loss D: 1.033 
Loss G: 0.7109 (0.7485) Acc G: 15.353% 
LR: 2.000e-04 

2023-03-02 02:00:25,225 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3069 (0.3873) Acc D Real: 62.500% 
Loss D Fake: 0.6780 (0.6447) Acc D Fake: 84.740% 
Loss D: 0.985 
Loss G: 0.7119 (0.7479) Acc G: 15.347% 
LR: 2.000e-04 

2023-03-02 02:00:25,233 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4149 (0.3877) Acc D Real: 62.478% 
Loss D Fake: 0.6770 (0.6452) Acc D Fake: 84.744% 
Loss D: 1.092 
Loss G: 0.7126 (0.7473) Acc G: 15.341% 
LR: 2.000e-04 

2023-03-02 02:00:25,240 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4553 (0.3889) Acc D Real: 62.405% 
Loss D Fake: 0.6768 (0.6457) Acc D Fake: 84.748% 
Loss D: 1.132 
Loss G: 0.7123 (0.7467) Acc G: 15.335% 
LR: 2.000e-04 

2023-03-02 02:00:25,248 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.5110 (0.3909) Acc D Real: 62.217% 
Loss D Fake: 0.6777 (0.6463) Acc D Fake: 84.752% 
Loss D: 1.189 
Loss G: 0.7106 (0.7461) Acc G: 15.330% 
LR: 2.000e-04 

2023-03-02 02:00:25,256 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4550 (0.3919) Acc D Real: 62.130% 
Loss D Fake: 0.6798 (0.6468) Acc D Fake: 84.756% 
Loss D: 1.135 
Loss G: 0.7082 (0.7455) Acc G: 15.324% 
LR: 2.000e-04 

2023-03-02 02:00:25,264 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3747 (0.3916) Acc D Real: 62.193% 
Loss D Fake: 0.6822 (0.6474) Acc D Fake: 84.760% 
Loss D: 1.057 
Loss G: 0.7059 (0.7449) Acc G: 15.319% 
LR: 2.000e-04 

2023-03-02 02:00:25,272 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2941 (0.3901) Acc D Real: 62.393% 
Loss D Fake: 0.6839 (0.6479) Acc D Fake: 84.764% 
Loss D: 0.978 
Loss G: 0.7048 (0.7443) Acc G: 15.314% 
LR: 2.000e-04 

2023-03-02 02:00:25,279 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3941 (0.3902) Acc D Real: 62.467% 
Loss D Fake: 0.6846 (0.6485) Acc D Fake: 84.768% 
Loss D: 1.079 
Loss G: 0.7042 (0.7437) Acc G: 15.309% 
LR: 2.000e-04 

2023-03-02 02:00:25,287 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3238 (0.3892) Acc D Real: 62.608% 
Loss D Fake: 0.6850 (0.6491) Acc D Fake: 84.771% 
Loss D: 1.009 
Loss G: 0.7040 (0.7431) Acc G: 15.305% 
LR: 2.000e-04 

2023-03-02 02:00:25,295 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2675 (0.3873) Acc D Real: 62.833% 
Loss D Fake: 0.6848 (0.6496) Acc D Fake: 84.775% 
Loss D: 0.952 
Loss G: 0.7049 (0.7425) Acc G: 15.300% 
LR: 2.000e-04 

2023-03-02 02:00:25,302 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2904 (0.3859) Acc D Real: 63.044% 
Loss D Fake: 0.6835 (0.6501) Acc D Fake: 84.778% 
Loss D: 0.974 
Loss G: 0.7066 (0.7420) Acc G: 15.296% 
LR: 2.000e-04 

2023-03-02 02:00:25,310 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3739 (0.3857) Acc D Real: 63.117% 
Loss D Fake: 0.6817 (0.6505) Acc D Fake: 84.781% 
Loss D: 1.056 
Loss G: 0.7082 (0.7415) Acc G: 15.291% 
LR: 2.000e-04 

2023-03-02 02:00:25,318 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3442 (0.3852) Acc D Real: 63.210% 
Loss D Fake: 0.6803 (0.6510) Acc D Fake: 84.784% 
Loss D: 1.025 
Loss G: 0.7096 (0.7410) Acc G: 15.287% 
LR: 2.000e-04 

2023-03-02 02:00:25,325 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3138 (0.3841) Acc D Real: 63.310% 
Loss D Fake: 0.6788 (0.6514) Acc D Fake: 84.787% 
Loss D: 0.993 
Loss G: 0.7115 (0.7406) Acc G: 15.283% 
LR: 2.000e-04 

2023-03-02 02:00:25,333 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3866 (0.3842) Acc D Real: 63.390% 
Loss D Fake: 0.6771 (0.6517) Acc D Fake: 84.790% 
Loss D: 1.064 
Loss G: 0.7131 (0.7402) Acc G: 15.279% 
LR: 2.000e-04 

2023-03-02 02:00:25,341 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4222 (0.3847) Acc D Real: 63.396% 
Loss D Fake: 0.6759 (0.6520) Acc D Fake: 84.793% 
Loss D: 1.098 
Loss G: 0.7139 (0.7399) Acc G: 15.275% 
LR: 2.000e-04 

2023-03-02 02:00:25,348 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2809 (0.3833) Acc D Real: 63.562% 
Loss D Fake: 0.6750 (0.6524) Acc D Fake: 84.796% 
Loss D: 0.956 
Loss G: 0.7154 (0.7395) Acc G: 15.272% 
LR: 2.000e-04 

2023-03-02 02:00:25,356 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.2997 (0.3822) Acc D Real: 63.740% 
Loss D Fake: 0.6732 (0.6526) Acc D Fake: 84.799% 
Loss D: 0.973 
Loss G: 0.7177 (0.7392) Acc G: 15.268% 
LR: 2.000e-04 

2023-03-02 02:00:25,364 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4154 (0.3826) Acc D Real: 63.736% 
Loss D Fake: 0.6712 (0.6529) Acc D Fake: 84.801% 
Loss D: 1.087 
Loss G: 0.7193 (0.7390) Acc G: 15.265% 
LR: 2.000e-04 

2023-03-02 02:00:25,371 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4167 (0.3831) Acc D Real: 63.712% 
Loss D Fake: 0.6700 (0.6531) Acc D Fake: 84.804% 
Loss D: 1.087 
Loss G: 0.7201 (0.7387) Acc G: 15.261% 
LR: 2.000e-04 

2023-03-02 02:00:25,379 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3064 (0.3821) Acc D Real: 63.831% 
Loss D Fake: 0.6693 (0.6533) Acc D Fake: 84.806% 
Loss D: 0.976 
Loss G: 0.7213 (0.7385) Acc G: 15.258% 
LR: 2.000e-04 

2023-03-02 02:00:25,387 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3672 (0.3819) Acc D Real: 63.846% 
Loss D Fake: 0.6680 (0.6535) Acc D Fake: 84.818% 
Loss D: 1.035 
Loss G: 0.7227 (0.7383) Acc G: 15.233% 
LR: 2.000e-04 

2023-03-02 02:00:25,394 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3147 (0.3811) Acc D Real: 63.910% 
Loss D Fake: 0.6667 (0.6537) Acc D Fake: 84.841% 
Loss D: 0.981 
Loss G: 0.7241 (0.7381) Acc G: 15.210% 
LR: 2.000e-04 

2023-03-02 02:00:25,402 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3159 (0.3802) Acc D Real: 63.972% 
Loss D Fake: 0.6652 (0.6538) Acc D Fake: 84.864% 
Loss D: 0.981 
Loss G: 0.7259 (0.7380) Acc G: 15.186% 
LR: 2.000e-04 

2023-03-02 02:00:25,410 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3795 (0.3802) Acc D Real: 63.960% 
Loss D Fake: 0.6637 (0.6539) Acc D Fake: 84.886% 
Loss D: 1.043 
Loss G: 0.7274 (0.7378) Acc G: 15.164% 
LR: 2.000e-04 

2023-03-02 02:00:25,417 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4597 (0.3812) Acc D Real: 63.826% 
Loss D Fake: 0.6626 (0.6540) Acc D Fake: 84.907% 
Loss D: 1.122 
Loss G: 0.7279 (0.7377) Acc G: 15.142% 
LR: 2.000e-04 

2023-03-02 02:00:25,427 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3816 (0.3812) Acc D Real: 63.835% 
Loss D Fake: 0.6625 (0.6541) Acc D Fake: 84.928% 
Loss D: 1.044 
Loss G: 0.7280 (0.7376) Acc G: 15.120% 
LR: 2.000e-04 

2023-03-02 02:00:25,434 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4275 (0.3817) Acc D Real: 63.758% 
Loss D Fake: 0.6626 (0.6542) Acc D Fake: 84.949% 
Loss D: 1.090 
Loss G: 0.7275 (0.7375) Acc G: 15.099% 
LR: 2.000e-04 

2023-03-02 02:00:25,442 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4812 (0.3829) Acc D Real: 63.621% 
Loss D Fake: 0.6635 (0.6543) Acc D Fake: 84.969% 
Loss D: 1.145 
Loss G: 0.7259 (0.7374) Acc G: 15.079% 
LR: 2.000e-04 

2023-03-02 02:00:25,449 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3624 (0.3827) Acc D Real: 63.640% 
Loss D Fake: 0.6652 (0.6545) Acc D Fake: 84.988% 
Loss D: 1.028 
Loss G: 0.7244 (0.7372) Acc G: 15.059% 
LR: 2.000e-04 

2023-03-02 02:00:25,456 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3381 (0.3822) Acc D Real: 63.712% 
Loss D Fake: 0.6663 (0.6546) Acc D Fake: 85.007% 
Loss D: 1.004 
Loss G: 0.7235 (0.7371) Acc G: 15.039% 
LR: 2.000e-04 

2023-03-02 02:00:25,464 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3331 (0.3816) Acc D Real: 63.787% 
Loss D Fake: 0.6668 (0.6547) Acc D Fake: 85.026% 
Loss D: 1.000 
Loss G: 0.7233 (0.7369) Acc G: 15.020% 
LR: 2.000e-04 

2023-03-02 02:00:25,471 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4297 (0.3821) Acc D Real: 63.700% 
Loss D Fake: 0.6670 (0.6549) Acc D Fake: 85.044% 
Loss D: 1.097 
Loss G: 0.7227 (0.7367) Acc G: 15.001% 
LR: 2.000e-04 

2023-03-02 02:00:25,479 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4502 (0.3829) Acc D Real: 63.611% 
Loss D Fake: 0.6678 (0.6550) Acc D Fake: 85.062% 
Loss D: 1.118 
Loss G: 0.7215 (0.7366) Acc G: 14.983% 
LR: 2.000e-04 

2023-03-02 02:00:25,486 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4427 (0.3835) Acc D Real: 63.528% 
Loss D Fake: 0.6693 (0.6552) Acc D Fake: 85.079% 
Loss D: 1.112 
Loss G: 0.7197 (0.7364) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,494 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3801 (0.3835) Acc D Real: 63.564% 
Loss D Fake: 0.6710 (0.6553) Acc D Fake: 85.078% 
Loss D: 1.051 
Loss G: 0.7180 (0.7362) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,501 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3801 (0.3835) Acc D Real: 63.528% 
Loss D Fake: 0.6725 (0.6555) Acc D Fake: 85.078% 
Loss D: 1.053 
Loss G: 0.7164 (0.7360) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,510 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3689 (0.3833) Acc D Real: 63.595% 
Loss D Fake: 0.6738 (0.6557) Acc D Fake: 85.077% 
Loss D: 1.043 
Loss G: 0.7152 (0.7358) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,518 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4254 (0.3838) Acc D Real: 63.540% 
Loss D Fake: 0.6751 (0.6559) Acc D Fake: 85.076% 
Loss D: 1.100 
Loss G: 0.7137 (0.7355) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,526 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3500 (0.3834) Acc D Real: 63.576% 
Loss D Fake: 0.6764 (0.6561) Acc D Fake: 85.075% 
Loss D: 1.026 
Loss G: 0.7125 (0.7353) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,533 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3492 (0.3831) Acc D Real: 63.610% 
Loss D Fake: 0.6774 (0.6563) Acc D Fake: 85.074% 
Loss D: 1.027 
Loss G: 0.7117 (0.7351) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,541 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2663 (0.3819) Acc D Real: 63.788% 
Loss D Fake: 0.6777 (0.6566) Acc D Fake: 85.074% 
Loss D: 0.944 
Loss G: 0.7120 (0.7348) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,548 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3878 (0.3819) Acc D Real: 63.803% 
Loss D Fake: 0.6772 (0.6568) Acc D Fake: 85.073% 
Loss D: 1.065 
Loss G: 0.7124 (0.7346) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,556 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3143 (0.3813) Acc D Real: 63.884% 
Loss D Fake: 0.6767 (0.6570) Acc D Fake: 85.072% 
Loss D: 0.991 
Loss G: 0.7131 (0.7344) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,564 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3709 (0.3812) Acc D Real: 63.888% 
Loss D Fake: 0.6759 (0.6571) Acc D Fake: 85.071% 
Loss D: 1.047 
Loss G: 0.7139 (0.7342) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,571 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3391 (0.3808) Acc D Real: 63.927% 
Loss D Fake: 0.6753 (0.6573) Acc D Fake: 85.071% 
Loss D: 1.014 
Loss G: 0.7147 (0.7340) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:25,579 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4345 (0.3813) Acc D Real: 63.882% 
Loss D Fake: 0.6746 (0.6575) Acc D Fake: 85.070% 
Loss D: 1.109 
Loss G: 0.7149 (0.7338) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:25,586 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4344 (0.3818) Acc D Real: 63.840% 
Loss D Fake: 0.6748 (0.6577) Acc D Fake: 85.069% 
Loss D: 1.109 
Loss G: 0.7145 (0.7336) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:25,594 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4346 (0.3823) Acc D Real: 63.760% 
Loss D Fake: 0.6755 (0.6578) Acc D Fake: 85.069% 
Loss D: 1.110 
Loss G: 0.7134 (0.7334) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 02:00:25,602 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3211 (0.3817) Acc D Real: 63.908% 
Loss D Fake: 0.6765 (0.6580) Acc D Fake: 85.068% 
Loss D: 0.998 
Loss G: 0.7127 (0.7332) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 02:00:25,610 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4373 (0.3822) Acc D Real: 63.858% 
Loss D Fake: 0.6771 (0.6582) Acc D Fake: 85.068% 
Loss D: 1.114 
Loss G: 0.7118 (0.7330) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 02:00:25,617 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3176 (0.3816) Acc D Real: 63.909% 
Loss D Fake: 0.6778 (0.6584) Acc D Fake: 85.066% 
Loss D: 0.995 
Loss G: 0.7114 (0.7328) Acc G: 14.970% 
LR: 2.000e-04 

2023-03-02 02:00:25,625 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4139 (0.3819) Acc D Real: 63.854% 
Loss D Fake: 0.6782 (0.6585) Acc D Fake: 85.051% 
Loss D: 1.092 
Loss G: 0.7108 (0.7326) Acc G: 14.983% 
LR: 2.000e-04 

2023-03-02 02:00:25,632 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3284 (0.3814) Acc D Real: 63.921% 
Loss D Fake: 0.6787 (0.6587) Acc D Fake: 85.035% 
Loss D: 1.007 
Loss G: 0.7106 (0.7324) Acc G: 14.998% 
LR: 2.000e-04 

2023-03-02 02:00:25,640 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2728 (0.3805) Acc D Real: 64.023% 
Loss D Fake: 0.6784 (0.6589) Acc D Fake: 85.020% 
Loss D: 0.951 
Loss G: 0.7115 (0.7323) Acc G: 15.013% 
LR: 2.000e-04 

2023-03-02 02:00:25,647 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3628 (0.3803) Acc D Real: 64.059% 
Loss D Fake: 0.6773 (0.6591) Acc D Fake: 85.005% 
Loss D: 1.040 
Loss G: 0.7127 (0.7321) Acc G: 15.028% 
LR: 2.000e-04 

2023-03-02 02:00:25,655 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3121 (0.3797) Acc D Real: 64.108% 
Loss D Fake: 0.6761 (0.6592) Acc D Fake: 84.990% 
Loss D: 0.988 
Loss G: 0.7142 (0.7319) Acc G: 15.042% 
LR: 2.000e-04 

2023-03-02 02:00:25,662 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3874 (0.3798) Acc D Real: 64.114% 
Loss D Fake: 0.6746 (0.6593) Acc D Fake: 84.979% 
Loss D: 1.062 
Loss G: 0.7156 (0.7318) Acc G: 15.042% 
LR: 2.000e-04 

2023-03-02 02:00:25,670 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3014 (0.3791) Acc D Real: 64.165% 
Loss D Fake: 0.6732 (0.6595) Acc D Fake: 84.979% 
Loss D: 0.975 
Loss G: 0.7174 (0.7317) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 02:00:25,677 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3127 (0.3785) Acc D Real: 64.224% 
Loss D Fake: 0.6713 (0.6596) Acc D Fake: 84.980% 
Loss D: 0.984 
Loss G: 0.7196 (0.7316) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 02:00:25,685 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3995 (0.3787) Acc D Real: 64.187% 
Loss D Fake: 0.6693 (0.6596) Acc D Fake: 84.980% 
Loss D: 1.069 
Loss G: 0.7213 (0.7315) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 02:00:25,692 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2326 (0.3775) Acc D Real: 64.298% 
Loss D Fake: 0.6676 (0.6597) Acc D Fake: 84.980% 
Loss D: 0.900 
Loss G: 0.7239 (0.7314) Acc G: 15.031% 
LR: 2.000e-04 

2023-03-02 02:00:25,701 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3622 (0.3774) Acc D Real: 64.322% 
Loss D Fake: 0.6651 (0.6598) Acc D Fake: 84.983% 
Loss D: 1.027 
Loss G: 0.7264 (0.7314) Acc G: 15.017% 
LR: 2.000e-04 

2023-03-02 02:00:25,709 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3436 (0.3771) Acc D Real: 64.321% 
Loss D Fake: 0.6629 (0.6598) Acc D Fake: 84.993% 
Loss D: 1.006 
Loss G: 0.7286 (0.7313) Acc G: 15.003% 
LR: 2.000e-04 

2023-03-02 02:00:25,716 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4861 (0.3780) Acc D Real: 64.195% 
Loss D Fake: 0.6613 (0.6598) Acc D Fake: 85.006% 
Loss D: 1.147 
Loss G: 0.7295 (0.7313) Acc G: 14.990% 
LR: 2.000e-04 

2023-03-02 02:00:25,725 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3657 (0.3779) Acc D Real: 64.210% 
Loss D Fake: 0.6609 (0.6598) Acc D Fake: 85.015% 
Loss D: 1.027 
Loss G: 0.7300 (0.7313) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 02:00:25,733 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3643 (0.3778) Acc D Real: 64.191% 
Loss D Fake: 0.6604 (0.6598) Acc D Fake: 85.018% 
Loss D: 1.025 
Loss G: 0.7304 (0.7313) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,741 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3668 (0.3777) Acc D Real: 64.197% 
Loss D Fake: 0.6601 (0.6598) Acc D Fake: 85.019% 
Loss D: 1.027 
Loss G: 0.7309 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 02:00:25,749 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4190 (0.3780) Acc D Real: 64.156% 
Loss D Fake: 0.6598 (0.6598) Acc D Fake: 85.019% 
Loss D: 1.079 
Loss G: 0.7307 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 02:00:25,757 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.5344 (0.3792) Acc D Real: 63.978% 
Loss D Fake: 0.6605 (0.6598) Acc D Fake: 85.018% 
Loss D: 1.195 
Loss G: 0.7292 (0.7313) Acc G: 14.961% 
LR: 2.000e-04 

2023-03-02 02:00:25,764 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2961 (0.3786) Acc D Real: 64.049% 
Loss D Fake: 0.6620 (0.6598) Acc D Fake: 85.018% 
Loss D: 0.958 
Loss G: 0.7282 (0.7313) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 02:00:25,772 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4506 (0.3791) Acc D Real: 63.973% 
Loss D Fake: 0.6629 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.113 
Loss G: 0.7268 (0.7312) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 02:00:25,779 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2554 (0.3782) Acc D Real: 64.062% 
Loss D Fake: 0.6639 (0.6599) Acc D Fake: 85.018% 
Loss D: 0.919 
Loss G: 0.7265 (0.7312) Acc G: 14.962% 
LR: 2.000e-04 

2023-03-02 02:00:25,787 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3622 (0.3781) Acc D Real: 64.061% 
Loss D Fake: 0.6638 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.026 
Loss G: 0.7267 (0.7312) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 02:00:25,794 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4141 (0.3783) Acc D Real: 64.009% 
Loss D Fake: 0.6637 (0.6599) Acc D Fake: 85.018% 
Loss D: 1.078 
Loss G: 0.7264 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 02:00:25,802 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.2540 (0.3774) Acc D Real: 64.103% 
Loss D Fake: 0.6637 (0.6600) Acc D Fake: 85.018% 
Loss D: 0.918 
Loss G: 0.7271 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 02:00:25,810 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3886 (0.3775) Acc D Real: 64.073% 
Loss D Fake: 0.6629 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.052 
Loss G: 0.7279 (0.7311) Acc G: 14.963% 
LR: 2.000e-04 

2023-03-02 02:00:25,817 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3385 (0.3772) Acc D Real: 64.101% 
Loss D Fake: 0.6622 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.001 
Loss G: 0.7287 (0.7310) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 02:00:25,825 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2737 (0.3764) Acc D Real: 64.181% 
Loss D Fake: 0.6612 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.935 
Loss G: 0.7303 (0.7310) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 02:00:25,832 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2856 (0.3758) Acc D Real: 64.246% 
Loss D Fake: 0.6594 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.945 
Loss G: 0.7327 (0.7311) Acc G: 14.964% 
LR: 2.000e-04 

2023-03-02 02:00:25,840 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3378 (0.3755) Acc D Real: 64.261% 
Loss D Fake: 0.6571 (0.6600) Acc D Fake: 85.017% 
Loss D: 0.995 
Loss G: 0.7351 (0.7311) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,847 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3863 (0.3756) Acc D Real: 64.234% 
Loss D Fake: 0.6550 (0.6600) Acc D Fake: 85.017% 
Loss D: 1.041 
Loss G: 0.7371 (0.7311) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,855 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4224 (0.3759) Acc D Real: 64.169% 
Loss D Fake: 0.6537 (0.6599) Acc D Fake: 85.017% 
Loss D: 1.076 
Loss G: 0.7380 (0.7312) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,862 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3896 (0.3760) Acc D Real: 64.146% 
Loss D Fake: 0.6532 (0.6599) Acc D Fake: 85.017% 
Loss D: 1.043 
Loss G: 0.7384 (0.7312) Acc G: 14.965% 
LR: 2.000e-04 

2023-03-02 02:00:25,870 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.2887 (0.3754) Acc D Real: 64.211% 
Loss D Fake: 0.6526 (0.6598) Acc D Fake: 85.017% 
Loss D: 0.941 
Loss G: 0.7395 (0.7313) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,878 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.2473 (0.3745) Acc D Real: 64.310% 
Loss D Fake: 0.6511 (0.6598) Acc D Fake: 85.016% 
Loss D: 0.898 
Loss G: 0.7419 (0.7314) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,885 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.5152 (0.3755) Acc D Real: 64.170% 
Loss D Fake: 0.6493 (0.6597) Acc D Fake: 85.016% 
Loss D: 1.164 
Loss G: 0.7429 (0.7314) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,893 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4833 (0.3762) Acc D Real: 64.056% 
Loss D Fake: 0.6491 (0.6596) Acc D Fake: 85.016% 
Loss D: 1.132 
Loss G: 0.7424 (0.7315) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,900 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4735 (0.3769) Acc D Real: 63.963% 
Loss D Fake: 0.6501 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.124 
Loss G: 0.7408 (0.7316) Acc G: 14.966% 
LR: 2.000e-04 

2023-03-02 02:00:25,908 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4191 (0.3772) Acc D Real: 63.913% 
Loss D Fake: 0.6518 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.071 
Loss G: 0.7386 (0.7316) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,915 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.5376 (0.3783) Acc D Real: 63.754% 
Loss D Fake: 0.6542 (0.6595) Acc D Fake: 85.016% 
Loss D: 1.192 
Loss G: 0.7352 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,923 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3193 (0.3779) Acc D Real: 63.785% 
Loss D Fake: 0.6573 (0.6594) Acc D Fake: 85.016% 
Loss D: 0.977 
Loss G: 0.7324 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,930 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3457 (0.3777) Acc D Real: 63.803% 
Loss D Fake: 0.6593 (0.6594) Acc D Fake: 85.016% 
Loss D: 1.005 
Loss G: 0.7308 (0.7317) Acc G: 14.967% 
LR: 2.000e-04 

2023-03-02 02:00:25,938 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4319 (0.3780) Acc D Real: 63.777% 
Loss D Fake: 0.6607 (0.6594) Acc D Fake: 85.016% 
Loss D: 1.093 
Loss G: 0.7290 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,945 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.2828 (0.3774) Acc D Real: 63.838% 
Loss D Fake: 0.6621 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.945 
Loss G: 0.7281 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,953 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3344 (0.3771) Acc D Real: 63.869% 
Loss D Fake: 0.6624 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.997 
Loss G: 0.7282 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,960 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4194 (0.3774) Acc D Real: 63.811% 
Loss D Fake: 0.6623 (0.6595) Acc D Fake: 85.015% 
Loss D: 1.082 
Loss G: 0.7279 (0.7316) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,968 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3111 (0.3770) Acc D Real: 63.852% 
Loss D Fake: 0.6626 (0.6595) Acc D Fake: 85.015% 
Loss D: 0.974 
Loss G: 0.7279 (0.7315) Acc G: 14.968% 
LR: 2.000e-04 

2023-03-02 02:00:25,977 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4436 (0.3774) Acc D Real: 63.778% 
Loss D Fake: 0.6627 (0.6595) Acc D Fake: 85.015% 
Loss D: 1.106 
Loss G: 0.7274 (0.7315) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:25,984 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4530 (0.3779) Acc D Real: 63.710% 
Loss D Fake: 0.6635 (0.6596) Acc D Fake: 85.015% 
Loss D: 1.117 
Loss G: 0.7261 (0.7315) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:25,992 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3022 (0.3774) Acc D Real: 63.714% 
Loss D Fake: 0.6646 (0.6596) Acc D Fake: 85.015% 
Loss D: 0.967 
Loss G: 0.7254 (0.7314) Acc G: 14.969% 
LR: 2.000e-04 

2023-03-02 02:00:26,229 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.344 | Generator Loss: 0.725 | Avg: 2.069 
2023-03-02 02:00:26,253 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.249 | Generator Loss: 0.725 | Avg: 1.974 
2023-03-02 02:00:26,278 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.278 | Generator Loss: 0.725 | Avg: 2.003 
2023-03-02 02:00:26,304 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.297 | Generator Loss: 0.725 | Avg: 2.023 
2023-03-02 02:00:26,331 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.299 | Generator Loss: 0.725 | Avg: 2.024 
2023-03-02 02:00:26,358 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.251 | Generator Loss: 0.725 | Avg: 1.976 
2023-03-02 02:00:26,385 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.224 | Generator Loss: 0.725 | Avg: 1.949 
2023-03-02 02:00:26,412 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.178 | Generator Loss: 0.725 | Avg: 1.904 
2023-03-02 02:00:26,438 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.152 | Generator Loss: 0.725 | Avg: 1.878 
2023-03-02 02:00:26,465 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.111 | Generator Loss: 0.725 | Avg: 1.836 
2023-03-02 02:00:26,492 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.078 | Generator Loss: 0.725 | Avg: 1.803 
2023-03-02 02:00:26,517 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.048 | Generator Loss: 0.725 | Avg: 1.773 
2023-03-02 02:00:26,543 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.023 | Generator Loss: 0.725 | Avg: 1.749 
2023-03-02 02:00:26,569 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.005 | Generator Loss: 0.725 | Avg: 1.731 
2023-03-02 02:00:26,597 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.021 | Generator Loss: 0.725 | Avg: 1.747 
2023-03-02 02:00:26,622 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.037 | Generator Loss: 0.725 | Avg: 1.763 
2023-03-02 02:00:26,647 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.053 | Generator Loss: 0.725 | Avg: 1.778 
2023-03-02 02:00:26,681 -                train: [    INFO] - 
Epoch: 13/20
2023-03-02 02:00:26,868 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3018 (0.3269) Acc D Real: 67.760% 
Loss D Fake: 0.6649 (0.6649) Acc D Fake: 85.000% 
Loss D: 0.967 
Loss G: 0.7257 (0.7254) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,876 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4261 (0.3600) Acc D Real: 63.212% 
Loss D Fake: 0.6644 (0.6648) Acc D Fake: 85.000% 
Loss D: 1.091 
Loss G: 0.7258 (0.7255) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,885 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.4169 (0.3742) Acc D Real: 61.549% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.082 
Loss G: 0.7253 (0.7255) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,903 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.2560 (0.3506) Acc D Real: 64.750% 
Loss D Fake: 0.6648 (0.6647) Acc D Fake: 85.000% 
Loss D: 0.921 
Loss G: 0.7258 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,910 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4857 (0.3731) Acc D Real: 61.988% 
Loss D Fake: 0.6644 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.150 
Loss G: 0.7256 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,917 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3009 (0.3628) Acc D Real: 63.586% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 0.966 
Loss G: 0.7258 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,925 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4407 (0.3725) Acc D Real: 62.461% 
Loss D Fake: 0.6646 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.105 
Loss G: 0.7253 (0.7256) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,932 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4179 (0.3776) Acc D Real: 62.031% 
Loss D Fake: 0.6652 (0.6647) Acc D Fake: 85.000% 
Loss D: 1.083 
Loss G: 0.7245 (0.7254) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,939 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4994 (0.3897) Acc D Real: 60.766% 
Loss D Fake: 0.6664 (0.6649) Acc D Fake: 85.000% 
Loss D: 1.166 
Loss G: 0.7225 (0.7252) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,946 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3758 (0.3885) Acc D Real: 60.767% 
Loss D Fake: 0.6684 (0.6652) Acc D Fake: 85.000% 
Loss D: 1.044 
Loss G: 0.7206 (0.7247) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,953 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.2947 (0.3807) Acc D Real: 62.044% 
Loss D Fake: 0.6699 (0.6656) Acc D Fake: 85.000% 
Loss D: 0.965 
Loss G: 0.7196 (0.7243) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:26,960 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4421 (0.3854) Acc D Real: 61.498% 
Loss D Fake: 0.6707 (0.6660) Acc D Fake: 84.976% 
Loss D: 1.113 
Loss G: 0.7183 (0.7238) Acc G: 15.064% 
LR: 2.000e-04 

2023-03-02 02:00:26,967 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.2279 (0.3741) Acc D Real: 62.999% 
Loss D Fake: 0.6716 (0.6664) Acc D Fake: 84.881% 
Loss D: 0.899 
Loss G: 0.7184 (0.7235) Acc G: 15.141% 
LR: 2.000e-04 

2023-03-02 02:00:26,974 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4772 (0.3810) Acc D Real: 62.319% 
Loss D Fake: 0.6714 (0.6667) Acc D Fake: 84.806% 
Loss D: 1.149 
Loss G: 0.7180 (0.7231) Acc G: 15.233% 
LR: 2.000e-04 

2023-03-02 02:00:26,981 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3333 (0.3780) Acc D Real: 62.760% 
Loss D Fake: 0.6719 (0.6671) Acc D Fake: 84.714% 
Loss D: 1.005 
Loss G: 0.7177 (0.7228) Acc G: 15.322% 
LR: 2.000e-04 

2023-03-02 02:00:26,989 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3662 (0.3773) Acc D Real: 62.751% 
Loss D Fake: 0.6720 (0.6673) Acc D Fake: 84.632% 
Loss D: 1.038 
Loss G: 0.7176 (0.7224) Acc G: 15.401% 
LR: 2.000e-04 

2023-03-02 02:00:26,996 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3993 (0.3786) Acc D Real: 62.636% 
Loss D Fake: 0.6722 (0.6676) Acc D Fake: 84.560% 
Loss D: 1.071 
Loss G: 0.7173 (0.7222) Acc G: 15.472% 
LR: 2.000e-04 

2023-03-02 02:00:27,003 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3054 (0.3747) Acc D Real: 63.021% 
Loss D Fake: 0.6723 (0.6679) Acc D Fake: 84.496% 
Loss D: 0.978 
Loss G: 0.7176 (0.7219) Acc G: 15.535% 
LR: 2.000e-04 

2023-03-02 02:00:27,010 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2958 (0.3708) Acc D Real: 63.539% 
Loss D Fake: 0.6716 (0.6680) Acc D Fake: 84.438% 
Loss D: 0.967 
Loss G: 0.7187 (0.7218) Acc G: 15.591% 
LR: 2.000e-04 

2023-03-02 02:00:27,016 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2692 (0.3659) Acc D Real: 64.174% 
Loss D Fake: 0.6702 (0.6682) Acc D Fake: 84.385% 
Loss D: 0.939 
Loss G: 0.7206 (0.7217) Acc G: 15.642% 
LR: 2.000e-04 

2023-03-02 02:00:27,023 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3718 (0.3662) Acc D Real: 64.176% 
Loss D Fake: 0.6684 (0.6682) Acc D Fake: 84.337% 
Loss D: 1.040 
Loss G: 0.7224 (0.7217) Acc G: 15.689% 
LR: 2.000e-04 

2023-03-02 02:00:27,030 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3592 (0.3659) Acc D Real: 64.149% 
Loss D Fake: 0.6669 (0.6681) Acc D Fake: 84.293% 
Loss D: 1.026 
Loss G: 0.7239 (0.7218) Acc G: 15.731% 
LR: 2.000e-04 

2023-03-02 02:00:27,037 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3751 (0.3663) Acc D Real: 64.115% 
Loss D Fake: 0.6656 (0.6680) Acc D Fake: 84.253% 
Loss D: 1.041 
Loss G: 0.7252 (0.7220) Acc G: 15.770% 
LR: 2.000e-04 

2023-03-02 02:00:27,044 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4115 (0.3681) Acc D Real: 63.842% 
Loss D Fake: 0.6646 (0.6679) Acc D Fake: 84.217% 
Loss D: 1.076 
Loss G: 0.7259 (0.7221) Acc G: 15.806% 
LR: 2.000e-04 

2023-03-02 02:00:27,051 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3584 (0.3677) Acc D Real: 63.886% 
Loss D Fake: 0.6641 (0.6677) Acc D Fake: 84.183% 
Loss D: 1.022 
Loss G: 0.7265 (0.7223) Acc G: 15.839% 
LR: 2.000e-04 

2023-03-02 02:00:27,058 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3161 (0.3658) Acc D Real: 64.086% 
Loss D Fake: 0.6634 (0.6676) Acc D Fake: 84.151% 
Loss D: 0.979 
Loss G: 0.7275 (0.7225) Acc G: 15.870% 
LR: 2.000e-04 

2023-03-02 02:00:27,065 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3646 (0.3657) Acc D Real: 64.031% 
Loss D Fake: 0.6624 (0.6674) Acc D Fake: 84.122% 
Loss D: 1.027 
Loss G: 0.7285 (0.7227) Acc G: 15.898% 
LR: 2.000e-04 

2023-03-02 02:00:27,073 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3213 (0.3642) Acc D Real: 64.217% 
Loss D Fake: 0.6613 (0.6672) Acc D Fake: 84.095% 
Loss D: 0.983 
Loss G: 0.7299 (0.7230) Acc G: 15.925% 
LR: 2.000e-04 

2023-03-02 02:00:27,080 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.2717 (0.3611) Acc D Real: 64.562% 
Loss D Fake: 0.6598 (0.6669) Acc D Fake: 84.069% 
Loss D: 0.932 
Loss G: 0.7319 (0.7233) Acc G: 15.950% 
LR: 2.000e-04 

2023-03-02 02:00:27,089 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.5039 (0.3657) Acc D Real: 63.871% 
Loss D Fake: 0.6583 (0.6666) Acc D Fake: 84.046% 
Loss D: 1.162 
Loss G: 0.7326 (0.7236) Acc G: 15.973% 
LR: 2.000e-04 

2023-03-02 02:00:27,097 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3229 (0.3644) Acc D Real: 63.994% 
Loss D Fake: 0.6580 (0.6664) Acc D Fake: 84.023% 
Loss D: 0.981 
Loss G: 0.7332 (0.7239) Acc G: 15.994% 
LR: 2.000e-04 

2023-03-02 02:00:27,105 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3396 (0.3636) Acc D Real: 64.148% 
Loss D Fake: 0.6573 (0.6661) Acc D Fake: 84.003% 
Loss D: 0.997 
Loss G: 0.7340 (0.7242) Acc G: 16.015% 
LR: 2.000e-04 

2023-03-02 02:00:27,112 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3576 (0.3635) Acc D Real: 64.263% 
Loss D Fake: 0.6565 (0.6658) Acc D Fake: 83.983% 
Loss D: 1.014 
Loss G: 0.7348 (0.7245) Acc G: 16.034% 
LR: 2.000e-04 

2023-03-02 02:00:27,120 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3586 (0.3633) Acc D Real: 64.198% 
Loss D Fake: 0.6559 (0.6655) Acc D Fake: 83.964% 
Loss D: 1.014 
Loss G: 0.7355 (0.7248) Acc G: 16.052% 
LR: 2.000e-04 

2023-03-02 02:00:27,128 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2302 (0.3596) Acc D Real: 64.631% 
Loss D Fake: 0.6550 (0.6652) Acc D Fake: 83.947% 
Loss D: 0.885 
Loss G: 0.7371 (0.7251) Acc G: 16.069% 
LR: 2.000e-04 

2023-03-02 02:00:27,135 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3236 (0.3587) Acc D Real: 64.686% 
Loss D Fake: 0.6532 (0.6649) Acc D Fake: 83.932% 
Loss D: 0.977 
Loss G: 0.7392 (0.7255) Acc G: 16.077% 
LR: 2.000e-04 

2023-03-02 02:00:27,143 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4077 (0.3599) Acc D Real: 64.445% 
Loss D Fake: 0.6516 (0.6646) Acc D Fake: 83.924% 
Loss D: 1.059 
Loss G: 0.7406 (0.7259) Acc G: 16.073% 
LR: 2.000e-04 

2023-03-02 02:00:27,151 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3198 (0.3589) Acc D Real: 64.497% 
Loss D Fake: 0.6505 (0.6642) Acc D Fake: 83.925% 
Loss D: 0.970 
Loss G: 0.7419 (0.7263) Acc G: 16.058% 
LR: 2.000e-04 

2023-03-02 02:00:27,159 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3862 (0.3596) Acc D Real: 64.423% 
Loss D Fake: 0.6493 (0.6638) Acc D Fake: 83.932% 
Loss D: 1.036 
Loss G: 0.7430 (0.7267) Acc G: 16.039% 
LR: 2.000e-04 

2023-03-02 02:00:27,166 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3881 (0.3603) Acc D Real: 64.295% 
Loss D Fake: 0.6486 (0.6635) Acc D Fake: 83.948% 
Loss D: 1.037 
Loss G: 0.7435 (0.7271) Acc G: 16.023% 
LR: 2.000e-04 

2023-03-02 02:00:27,174 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3907 (0.3610) Acc D Real: 64.187% 
Loss D Fake: 0.6483 (0.6631) Acc D Fake: 83.952% 
Loss D: 1.039 
Loss G: 0.7437 (0.7275) Acc G: 16.018% 
LR: 2.000e-04 

2023-03-02 02:00:27,181 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3921 (0.3617) Acc D Real: 64.059% 
Loss D Fake: 0.6483 (0.6628) Acc D Fake: 83.943% 
Loss D: 1.040 
Loss G: 0.7435 (0.7279) Acc G: 16.027% 
LR: 2.000e-04 

2023-03-02 02:00:27,189 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.1968 (0.3580) Acc D Real: 64.496% 
Loss D Fake: 0.6481 (0.6624) Acc D Fake: 83.933% 
Loss D: 0.845 
Loss G: 0.7447 (0.7283) Acc G: 16.033% 
LR: 2.000e-04 

2023-03-02 02:00:27,196 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2628 (0.3559) Acc D Real: 64.747% 
Loss D Fake: 0.6464 (0.6621) Acc D Fake: 83.925% 
Loss D: 0.909 
Loss G: 0.7471 (0.7287) Acc G: 16.028% 
LR: 2.000e-04 

2023-03-02 02:00:27,204 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3501 (0.3558) Acc D Real: 64.740% 
Loss D Fake: 0.6442 (0.6617) Acc D Fake: 83.932% 
Loss D: 0.994 
Loss G: 0.7494 (0.7292) Acc G: 16.007% 
LR: 2.000e-04 

2023-03-02 02:00:27,211 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3742 (0.3561) Acc D Real: 64.664% 
Loss D Fake: 0.6424 (0.6613) Acc D Fake: 83.951% 
Loss D: 1.017 
Loss G: 0.7513 (0.7296) Acc G: 15.987% 
LR: 2.000e-04 

2023-03-02 02:00:27,219 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3718 (0.3565) Acc D Real: 64.588% 
Loss D Fake: 0.6409 (0.6608) Acc D Fake: 83.980% 
Loss D: 1.013 
Loss G: 0.7527 (0.7301) Acc G: 15.932% 
LR: 2.000e-04 

2023-03-02 02:00:27,226 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3062 (0.3554) Acc D Real: 64.691% 
Loss D Fake: 0.6397 (0.6604) Acc D Fake: 84.033% 
Loss D: 0.946 
Loss G: 0.7543 (0.7306) Acc G: 15.879% 
LR: 2.000e-04 

2023-03-02 02:00:27,234 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3908 (0.3562) Acc D Real: 64.581% 
Loss D Fake: 0.6384 (0.6600) Acc D Fake: 84.085% 
Loss D: 1.029 
Loss G: 0.7556 (0.7311) Acc G: 15.830% 
LR: 2.000e-04 

2023-03-02 02:00:27,241 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3540 (0.3561) Acc D Real: 64.572% 
Loss D Fake: 0.6374 (0.6595) Acc D Fake: 84.132% 
Loss D: 0.991 
Loss G: 0.7567 (0.7316) Acc G: 15.782% 
LR: 2.000e-04 

2023-03-02 02:00:27,249 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3694 (0.3564) Acc D Real: 64.517% 
Loss D Fake: 0.6365 (0.6591) Acc D Fake: 84.170% 
Loss D: 1.006 
Loss G: 0.7575 (0.7321) Acc G: 15.735% 
LR: 2.000e-04 

2023-03-02 02:00:27,256 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.5251 (0.3595) Acc D Real: 64.075% 
Loss D Fake: 0.6363 (0.6587) Acc D Fake: 84.195% 
Loss D: 1.161 
Loss G: 0.7567 (0.7326) Acc G: 15.714% 
LR: 2.000e-04 

2023-03-02 02:00:27,264 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2791 (0.3581) Acc D Real: 64.218% 
Loss D Fake: 0.6372 (0.6583) Acc D Fake: 84.210% 
Loss D: 0.916 
Loss G: 0.7562 (0.7330) Acc G: 15.699% 
LR: 2.000e-04 

2023-03-02 02:00:27,272 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3703 (0.3583) Acc D Real: 64.159% 
Loss D Fake: 0.6375 (0.6579) Acc D Fake: 84.224% 
Loss D: 1.008 
Loss G: 0.7558 (0.7334) Acc G: 15.684% 
LR: 2.000e-04 

2023-03-02 02:00:27,279 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3452 (0.3580) Acc D Real: 64.150% 
Loss D Fake: 0.6378 (0.6575) Acc D Fake: 84.239% 
Loss D: 0.983 
Loss G: 0.7556 (0.7338) Acc G: 15.668% 
LR: 2.000e-04 

2023-03-02 02:00:27,287 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2855 (0.3568) Acc D Real: 64.308% 
Loss D Fake: 0.6377 (0.6572) Acc D Fake: 84.253% 
Loss D: 0.923 
Loss G: 0.7561 (0.7342) Acc G: 15.640% 
LR: 2.000e-04 

2023-03-02 02:00:27,294 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2794 (0.3554) Acc D Real: 64.460% 
Loss D Fake: 0.6369 (0.6568) Acc D Fake: 84.285% 
Loss D: 0.916 
Loss G: 0.7575 (0.7346) Acc G: 15.600% 
LR: 2.000e-04 

2023-03-02 02:00:27,302 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3208 (0.3549) Acc D Real: 64.503% 
Loss D Fake: 0.6355 (0.6565) Acc D Fake: 84.326% 
Loss D: 0.956 
Loss G: 0.7591 (0.7350) Acc G: 15.561% 
LR: 2.000e-04 

2023-03-02 02:00:27,309 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3235 (0.3543) Acc D Real: 64.550% 
Loss D Fake: 0.6341 (0.6561) Acc D Fake: 84.365% 
Loss D: 0.958 
Loss G: 0.7608 (0.7355) Acc G: 15.524% 
LR: 2.000e-04 

2023-03-02 02:00:27,317 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3726 (0.3546) Acc D Real: 64.495% 
Loss D Fake: 0.6327 (0.6557) Acc D Fake: 84.402% 
Loss D: 1.005 
Loss G: 0.7624 (0.7359) Acc G: 15.488% 
LR: 2.000e-04 

2023-03-02 02:00:27,324 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2408 (0.3528) Acc D Real: 64.713% 
Loss D Fake: 0.6312 (0.6553) Acc D Fake: 84.439% 
Loss D: 0.872 
Loss G: 0.7646 (0.7364) Acc G: 15.454% 
LR: 2.000e-04 

2023-03-02 02:00:27,332 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4683 (0.3546) Acc D Real: 64.478% 
Loss D Fake: 0.6294 (0.6549) Acc D Fake: 84.474% 
Loss D: 1.098 
Loss G: 0.7659 (0.7368) Acc G: 15.420% 
LR: 2.000e-04 

2023-03-02 02:00:27,339 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3780 (0.3550) Acc D Real: 64.425% 
Loss D Fake: 0.6287 (0.6545) Acc D Fake: 84.508% 
Loss D: 1.007 
Loss G: 0.7666 (0.7373) Acc G: 15.387% 
LR: 2.000e-04 

2023-03-02 02:00:27,347 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3120 (0.3543) Acc D Real: 64.486% 
Loss D Fake: 0.6281 (0.6541) Acc D Fake: 84.542% 
Loss D: 0.940 
Loss G: 0.7676 (0.7378) Acc G: 15.356% 
LR: 2.000e-04 

2023-03-02 02:00:27,355 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3824 (0.3548) Acc D Real: 64.443% 
Loss D Fake: 0.6273 (0.6537) Acc D Fake: 84.574% 
Loss D: 1.010 
Loss G: 0.7684 (0.7382) Acc G: 15.325% 
LR: 2.000e-04 

2023-03-02 02:00:27,362 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2860 (0.3537) Acc D Real: 64.555% 
Loss D Fake: 0.6265 (0.6533) Acc D Fake: 84.605% 
Loss D: 0.913 
Loss G: 0.7697 (0.7387) Acc G: 15.295% 
LR: 2.000e-04 

2023-03-02 02:00:27,370 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3553 (0.3538) Acc D Real: 64.540% 
Loss D Fake: 0.6253 (0.6529) Acc D Fake: 84.635% 
Loss D: 0.981 
Loss G: 0.7710 (0.7392) Acc G: 15.267% 
LR: 2.000e-04 

2023-03-02 02:00:27,377 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4200 (0.3547) Acc D Real: 64.416% 
Loss D Fake: 0.6245 (0.6525) Acc D Fake: 84.665% 
Loss D: 1.044 
Loss G: 0.7715 (0.7396) Acc G: 15.239% 
LR: 2.000e-04 

2023-03-02 02:00:27,385 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4867 (0.3566) Acc D Real: 64.173% 
Loss D Fake: 0.6245 (0.6521) Acc D Fake: 84.693% 
Loss D: 1.111 
Loss G: 0.7706 (0.7401) Acc G: 15.211% 
LR: 2.000e-04 

2023-03-02 02:00:27,393 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4426 (0.3578) Acc D Real: 64.007% 
Loss D Fake: 0.6258 (0.6517) Acc D Fake: 84.721% 
Loss D: 1.068 
Loss G: 0.7688 (0.7405) Acc G: 15.185% 
LR: 2.000e-04 

2023-03-02 02:00:27,400 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3494 (0.3577) Acc D Real: 64.010% 
Loss D Fake: 0.6274 (0.6513) Acc D Fake: 84.748% 
Loss D: 0.977 
Loss G: 0.7671 (0.7409) Acc G: 15.159% 
LR: 2.000e-04 

2023-03-02 02:00:27,408 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3095 (0.3570) Acc D Real: 64.094% 
Loss D Fake: 0.6285 (0.6510) Acc D Fake: 84.775% 
Loss D: 0.938 
Loss G: 0.7663 (0.7412) Acc G: 15.134% 
LR: 2.000e-04 

2023-03-02 02:00:27,416 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3902 (0.3575) Acc D Real: 64.011% 
Loss D Fake: 0.6291 (0.6507) Acc D Fake: 84.800% 
Loss D: 1.019 
Loss G: 0.7653 (0.7415) Acc G: 15.110% 
LR: 2.000e-04 

2023-03-02 02:00:27,424 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3580 (0.3575) Acc D Real: 64.019% 
Loss D Fake: 0.6299 (0.6505) Acc D Fake: 84.825% 
Loss D: 0.988 
Loss G: 0.7645 (0.7418) Acc G: 15.086% 
LR: 2.000e-04 

2023-03-02 02:00:27,431 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3833 (0.3578) Acc D Real: 63.988% 
Loss D Fake: 0.6305 (0.6502) Acc D Fake: 84.849% 
Loss D: 1.014 
Loss G: 0.7639 (0.7421) Acc G: 15.063% 
LR: 2.000e-04 

2023-03-02 02:00:27,439 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2595 (0.3566) Acc D Real: 64.129% 
Loss D Fake: 0.6307 (0.6499) Acc D Fake: 84.873% 
Loss D: 0.890 
Loss G: 0.7642 (0.7424) Acc G: 15.041% 
LR: 2.000e-04 

2023-03-02 02:00:27,446 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3695 (0.3567) Acc D Real: 64.092% 
Loss D Fake: 0.6303 (0.6497) Acc D Fake: 84.896% 
Loss D: 1.000 
Loss G: 0.7646 (0.7427) Acc G: 15.019% 
LR: 2.000e-04 

2023-03-02 02:00:27,454 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3301 (0.3564) Acc D Real: 64.123% 
Loss D Fake: 0.6299 (0.6494) Acc D Fake: 84.918% 
Loss D: 0.960 
Loss G: 0.7651 (0.7430) Acc G: 14.997% 
LR: 2.000e-04 

2023-03-02 02:00:27,462 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4028 (0.3570) Acc D Real: 64.053% 
Loss D Fake: 0.6296 (0.6492) Acc D Fake: 84.940% 
Loss D: 1.032 
Loss G: 0.7653 (0.7433) Acc G: 14.977% 
LR: 2.000e-04 

2023-03-02 02:00:27,469 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4387 (0.3580) Acc D Real: 63.922% 
Loss D Fake: 0.6296 (0.6489) Acc D Fake: 84.961% 
Loss D: 1.068 
Loss G: 0.7649 (0.7435) Acc G: 14.956% 
LR: 2.000e-04 

2023-03-02 02:00:27,477 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3218 (0.3575) Acc D Real: 63.955% 
Loss D Fake: 0.6301 (0.6487) Acc D Fake: 84.982% 
Loss D: 0.952 
Loss G: 0.7645 (0.7438) Acc G: 14.936% 
LR: 2.000e-04 

2023-03-02 02:00:27,484 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3762 (0.3578) Acc D Real: 63.924% 
Loss D Fake: 0.6303 (0.6485) Acc D Fake: 85.003% 
Loss D: 1.006 
Loss G: 0.7642 (0.7440) Acc G: 14.917% 
LR: 2.000e-04 

2023-03-02 02:00:27,492 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4628 (0.3590) Acc D Real: 63.759% 
Loss D Fake: 0.6308 (0.6483) Acc D Fake: 85.022% 
Loss D: 1.094 
Loss G: 0.7631 (0.7443) Acc G: 14.898% 
LR: 2.000e-04 

2023-03-02 02:00:27,499 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3288 (0.3587) Acc D Real: 63.791% 
Loss D Fake: 0.6319 (0.6481) Acc D Fake: 85.042% 
Loss D: 0.961 
Loss G: 0.7621 (0.7445) Acc G: 14.880% 
LR: 2.000e-04 

2023-03-02 02:00:27,507 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4342 (0.3595) Acc D Real: 63.662% 
Loss D Fake: 0.6328 (0.6479) Acc D Fake: 85.061% 
Loss D: 1.067 
Loss G: 0.7607 (0.7447) Acc G: 14.862% 
LR: 2.000e-04 

2023-03-02 02:00:27,515 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3712 (0.3597) Acc D Real: 63.627% 
Loss D Fake: 0.6341 (0.6478) Acc D Fake: 85.079% 
Loss D: 1.005 
Loss G: 0.7592 (0.7448) Acc G: 14.844% 
LR: 2.000e-04 

2023-03-02 02:00:27,522 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4158 (0.3603) Acc D Real: 63.540% 
Loss D Fake: 0.6354 (0.6476) Acc D Fake: 85.097% 
Loss D: 1.051 
Loss G: 0.7576 (0.7450) Acc G: 14.827% 
LR: 2.000e-04 

2023-03-02 02:00:27,530 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4580 (0.3614) Acc D Real: 63.394% 
Loss D Fake: 0.6370 (0.6475) Acc D Fake: 85.115% 
Loss D: 1.095 
Loss G: 0.7553 (0.7451) Acc G: 14.810% 
LR: 2.000e-04 

2023-03-02 02:00:27,538 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4761 (0.3627) Acc D Real: 63.226% 
Loss D Fake: 0.6393 (0.6474) Acc D Fake: 85.132% 
Loss D: 1.115 
Loss G: 0.7523 (0.7452) Acc G: 14.812% 
LR: 2.000e-04 

2023-03-02 02:00:27,545 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3462 (0.3625) Acc D Real: 63.227% 
Loss D Fake: 0.6419 (0.6473) Acc D Fake: 85.130% 
Loss D: 0.988 
Loss G: 0.7497 (0.7452) Acc G: 14.815% 
LR: 2.000e-04 

2023-03-02 02:00:27,553 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3628 (0.3625) Acc D Real: 63.214% 
Loss D Fake: 0.6440 (0.6473) Acc D Fake: 85.129% 
Loss D: 1.007 
Loss G: 0.7475 (0.7452) Acc G: 14.817% 
LR: 2.000e-04 

2023-03-02 02:00:27,560 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3170 (0.3620) Acc D Real: 63.254% 
Loss D Fake: 0.6456 (0.6473) Acc D Fake: 85.128% 
Loss D: 0.963 
Loss G: 0.7461 (0.7452) Acc G: 14.819% 
LR: 2.000e-04 

2023-03-02 02:00:27,568 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4073 (0.3625) Acc D Real: 63.197% 
Loss D Fake: 0.6467 (0.6473) Acc D Fake: 85.126% 
Loss D: 1.054 
Loss G: 0.7447 (0.7452) Acc G: 14.820% 
LR: 2.000e-04 

2023-03-02 02:00:27,575 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2989 (0.3618) Acc D Real: 63.264% 
Loss D Fake: 0.6477 (0.6473) Acc D Fake: 85.125% 
Loss D: 0.947 
Loss G: 0.7441 (0.7452) Acc G: 14.822% 
LR: 2.000e-04 

2023-03-02 02:00:27,583 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3432 (0.3616) Acc D Real: 63.283% 
Loss D Fake: 0.6480 (0.6473) Acc D Fake: 85.124% 
Loss D: 0.991 
Loss G: 0.7439 (0.7452) Acc G: 14.824% 
LR: 2.000e-04 

2023-03-02 02:00:27,591 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4231 (0.3623) Acc D Real: 63.175% 
Loss D Fake: 0.6482 (0.6473) Acc D Fake: 85.122% 
Loss D: 1.071 
Loss G: 0.7433 (0.7452) Acc G: 14.826% 
LR: 2.000e-04 

2023-03-02 02:00:27,598 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3396 (0.3620) Acc D Real: 63.189% 
Loss D Fake: 0.6489 (0.6473) Acc D Fake: 85.121% 
Loss D: 0.989 
Loss G: 0.7427 (0.7452) Acc G: 14.828% 
LR: 2.000e-04 

2023-03-02 02:00:27,606 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2268 (0.3607) Acc D Real: 63.351% 
Loss D Fake: 0.6489 (0.6473) Acc D Fake: 85.120% 
Loss D: 0.876 
Loss G: 0.7435 (0.7452) Acc G: 14.830% 
LR: 2.000e-04 

2023-03-02 02:00:27,614 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2348 (0.3594) Acc D Real: 63.501% 
Loss D Fake: 0.6475 (0.6473) Acc D Fake: 85.119% 
Loss D: 0.882 
Loss G: 0.7457 (0.7452) Acc G: 14.831% 
LR: 2.000e-04 

2023-03-02 02:00:27,621 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4248 (0.3600) Acc D Real: 63.412% 
Loss D Fake: 0.6456 (0.6473) Acc D Fake: 85.118% 
Loss D: 1.070 
Loss G: 0.7473 (0.7452) Acc G: 14.833% 
LR: 2.000e-04 

2023-03-02 02:00:27,629 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4551 (0.3610) Acc D Real: 63.275% 
Loss D Fake: 0.6448 (0.6473) Acc D Fake: 85.116% 
Loss D: 1.100 
Loss G: 0.7476 (0.7452) Acc G: 14.835% 
LR: 2.000e-04 

2023-03-02 02:00:27,637 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3269 (0.3607) Acc D Real: 63.315% 
Loss D Fake: 0.6447 (0.6473) Acc D Fake: 85.115% 
Loss D: 0.972 
Loss G: 0.7479 (0.7452) Acc G: 14.836% 
LR: 2.000e-04 

2023-03-02 02:00:27,644 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3853 (0.3609) Acc D Real: 63.286% 
Loss D Fake: 0.6444 (0.6472) Acc D Fake: 85.114% 
Loss D: 1.030 
Loss G: 0.7481 (0.7453) Acc G: 14.838% 
LR: 2.000e-04 

2023-03-02 02:00:27,652 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.4358 (0.3616) Acc D Real: 63.182% 
Loss D Fake: 0.6445 (0.6472) Acc D Fake: 85.113% 
Loss D: 1.080 
Loss G: 0.7476 (0.7453) Acc G: 14.839% 
LR: 2.000e-04 

2023-03-02 02:00:27,660 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3900 (0.3619) Acc D Real: 63.141% 
Loss D Fake: 0.6451 (0.6472) Acc D Fake: 85.112% 
Loss D: 1.035 
Loss G: 0.7469 (0.7453) Acc G: 14.841% 
LR: 2.000e-04 

2023-03-02 02:00:27,667 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3448 (0.3617) Acc D Real: 63.151% 
Loss D Fake: 0.6456 (0.6472) Acc D Fake: 85.111% 
Loss D: 0.990 
Loss G: 0.7464 (0.7453) Acc G: 14.842% 
LR: 2.000e-04 

2023-03-02 02:00:27,675 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4451 (0.3625) Acc D Real: 63.048% 
Loss D Fake: 0.6462 (0.6472) Acc D Fake: 85.110% 
Loss D: 1.091 
Loss G: 0.7454 (0.7453) Acc G: 14.844% 
LR: 2.000e-04 

2023-03-02 02:00:27,682 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3427 (0.3623) Acc D Real: 63.064% 
Loss D Fake: 0.6471 (0.6472) Acc D Fake: 85.109% 
Loss D: 0.990 
Loss G: 0.7445 (0.7453) Acc G: 14.845% 
LR: 2.000e-04 

2023-03-02 02:00:27,690 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3785 (0.3624) Acc D Real: 63.035% 
Loss D Fake: 0.6478 (0.6472) Acc D Fake: 85.108% 
Loss D: 1.026 
Loss G: 0.7438 (0.7453) Acc G: 14.847% 
LR: 2.000e-04 

2023-03-02 02:00:27,698 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3685 (0.3625) Acc D Real: 63.029% 
Loss D Fake: 0.6484 (0.6472) Acc D Fake: 85.107% 
Loss D: 1.017 
Loss G: 0.7432 (0.7453) Acc G: 14.848% 
LR: 2.000e-04 

2023-03-02 02:00:27,705 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3453 (0.3623) Acc D Real: 63.046% 
Loss D Fake: 0.6487 (0.6472) Acc D Fake: 85.106% 
Loss D: 0.994 
Loss G: 0.7431 (0.7452) Acc G: 14.849% 
LR: 2.000e-04 

2023-03-02 02:00:27,713 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3448 (0.3622) Acc D Real: 63.063% 
Loss D Fake: 0.6487 (0.6472) Acc D Fake: 85.105% 
Loss D: 0.994 
Loss G: 0.7432 (0.7452) Acc G: 14.851% 
LR: 2.000e-04 

2023-03-02 02:00:27,720 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.4239 (0.3627) Acc D Real: 62.981% 
Loss D Fake: 0.6488 (0.6472) Acc D Fake: 85.104% 
Loss D: 1.073 
Loss G: 0.7427 (0.7452) Acc G: 14.852% 
LR: 2.000e-04 

2023-03-02 02:00:27,728 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3286 (0.3624) Acc D Real: 63.007% 
Loss D Fake: 0.6492 (0.6472) Acc D Fake: 85.103% 
Loss D: 0.978 
Loss G: 0.7426 (0.7452) Acc G: 14.853% 
LR: 2.000e-04 

2023-03-02 02:00:27,736 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.4498 (0.3632) Acc D Real: 62.912% 
Loss D Fake: 0.6494 (0.6473) Acc D Fake: 85.102% 
Loss D: 1.099 
Loss G: 0.7418 (0.7452) Acc G: 14.855% 
LR: 2.000e-04 

2023-03-02 02:00:27,744 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.5587 (0.3649) Acc D Real: 62.692% 
Loss D Fake: 0.6508 (0.6473) Acc D Fake: 85.101% 
Loss D: 1.209 
Loss G: 0.7393 (0.7451) Acc G: 14.856% 
LR: 2.000e-04 

2023-03-02 02:00:27,751 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3665 (0.3649) Acc D Real: 62.691% 
Loss D Fake: 0.6532 (0.6473) Acc D Fake: 85.101% 
Loss D: 1.020 
Loss G: 0.7369 (0.7450) Acc G: 14.857% 
LR: 2.000e-04 

2023-03-02 02:00:27,759 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3382 (0.3647) Acc D Real: 62.706% 
Loss D Fake: 0.6550 (0.6474) Acc D Fake: 85.100% 
Loss D: 0.993 
Loss G: 0.7353 (0.7450) Acc G: 14.858% 
LR: 2.000e-04 

2023-03-02 02:00:27,767 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3729 (0.3647) Acc D Real: 62.687% 
Loss D Fake: 0.6564 (0.6475) Acc D Fake: 85.099% 
Loss D: 1.029 
Loss G: 0.7338 (0.7449) Acc G: 14.859% 
LR: 2.000e-04 

2023-03-02 02:00:27,774 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2975 (0.3642) Acc D Real: 62.747% 
Loss D Fake: 0.6574 (0.6476) Acc D Fake: 85.098% 
Loss D: 0.955 
Loss G: 0.7332 (0.7448) Acc G: 14.861% 
LR: 2.000e-04 

2023-03-02 02:00:27,782 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3137 (0.3638) Acc D Real: 62.789% 
Loss D Fake: 0.6577 (0.6476) Acc D Fake: 85.097% 
Loss D: 0.971 
Loss G: 0.7332 (0.7447) Acc G: 14.862% 
LR: 2.000e-04 

2023-03-02 02:00:27,789 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3983 (0.3640) Acc D Real: 62.738% 
Loss D Fake: 0.6576 (0.6477) Acc D Fake: 85.097% 
Loss D: 1.056 
Loss G: 0.7330 (0.7446) Acc G: 14.863% 
LR: 2.000e-04 

2023-03-02 02:00:27,797 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3320 (0.3638) Acc D Real: 62.757% 
Loss D Fake: 0.6577 (0.6478) Acc D Fake: 85.096% 
Loss D: 0.990 
Loss G: 0.7331 (0.7445) Acc G: 14.864% 
LR: 2.000e-04 

2023-03-02 02:00:27,805 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.4001 (0.3641) Acc D Real: 62.713% 
Loss D Fake: 0.6577 (0.6479) Acc D Fake: 85.095% 
Loss D: 1.058 
Loss G: 0.7331 (0.7444) Acc G: 14.865% 
LR: 2.000e-04 

2023-03-02 02:00:27,812 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3171 (0.3637) Acc D Real: 62.746% 
Loss D Fake: 0.6577 (0.6480) Acc D Fake: 85.094% 
Loss D: 0.975 
Loss G: 0.7333 (0.7443) Acc G: 14.866% 
LR: 2.000e-04 

2023-03-02 02:00:27,820 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3464 (0.3636) Acc D Real: 62.758% 
Loss D Fake: 0.6572 (0.6480) Acc D Fake: 85.094% 
Loss D: 1.004 
Loss G: 0.7339 (0.7442) Acc G: 14.867% 
LR: 2.000e-04 

2023-03-02 02:00:27,828 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4347 (0.3641) Acc D Real: 62.669% 
Loss D Fake: 0.6570 (0.6481) Acc D Fake: 85.093% 
Loss D: 1.092 
Loss G: 0.7336 (0.7441) Acc G: 14.868% 
LR: 2.000e-04 

2023-03-02 02:00:27,836 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2583 (0.3633) Acc D Real: 62.766% 
Loss D Fake: 0.6571 (0.6482) Acc D Fake: 85.092% 
Loss D: 0.915 
Loss G: 0.7342 (0.7441) Acc G: 14.869% 
LR: 2.000e-04 

2023-03-02 02:00:27,843 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4142 (0.3637) Acc D Real: 62.705% 
Loss D Fake: 0.6565 (0.6482) Acc D Fake: 85.091% 
Loss D: 1.071 
Loss G: 0.7345 (0.7440) Acc G: 14.870% 
LR: 2.000e-04 

2023-03-02 02:00:27,851 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2939 (0.3632) Acc D Real: 62.758% 
Loss D Fake: 0.6562 (0.6483) Acc D Fake: 85.091% 
Loss D: 0.950 
Loss G: 0.7351 (0.7439) Acc G: 14.871% 
LR: 2.000e-04 

2023-03-02 02:00:27,859 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3257 (0.3629) Acc D Real: 62.784% 
Loss D Fake: 0.6554 (0.6484) Acc D Fake: 85.090% 
Loss D: 0.981 
Loss G: 0.7362 (0.7439) Acc G: 14.872% 
LR: 2.000e-04 

2023-03-02 02:00:27,866 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3159 (0.3625) Acc D Real: 62.819% 
Loss D Fake: 0.6543 (0.6484) Acc D Fake: 85.089% 
Loss D: 0.970 
Loss G: 0.7376 (0.7438) Acc G: 14.873% 
LR: 2.000e-04 

2023-03-02 02:00:27,874 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4242 (0.3630) Acc D Real: 62.747% 
Loss D Fake: 0.6532 (0.6484) Acc D Fake: 85.089% 
Loss D: 1.077 
Loss G: 0.7384 (0.7438) Acc G: 14.874% 
LR: 2.000e-04 

2023-03-02 02:00:27,881 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3855 (0.3631) Acc D Real: 62.724% 
Loss D Fake: 0.6528 (0.6485) Acc D Fake: 85.088% 
Loss D: 1.038 
Loss G: 0.7387 (0.7437) Acc G: 14.875% 
LR: 2.000e-04 

2023-03-02 02:00:27,889 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3428 (0.3630) Acc D Real: 62.738% 
Loss D Fake: 0.6525 (0.6485) Acc D Fake: 85.087% 
Loss D: 0.995 
Loss G: 0.7391 (0.7437) Acc G: 14.876% 
LR: 2.000e-04 

2023-03-02 02:00:27,896 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.2692 (0.3623) Acc D Real: 62.820% 
Loss D Fake: 0.6519 (0.6485) Acc D Fake: 85.087% 
Loss D: 0.921 
Loss G: 0.7403 (0.7437) Acc G: 14.877% 
LR: 2.000e-04 

2023-03-02 02:00:27,904 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2709 (0.3616) Acc D Real: 62.891% 
Loss D Fake: 0.6504 (0.6485) Acc D Fake: 85.086% 
Loss D: 0.921 
Loss G: 0.7423 (0.7437) Acc G: 14.878% 
LR: 2.000e-04 

2023-03-02 02:00:27,912 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3838 (0.3618) Acc D Real: 62.860% 
Loss D Fake: 0.6486 (0.6485) Acc D Fake: 85.085% 
Loss D: 1.032 
Loss G: 0.7441 (0.7437) Acc G: 14.879% 
LR: 2.000e-04 

2023-03-02 02:00:27,919 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3471 (0.3617) Acc D Real: 62.867% 
Loss D Fake: 0.6472 (0.6485) Acc D Fake: 85.085% 
Loss D: 0.994 
Loss G: 0.7455 (0.7437) Acc G: 14.879% 
LR: 2.000e-04 

2023-03-02 02:00:27,927 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3605 (0.3617) Acc D Real: 62.856% 
Loss D Fake: 0.6461 (0.6485) Acc D Fake: 85.084% 
Loss D: 1.007 
Loss G: 0.7466 (0.7437) Acc G: 14.880% 
LR: 2.000e-04 

2023-03-02 02:00:27,935 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3642 (0.3617) Acc D Real: 62.844% 
Loss D Fake: 0.6453 (0.6485) Acc D Fake: 85.084% 
Loss D: 1.009 
Loss G: 0.7474 (0.7437) Acc G: 14.881% 
LR: 2.000e-04 

2023-03-02 02:00:27,942 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3661 (0.3617) Acc D Real: 62.837% 
Loss D Fake: 0.6447 (0.6485) Acc D Fake: 85.083% 
Loss D: 1.011 
Loss G: 0.7480 (0.7438) Acc G: 14.882% 
LR: 2.000e-04 

2023-03-02 02:00:27,950 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3453 (0.3616) Acc D Real: 62.842% 
Loss D Fake: 0.6441 (0.6484) Acc D Fake: 85.082% 
Loss D: 0.989 
Loss G: 0.7488 (0.7438) Acc G: 14.883% 
LR: 2.000e-04 

2023-03-02 02:00:27,957 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2972 (0.3612) Acc D Real: 62.892% 
Loss D Fake: 0.6433 (0.6484) Acc D Fake: 85.082% 
Loss D: 0.941 
Loss G: 0.7500 (0.7438) Acc G: 14.884% 
LR: 2.000e-04 

2023-03-02 02:00:27,965 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2816 (0.3606) Acc D Real: 62.958% 
Loss D Fake: 0.6420 (0.6484) Acc D Fake: 85.081% 
Loss D: 0.924 
Loss G: 0.7518 (0.7439) Acc G: 14.884% 
LR: 2.000e-04 

2023-03-02 02:00:27,972 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.4506 (0.3612) Acc D Real: 62.872% 
Loss D Fake: 0.6406 (0.6483) Acc D Fake: 85.081% 
Loss D: 1.091 
Loss G: 0.7527 (0.7440) Acc G: 14.885% 
LR: 2.000e-04 

2023-03-02 02:00:27,980 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3712 (0.3613) Acc D Real: 62.858% 
Loss D Fake: 0.6402 (0.6482) Acc D Fake: 85.080% 
Loss D: 1.011 
Loss G: 0.7530 (0.7440) Acc G: 14.886% 
LR: 2.000e-04 

2023-03-02 02:00:27,987 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3891 (0.3615) Acc D Real: 62.829% 
Loss D Fake: 0.6400 (0.6482) Acc D Fake: 85.080% 
Loss D: 1.029 
Loss G: 0.7531 (0.7441) Acc G: 14.887% 
LR: 2.000e-04 

2023-03-02 02:00:27,995 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3746 (0.3616) Acc D Real: 62.816% 
Loss D Fake: 0.6400 (0.6481) Acc D Fake: 85.079% 
Loss D: 1.015 
Loss G: 0.7531 (0.7441) Acc G: 14.888% 
LR: 2.000e-04 

2023-03-02 02:00:28,002 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2826 (0.3611) Acc D Real: 62.879% 
Loss D Fake: 0.6399 (0.6481) Acc D Fake: 85.079% 
Loss D: 0.922 
Loss G: 0.7536 (0.7442) Acc G: 14.888% 
LR: 2.000e-04 

2023-03-02 02:00:28,010 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3217 (0.3608) Acc D Real: 62.909% 
Loss D Fake: 0.6391 (0.6480) Acc D Fake: 85.078% 
Loss D: 0.961 
Loss G: 0.7546 (0.7443) Acc G: 14.889% 
LR: 2.000e-04 

2023-03-02 02:00:28,017 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4557 (0.3614) Acc D Real: 62.830% 
Loss D Fake: 0.6385 (0.6480) Acc D Fake: 85.078% 
Loss D: 1.094 
Loss G: 0.7548 (0.7443) Acc G: 14.890% 
LR: 2.000e-04 

2023-03-02 02:00:28,024 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3631 (0.3614) Acc D Real: 62.820% 
Loss D Fake: 0.6385 (0.6479) Acc D Fake: 85.077% 
Loss D: 1.002 
Loss G: 0.7547 (0.7444) Acc G: 14.890% 
LR: 2.000e-04 

2023-03-02 02:00:28,032 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4050 (0.3617) Acc D Real: 62.774% 
Loss D Fake: 0.6388 (0.6478) Acc D Fake: 85.077% 
Loss D: 1.044 
Loss G: 0.7542 (0.7445) Acc G: 14.891% 
LR: 2.000e-04 

2023-03-02 02:00:28,039 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4276 (0.3621) Acc D Real: 62.711% 
Loss D Fake: 0.6395 (0.6478) Acc D Fake: 85.076% 
Loss D: 1.067 
Loss G: 0.7530 (0.7445) Acc G: 14.892% 
LR: 2.000e-04 

2023-03-02 02:00:28,046 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.2757 (0.3616) Acc D Real: 62.771% 
Loss D Fake: 0.6404 (0.6477) Acc D Fake: 85.076% 
Loss D: 0.916 
Loss G: 0.7526 (0.7446) Acc G: 14.893% 
LR: 2.000e-04 

2023-03-02 02:00:28,053 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2812 (0.3611) Acc D Real: 62.775% 
Loss D Fake: 0.6403 (0.6477) Acc D Fake: 85.076% 
Loss D: 0.922 
Loss G: 0.7530 (0.7446) Acc G: 14.893% 
LR: 2.000e-04 

2023-03-02 02:00:28,281 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.339 | Generator Loss: 0.753 | Avg: 2.092 
2023-03-02 02:00:28,304 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.222 | Generator Loss: 0.753 | Avg: 1.975 
2023-03-02 02:00:28,327 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.255 | Generator Loss: 0.753 | Avg: 2.008 
2023-03-02 02:00:28,357 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.281 | Generator Loss: 0.753 | Avg: 2.034 
2023-03-02 02:00:28,386 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.282 | Generator Loss: 0.753 | Avg: 2.035 
2023-03-02 02:00:28,413 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.232 | Generator Loss: 0.753 | Avg: 1.985 
2023-03-02 02:00:28,440 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.198 | Generator Loss: 0.753 | Avg: 1.951 
2023-03-02 02:00:28,467 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.150 | Generator Loss: 0.753 | Avg: 1.903 
2023-03-02 02:00:28,494 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.120 | Generator Loss: 0.753 | Avg: 1.872 
2023-03-02 02:00:28,521 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.077 | Generator Loss: 0.753 | Avg: 1.830 
2023-03-02 02:00:28,547 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.043 | Generator Loss: 0.753 | Avg: 1.796 
2023-03-02 02:00:28,574 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.013 | Generator Loss: 0.753 | Avg: 1.766 
2023-03-02 02:00:28,600 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.989 | Generator Loss: 0.753 | Avg: 1.742 
2023-03-02 02:00:28,628 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.969 | Generator Loss: 0.753 | Avg: 1.722 
2023-03-02 02:00:28,654 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.985 | Generator Loss: 0.753 | Avg: 1.738 
2023-03-02 02:00:28,681 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.001 | Generator Loss: 0.753 | Avg: 1.754 
2023-03-02 02:00:28,707 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.017 | Generator Loss: 0.753 | Avg: 1.770 
2023-03-02 02:00:28,741 -                train: [    INFO] - 
Epoch: 14/20
2023-03-02 02:00:28,923 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3609 (0.3702) Acc D Real: 60.833% 
Loss D Fake: 0.6397 (0.6398) Acc D Fake: 85.000% 
Loss D: 1.001 
Loss G: 0.7535 (0.7534) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,931 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3156 (0.3520) Acc D Real: 62.778% 
Loss D Fake: 0.6394 (0.6397) Acc D Fake: 85.000% 
Loss D: 0.955 
Loss G: 0.7540 (0.7536) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,938 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2564 (0.3281) Acc D Real: 65.651% 
Loss D Fake: 0.6388 (0.6395) Acc D Fake: 85.000% 
Loss D: 0.895 
Loss G: 0.7552 (0.7540) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,959 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3878 (0.3400) Acc D Real: 64.365% 
Loss D Fake: 0.6376 (0.6391) Acc D Fake: 85.000% 
Loss D: 1.025 
Loss G: 0.7563 (0.7545) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,966 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3825 (0.3471) Acc D Real: 63.377% 
Loss D Fake: 0.6370 (0.6387) Acc D Fake: 85.000% 
Loss D: 1.019 
Loss G: 0.7568 (0.7549) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,973 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.2976 (0.3400) Acc D Real: 64.286% 
Loss D Fake: 0.6365 (0.6384) Acc D Fake: 85.000% 
Loss D: 0.934 
Loss G: 0.7576 (0.7553) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,980 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4533 (0.3542) Acc D Real: 62.546% 
Loss D Fake: 0.6360 (0.6381) Acc D Fake: 85.000% 
Loss D: 1.089 
Loss G: 0.7577 (0.7556) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,987 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.2071 (0.3378) Acc D Real: 64.554% 
Loss D Fake: 0.6358 (0.6379) Acc D Fake: 85.000% 
Loss D: 0.843 
Loss G: 0.7587 (0.7559) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:28,994 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3376 (0.3378) Acc D Real: 64.427% 
Loss D Fake: 0.6347 (0.6375) Acc D Fake: 85.000% 
Loss D: 0.972 
Loss G: 0.7599 (0.7563) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,001 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3386 (0.3379) Acc D Real: 64.470% 
Loss D Fake: 0.6336 (0.6372) Acc D Fake: 85.000% 
Loss D: 0.972 
Loss G: 0.7612 (0.7568) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,008 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4165 (0.3444) Acc D Real: 63.728% 
Loss D Fake: 0.6327 (0.6368) Acc D Fake: 85.000% 
Loss D: 1.049 
Loss G: 0.7617 (0.7572) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,015 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3474 (0.3447) Acc D Real: 63.722% 
Loss D Fake: 0.6325 (0.6365) Acc D Fake: 85.000% 
Loss D: 0.980 
Loss G: 0.7620 (0.7575) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,022 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3012 (0.3416) Acc D Real: 64.126% 
Loss D Fake: 0.6321 (0.6362) Acc D Fake: 85.000% 
Loss D: 0.933 
Loss G: 0.7627 (0.7579) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,029 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4768 (0.3506) Acc D Real: 63.045% 
Loss D Fake: 0.6318 (0.6359) Acc D Fake: 85.000% 
Loss D: 1.109 
Loss G: 0.7623 (0.7582) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,036 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3687 (0.3517) Acc D Real: 62.923% 
Loss D Fake: 0.6324 (0.6357) Acc D Fake: 85.000% 
Loss D: 1.001 
Loss G: 0.7616 (0.7584) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,043 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3662 (0.3526) Acc D Real: 62.809% 
Loss D Fake: 0.6331 (0.6355) Acc D Fake: 85.000% 
Loss D: 0.999 
Loss G: 0.7608 (0.7585) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,050 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.2835 (0.3487) Acc D Real: 63.319% 
Loss D Fake: 0.6335 (0.6354) Acc D Fake: 85.000% 
Loss D: 0.917 
Loss G: 0.7608 (0.7587) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,057 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2270 (0.3423) Acc D Real: 64.093% 
Loss D Fake: 0.6330 (0.6353) Acc D Fake: 85.000% 
Loss D: 0.860 
Loss G: 0.7620 (0.7588) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,064 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4083 (0.3456) Acc D Real: 63.669% 
Loss D Fake: 0.6319 (0.6351) Acc D Fake: 85.000% 
Loss D: 1.040 
Loss G: 0.7628 (0.7590) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,071 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3442 (0.3456) Acc D Real: 63.700% 
Loss D Fake: 0.6315 (0.6349) Acc D Fake: 85.000% 
Loss D: 0.976 
Loss G: 0.7633 (0.7592) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,078 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3267 (0.3447) Acc D Real: 63.866% 
Loss D Fake: 0.6310 (0.6347) Acc D Fake: 85.000% 
Loss D: 0.958 
Loss G: 0.7641 (0.7595) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,085 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3194 (0.3436) Acc D Real: 64.008% 
Loss D Fake: 0.6302 (0.6346) Acc D Fake: 85.000% 
Loss D: 0.950 
Loss G: 0.7651 (0.7597) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,092 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.2343 (0.3390) Acc D Real: 64.575% 
Loss D Fake: 0.6291 (0.6343) Acc D Fake: 85.000% 
Loss D: 0.863 
Loss G: 0.7670 (0.7600) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,099 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3502 (0.3395) Acc D Real: 64.538% 
Loss D Fake: 0.6273 (0.6340) Acc D Fake: 85.000% 
Loss D: 0.978 
Loss G: 0.7689 (0.7604) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,106 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3673 (0.3406) Acc D Real: 64.391% 
Loss D Fake: 0.6259 (0.6337) Acc D Fake: 85.000% 
Loss D: 0.993 
Loss G: 0.7703 (0.7608) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,113 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.2831 (0.3384) Acc D Real: 64.668% 
Loss D Fake: 0.6247 (0.6334) Acc D Fake: 85.000% 
Loss D: 0.908 
Loss G: 0.7722 (0.7612) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,120 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4141 (0.3411) Acc D Real: 64.345% 
Loss D Fake: 0.6233 (0.6330) Acc D Fake: 85.000% 
Loss D: 1.037 
Loss G: 0.7733 (0.7616) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,128 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3892 (0.3428) Acc D Real: 64.161% 
Loss D Fake: 0.6226 (0.6327) Acc D Fake: 85.000% 
Loss D: 1.012 
Loss G: 0.7738 (0.7620) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,135 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3249 (0.3422) Acc D Real: 64.224% 
Loss D Fake: 0.6224 (0.6323) Acc D Fake: 85.000% 
Loss D: 0.947 
Loss G: 0.7742 (0.7624) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,143 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3957 (0.3439) Acc D Real: 64.017% 
Loss D Fake: 0.6221 (0.6320) Acc D Fake: 85.000% 
Loss D: 1.018 
Loss G: 0.7742 (0.7628) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,150 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3921 (0.3454) Acc D Real: 63.813% 
Loss D Fake: 0.6223 (0.6317) Acc D Fake: 85.000% 
Loss D: 1.014 
Loss G: 0.7738 (0.7632) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,158 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3055 (0.3442) Acc D Real: 63.971% 
Loss D Fake: 0.6226 (0.6314) Acc D Fake: 85.000% 
Loss D: 0.928 
Loss G: 0.7737 (0.7635) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,165 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3148 (0.3433) Acc D Real: 64.107% 
Loss D Fake: 0.6225 (0.6312) Acc D Fake: 85.000% 
Loss D: 0.937 
Loss G: 0.7741 (0.7638) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,172 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2342 (0.3402) Acc D Real: 64.497% 
Loss D Fake: 0.6217 (0.6309) Acc D Fake: 85.000% 
Loss D: 0.856 
Loss G: 0.7756 (0.7641) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,180 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3979 (0.3418) Acc D Real: 64.372% 
Loss D Fake: 0.6204 (0.6306) Acc D Fake: 85.000% 
Loss D: 1.018 
Loss G: 0.7768 (0.7645) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,187 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4137 (0.3438) Acc D Real: 64.131% 
Loss D Fake: 0.6198 (0.6303) Acc D Fake: 85.000% 
Loss D: 1.033 
Loss G: 0.7771 (0.7648) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,195 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3220 (0.3432) Acc D Real: 64.221% 
Loss D Fake: 0.6197 (0.6300) Acc D Fake: 85.000% 
Loss D: 0.942 
Loss G: 0.7774 (0.7652) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,202 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.4420 (0.3457) Acc D Real: 63.904% 
Loss D Fake: 0.6196 (0.6298) Acc D Fake: 85.000% 
Loss D: 1.062 
Loss G: 0.7769 (0.7655) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,210 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.5087 (0.3498) Acc D Real: 63.439% 
Loss D Fake: 0.6207 (0.6295) Acc D Fake: 85.000% 
Loss D: 1.129 
Loss G: 0.7748 (0.7657) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,217 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3085 (0.3488) Acc D Real: 63.580% 
Loss D Fake: 0.6225 (0.6294) Acc D Fake: 85.000% 
Loss D: 0.931 
Loss G: 0.7730 (0.7659) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,225 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3930 (0.3499) Acc D Real: 63.439% 
Loss D Fake: 0.6238 (0.6292) Acc D Fake: 85.000% 
Loss D: 1.017 
Loss G: 0.7713 (0.7660) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,232 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3691 (0.3503) Acc D Real: 63.391% 
Loss D Fake: 0.6253 (0.6291) Acc D Fake: 85.000% 
Loss D: 0.994 
Loss G: 0.7696 (0.7661) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,240 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2809 (0.3487) Acc D Real: 63.584% 
Loss D Fake: 0.6265 (0.6291) Acc D Fake: 85.000% 
Loss D: 0.907 
Loss G: 0.7687 (0.7661) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,248 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.4169 (0.3502) Acc D Real: 63.411% 
Loss D Fake: 0.6271 (0.6290) Acc D Fake: 85.000% 
Loss D: 1.044 
Loss G: 0.7677 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,255 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3498 (0.3502) Acc D Real: 63.423% 
Loss D Fake: 0.6281 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.978 
Loss G: 0.7667 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,262 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3447 (0.3501) Acc D Real: 63.431% 
Loss D Fake: 0.6288 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.973 
Loss G: 0.7660 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,270 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2749 (0.3485) Acc D Real: 63.630% 
Loss D Fake: 0.6291 (0.6290) Acc D Fake: 85.000% 
Loss D: 0.904 
Loss G: 0.7661 (0.7662) Acc G: 15.000% 
LR: 2.000e-04 

2023-03-02 02:00:29,277 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3504 (0.3486) Acc D Real: 63.658% 
Loss D Fake: 0.6288 (0.6290) Acc D Fake: 84.971% 
Loss D: 0.979 
Loss G: 0.7665 (0.7662) Acc G: 15.033% 
LR: 2.000e-04 

2023-03-02 02:00:29,285 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3051 (0.3477) Acc D Real: 63.780% 
Loss D Fake: 0.6283 (0.6290) Acc D Fake: 84.939% 
Loss D: 0.933 
Loss G: 0.7674 (0.7662) Acc G: 15.066% 
LR: 2.000e-04 

2023-03-02 02:00:29,292 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3279 (0.3473) Acc D Real: 63.841% 
Loss D Fake: 0.6274 (0.6290) Acc D Fake: 84.907% 
Loss D: 0.955 
Loss G: 0.7686 (0.7663) Acc G: 15.097% 
LR: 2.000e-04 

2023-03-02 02:00:29,300 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4334 (0.3490) Acc D Real: 63.632% 
Loss D Fake: 0.6266 (0.6289) Acc D Fake: 84.877% 
Loss D: 1.060 
Loss G: 0.7689 (0.7663) Acc G: 15.127% 
LR: 2.000e-04 

2023-03-02 02:00:29,307 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3259 (0.3485) Acc D Real: 63.683% 
Loss D Fake: 0.6266 (0.6289) Acc D Fake: 84.848% 
Loss D: 0.952 
Loss G: 0.7691 (0.7664) Acc G: 15.156% 
LR: 2.000e-04 

2023-03-02 02:00:29,315 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.4352 (0.3502) Acc D Real: 63.487% 
Loss D Fake: 0.6265 (0.6288) Acc D Fake: 84.820% 
Loss D: 1.062 
Loss G: 0.7687 (0.7664) Acc G: 15.184% 
LR: 2.000e-04 

2023-03-02 02:00:29,322 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2761 (0.3488) Acc D Real: 63.658% 
Loss D Fake: 0.6269 (0.6288) Acc D Fake: 84.793% 
Loss D: 0.903 
Loss G: 0.7687 (0.7664) Acc G: 15.211% 
LR: 2.000e-04 

2023-03-02 02:00:29,329 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4332 (0.3503) Acc D Real: 63.461% 
Loss D Fake: 0.6269 (0.6288) Acc D Fake: 84.767% 
Loss D: 1.060 
Loss G: 0.7683 (0.7665) Acc G: 15.237% 
LR: 2.000e-04 

2023-03-02 02:00:29,337 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3274 (0.3499) Acc D Real: 63.527% 
Loss D Fake: 0.6273 (0.6287) Acc D Fake: 84.741% 
Loss D: 0.955 
Loss G: 0.7680 (0.7665) Acc G: 15.262% 
LR: 2.000e-04 

2023-03-02 02:00:29,344 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3273 (0.3495) Acc D Real: 63.583% 
Loss D Fake: 0.6273 (0.6287) Acc D Fake: 84.717% 
Loss D: 0.955 
Loss G: 0.7682 (0.7665) Acc G: 15.286% 
LR: 2.000e-04 

2023-03-02 02:00:29,352 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3397 (0.3494) Acc D Real: 63.589% 
Loss D Fake: 0.6271 (0.6287) Acc D Fake: 84.694% 
Loss D: 0.967 
Loss G: 0.7685 (0.7666) Acc G: 15.310% 
LR: 2.000e-04 

2023-03-02 02:00:29,360 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.4343 (0.3508) Acc D Real: 63.411% 
Loss D Fake: 0.6271 (0.6287) Acc D Fake: 84.671% 
Loss D: 1.061 
Loss G: 0.7680 (0.7666) Acc G: 15.332% 
LR: 2.000e-04 

2023-03-02 02:00:29,367 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4091 (0.3517) Acc D Real: 63.311% 
Loss D Fake: 0.6278 (0.6286) Acc D Fake: 84.649% 
Loss D: 1.037 
Loss G: 0.7668 (0.7666) Acc G: 15.354% 
LR: 2.000e-04 

2023-03-02 02:00:29,375 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2302 (0.3498) Acc D Real: 63.543% 
Loss D Fake: 0.6286 (0.6286) Acc D Fake: 84.628% 
Loss D: 0.859 
Loss G: 0.7668 (0.7666) Acc G: 15.376% 
LR: 2.000e-04 

2023-03-02 02:00:29,382 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3635 (0.3500) Acc D Real: 63.514% 
Loss D Fake: 0.6283 (0.6286) Acc D Fake: 84.607% 
Loss D: 0.992 
Loss G: 0.7670 (0.7666) Acc G: 15.396% 
LR: 2.000e-04 

2023-03-02 02:00:29,390 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3895 (0.3506) Acc D Real: 63.429% 
Loss D Fake: 0.6283 (0.6286) Acc D Fake: 84.587% 
Loss D: 1.018 
Loss G: 0.7668 (0.7666) Acc G: 15.416% 
LR: 2.000e-04 

2023-03-02 02:00:29,397 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4309 (0.3518) Acc D Real: 63.276% 
Loss D Fake: 0.6288 (0.6286) Acc D Fake: 84.568% 
Loss D: 1.060 
Loss G: 0.7658 (0.7666) Acc G: 15.435% 
LR: 2.000e-04 

2023-03-02 02:00:29,405 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3204 (0.3514) Acc D Real: 63.329% 
Loss D Fake: 0.6297 (0.6286) Acc D Fake: 84.549% 
Loss D: 0.950 
Loss G: 0.7650 (0.7666) Acc G: 15.454% 
LR: 2.000e-04 

2023-03-02 02:00:29,412 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2593 (0.3500) Acc D Real: 63.497% 
Loss D Fake: 0.6300 (0.6287) Acc D Fake: 84.531% 
Loss D: 0.889 
Loss G: 0.7652 (0.7665) Acc G: 15.472% 
LR: 2.000e-04 

2023-03-02 02:00:29,420 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4304 (0.3512) Acc D Real: 63.338% 
Loss D Fake: 0.6297 (0.6287) Acc D Fake: 84.514% 
Loss D: 1.060 
Loss G: 0.7649 (0.7665) Acc G: 15.489% 
LR: 2.000e-04 

2023-03-02 02:00:29,428 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3626 (0.3513) Acc D Real: 63.305% 
Loss D Fake: 0.6302 (0.6287) Acc D Fake: 84.497% 
Loss D: 0.993 
Loss G: 0.7644 (0.7665) Acc G: 15.506% 
LR: 2.000e-04 

2023-03-02 02:00:29,435 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2670 (0.3501) Acc D Real: 63.465% 
Loss D Fake: 0.6304 (0.6287) Acc D Fake: 84.480% 
Loss D: 0.897 
Loss G: 0.7647 (0.7665) Acc G: 15.523% 
LR: 2.000e-04 

2023-03-02 02:00:29,443 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3404 (0.3500) Acc D Real: 63.477% 
Loss D Fake: 0.6299 (0.6287) Acc D Fake: 84.464% 
Loss D: 0.970 
Loss G: 0.7654 (0.7665) Acc G: 15.539% 
LR: 2.000e-04 

2023-03-02 02:00:29,450 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.4122 (0.3509) Acc D Real: 63.373% 
Loss D Fake: 0.6295 (0.6288) Acc D Fake: 84.448% 
Loss D: 1.042 
Loss G: 0.7655 (0.7664) Acc G: 15.555% 
LR: 2.000e-04 

2023-03-02 02:00:29,458 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3880 (0.3514) Acc D Real: 63.318% 
Loss D Fake: 0.6296 (0.6288) Acc D Fake: 84.433% 
Loss D: 1.018 
Loss G: 0.7651 (0.7664) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 02:00:29,466 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2808 (0.3504) Acc D Real: 63.440% 
Loss D Fake: 0.6298 (0.6288) Acc D Fake: 84.418% 
Loss D: 0.911 
Loss G: 0.7654 (0.7664) Acc G: 15.585% 
LR: 2.000e-04 

2023-03-02 02:00:29,473 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3676 (0.3506) Acc D Real: 63.412% 
Loss D Fake: 0.6294 (0.6288) Acc D Fake: 84.403% 
Loss D: 0.997 
Loss G: 0.7658 (0.7664) Acc G: 15.599% 
LR: 2.000e-04 

2023-03-02 02:00:29,481 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4320 (0.3517) Acc D Real: 63.278% 
Loss D Fake: 0.6294 (0.6288) Acc D Fake: 84.389% 
Loss D: 1.061 
Loss G: 0.7653 (0.7664) Acc G: 15.613% 
LR: 2.000e-04 

2023-03-02 02:00:29,489 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3477 (0.3517) Acc D Real: 63.282% 
Loss D Fake: 0.6300 (0.6288) Acc D Fake: 84.376% 
Loss D: 0.978 
Loss G: 0.7646 (0.7664) Acc G: 15.627% 
LR: 2.000e-04 

2023-03-02 02:00:29,497 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2937 (0.3509) Acc D Real: 63.365% 
Loss D Fake: 0.6304 (0.6288) Acc D Fake: 84.371% 
Loss D: 0.924 
Loss G: 0.7646 (0.7663) Acc G: 15.622% 
LR: 2.000e-04 

2023-03-02 02:00:29,505 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3275 (0.3506) Acc D Real: 63.408% 
Loss D Fake: 0.6302 (0.6289) Acc D Fake: 84.379% 
Loss D: 0.958 
Loss G: 0.7649 (0.7663) Acc G: 15.614% 
LR: 2.000e-04 

2023-03-02 02:00:29,512 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4051 (0.3513) Acc D Real: 63.305% 
Loss D Fake: 0.6300 (0.6289) Acc D Fake: 84.387% 
Loss D: 1.035 
Loss G: 0.7648 (0.7663) Acc G: 15.606% 
LR: 2.000e-04 

2023-03-02 02:00:29,519 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4048 (0.3520) Acc D Real: 63.216% 
Loss D Fake: 0.6304 (0.6289) Acc D Fake: 84.394% 
Loss D: 1.035 
Loss G: 0.7640 (0.7663) Acc G: 15.599% 
LR: 2.000e-04 

2023-03-02 02:00:29,527 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2742 (0.3510) Acc D Real: 63.324% 
Loss D Fake: 0.6310 (0.6289) Acc D Fake: 84.402% 
Loss D: 0.905 
Loss G: 0.7638 (0.7662) Acc G: 15.591% 
LR: 2.000e-04 

2023-03-02 02:00:29,534 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3189 (0.3506) Acc D Real: 63.379% 
Loss D Fake: 0.6309 (0.6289) Acc D Fake: 84.409% 
Loss D: 0.950 
Loss G: 0.7642 (0.7662) Acc G: 15.584% 
LR: 2.000e-04 

2023-03-02 02:00:29,542 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3632 (0.3508) Acc D Real: 63.361% 
Loss D Fake: 0.6305 (0.6290) Acc D Fake: 84.416% 
Loss D: 0.994 
Loss G: 0.7646 (0.7662) Acc G: 15.577% 
LR: 2.000e-04 

2023-03-02 02:00:29,549 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3699 (0.3510) Acc D Real: 63.338% 
Loss D Fake: 0.6302 (0.6290) Acc D Fake: 84.423% 
Loss D: 1.000 
Loss G: 0.7648 (0.7662) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 02:00:29,556 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3630 (0.3511) Acc D Real: 63.314% 
Loss D Fake: 0.6302 (0.6290) Acc D Fake: 84.430% 
Loss D: 0.993 
Loss G: 0.7647 (0.7662) Acc G: 15.564% 
LR: 2.000e-04 

2023-03-02 02:00:29,564 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2499 (0.3500) Acc D Real: 63.461% 
Loss D Fake: 0.6300 (0.6290) Acc D Fake: 84.436% 
Loss D: 0.880 
Loss G: 0.7655 (0.7662) Acc G: 15.557% 
LR: 2.000e-04 

2023-03-02 02:00:29,571 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3035 (0.3494) Acc D Real: 63.528% 
Loss D Fake: 0.6290 (0.6290) Acc D Fake: 84.442% 
Loss D: 0.932 
Loss G: 0.7669 (0.7662) Acc G: 15.551% 
LR: 2.000e-04 

2023-03-02 02:00:29,579 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2558 (0.3484) Acc D Real: 63.658% 
Loss D Fake: 0.6277 (0.6290) Acc D Fake: 84.449% 
Loss D: 0.884 
Loss G: 0.7689 (0.7662) Acc G: 15.545% 
LR: 2.000e-04 

2023-03-02 02:00:29,586 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3715 (0.3487) Acc D Real: 63.628% 
Loss D Fake: 0.6260 (0.6289) Acc D Fake: 84.455% 
Loss D: 0.997 
Loss G: 0.7707 (0.7662) Acc G: 15.539% 
LR: 2.000e-04 

2023-03-02 02:00:29,594 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3637 (0.3488) Acc D Real: 63.594% 
Loss D Fake: 0.6247 (0.6289) Acc D Fake: 84.461% 
Loss D: 0.988 
Loss G: 0.7719 (0.7663) Acc G: 15.533% 
LR: 2.000e-04 

2023-03-02 02:00:29,601 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3258 (0.3486) Acc D Real: 63.629% 
Loss D Fake: 0.6238 (0.6288) Acc D Fake: 84.467% 
Loss D: 0.950 
Loss G: 0.7730 (0.7664) Acc G: 15.527% 
LR: 2.000e-04 

2023-03-02 02:00:29,608 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4357 (0.3495) Acc D Real: 63.516% 
Loss D Fake: 0.6231 (0.6288) Acc D Fake: 84.472% 
Loss D: 1.059 
Loss G: 0.7733 (0.7665) Acc G: 15.521% 
LR: 2.000e-04 

2023-03-02 02:00:29,616 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2499 (0.3484) Acc D Real: 63.642% 
Loss D Fake: 0.6229 (0.6287) Acc D Fake: 84.478% 
Loss D: 0.873 
Loss G: 0.7741 (0.7665) Acc G: 15.516% 
LR: 2.000e-04 

2023-03-02 02:00:29,624 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2602 (0.3475) Acc D Real: 63.771% 
Loss D Fake: 0.6218 (0.6286) Acc D Fake: 84.484% 
Loss D: 0.882 
Loss G: 0.7758 (0.7666) Acc G: 15.510% 
LR: 2.000e-04 

2023-03-02 02:00:29,632 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2905 (0.3469) Acc D Real: 63.866% 
Loss D Fake: 0.6201 (0.6286) Acc D Fake: 84.489% 
Loss D: 0.911 
Loss G: 0.7782 (0.7668) Acc G: 15.505% 
LR: 2.000e-04 

2023-03-02 02:00:29,640 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.4350 (0.3478) Acc D Real: 63.748% 
Loss D Fake: 0.6185 (0.6285) Acc D Fake: 84.494% 
Loss D: 1.054 
Loss G: 0.7794 (0.7669) Acc G: 15.500% 
LR: 2.000e-04 

2023-03-02 02:00:29,648 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3884 (0.3482) Acc D Real: 63.690% 
Loss D Fake: 0.6180 (0.6283) Acc D Fake: 84.499% 
Loss D: 1.006 
Loss G: 0.7797 (0.7670) Acc G: 15.495% 
LR: 2.000e-04 

2023-03-02 02:00:29,655 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4327 (0.3491) Acc D Real: 63.567% 
Loss D Fake: 0.6181 (0.6282) Acc D Fake: 84.504% 
Loss D: 1.051 
Loss G: 0.7789 (0.7671) Acc G: 15.490% 
LR: 2.000e-04 

2023-03-02 02:00:29,664 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3622 (0.3492) Acc D Real: 63.548% 
Loss D Fake: 0.6191 (0.6282) Acc D Fake: 84.509% 
Loss D: 0.981 
Loss G: 0.7777 (0.7672) Acc G: 15.485% 
LR: 2.000e-04 

2023-03-02 02:00:29,672 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4323 (0.3501) Acc D Real: 63.448% 
Loss D Fake: 0.6203 (0.6281) Acc D Fake: 84.514% 
Loss D: 1.053 
Loss G: 0.7758 (0.7673) Acc G: 15.480% 
LR: 2.000e-04 

2023-03-02 02:00:29,681 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3444 (0.3500) Acc D Real: 63.454% 
Loss D Fake: 0.6219 (0.6280) Acc D Fake: 84.519% 
Loss D: 0.966 
Loss G: 0.7741 (0.7674) Acc G: 15.475% 
LR: 2.000e-04 

2023-03-02 02:00:29,689 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3491 (0.3500) Acc D Real: 63.462% 
Loss D Fake: 0.6232 (0.6280) Acc D Fake: 84.524% 
Loss D: 0.972 
Loss G: 0.7727 (0.7674) Acc G: 15.471% 
LR: 2.000e-04 

2023-03-02 02:00:29,697 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2764 (0.3493) Acc D Real: 63.542% 
Loss D Fake: 0.6240 (0.6279) Acc D Fake: 84.528% 
Loss D: 0.900 
Loss G: 0.7722 (0.7675) Acc G: 15.466% 
LR: 2.000e-04 

2023-03-02 02:00:29,704 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2843 (0.3487) Acc D Real: 63.629% 
Loss D Fake: 0.6240 (0.6279) Acc D Fake: 84.533% 
Loss D: 0.908 
Loss G: 0.7727 (0.7675) Acc G: 15.462% 
LR: 2.000e-04 

2023-03-02 02:00:29,713 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3021 (0.3482) Acc D Real: 63.687% 
Loss D Fake: 0.6233 (0.6278) Acc D Fake: 84.537% 
Loss D: 0.925 
Loss G: 0.7738 (0.7676) Acc G: 15.457% 
LR: 2.000e-04 

2023-03-02 02:00:29,721 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3696 (0.3484) Acc D Real: 63.660% 
Loss D Fake: 0.6224 (0.6278) Acc D Fake: 84.541% 
Loss D: 0.992 
Loss G: 0.7745 (0.7677) Acc G: 15.453% 
LR: 2.000e-04 

2023-03-02 02:00:29,730 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2611 (0.3476) Acc D Real: 63.762% 
Loss D Fake: 0.6217 (0.6277) Acc D Fake: 84.546% 
Loss D: 0.883 
Loss G: 0.7759 (0.7677) Acc G: 15.449% 
LR: 2.000e-04 

2023-03-02 02:00:29,738 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3663 (0.3478) Acc D Real: 63.741% 
Loss D Fake: 0.6205 (0.6277) Acc D Fake: 84.550% 
Loss D: 0.987 
Loss G: 0.7772 (0.7678) Acc G: 15.445% 
LR: 2.000e-04 

2023-03-02 02:00:29,746 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4085 (0.3483) Acc D Real: 63.658% 
Loss D Fake: 0.6198 (0.6276) Acc D Fake: 84.554% 
Loss D: 1.028 
Loss G: 0.7774 (0.7679) Acc G: 15.441% 
LR: 2.000e-04 

2023-03-02 02:00:29,754 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2586 (0.3475) Acc D Real: 63.757% 
Loss D Fake: 0.6197 (0.6275) Acc D Fake: 84.558% 
Loss D: 0.878 
Loss G: 0.7780 (0.7680) Acc G: 15.437% 
LR: 2.000e-04 

2023-03-02 02:00:29,761 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3431 (0.3475) Acc D Real: 63.753% 
Loss D Fake: 0.6190 (0.6275) Acc D Fake: 84.562% 
Loss D: 0.962 
Loss G: 0.7788 (0.7681) Acc G: 15.433% 
LR: 2.000e-04 

2023-03-02 02:00:29,769 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3918 (0.3479) Acc D Real: 63.697% 
Loss D Fake: 0.6186 (0.6274) Acc D Fake: 84.566% 
Loss D: 1.010 
Loss G: 0.7789 (0.7682) Acc G: 15.429% 
LR: 2.000e-04 

2023-03-02 02:00:29,776 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2347 (0.3469) Acc D Real: 63.821% 
Loss D Fake: 0.6184 (0.6273) Acc D Fake: 84.570% 
Loss D: 0.853 
Loss G: 0.7798 (0.7683) Acc G: 15.425% 
LR: 2.000e-04 

2023-03-02 02:00:29,784 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.5076 (0.3483) Acc D Real: 63.649% 
Loss D Fake: 0.6179 (0.6272) Acc D Fake: 84.573% 
Loss D: 1.126 
Loss G: 0.7794 (0.7684) Acc G: 15.422% 
LR: 2.000e-04 

2023-03-02 02:00:29,793 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3702 (0.3485) Acc D Real: 63.624% 
Loss D Fake: 0.6187 (0.6271) Acc D Fake: 84.566% 
Loss D: 0.989 
Loss G: 0.7784 (0.7685) Acc G: 15.432% 
LR: 2.000e-04 

2023-03-02 02:00:29,801 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3673 (0.3486) Acc D Real: 63.602% 
Loss D Fake: 0.6195 (0.6271) Acc D Fake: 84.556% 
Loss D: 0.987 
Loss G: 0.7774 (0.7686) Acc G: 15.442% 
LR: 2.000e-04 

2023-03-02 02:00:29,809 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4544 (0.3495) Acc D Real: 63.491% 
Loss D Fake: 0.6206 (0.6270) Acc D Fake: 84.545% 
Loss D: 1.075 
Loss G: 0.7755 (0.7686) Acc G: 15.453% 
LR: 2.000e-04 

2023-03-02 02:00:29,817 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.4137 (0.3501) Acc D Real: 63.420% 
Loss D Fake: 0.6225 (0.6270) Acc D Fake: 84.521% 
Loss D: 1.036 
Loss G: 0.7729 (0.7687) Acc G: 15.477% 
LR: 2.000e-04 

2023-03-02 02:00:29,825 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2940 (0.3496) Acc D Real: 63.466% 
Loss D Fake: 0.6246 (0.6270) Acc D Fake: 84.497% 
Loss D: 0.919 
Loss G: 0.7709 (0.7687) Acc G: 15.501% 
LR: 2.000e-04 

2023-03-02 02:00:29,833 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3265 (0.3494) Acc D Real: 63.490% 
Loss D Fake: 0.6258 (0.6270) Acc D Fake: 84.474% 
Loss D: 0.952 
Loss G: 0.7697 (0.7687) Acc G: 15.524% 
LR: 2.000e-04 

2023-03-02 02:00:29,840 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2521 (0.3486) Acc D Real: 63.584% 
Loss D Fake: 0.6264 (0.6270) Acc D Fake: 84.451% 
Loss D: 0.879 
Loss G: 0.7697 (0.7687) Acc G: 15.547% 
LR: 2.000e-04 

2023-03-02 02:00:29,848 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3393 (0.3485) Acc D Real: 63.591% 
Loss D Fake: 0.6262 (0.6269) Acc D Fake: 84.428% 
Loss D: 0.966 
Loss G: 0.7700 (0.7687) Acc G: 15.570% 
LR: 2.000e-04 

2023-03-02 02:00:29,856 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4094 (0.3490) Acc D Real: 63.525% 
Loss D Fake: 0.6262 (0.6269) Acc D Fake: 84.406% 
Loss D: 1.036 
Loss G: 0.7695 (0.7687) Acc G: 15.592% 
LR: 2.000e-04 

2023-03-02 02:00:29,864 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3403 (0.3490) Acc D Real: 63.530% 
Loss D Fake: 0.6267 (0.6269) Acc D Fake: 84.384% 
Loss D: 0.967 
Loss G: 0.7690 (0.7687) Acc G: 15.614% 
LR: 2.000e-04 

2023-03-02 02:00:29,872 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.2975 (0.3486) Acc D Real: 63.581% 
Loss D Fake: 0.6270 (0.6269) Acc D Fake: 84.363% 
Loss D: 0.924 
Loss G: 0.7691 (0.7687) Acc G: 15.636% 
LR: 2.000e-04 

2023-03-02 02:00:29,880 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4319 (0.3492) Acc D Real: 63.488% 
Loss D Fake: 0.6270 (0.6269) Acc D Fake: 84.341% 
Loss D: 1.059 
Loss G: 0.7684 (0.7687) Acc G: 15.657% 
LR: 2.000e-04 

2023-03-02 02:00:29,888 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3488 (0.3492) Acc D Real: 63.486% 
Loss D Fake: 0.6278 (0.6269) Acc D Fake: 84.320% 
Loss D: 0.977 
Loss G: 0.7675 (0.7687) Acc G: 15.678% 
LR: 2.000e-04 

2023-03-02 02:00:29,896 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3044 (0.3489) Acc D Real: 63.526% 
Loss D Fake: 0.6283 (0.6270) Acc D Fake: 84.300% 
Loss D: 0.933 
Loss G: 0.7673 (0.7687) Acc G: 15.698% 
LR: 2.000e-04 

2023-03-02 02:00:29,904 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2942 (0.3484) Acc D Real: 63.572% 
Loss D Fake: 0.6282 (0.6270) Acc D Fake: 84.292% 
Loss D: 0.922 
Loss G: 0.7677 (0.7687) Acc G: 15.706% 
LR: 2.000e-04 

2023-03-02 02:00:29,911 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.4521 (0.3492) Acc D Real: 63.471% 
Loss D Fake: 0.6281 (0.6270) Acc D Fake: 84.285% 
Loss D: 1.080 
Loss G: 0.7672 (0.7687) Acc G: 15.713% 
LR: 2.000e-04 

2023-03-02 02:00:29,919 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.2497 (0.3485) Acc D Real: 63.562% 
Loss D Fake: 0.6285 (0.6270) Acc D Fake: 84.278% 
Loss D: 0.878 
Loss G: 0.7673 (0.7687) Acc G: 15.720% 
LR: 2.000e-04 

2023-03-02 02:00:29,926 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4468 (0.3492) Acc D Real: 63.455% 
Loss D Fake: 0.6285 (0.6270) Acc D Fake: 84.271% 
Loss D: 1.075 
Loss G: 0.7668 (0.7686) Acc G: 15.728% 
LR: 2.000e-04 

2023-03-02 02:00:29,934 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4077 (0.3496) Acc D Real: 63.400% 
Loss D Fake: 0.6293 (0.6270) Acc D Fake: 84.264% 
Loss D: 1.037 
Loss G: 0.7655 (0.7686) Acc G: 15.735% 
LR: 2.000e-04 

2023-03-02 02:00:29,941 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4059 (0.3501) Acc D Real: 63.341% 
Loss D Fake: 0.6307 (0.6270) Acc D Fake: 84.257% 
Loss D: 1.037 
Loss G: 0.7636 (0.7686) Acc G: 15.742% 
LR: 2.000e-04 

2023-03-02 02:00:29,949 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2717 (0.3495) Acc D Real: 63.410% 
Loss D Fake: 0.6321 (0.6271) Acc D Fake: 84.250% 
Loss D: 0.904 
Loss G: 0.7625 (0.7685) Acc G: 15.748% 
LR: 2.000e-04 

2023-03-02 02:00:29,956 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3675 (0.3496) Acc D Real: 63.391% 
Loss D Fake: 0.6328 (0.6271) Acc D Fake: 84.243% 
Loss D: 1.000 
Loss G: 0.7615 (0.7685) Acc G: 15.755% 
LR: 2.000e-04 

2023-03-02 02:00:29,964 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3397 (0.3495) Acc D Real: 63.397% 
Loss D Fake: 0.6337 (0.6272) Acc D Fake: 84.237% 
Loss D: 0.973 
Loss G: 0.7606 (0.7684) Acc G: 15.762% 
LR: 2.000e-04 

2023-03-02 02:00:29,971 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3066 (0.3492) Acc D Real: 63.420% 
Loss D Fake: 0.6343 (0.6272) Acc D Fake: 84.230% 
Loss D: 0.941 
Loss G: 0.7603 (0.7684) Acc G: 15.768% 
LR: 2.000e-04 

2023-03-02 02:00:29,979 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3154 (0.3490) Acc D Real: 63.442% 
Loss D Fake: 0.6344 (0.6273) Acc D Fake: 84.224% 
Loss D: 0.950 
Loss G: 0.7602 (0.7683) Acc G: 15.775% 
LR: 2.000e-04 

2023-03-02 02:00:29,986 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3346 (0.3489) Acc D Real: 63.454% 
Loss D Fake: 0.6344 (0.6273) Acc D Fake: 84.218% 
Loss D: 0.969 
Loss G: 0.7603 (0.7683) Acc G: 15.781% 
LR: 2.000e-04 

2023-03-02 02:00:29,994 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4036 (0.3493) Acc D Real: 63.405% 
Loss D Fake: 0.6345 (0.6274) Acc D Fake: 84.211% 
Loss D: 1.038 
Loss G: 0.7598 (0.7682) Acc G: 15.787% 
LR: 2.000e-04 

2023-03-02 02:00:30,001 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4200 (0.3498) Acc D Real: 63.336% 
Loss D Fake: 0.6353 (0.6274) Acc D Fake: 84.205% 
Loss D: 1.055 
Loss G: 0.7585 (0.7681) Acc G: 15.793% 
LR: 2.000e-04 

2023-03-02 02:00:30,008 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2945 (0.3494) Acc D Real: 63.378% 
Loss D Fake: 0.6363 (0.6275) Acc D Fake: 84.199% 
Loss D: 0.931 
Loss G: 0.7577 (0.7681) Acc G: 15.799% 
LR: 2.000e-04 

2023-03-02 02:00:30,016 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3359 (0.3493) Acc D Real: 63.389% 
Loss D Fake: 0.6368 (0.6276) Acc D Fake: 84.182% 
Loss D: 0.973 
Loss G: 0.7573 (0.7680) Acc G: 15.815% 
LR: 2.000e-04 

2023-03-02 02:00:30,023 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2523 (0.3486) Acc D Real: 63.472% 
Loss D Fake: 0.6368 (0.6276) Acc D Fake: 84.165% 
Loss D: 0.889 
Loss G: 0.7579 (0.7679) Acc G: 15.833% 
LR: 2.000e-04 

2023-03-02 02:00:30,030 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3865 (0.3489) Acc D Real: 63.437% 
Loss D Fake: 0.6363 (0.6277) Acc D Fake: 84.137% 
Loss D: 1.023 
Loss G: 0.7582 (0.7678) Acc G: 15.861% 
LR: 2.000e-04 

2023-03-02 02:00:30,038 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3601 (0.3490) Acc D Real: 63.423% 
Loss D Fake: 0.6362 (0.6277) Acc D Fake: 84.109% 
Loss D: 0.996 
Loss G: 0.7581 (0.7678) Acc G: 15.889% 
LR: 2.000e-04 

2023-03-02 02:00:30,045 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3796 (0.3492) Acc D Real: 63.388% 
Loss D Fake: 0.6366 (0.6278) Acc D Fake: 84.081% 
Loss D: 1.016 
Loss G: 0.7573 (0.7677) Acc G: 15.917% 
LR: 2.000e-04 

2023-03-02 02:00:30,052 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.2902 (0.3488) Acc D Real: 63.430% 
Loss D Fake: 0.6373 (0.6279) Acc D Fake: 84.054% 
Loss D: 0.927 
Loss G: 0.7569 (0.7676) Acc G: 15.944% 
LR: 2.000e-04 

2023-03-02 02:00:30,060 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.2391 (0.3481) Acc D Real: 63.527% 
Loss D Fake: 0.6372 (0.6279) Acc D Fake: 84.027% 
Loss D: 0.876 
Loss G: 0.7577 (0.7676) Acc G: 15.971% 
LR: 2.000e-04 

2023-03-02 02:00:30,067 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3527 (0.3481) Acc D Real: 63.509% 
Loss D Fake: 0.6363 (0.6280) Acc D Fake: 84.001% 
Loss D: 0.989 
Loss G: 0.7586 (0.7675) Acc G: 15.997% 
LR: 2.000e-04 

2023-03-02 02:00:30,075 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.2755 (0.3476) Acc D Real: 63.571% 
Loss D Fake: 0.6355 (0.6280) Acc D Fake: 83.975% 
Loss D: 0.911 
Loss G: 0.7599 (0.7675) Acc G: 16.023% 
LR: 2.000e-04 

2023-03-02 02:00:30,082 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3292 (0.3475) Acc D Real: 63.579% 
Loss D Fake: 0.6343 (0.6281) Acc D Fake: 83.954% 
Loss D: 0.963 
Loss G: 0.7613 (0.7674) Acc G: 16.039% 
LR: 2.000e-04 

2023-03-02 02:00:30,090 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3367 (0.3474) Acc D Real: 63.582% 
Loss D Fake: 0.6332 (0.6281) Acc D Fake: 83.939% 
Loss D: 0.970 
Loss G: 0.7626 (0.7674) Acc G: 16.054% 
LR: 2.000e-04 

2023-03-02 02:00:30,097 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3598 (0.3475) Acc D Real: 63.563% 
Loss D Fake: 0.6323 (0.6281) Acc D Fake: 83.924% 
Loss D: 0.992 
Loss G: 0.7634 (0.7674) Acc G: 16.069% 
LR: 2.000e-04 

2023-03-02 02:00:30,105 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3966 (0.3478) Acc D Real: 63.513% 
Loss D Fake: 0.6320 (0.6281) Acc D Fake: 83.910% 
Loss D: 1.029 
Loss G: 0.7634 (0.7673) Acc G: 16.083% 
LR: 2.000e-04 

2023-03-02 02:00:30,112 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.5214 (0.3489) Acc D Real: 63.501% 
Loss D Fake: 0.6326 (0.6282) Acc D Fake: 83.909% 
Loss D: 1.154 
Loss G: 0.7620 (0.7673) Acc G: 16.084% 
LR: 2.000e-04 

2023-03-02 02:00:30,335 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.329 | Generator Loss: 0.762 | Avg: 2.091 
2023-03-02 02:00:30,357 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.203 | Generator Loss: 0.762 | Avg: 1.964 
2023-03-02 02:00:30,381 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.234 | Generator Loss: 0.762 | Avg: 1.996 
2023-03-02 02:00:30,407 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.268 | Generator Loss: 0.762 | Avg: 2.029 
2023-03-02 02:00:30,435 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.267 | Generator Loss: 0.762 | Avg: 2.029 
2023-03-02 02:00:30,461 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.216 | Generator Loss: 0.762 | Avg: 1.978 
2023-03-02 02:00:30,487 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.178 | Generator Loss: 0.762 | Avg: 1.940 
2023-03-02 02:00:30,514 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.130 | Generator Loss: 0.762 | Avg: 1.892 
2023-03-02 02:00:30,540 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.099 | Generator Loss: 0.762 | Avg: 1.860 
2023-03-02 02:00:30,569 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.057 | Generator Loss: 0.762 | Avg: 1.819 
2023-03-02 02:00:30,596 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.024 | Generator Loss: 0.762 | Avg: 1.785 
2023-03-02 02:00:30,622 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 0.994 | Generator Loss: 0.762 | Avg: 1.756 
2023-03-02 02:00:30,649 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 0.970 | Generator Loss: 0.762 | Avg: 1.732 
2023-03-02 02:00:30,676 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 0.951 | Generator Loss: 0.762 | Avg: 1.712 
2023-03-02 02:00:30,703 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 0.965 | Generator Loss: 0.762 | Avg: 1.727 
2023-03-02 02:00:30,730 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 0.980 | Generator Loss: 0.762 | Avg: 1.742 
2023-03-02 02:00:30,756 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 0.997 | Generator Loss: 0.762 | Avg: 1.759 
2023-03-02 02:00:30,793 -                train: [    INFO] - 
Epoch: 15/20
2023-03-02 02:00:30,983 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.2893 (0.2944) Acc D Real: 69.661% 
Loss D Fake: 0.6349 (0.6344) Acc D Fake: 81.667% 
Loss D: 0.924 
Loss G: 0.7599 (0.7603) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 02:00:30,991 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3124 (0.3004) Acc D Real: 68.681% 
Loss D Fake: 0.6353 (0.6347) Acc D Fake: 81.667% 
Loss D: 0.948 
Loss G: 0.7596 (0.7600) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 02:00:30,999 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2504 (0.2879) Acc D Real: 70.182% 
Loss D Fake: 0.6354 (0.6349) Acc D Fake: 81.667% 
Loss D: 0.886 
Loss G: 0.7600 (0.7600) Acc G: 18.333% 
LR: 2.000e-04 

2023-03-02 02:00:31,016 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3118 (0.2927) Acc D Real: 69.469% 
Loss D Fake: 0.6350 (0.6349) Acc D Fake: 81.365% 
Loss D: 0.947 
Loss G: 0.7604 (0.7601) Acc G: 18.667% 
LR: 2.000e-04 

2023-03-02 02:00:31,024 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2895 (0.2921) Acc D Real: 69.392% 
Loss D Fake: 0.6347 (0.6349) Acc D Fake: 80.859% 
Loss D: 0.924 
Loss G: 0.7610 (0.7602) Acc G: 19.167% 
LR: 2.000e-04 

2023-03-02 02:00:31,031 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3595 (0.3018) Acc D Real: 68.341% 
Loss D Fake: 0.6342 (0.6348) Acc D Fake: 80.499% 
Loss D: 0.994 
Loss G: 0.7614 (0.7604) Acc G: 19.524% 
LR: 2.000e-04 

2023-03-02 02:00:31,038 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2021 (0.2893) Acc D Real: 69.824% 
Loss D Fake: 0.6338 (0.6347) Acc D Fake: 80.436% 
Loss D: 0.836 
Loss G: 0.7626 (0.7607) Acc G: 19.583% 
LR: 2.000e-04 

2023-03-02 02:00:31,045 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.2730 (0.2875) Acc D Real: 70.139% 
Loss D Fake: 0.6325 (0.6344) Acc D Fake: 80.388% 
Loss D: 0.905 
Loss G: 0.7645 (0.7611) Acc G: 19.630% 
LR: 2.000e-04 

2023-03-02 02:00:31,052 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.2519 (0.2839) Acc D Real: 70.620% 
Loss D Fake: 0.6309 (0.6341) Acc D Fake: 80.349% 
Loss D: 0.883 
Loss G: 0.7667 (0.7617) Acc G: 19.667% 
LR: 2.000e-04 

2023-03-02 02:00:31,059 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4106 (0.2955) Acc D Real: 69.205% 
Loss D Fake: 0.6294 (0.6336) Acc D Fake: 80.317% 
Loss D: 1.040 
Loss G: 0.7679 (0.7622) Acc G: 19.697% 
LR: 2.000e-04 

2023-03-02 02:00:31,067 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4072 (0.3048) Acc D Real: 68.121% 
Loss D Fake: 0.6291 (0.6332) Acc D Fake: 80.291% 
Loss D: 1.036 
Loss G: 0.7677 (0.7627) Acc G: 19.722% 
LR: 2.000e-04 

2023-03-02 02:00:31,074 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.3625 (0.3092) Acc D Real: 67.660% 
Loss D Fake: 0.6297 (0.6330) Acc D Fake: 80.260% 
Loss D: 0.992 
Loss G: 0.7668 (0.7630) Acc G: 19.744% 
LR: 2.000e-04 

2023-03-02 02:00:31,081 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4793 (0.3214) Acc D Real: 66.187% 
Loss D Fake: 0.6312 (0.6328) Acc D Fake: 80.134% 
Loss D: 1.111 
Loss G: 0.7643 (0.7631) Acc G: 19.881% 
LR: 2.000e-04 

2023-03-02 02:00:31,088 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2502 (0.3166) Acc D Real: 66.771% 
Loss D Fake: 0.6335 (0.6329) Acc D Fake: 80.014% 
Loss D: 0.884 
Loss G: 0.7623 (0.7631) Acc G: 20.000% 
LR: 2.000e-04 

2023-03-02 02:00:31,095 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3159 (0.3166) Acc D Real: 66.758% 
Loss D Fake: 0.6351 (0.6330) Acc D Fake: 79.909% 
Loss D: 0.951 
Loss G: 0.7607 (0.7629) Acc G: 20.104% 
LR: 2.000e-04 

2023-03-02 02:00:31,105 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4974 (0.3272) Acc D Real: 65.417% 
Loss D Fake: 0.6372 (0.6333) Acc D Fake: 79.816% 
Loss D: 1.135 
Loss G: 0.7575 (0.7626) Acc G: 20.196% 
LR: 2.000e-04 

2023-03-02 02:00:31,112 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3391 (0.3279) Acc D Real: 65.336% 
Loss D Fake: 0.6405 (0.6337) Acc D Fake: 79.641% 
Loss D: 0.980 
Loss G: 0.7540 (0.7621) Acc G: 20.370% 
LR: 2.000e-04 

2023-03-02 02:00:31,119 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.2985 (0.3263) Acc D Real: 65.573% 
Loss D Fake: 0.6439 (0.6342) Acc D Fake: 79.485% 
Loss D: 0.942 
Loss G: 0.7508 (0.7615) Acc G: 20.526% 
LR: 2.000e-04 

2023-03-02 02:00:31,126 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.2685 (0.3234) Acc D Real: 65.919% 
Loss D Fake: 0.6472 (0.6349) Acc D Fake: 79.260% 
Loss D: 0.916 
Loss G: 0.7477 (0.7608) Acc G: 20.750% 
LR: 2.000e-04 

2023-03-02 02:00:31,133 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4082 (0.3275) Acc D Real: 65.432% 
Loss D Fake: 0.6516 (0.6357) Acc D Fake: 79.015% 
Loss D: 1.060 
Loss G: 0.7426 (0.7600) Acc G: 21.017% 
LR: 2.000e-04 

2023-03-02 02:00:31,140 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3972 (0.3306) Acc D Real: 64.979% 
Loss D Fake: 0.6600 (0.6368) Acc D Fake: 78.681% 
Loss D: 1.057 
Loss G: 0.7339 (0.7588) Acc G: 21.349% 
LR: 2.000e-04 

2023-03-02 02:00:31,146 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4000 (0.3337) Acc D Real: 64.563% 
Loss D Fake: 0.6807 (0.6387) Acc D Fake: 78.132% 
Loss D: 1.081 
Loss G: 0.7146 (0.7569) Acc G: 21.880% 
LR: 2.000e-04 

2023-03-02 02:00:31,153 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3370 (0.3338) Acc D Real: 64.570% 
Loss D Fake: 3.6728 (0.7651) Acc D Fake: 74.876% 
Loss D: 4.010 
Loss G: 0.1368 (0.7310) Acc G: 25.135% 
LR: 2.000e-04 

2023-03-02 02:00:31,161 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.2230 (0.3294) Acc D Real: 65.110% 
Loss D Fake: 3.7956 (0.8863) Acc D Fake: 71.881% 
Loss D: 4.019 
Loss G: 0.1183 (0.7065) Acc G: 28.129% 
LR: 2.000e-04 

2023-03-02 02:00:31,169 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.2242 (0.3253) Acc D Real: 65.651% 
Loss D Fake: 3.8407 (1.0000) Acc D Fake: 69.117% 
Loss D: 4.065 
Loss G: 0.1100 (0.6836) Acc G: 30.893% 
LR: 2.000e-04 

2023-03-02 02:00:31,176 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4731 (0.3308) Acc D Real: 65.085% 
Loss D Fake: 3.8553 (1.1057) Acc D Fake: 66.557% 
Loss D: 4.328 
Loss G: 0.1050 (0.6621) Acc G: 33.453% 
LR: 2.000e-04 

2023-03-02 02:00:31,184 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3442 (0.3313) Acc D Real: 65.086% 
Loss D Fake: 3.8532 (1.2038) Acc D Fake: 64.180% 
Loss D: 4.197 
Loss G: 0.1018 (0.6421) Acc G: 35.830% 
LR: 2.000e-04 

2023-03-02 02:00:31,192 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.2801 (0.3295) Acc D Real: 65.665% 
Loss D Fake: 3.8407 (1.2948) Acc D Fake: 61.967% 
Loss D: 4.121 
Loss G: 0.0995 (0.6234) Acc G: 38.042% 
LR: 2.000e-04 

2023-03-02 02:00:31,200 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3498 (0.3302) Acc D Real: 66.128% 
Loss D Fake: 3.8210 (1.3790) Acc D Fake: 59.901% 
Loss D: 4.171 
Loss G: 0.0979 (0.6059) Acc G: 40.108% 
LR: 2.000e-04 

2023-03-02 02:00:31,207 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.2507 (0.3276) Acc D Real: 66.951% 
Loss D Fake: 3.7964 (1.4569) Acc D Fake: 57.969% 
Loss D: 4.047 
Loss G: 0.0968 (0.5895) Acc G: 42.040% 
LR: 2.000e-04 

2023-03-02 02:00:31,215 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3266 (0.3276) Acc D Real: 67.812% 
Loss D Fake: 3.7683 (1.5292) Acc D Fake: 56.157% 
Loss D: 4.095 
Loss G: 0.0961 (0.5741) Acc G: 43.851% 
LR: 2.000e-04 

2023-03-02 02:00:31,223 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.2921 (0.3265) Acc D Real: 68.670% 
Loss D Fake: 3.7375 (1.5961) Acc D Fake: 54.455% 
Loss D: 4.030 
Loss G: 0.0956 (0.5596) Acc G: 45.552% 
LR: 2.000e-04 

2023-03-02 02:00:31,231 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.2486 (0.3242) Acc D Real: 69.522% 
Loss D Fake: 3.7049 (1.6581) Acc D Fake: 52.854% 
Loss D: 3.953 
Loss G: 0.0953 (0.5459) Acc G: 47.154% 
LR: 2.000e-04 

2023-03-02 02:00:31,238 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.2468 (0.3220) Acc D Real: 70.329% 
Loss D Fake: 3.6709 (1.7156) Acc D Fake: 51.344% 
Loss D: 3.918 
Loss G: 0.0952 (0.5330) Acc G: 48.664% 
LR: 2.000e-04 

2023-03-02 02:00:31,246 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.2814 (0.3209) Acc D Real: 71.092% 
Loss D Fake: 3.6359 (1.7690) Acc D Fake: 49.918% 
Loss D: 3.917 
Loss G: 0.0953 (0.5209) Acc G: 50.090% 
LR: 2.000e-04 

2023-03-02 02:00:31,255 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.2742 (0.3196) Acc D Real: 71.806% 
Loss D Fake: 3.6002 (1.8185) Acc D Fake: 48.568% 
Loss D: 3.874 
Loss G: 0.0955 (0.5094) Acc G: 51.439% 
LR: 2.000e-04 

2023-03-02 02:00:31,263 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2549 (0.3179) Acc D Real: 72.481% 
Loss D Fake: 3.5640 (1.8644) Acc D Fake: 47.290% 
Loss D: 3.819 
Loss G: 0.0958 (0.4985) Acc G: 52.717% 
LR: 2.000e-04 

2023-03-02 02:00:31,270 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2453 (0.3161) Acc D Real: 73.125% 
Loss D Fake: 3.5275 (1.9070) Acc D Fake: 46.078% 
Loss D: 3.773 
Loss G: 0.0961 (0.4882) Acc G: 53.929% 
LR: 2.000e-04 

2023-03-02 02:00:31,278 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.2604 (0.3147) Acc D Real: 73.724% 
Loss D Fake: 3.4909 (1.9466) Acc D Fake: 44.926% 
Loss D: 3.751 
Loss G: 0.0966 (0.4784) Acc G: 55.081% 
LR: 2.000e-04 

2023-03-02 02:00:31,288 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.2420 (0.3129) Acc D Real: 74.318% 
Loss D Fake: 3.4543 (1.9834) Acc D Fake: 43.830% 
Loss D: 3.696 
Loss G: 0.0972 (0.4691) Acc G: 56.176% 
LR: 2.000e-04 

2023-03-02 02:00:31,298 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.2630 (0.3117) Acc D Real: 74.876% 
Loss D Fake: 3.4176 (2.0176) Acc D Fake: 42.786% 
Loss D: 3.681 
Loss G: 0.0978 (0.4602) Acc G: 57.220% 
LR: 2.000e-04 

2023-03-02 02:00:31,308 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3001 (0.3114) Acc D Real: 75.389% 
Loss D Fake: 3.3811 (2.0493) Acc D Fake: 41.791% 
Loss D: 3.681 
Loss G: 0.0985 (0.4518) Acc G: 58.215% 
LR: 2.000e-04 

2023-03-02 02:00:31,318 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.2841 (0.3108) Acc D Real: 75.898% 
Loss D Fake: 3.3448 (2.0787) Acc D Fake: 40.842% 
Loss D: 3.629 
Loss G: 0.0992 (0.4438) Acc G: 59.164% 
LR: 2.000e-04 

2023-03-02 02:00:31,328 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.2141 (0.3087) Acc D Real: 76.403% 
Loss D Fake: 3.3086 (2.1060) Acc D Fake: 39.934% 
Loss D: 3.523 
Loss G: 0.0999 (0.4362) Acc G: 60.072% 
LR: 2.000e-04 

2023-03-02 02:00:31,338 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.1831 (0.3059) Acc D Real: 76.894% 
Loss D Fake: 3.2727 (2.1314) Acc D Fake: 39.066% 
Loss D: 3.456 
Loss G: 0.1008 (0.4289) Acc G: 60.940% 
LR: 2.000e-04 

2023-03-02 02:00:31,348 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.2724 (0.3052) Acc D Real: 77.342% 
Loss D Fake: 3.2372 (2.1549) Acc D Fake: 38.235% 
Loss D: 3.510 
Loss G: 0.1016 (0.4219) Acc G: 61.771% 
LR: 2.000e-04 

2023-03-02 02:00:31,357 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.2163 (0.3034) Acc D Real: 77.788% 
Loss D Fake: 3.2017 (2.1767) Acc D Fake: 37.438% 
Loss D: 3.418 
Loss G: 0.1026 (0.4153) Acc G: 62.567% 
LR: 2.000e-04 

2023-03-02 02:00:31,364 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2622 (0.3025) Acc D Real: 78.202% 
Loss D Fake: 3.1665 (2.1969) Acc D Fake: 36.674% 
Loss D: 3.429 
Loss G: 0.1036 (0.4089) Acc G: 63.331% 
LR: 2.000e-04 

2023-03-02 02:00:31,372 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.2173 (0.3008) Acc D Real: 78.617% 
Loss D Fake: 3.1314 (2.2156) Acc D Fake: 35.941% 
Loss D: 3.349 
Loss G: 0.1046 (0.4028) Acc G: 64.065% 
LR: 2.000e-04 

2023-03-02 02:00:31,379 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2349 (0.2995) Acc D Real: 79.008% 
Loss D Fake: 3.0966 (2.2329) Acc D Fake: 35.236% 
Loss D: 3.331 
Loss G: 0.1057 (0.3970) Acc G: 64.769% 
LR: 2.000e-04 

2023-03-02 02:00:31,386 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.2453 (0.2985) Acc D Real: 79.371% 
Loss D Fake: 3.0619 (2.2488) Acc D Fake: 34.558% 
Loss D: 3.307 
Loss G: 0.1069 (0.3914) Acc G: 65.447% 
LR: 2.000e-04 

2023-03-02 02:00:31,394 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.2341 (0.2973) Acc D Real: 79.730% 
Loss D Fake: 3.0273 (2.2635) Acc D Fake: 33.906% 
Loss D: 3.261 
Loss G: 0.1082 (0.3861) Acc G: 66.099% 
LR: 2.000e-04 

2023-03-02 02:00:31,401 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.2200 (0.2958) Acc D Real: 80.074% 
Loss D Fake: 2.9929 (2.2770) Acc D Fake: 33.278% 
Loss D: 3.213 
Loss G: 0.1095 (0.3810) Acc G: 66.726% 
LR: 2.000e-04 

2023-03-02 02:00:31,409 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.2438 (0.2949) Acc D Real: 80.412% 
Loss D Fake: 2.9588 (2.2894) Acc D Fake: 32.673% 
Loss D: 3.203 
Loss G: 0.1109 (0.3760) Acc G: 67.331% 
LR: 2.000e-04 

2023-03-02 02:00:31,416 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.2229 (0.2936) Acc D Real: 80.742% 
Loss D Fake: 2.9250 (2.3008) Acc D Fake: 32.090% 
Loss D: 3.148 
Loss G: 0.1123 (0.3713) Acc G: 67.915% 
LR: 2.000e-04 

2023-03-02 02:00:31,424 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.2325 (0.2925) Acc D Real: 81.067% 
Loss D Fake: 2.8914 (2.3111) Acc D Fake: 31.527% 
Loss D: 3.124 
Loss G: 0.1137 (0.3668) Acc G: 68.478% 
LR: 2.000e-04 

2023-03-02 02:00:31,431 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.2207 (0.2913) Acc D Real: 81.376% 
Loss D Fake: 2.8580 (2.3206) Acc D Fake: 30.983% 
Loss D: 3.079 
Loss G: 0.1152 (0.3625) Acc G: 69.021% 
LR: 2.000e-04 

2023-03-02 02:00:31,439 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.1689 (0.2892) Acc D Real: 81.685% 
Loss D Fake: 2.8250 (2.3291) Acc D Fake: 30.458% 
Loss D: 2.994 
Loss G: 0.1167 (0.3583) Acc G: 69.546% 
LR: 2.000e-04 

2023-03-02 02:00:31,446 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.2137 (0.2880) Acc D Real: 81.976% 
Loss D Fake: 2.7924 (2.3368) Acc D Fake: 29.951% 
Loss D: 3.006 
Loss G: 0.1183 (0.3543) Acc G: 70.054% 
LR: 2.000e-04 

2023-03-02 02:00:31,454 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2297 (0.2870) Acc D Real: 82.252% 
Loss D Fake: 2.7600 (2.3438) Acc D Fake: 29.460% 
Loss D: 2.990 
Loss G: 0.1199 (0.3505) Acc G: 70.545% 
LR: 2.000e-04 

2023-03-02 02:00:31,462 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2197 (0.2859) Acc D Real: 82.524% 
Loss D Fake: 2.7278 (2.3500) Acc D Fake: 28.984% 
Loss D: 2.947 
Loss G: 0.1216 (0.3468) Acc G: 71.020% 
LR: 2.000e-04 

2023-03-02 02:00:31,469 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2050 (0.2846) Acc D Real: 82.784% 
Loss D Fake: 2.6959 (2.3555) Acc D Fake: 28.524% 
Loss D: 2.901 
Loss G: 0.1233 (0.3432) Acc G: 71.480% 
LR: 2.000e-04 

2023-03-02 02:00:31,477 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.2906 (0.2847) Acc D Real: 83.042% 
Loss D Fake: 2.6641 (2.3603) Acc D Fake: 28.079% 
Loss D: 2.955 
Loss G: 0.1251 (0.3398) Acc G: 71.925% 
LR: 2.000e-04 

2023-03-02 02:00:31,484 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2176 (0.2837) Acc D Real: 83.297% 
Loss D Fake: 2.6324 (2.3645) Acc D Fake: 27.647% 
Loss D: 2.850 
Loss G: 0.1270 (0.3365) Acc G: 72.357% 
LR: 2.000e-04 

2023-03-02 02:00:31,492 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2415 (0.2831) Acc D Real: 83.527% 
Loss D Fake: 2.6009 (2.3681) Acc D Fake: 27.228% 
Loss D: 2.842 
Loss G: 0.1289 (0.3334) Acc G: 72.776% 
LR: 2.000e-04 

2023-03-02 02:00:31,499 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.2003 (0.2818) Acc D Real: 83.764% 
Loss D Fake: 2.5696 (2.3711) Acc D Fake: 26.821% 
Loss D: 2.770 
Loss G: 0.1309 (0.3304) Acc G: 73.183% 
LR: 2.000e-04 

2023-03-02 02:00:31,507 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.2165 (0.2809) Acc D Real: 83.997% 
Loss D Fake: 2.5386 (2.3735) Acc D Fake: 26.427% 
Loss D: 2.755 
Loss G: 0.1329 (0.3275) Acc G: 73.577% 
LR: 2.000e-04 

2023-03-02 02:00:31,515 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.1871 (0.2795) Acc D Real: 84.220% 
Loss D Fake: 2.5079 (2.3755) Acc D Fake: 26.044% 
Loss D: 2.695 
Loss G: 0.1350 (0.3247) Acc G: 73.960% 
LR: 2.000e-04 

2023-03-02 02:00:31,522 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.2120 (0.2785) Acc D Real: 84.435% 
Loss D Fake: 2.4776 (2.3769) Acc D Fake: 25.672% 
Loss D: 2.690 
Loss G: 0.1371 (0.3220) Acc G: 74.332% 
LR: 2.000e-04 

2023-03-02 02:00:31,530 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.2119 (0.2776) Acc D Real: 84.649% 
Loss D Fake: 2.4476 (2.3779) Acc D Fake: 25.310% 
Loss D: 2.659 
Loss G: 0.1393 (0.3194) Acc G: 74.693% 
LR: 2.000e-04 

2023-03-02 02:00:31,537 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.1868 (0.2763) Acc D Real: 84.858% 
Loss D Fake: 2.4176 (2.3785) Acc D Fake: 24.959% 
Loss D: 2.604 
Loss G: 0.1416 (0.3170) Acc G: 75.045% 
LR: 2.000e-04 

2023-03-02 02:00:31,545 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.1954 (0.2752) Acc D Real: 85.060% 
Loss D Fake: 2.3880 (2.3786) Acc D Fake: 24.617% 
Loss D: 2.583 
Loss G: 0.1439 (0.3146) Acc G: 75.387% 
LR: 2.000e-04 

2023-03-02 02:00:31,553 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2198 (0.2745) Acc D Real: 85.256% 
Loss D Fake: 2.3585 (2.3783) Acc D Fake: 24.284% 
Loss D: 2.578 
Loss G: 0.1463 (0.3123) Acc G: 75.719% 
LR: 2.000e-04 

2023-03-02 02:00:31,560 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.1818 (0.2732) Acc D Real: 85.451% 
Loss D Fake: 2.3292 (2.3777) Acc D Fake: 23.960% 
Loss D: 2.511 
Loss G: 0.1488 (0.3101) Acc G: 76.043% 
LR: 2.000e-04 

2023-03-02 02:00:31,568 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.1731 (0.2719) Acc D Real: 85.641% 
Loss D Fake: 2.3003 (2.3767) Acc D Fake: 23.645% 
Loss D: 2.473 
Loss G: 0.1513 (0.3080) Acc G: 76.358% 
LR: 2.000e-04 

2023-03-02 02:00:31,575 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.2104 (0.2711) Acc D Real: 85.824% 
Loss D Fake: 2.2718 (2.3753) Acc D Fake: 23.338% 
Loss D: 2.482 
Loss G: 0.1539 (0.3060) Acc G: 76.665% 
LR: 2.000e-04 

2023-03-02 02:00:31,583 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.1957 (0.2702) Acc D Real: 86.003% 
Loss D Fake: 2.2434 (2.3736) Acc D Fake: 23.039% 
Loss D: 2.439 
Loss G: 0.1566 (0.3041) Acc G: 76.964% 
LR: 2.000e-04 

2023-03-02 02:00:31,590 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.1955 (0.2692) Acc D Real: 86.177% 
Loss D Fake: 2.2153 (2.3716) Acc D Fake: 22.747% 
Loss D: 2.411 
Loss G: 0.1593 (0.3023) Acc G: 77.256% 
LR: 2.000e-04 

2023-03-02 02:00:31,598 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.2055 (0.2684) Acc D Real: 86.346% 
Loss D Fake: 2.1874 (2.3693) Acc D Fake: 22.463% 
Loss D: 2.393 
Loss G: 0.1622 (0.3005) Acc G: 77.540% 
LR: 2.000e-04 

2023-03-02 02:00:31,605 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2087 (0.2677) Acc D Real: 86.512% 
Loss D Fake: 2.1597 (2.3667) Acc D Fake: 22.186% 
Loss D: 2.368 
Loss G: 0.1651 (0.2989) Acc G: 77.818% 
LR: 2.000e-04 

2023-03-02 02:00:31,613 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.2012 (0.2669) Acc D Real: 86.670% 
Loss D Fake: 2.1321 (2.3639) Acc D Fake: 21.915% 
Loss D: 2.333 
Loss G: 0.1681 (0.2973) Acc G: 78.088% 
LR: 2.000e-04 

2023-03-02 02:00:31,620 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.1979 (0.2660) Acc D Real: 86.830% 
Loss D Fake: 2.1049 (2.3607) Acc D Fake: 21.651% 
Loss D: 2.303 
Loss G: 0.1712 (0.2958) Acc G: 78.352% 
LR: 2.000e-04 

2023-03-02 02:00:31,628 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.2188 (0.2655) Acc D Real: 86.979% 
Loss D Fake: 2.0780 (2.3574) Acc D Fake: 21.393% 
Loss D: 2.297 
Loss G: 0.1743 (0.2943) Acc G: 78.610% 
LR: 2.000e-04 

2023-03-02 02:00:31,635 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.2084 (0.2648) Acc D Real: 87.129% 
Loss D Fake: 2.0512 (2.3538) Acc D Fake: 21.142% 
Loss D: 2.260 
Loss G: 0.1775 (0.2929) Acc G: 78.862% 
LR: 2.000e-04 

2023-03-02 02:00:31,643 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.1838 (0.2639) Acc D Real: 87.279% 
Loss D Fake: 2.0248 (2.3499) Acc D Fake: 20.896% 
Loss D: 2.209 
Loss G: 0.1808 (0.2916) Acc G: 79.107% 
LR: 2.000e-04 

2023-03-02 02:00:31,651 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2099 (0.2632) Acc D Real: 87.425% 
Loss D Fake: 1.9988 (2.3459) Acc D Fake: 20.656% 
Loss D: 2.209 
Loss G: 0.1842 (0.2904) Acc G: 79.347% 
LR: 2.000e-04 

2023-03-02 02:00:31,659 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.2220 (0.2628) Acc D Real: 87.568% 
Loss D Fake: 1.9729 (2.3417) Acc D Fake: 20.421% 
Loss D: 2.195 
Loss G: 0.1876 (0.2892) Acc G: 79.582% 
LR: 2.000e-04 

2023-03-02 02:00:31,667 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.2046 (0.2621) Acc D Real: 87.708% 
Loss D Fake: 1.9470 (2.3372) Acc D Fake: 20.191% 
Loss D: 2.152 
Loss G: 0.1912 (0.2881) Acc G: 79.812% 
LR: 2.000e-04 

2023-03-02 02:00:31,674 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.2019 (0.2614) Acc D Real: 87.844% 
Loss D Fake: 1.9214 (2.3326) Acc D Fake: 19.967% 
Loss D: 2.123 
Loss G: 0.1949 (0.2871) Acc G: 80.036% 
LR: 2.000e-04 

2023-03-02 02:00:31,682 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.2091 (0.2609) Acc D Real: 87.978% 
Loss D Fake: 1.8962 (2.3278) Acc D Fake: 19.748% 
Loss D: 2.105 
Loss G: 0.1986 (0.2861) Acc G: 80.255% 
LR: 2.000e-04 

2023-03-02 02:00:31,689 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.2023 (0.2602) Acc D Real: 88.106% 
Loss D Fake: 1.8714 (2.3229) Acc D Fake: 19.533% 
Loss D: 2.074 
Loss G: 0.2023 (0.2852) Acc G: 80.470% 
LR: 2.000e-04 

2023-03-02 02:00:31,696 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.1993 (0.2596) Acc D Real: 88.234% 
Loss D Fake: 1.8472 (2.3177) Acc D Fake: 19.323% 
Loss D: 2.047 
Loss G: 0.2061 (0.2844) Acc G: 80.680% 
LR: 2.000e-04 

2023-03-02 02:00:31,704 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.2124 (0.2591) Acc D Real: 88.354% 
Loss D Fake: 1.8233 (2.3125) Acc D Fake: 19.117% 
Loss D: 2.036 
Loss G: 0.2100 (0.2836) Acc G: 80.885% 
LR: 2.000e-04 

2023-03-02 02:00:31,711 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2128 (0.2586) Acc D Real: 88.467% 
Loss D Fake: 1.7996 (2.3071) Acc D Fake: 18.916% 
Loss D: 2.012 
Loss G: 0.2139 (0.2828) Acc G: 81.087% 
LR: 2.000e-04 

2023-03-02 02:00:31,719 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.2180 (0.2582) Acc D Real: 88.587% 
Loss D Fake: 1.7762 (2.3016) Acc D Fake: 18.719% 
Loss D: 1.994 
Loss G: 0.2180 (0.2822) Acc G: 81.284% 
LR: 2.000e-04 

2023-03-02 02:00:31,726 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.2155 (0.2577) Acc D Real: 88.705% 
Loss D Fake: 1.7532 (2.2959) Acc D Fake: 18.526% 
Loss D: 1.969 
Loss G: 0.2221 (0.2815) Acc G: 81.477% 
LR: 2.000e-04 

2023-03-02 02:00:31,734 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.2074 (0.2572) Acc D Real: 88.820% 
Loss D Fake: 1.7306 (2.2901) Acc D Fake: 18.337% 
Loss D: 1.938 
Loss G: 0.2262 (0.2810) Acc G: 81.666% 
LR: 2.000e-04 

2023-03-02 02:00:31,741 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.2149 (0.2568) Acc D Real: 88.933% 
Loss D Fake: 1.7083 (2.2843) Acc D Fake: 18.152% 
Loss D: 1.923 
Loss G: 0.2304 (0.2805) Acc G: 81.851% 
LR: 2.000e-04 

2023-03-02 02:00:31,748 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2212 (0.2564) Acc D Real: 89.044% 
Loss D Fake: 1.6866 (2.2783) Acc D Fake: 17.970% 
Loss D: 1.908 
Loss G: 0.2346 (0.2800) Acc G: 82.032% 
LR: 2.000e-04 

2023-03-02 02:00:31,756 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.2182 (0.2561) Acc D Real: 89.152% 
Loss D Fake: 1.6651 (2.2722) Acc D Fake: 17.792% 
Loss D: 1.883 
Loss G: 0.2389 (0.2796) Acc G: 82.210% 
LR: 2.000e-04 

2023-03-02 02:00:31,763 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2349 (0.2558) Acc D Real: 89.252% 
Loss D Fake: 1.6441 (2.2660) Acc D Fake: 17.618% 
Loss D: 1.879 
Loss G: 0.2433 (0.2792) Acc G: 82.385% 
LR: 2.000e-04 

2023-03-02 02:00:31,771 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2143 (0.2554) Acc D Real: 89.356% 
Loss D Fake: 1.6233 (2.2598) Acc D Fake: 17.447% 
Loss D: 1.838 
Loss G: 0.2477 (0.2789) Acc G: 82.556% 
LR: 2.000e-04 

2023-03-02 02:00:31,779 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.2348 (0.2552) Acc D Real: 89.459% 
Loss D Fake: 1.6028 (2.2535) Acc D Fake: 17.279% 
Loss D: 1.838 
Loss G: 0.2521 (0.2787) Acc G: 82.723% 
LR: 2.000e-04 

2023-03-02 02:00:31,786 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.2365 (0.2551) Acc D Real: 89.553% 
Loss D Fake: 1.5827 (2.2471) Acc D Fake: 17.115% 
Loss D: 1.819 
Loss G: 0.2567 (0.2785) Acc G: 82.888% 
LR: 2.000e-04 

2023-03-02 02:00:31,794 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2402 (0.2549) Acc D Real: 89.647% 
Loss D Fake: 1.5625 (2.2406) Acc D Fake: 16.953% 
Loss D: 1.803 
Loss G: 0.2614 (0.2783) Acc G: 83.049% 
LR: 2.000e-04 

2023-03-02 02:00:31,801 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.2319 (0.2547) Acc D Real: 89.744% 
Loss D Fake: 1.5427 (2.2341) Acc D Fake: 16.795% 
Loss D: 1.775 
Loss G: 0.2661 (0.2782) Acc G: 83.208% 
LR: 2.000e-04 

2023-03-02 02:00:31,809 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2464 (0.2546) Acc D Real: 89.837% 
Loss D Fake: 1.5232 (2.2275) Acc D Fake: 16.639% 
Loss D: 1.770 
Loss G: 0.2708 (0.2781) Acc G: 83.363% 
LR: 2.000e-04 

2023-03-02 02:00:31,816 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.2401 (0.2545) Acc D Real: 89.930% 
Loss D Fake: 1.5042 (2.2209) Acc D Fake: 16.487% 
Loss D: 1.744 
Loss G: 0.2755 (0.2781) Acc G: 83.516% 
LR: 2.000e-04 

2023-03-02 02:00:31,823 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.2490 (0.2545) Acc D Real: 90.021% 
Loss D Fake: 1.4858 (2.2142) Acc D Fake: 16.337% 
Loss D: 1.735 
Loss G: 0.2802 (0.2781) Acc G: 83.666% 
LR: 2.000e-04 

2023-03-02 02:00:31,831 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.2320 (0.2542) Acc D Real: 90.108% 
Loss D Fake: 1.4678 (2.2075) Acc D Fake: 16.189% 
Loss D: 1.700 
Loss G: 0.2850 (0.2782) Acc G: 83.813% 
LR: 2.000e-04 

2023-03-02 02:00:31,838 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.2438 (0.2542) Acc D Real: 90.196% 
Loss D Fake: 1.4499 (2.2007) Acc D Fake: 16.045% 
Loss D: 1.694 
Loss G: 0.2899 (0.2783) Acc G: 83.957% 
LR: 2.000e-04 

2023-03-02 02:00:31,845 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.2497 (0.2541) Acc D Real: 90.283% 
Loss D Fake: 1.4324 (2.1939) Acc D Fake: 15.903% 
Loss D: 1.682 
Loss G: 0.2947 (0.2784) Acc G: 84.099% 
LR: 2.000e-04 

2023-03-02 02:00:31,853 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2548 (0.2541) Acc D Real: 90.368% 
Loss D Fake: 1.4155 (2.1871) Acc D Fake: 15.763% 
Loss D: 1.670 
Loss G: 0.2995 (0.2786) Acc G: 84.239% 
LR: 2.000e-04 

2023-03-02 02:00:31,860 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.2302 (0.2539) Acc D Real: 90.452% 
Loss D Fake: 1.3989 (2.1802) Acc D Fake: 15.626% 
Loss D: 1.629 
Loss G: 0.3044 (0.2788) Acc G: 84.376% 
LR: 2.000e-04 

2023-03-02 02:00:31,868 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2317 (0.2537) Acc D Real: 90.530% 
Loss D Fake: 1.3820 (2.1734) Acc D Fake: 15.492% 
Loss D: 1.614 
Loss G: 0.3096 (0.2791) Acc G: 84.511% 
LR: 2.000e-04 

2023-03-02 02:00:31,875 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.2481 (0.2537) Acc D Real: 90.610% 
Loss D Fake: 1.3651 (2.1665) Acc D Fake: 15.359% 
Loss D: 1.613 
Loss G: 0.3148 (0.2794) Acc G: 84.643% 
LR: 2.000e-04 

2023-03-02 02:00:31,883 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.2680 (0.2538) Acc D Real: 90.689% 
Loss D Fake: 1.3485 (2.1595) Acc D Fake: 15.229% 
Loss D: 1.617 
Loss G: 0.3200 (0.2798) Acc G: 84.773% 
LR: 2.000e-04 

2023-03-02 02:00:31,890 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2683 (0.2539) Acc D Real: 90.766% 
Loss D Fake: 1.3324 (2.1526) Acc D Fake: 15.101% 
Loss D: 1.601 
Loss G: 0.3251 (0.2801) Acc G: 84.901% 
LR: 2.000e-04 

2023-03-02 02:00:31,897 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.2610 (0.2540) Acc D Real: 90.843% 
Loss D Fake: 1.3166 (2.1456) Acc D Fake: 14.975% 
Loss D: 1.578 
Loss G: 0.3304 (0.2806) Acc G: 85.027% 
LR: 2.000e-04 

2023-03-02 02:00:31,905 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.2669 (0.2541) Acc D Real: 90.917% 
Loss D Fake: 1.3011 (2.1386) Acc D Fake: 14.851% 
Loss D: 1.568 
Loss G: 0.3356 (0.2810) Acc G: 85.151% 
LR: 2.000e-04 

2023-03-02 02:00:31,912 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.2503 (0.2541) Acc D Real: 90.992% 
Loss D Fake: 1.2858 (2.1316) Acc D Fake: 14.730% 
Loss D: 1.536 
Loss G: 0.3410 (0.2815) Acc G: 85.272% 
LR: 2.000e-04 

2023-03-02 02:00:31,920 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.2820 (0.2543) Acc D Real: 91.062% 
Loss D Fake: 1.2707 (2.1246) Acc D Fake: 14.610% 
Loss D: 1.553 
Loss G: 0.3463 (0.2820) Acc G: 85.392% 
LR: 2.000e-04 

2023-03-02 02:00:31,927 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.2569 (0.2543) Acc D Real: 91.134% 
Loss D Fake: 1.2560 (2.1176) Acc D Fake: 14.492% 
Loss D: 1.513 
Loss G: 0.3516 (0.2826) Acc G: 85.510% 
LR: 2.000e-04 

2023-03-02 02:00:31,934 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2888 (0.2546) Acc D Real: 91.202% 
Loss D Fake: 1.2415 (2.1106) Acc D Fake: 14.376% 
Loss D: 1.530 
Loss G: 0.3569 (0.2832) Acc G: 85.626% 
LR: 2.000e-04 

2023-03-02 02:00:31,942 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.2749 (0.2547) Acc D Real: 91.267% 
Loss D Fake: 1.2276 (2.1036) Acc D Fake: 14.262% 
Loss D: 1.503 
Loss G: 0.3622 (0.2838) Acc G: 85.740% 
LR: 2.000e-04 

2023-03-02 02:00:31,949 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.2922 (0.2550) Acc D Real: 91.331% 
Loss D Fake: 1.2141 (2.0966) Acc D Fake: 14.150% 
Loss D: 1.506 
Loss G: 0.3674 (0.2845) Acc G: 85.852% 
LR: 2.000e-04 

2023-03-02 02:00:31,957 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.2888 (0.2553) Acc D Real: 91.392% 
Loss D Fake: 1.2010 (2.0896) Acc D Fake: 14.039% 
Loss D: 1.490 
Loss G: 0.3726 (0.2852) Acc G: 85.963% 
LR: 2.000e-04 

2023-03-02 02:00:31,964 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2854 (0.2555) Acc D Real: 91.459% 
Loss D Fake: 1.1882 (2.0826) Acc D Fake: 13.930% 
Loss D: 1.474 
Loss G: 0.3777 (0.2859) Acc G: 86.072% 
LR: 2.000e-04 

2023-03-02 02:00:31,972 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.2842 (0.2557) Acc D Real: 91.519% 
Loss D Fake: 1.1758 (2.0757) Acc D Fake: 13.823% 
Loss D: 1.460 
Loss G: 0.3828 (0.2866) Acc G: 86.179% 
LR: 2.000e-04 

2023-03-02 02:00:31,979 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2819 (0.2559) Acc D Real: 91.584% 
Loss D Fake: 1.1637 (2.0687) Acc D Fake: 13.718% 
Loss D: 1.446 
Loss G: 0.3879 (0.2874) Acc G: 86.284% 
LR: 2.000e-04 

2023-03-02 02:00:31,987 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3102 (0.2564) Acc D Real: 91.645% 
Loss D Fake: 1.1519 (2.0617) Acc D Fake: 13.614% 
Loss D: 1.462 
Loss G: 0.3928 (0.2882) Acc G: 86.388% 
LR: 2.000e-04 

2023-03-02 02:00:31,995 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3063 (0.2567) Acc D Real: 91.703% 
Loss D Fake: 1.1406 (2.0548) Acc D Fake: 13.512% 
Loss D: 1.447 
Loss G: 0.3977 (0.2890) Acc G: 86.490% 
LR: 2.000e-04 

2023-03-02 02:00:32,002 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2894 (0.2570) Acc D Real: 91.761% 
Loss D Fake: 1.1295 (2.0479) Acc D Fake: 13.411% 
Loss D: 1.419 
Loss G: 0.4027 (0.2899) Acc G: 86.591% 
LR: 2.000e-04 

2023-03-02 02:00:32,010 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3045 (0.2573) Acc D Real: 91.821% 
Loss D Fake: 1.1185 (2.0410) Acc D Fake: 13.311% 
Loss D: 1.423 
Loss G: 0.4076 (0.2907) Acc G: 86.691% 
LR: 2.000e-04 

2023-03-02 02:00:32,017 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2499 (0.2573) Acc D Real: 91.881% 
Loss D Fake: 1.1076 (2.0342) Acc D Fake: 13.213% 
Loss D: 1.358 
Loss G: 0.4128 (0.2916) Acc G: 86.788% 
LR: 2.000e-04 

2023-03-02 02:00:32,024 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3569 (0.2580) Acc D Real: 91.940% 
Loss D Fake: 1.0967 (2.0273) Acc D Fake: 13.117% 
Loss D: 1.454 
Loss G: 0.4176 (0.2926) Acc G: 86.885% 
LR: 2.000e-04 

2023-03-02 02:00:32,031 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.2978 (0.2583) Acc D Real: 91.997% 
Loss D Fake: 1.0868 (2.0205) Acc D Fake: 13.022% 
Loss D: 1.385 
Loss G: 0.4224 (0.2935) Acc G: 86.980% 
LR: 2.000e-04 

2023-03-02 02:00:32,039 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3283 (0.2588) Acc D Real: 92.052% 
Loss D Fake: 1.0769 (2.0137) Acc D Fake: 12.928% 
Loss D: 1.405 
Loss G: 0.4272 (0.2945) Acc G: 87.074% 
LR: 2.000e-04 

2023-03-02 02:00:32,046 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3231 (0.2593) Acc D Real: 92.109% 
Loss D Fake: 1.0674 (2.0070) Acc D Fake: 12.836% 
Loss D: 1.391 
Loss G: 0.4317 (0.2954) Acc G: 87.166% 
LR: 2.000e-04 

2023-03-02 02:00:32,053 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3093 (0.2596) Acc D Real: 92.165% 
Loss D Fake: 1.0585 (2.0002) Acc D Fake: 12.745% 
Loss D: 1.368 
Loss G: 0.4362 (0.2964) Acc G: 87.257% 
LR: 2.000e-04 

2023-03-02 02:00:32,061 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3190 (0.2600) Acc D Real: 92.219% 
Loss D Fake: 1.0497 (1.9935) Acc D Fake: 12.655% 
Loss D: 1.369 
Loss G: 0.4406 (0.2974) Acc G: 87.347% 
LR: 2.000e-04 

2023-03-02 02:00:32,068 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3361 (0.2606) Acc D Real: 92.274% 
Loss D Fake: 1.0413 (1.9869) Acc D Fake: 12.567% 
Loss D: 1.377 
Loss G: 0.4449 (0.2985) Acc G: 87.435% 
LR: 2.000e-04 

2023-03-02 02:00:32,075 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.2910 (0.2608) Acc D Real: 92.327% 
Loss D Fake: 1.0331 (1.9803) Acc D Fake: 12.479% 
Loss D: 1.324 
Loss G: 0.4492 (0.2995) Acc G: 87.522% 
LR: 2.000e-04 

2023-03-02 02:00:32,082 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3451 (0.2614) Acc D Real: 92.378% 
Loss D Fake: 1.0250 (1.9737) Acc D Fake: 12.393% 
Loss D: 1.370 
Loss G: 0.4534 (0.3006) Acc G: 87.608% 
LR: 2.000e-04 

2023-03-02 02:00:32,090 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3118 (0.2617) Acc D Real: 92.430% 
Loss D Fake: 1.0172 (1.9671) Acc D Fake: 12.308% 
Loss D: 1.329 
Loss G: 0.4576 (0.3017) Acc G: 87.693% 
LR: 2.000e-04 

2023-03-02 02:00:32,097 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3099 (0.2620) Acc D Real: 92.477% 
Loss D Fake: 1.0094 (1.9606) Acc D Fake: 12.225% 
Loss D: 1.319 
Loss G: 0.4619 (0.3028) Acc G: 87.777% 
LR: 2.000e-04 

2023-03-02 02:00:32,104 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3473 (0.2626) Acc D Real: 92.528% 
Loss D Fake: 1.0016 (1.9541) Acc D Fake: 12.142% 
Loss D: 1.349 
Loss G: 0.4661 (0.3039) Acc G: 87.860% 
LR: 2.000e-04 

2023-03-02 02:00:32,112 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3298 (0.2631) Acc D Real: 92.575% 
Loss D Fake: 0.9942 (1.9477) Acc D Fake: 12.061% 
Loss D: 1.324 
Loss G: 0.4703 (0.3050) Acc G: 87.941% 
LR: 2.000e-04 

2023-03-02 02:00:32,119 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3353 (0.2635) Acc D Real: 92.621% 
Loss D Fake: 0.9870 (1.9413) Acc D Fake: 11.980% 
Loss D: 1.322 
Loss G: 0.4744 (0.3061) Acc G: 88.022% 
LR: 2.000e-04 

2023-03-02 02:00:32,126 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3685 (0.2642) Acc D Real: 92.666% 
Loss D Fake: 0.9799 (1.9349) Acc D Fake: 11.901% 
Loss D: 1.348 
Loss G: 0.4783 (0.3072) Acc G: 88.101% 
LR: 2.000e-04 

2023-03-02 02:00:32,134 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3485 (0.2648) Acc D Real: 92.715% 
Loss D Fake: 0.9734 (1.9286) Acc D Fake: 11.823% 
Loss D: 1.322 
Loss G: 0.4820 (0.3084) Acc G: 88.179% 
LR: 2.000e-04 

2023-03-02 02:00:32,142 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3263 (0.2652) Acc D Real: 92.762% 
Loss D Fake: 0.9671 (1.9223) Acc D Fake: 11.745% 
Loss D: 1.293 
Loss G: 0.4857 (0.3096) Acc G: 88.256% 
LR: 2.000e-04 

2023-03-02 02:00:32,150 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3396 (0.2657) Acc D Real: 92.809% 
Loss D Fake: 0.9608 (1.9161) Acc D Fake: 11.669% 
Loss D: 1.300 
Loss G: 0.4894 (0.3107) Acc G: 88.333% 
LR: 2.000e-04 

2023-03-02 02:00:32,157 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3888 (0.2665) Acc D Real: 92.854% 
Loss D Fake: 0.9549 (1.9099) Acc D Fake: 11.594% 
Loss D: 1.344 
Loss G: 0.4928 (0.3119) Acc G: 88.408% 
LR: 2.000e-04 

2023-03-02 02:00:32,164 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3289 (0.2669) Acc D Real: 92.899% 
Loss D Fake: 0.9496 (1.9037) Acc D Fake: 11.519% 
Loss D: 1.278 
Loss G: 0.4961 (0.3131) Acc G: 88.482% 
LR: 2.000e-04 

2023-03-02 02:00:32,172 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3358 (0.2673) Acc D Real: 92.940% 
Loss D Fake: 0.9441 (1.8976) Acc D Fake: 11.446% 
Loss D: 1.280 
Loss G: 0.4995 (0.3143) Acc G: 88.556% 
LR: 2.000e-04 

2023-03-02 02:00:32,179 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2986 (0.2675) Acc D Real: 92.944% 
Loss D Fake: 0.9384 (1.8915) Acc D Fake: 11.439% 
Loss D: 1.237 
Loss G: 0.5033 (0.3155) Acc G: 88.562% 
LR: 2.000e-04 

2023-03-02 02:00:32,387 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 1.100 | Generator Loss: 0.503 | Avg: 1.603 
2023-03-02 02:00:32,410 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 1.165 | Generator Loss: 0.503 | Avg: 1.668 
2023-03-02 02:00:32,433 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 1.148 | Generator Loss: 0.503 | Avg: 1.651 
2023-03-02 02:00:32,462 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 1.130 | Generator Loss: 0.503 | Avg: 1.633 
2023-03-02 02:00:32,488 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 1.131 | Generator Loss: 0.503 | Avg: 1.634 
2023-03-02 02:00:32,515 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 1.157 | Generator Loss: 0.503 | Avg: 1.660 
2023-03-02 02:00:32,542 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 1.178 | Generator Loss: 0.503 | Avg: 1.681 
2023-03-02 02:00:32,568 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.203 | Generator Loss: 0.503 | Avg: 1.707 
2023-03-02 02:00:32,595 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.219 | Generator Loss: 0.503 | Avg: 1.723 
2023-03-02 02:00:32,622 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.240 | Generator Loss: 0.503 | Avg: 1.743 
2023-03-02 02:00:32,649 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.257 | Generator Loss: 0.503 | Avg: 1.761 
2023-03-02 02:00:32,675 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.271 | Generator Loss: 0.503 | Avg: 1.774 
2023-03-02 02:00:32,705 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.283 | Generator Loss: 0.503 | Avg: 1.787 
2023-03-02 02:00:32,731 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.294 | Generator Loss: 0.503 | Avg: 1.797 
2023-03-02 02:00:32,757 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.287 | Generator Loss: 0.503 | Avg: 1.790 
2023-03-02 02:00:32,782 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.280 | Generator Loss: 0.503 | Avg: 1.783 
2023-03-02 02:00:32,808 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.271 | Generator Loss: 0.503 | Avg: 1.774 
2023-03-02 02:00:32,846 -                train: [    INFO] - 
Epoch: 16/20
2023-03-02 02:00:33,051 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3972 (0.3875) Acc D Real: 99.922% 
Loss D Fake: 0.9266 (0.9294) Acc D Fake: 0.000% 
Loss D: 1.324 
Loss G: 0.5103 (0.5087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,059 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3507 (0.3752) Acc D Real: 99.948% 
Loss D Fake: 0.9216 (0.9268) Acc D Fake: 0.000% 
Loss D: 1.272 
Loss G: 0.5135 (0.5103) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,066 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3154 (0.3603) Acc D Real: 99.961% 
Loss D Fake: 0.9166 (0.9243) Acc D Fake: 0.000% 
Loss D: 1.232 
Loss G: 0.5168 (0.5119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,081 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4278 (0.3738) Acc D Real: 99.823% 
Loss D Fake: 0.9117 (0.9217) Acc D Fake: 0.000% 
Loss D: 1.340 
Loss G: 0.5198 (0.5135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,088 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.4335 (0.3837) Acc D Real: 99.818% 
Loss D Fake: 0.9076 (0.9194) Acc D Fake: 0.000% 
Loss D: 1.341 
Loss G: 0.5221 (0.5149) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,096 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3103 (0.3733) Acc D Real: 99.814% 
Loss D Fake: 0.9042 (0.9172) Acc D Fake: 0.000% 
Loss D: 1.214 
Loss G: 0.5246 (0.5163) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,103 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.2945 (0.3634) Acc D Real: 99.818% 
Loss D Fake: 0.9001 (0.9151) Acc D Fake: 0.000% 
Loss D: 1.195 
Loss G: 0.5275 (0.5177) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,110 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3716 (0.3643) Acc D Real: 99.809% 
Loss D Fake: 0.8956 (0.9129) Acc D Fake: 0.000% 
Loss D: 1.267 
Loss G: 0.5305 (0.5191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,117 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3537 (0.3633) Acc D Real: 99.807% 
Loss D Fake: 0.8913 (0.9107) Acc D Fake: 0.000% 
Loss D: 1.245 
Loss G: 0.5334 (0.5205) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,124 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3819 (0.3649) Acc D Real: 99.692% 
Loss D Fake: 0.8871 (0.9086) Acc D Fake: 0.000% 
Loss D: 1.269 
Loss G: 0.5362 (0.5220) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,132 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3706 (0.3654) Acc D Real: 99.614% 
Loss D Fake: 0.8830 (0.9065) Acc D Fake: 0.000% 
Loss D: 1.254 
Loss G: 0.5391 (0.5234) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,139 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4059 (0.3685) Acc D Real: 99.591% 
Loss D Fake: 0.8790 (0.9043) Acc D Fake: 0.000% 
Loss D: 1.285 
Loss G: 0.5416 (0.5248) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,146 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3660 (0.3683) Acc D Real: 99.587% 
Loss D Fake: 0.8755 (0.9023) Acc D Fake: 0.000% 
Loss D: 1.241 
Loss G: 0.5441 (0.5262) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,153 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4337 (0.3727) Acc D Real: 99.549% 
Loss D Fake: 0.8722 (0.9003) Acc D Fake: 0.000% 
Loss D: 1.306 
Loss G: 0.5461 (0.5275) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,160 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3242 (0.3697) Acc D Real: 99.538% 
Loss D Fake: 0.8694 (0.8983) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.5483 (0.5288) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,168 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3177 (0.3666) Acc D Real: 99.519% 
Loss D Fake: 0.8659 (0.8964) Acc D Fake: 0.000% 
Loss D: 1.184 
Loss G: 0.5510 (0.5301) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,175 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3361 (0.3649) Acc D Real: 99.511% 
Loss D Fake: 0.8620 (0.8945) Acc D Fake: 0.000% 
Loss D: 1.198 
Loss G: 0.5539 (0.5314) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,182 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3644 (0.3649) Acc D Real: 99.501% 
Loss D Fake: 0.8579 (0.8926) Acc D Fake: 0.000% 
Loss D: 1.222 
Loss G: 0.5568 (0.5328) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,189 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3802 (0.3657) Acc D Real: 99.477% 
Loss D Fake: 0.8541 (0.8907) Acc D Fake: 0.000% 
Loss D: 1.234 
Loss G: 0.5595 (0.5341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,196 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3530 (0.3651) Acc D Real: 99.472% 
Loss D Fake: 0.8505 (0.8888) Acc D Fake: 0.000% 
Loss D: 1.203 
Loss G: 0.5621 (0.5354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,204 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3907 (0.3662) Acc D Real: 99.427% 
Loss D Fake: 0.8469 (0.8869) Acc D Fake: 0.000% 
Loss D: 1.238 
Loss G: 0.5646 (0.5368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,211 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4192 (0.3685) Acc D Real: 99.404% 
Loss D Fake: 0.8438 (0.8850) Acc D Fake: 0.000% 
Loss D: 1.263 
Loss G: 0.5668 (0.5381) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,218 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4095 (0.3702) Acc D Real: 99.368% 
Loss D Fake: 0.8410 (0.8832) Acc D Fake: 0.000% 
Loss D: 1.250 
Loss G: 0.5687 (0.5393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,225 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3961 (0.3713) Acc D Real: 99.358% 
Loss D Fake: 0.8386 (0.8814) Acc D Fake: 0.000% 
Loss D: 1.235 
Loss G: 0.5705 (0.5406) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,232 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3245 (0.3695) Acc D Real: 99.353% 
Loss D Fake: 0.8362 (0.8796) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.5725 (0.5418) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,239 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3674 (0.3694) Acc D Real: 99.356% 
Loss D Fake: 0.8335 (0.8779) Acc D Fake: 0.000% 
Loss D: 1.201 
Loss G: 0.5746 (0.5430) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,247 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4508 (0.3723) Acc D Real: 99.340% 
Loss D Fake: 0.8310 (0.8762) Acc D Fake: 0.000% 
Loss D: 1.282 
Loss G: 0.5762 (0.5442) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,255 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3583 (0.3718) Acc D Real: 99.330% 
Loss D Fake: 0.8289 (0.8746) Acc D Fake: 0.000% 
Loss D: 1.187 
Loss G: 0.5778 (0.5454) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,263 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3979 (0.3727) Acc D Real: 99.321% 
Loss D Fake: 0.8268 (0.8730) Acc D Fake: 0.000% 
Loss D: 1.225 
Loss G: 0.5794 (0.5465) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,271 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4174 (0.3741) Acc D Real: 99.299% 
Loss D Fake: 0.8249 (0.8715) Acc D Fake: 0.000% 
Loss D: 1.242 
Loss G: 0.5807 (0.5476) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,278 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3871 (0.3745) Acc D Real: 99.297% 
Loss D Fake: 0.8233 (0.8700) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.5820 (0.5487) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,286 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3206 (0.3729) Acc D Real: 99.233% 
Loss D Fake: 0.8214 (0.8685) Acc D Fake: 0.000% 
Loss D: 1.142 
Loss G: 0.5838 (0.5498) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,294 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3881 (0.3733) Acc D Real: 99.223% 
Loss D Fake: 0.8190 (0.8670) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5856 (0.5508) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,302 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3592 (0.3729) Acc D Real: 99.213% 
Loss D Fake: 0.8166 (0.8656) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.5876 (0.5519) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,309 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3926 (0.3735) Acc D Real: 99.203% 
Loss D Fake: 0.8141 (0.8642) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5895 (0.5529) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,317 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4169 (0.3747) Acc D Real: 99.185% 
Loss D Fake: 0.8118 (0.8627) Acc D Fake: 0.000% 
Loss D: 1.229 
Loss G: 0.5911 (0.5539) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,325 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4077 (0.3755) Acc D Real: 99.132% 
Loss D Fake: 0.8099 (0.8614) Acc D Fake: 0.000% 
Loss D: 1.218 
Loss G: 0.5926 (0.5550) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,332 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3482 (0.3748) Acc D Real: 99.125% 
Loss D Fake: 0.8079 (0.8600) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.5944 (0.5560) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,340 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3854 (0.3751) Acc D Real: 99.124% 
Loss D Fake: 0.8056 (0.8586) Acc D Fake: 0.000% 
Loss D: 1.191 
Loss G: 0.5962 (0.5570) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,348 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3663 (0.3749) Acc D Real: 99.122% 
Loss D Fake: 0.8034 (0.8573) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.5980 (0.5580) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,356 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4058 (0.3756) Acc D Real: 99.108% 
Loss D Fake: 0.8012 (0.8559) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.5997 (0.5590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,364 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3906 (0.3760) Acc D Real: 99.113% 
Loss D Fake: 0.7992 (0.8546) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6013 (0.5599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,371 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4281 (0.3771) Acc D Real: 99.100% 
Loss D Fake: 0.7973 (0.8533) Acc D Fake: 0.000% 
Loss D: 1.225 
Loss G: 0.6026 (0.5609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,379 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3784 (0.3772) Acc D Real: 99.103% 
Loss D Fake: 0.7958 (0.8520) Acc D Fake: 0.000% 
Loss D: 1.174 
Loss G: 0.6039 (0.5619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,387 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3733 (0.3771) Acc D Real: 99.096% 
Loss D Fake: 0.7941 (0.8508) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.6053 (0.5628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,395 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3629 (0.3768) Acc D Real: 99.066% 
Loss D Fake: 0.7923 (0.8495) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6069 (0.5638) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,402 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3788 (0.3768) Acc D Real: 99.071% 
Loss D Fake: 0.7903 (0.8483) Acc D Fake: 0.000% 
Loss D: 1.169 
Loss G: 0.6085 (0.5647) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,410 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3437 (0.3762) Acc D Real: 99.073% 
Loss D Fake: 0.7883 (0.8471) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6104 (0.5656) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,418 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.4053 (0.3767) Acc D Real: 99.057% 
Loss D Fake: 0.7861 (0.8459) Acc D Fake: 0.000% 
Loss D: 1.191 
Loss G: 0.6121 (0.5665) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,426 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4039 (0.3773) Acc D Real: 99.059% 
Loss D Fake: 0.7841 (0.8447) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.6137 (0.5675) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,433 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3435 (0.3766) Acc D Real: 99.065% 
Loss D Fake: 0.7822 (0.8435) Acc D Fake: 0.000% 
Loss D: 1.126 
Loss G: 0.6154 (0.5684) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,441 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3705 (0.3765) Acc D Real: 99.066% 
Loss D Fake: 0.7801 (0.8423) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6172 (0.5693) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,448 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3498 (0.3760) Acc D Real: 99.065% 
Loss D Fake: 0.7780 (0.8411) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6191 (0.5702) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,455 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3765 (0.3760) Acc D Real: 99.072% 
Loss D Fake: 0.7757 (0.8399) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6210 (0.5712) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,463 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4050 (0.3765) Acc D Real: 99.073% 
Loss D Fake: 0.7736 (0.8387) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6227 (0.5721) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,471 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3768 (0.3765) Acc D Real: 99.075% 
Loss D Fake: 0.7717 (0.8375) Acc D Fake: 0.000% 
Loss D: 1.149 
Loss G: 0.6243 (0.5730) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,479 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3939 (0.3768) Acc D Real: 99.060% 
Loss D Fake: 0.7698 (0.8364) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6259 (0.5739) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,487 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4080 (0.3774) Acc D Real: 99.060% 
Loss D Fake: 0.7681 (0.8352) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6273 (0.5748) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,494 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3770 (0.3774) Acc D Real: 99.050% 
Loss D Fake: 0.7665 (0.8341) Acc D Fake: 0.000% 
Loss D: 1.144 
Loss G: 0.6286 (0.5757) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,501 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3924 (0.3776) Acc D Real: 99.045% 
Loss D Fake: 0.7650 (0.8329) Acc D Fake: 0.000% 
Loss D: 1.157 
Loss G: 0.6299 (0.5766) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,509 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.4249 (0.3784) Acc D Real: 99.044% 
Loss D Fake: 0.7636 (0.8318) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6310 (0.5775) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,517 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.4568 (0.3796) Acc D Real: 99.043% 
Loss D Fake: 0.7626 (0.8307) Acc D Fake: 0.000% 
Loss D: 1.219 
Loss G: 0.6316 (0.5783) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,525 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.4018 (0.3800) Acc D Real: 99.038% 
Loss D Fake: 0.7621 (0.8296) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6321 (0.5792) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,532 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.3735 (0.3799) Acc D Real: 99.040% 
Loss D Fake: 0.7614 (0.8286) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6328 (0.5800) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,540 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3723 (0.3797) Acc D Real: 99.041% 
Loss D Fake: 0.7605 (0.8275) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6338 (0.5808) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,547 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4916 (0.3814) Acc D Real: 99.042% 
Loss D Fake: 0.7596 (0.8265) Acc D Fake: 0.000% 
Loss D: 1.251 
Loss G: 0.6342 (0.5816) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,555 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3915 (0.3816) Acc D Real: 99.046% 
Loss D Fake: 0.7593 (0.8255) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6345 (0.5824) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,562 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.4039 (0.3819) Acc D Real: 99.038% 
Loss D Fake: 0.7589 (0.8246) Acc D Fake: 0.000% 
Loss D: 1.163 
Loss G: 0.6348 (0.5831) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,570 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4527 (0.3829) Acc D Real: 99.034% 
Loss D Fake: 0.7585 (0.8236) Acc D Fake: 0.000% 
Loss D: 1.211 
Loss G: 0.6350 (0.5839) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,577 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4135 (0.3833) Acc D Real: 99.028% 
Loss D Fake: 0.7585 (0.8227) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6350 (0.5846) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,585 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3340 (0.3826) Acc D Real: 99.034% 
Loss D Fake: 0.7582 (0.8218) Acc D Fake: 0.000% 
Loss D: 1.092 
Loss G: 0.6355 (0.5853) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,592 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3958 (0.3828) Acc D Real: 99.036% 
Loss D Fake: 0.7576 (0.8209) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6361 (0.5860) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,600 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4557 (0.3838) Acc D Real: 99.032% 
Loss D Fake: 0.7570 (0.8201) Acc D Fake: 0.000% 
Loss D: 1.213 
Loss G: 0.6364 (0.5867) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,607 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.4070 (0.3841) Acc D Real: 99.028% 
Loss D Fake: 0.7568 (0.8192) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6366 (0.5874) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,614 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3598 (0.3838) Acc D Real: 99.033% 
Loss D Fake: 0.7565 (0.8184) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6370 (0.5880) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,622 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4064 (0.3841) Acc D Real: 99.036% 
Loss D Fake: 0.7560 (0.8176) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6374 (0.5887) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,630 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3993 (0.3843) Acc D Real: 99.038% 
Loss D Fake: 0.7555 (0.8168) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6379 (0.5893) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,637 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4129 (0.3847) Acc D Real: 99.038% 
Loss D Fake: 0.7550 (0.8160) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6383 (0.5899) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,645 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4018 (0.3849) Acc D Real: 99.041% 
Loss D Fake: 0.7545 (0.8153) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.6387 (0.5905) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,653 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3621 (0.3846) Acc D Real: 99.045% 
Loss D Fake: 0.7540 (0.8145) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6393 (0.5911) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,660 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3413 (0.3841) Acc D Real: 99.050% 
Loss D Fake: 0.7531 (0.8137) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6403 (0.5917) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,668 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.4016 (0.3843) Acc D Real: 99.039% 
Loss D Fake: 0.7519 (0.8130) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6413 (0.5923) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,676 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4293 (0.3848) Acc D Real: 99.040% 
Loss D Fake: 0.7509 (0.8123) Acc D Fake: 0.000% 
Loss D: 1.180 
Loss G: 0.6422 (0.5929) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,683 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4138 (0.3852) Acc D Real: 99.040% 
Loss D Fake: 0.7501 (0.8115) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6428 (0.5935) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,691 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4034 (0.3854) Acc D Real: 99.043% 
Loss D Fake: 0.7494 (0.8108) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6434 (0.5941) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,700 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.4416 (0.3860) Acc D Real: 99.041% 
Loss D Fake: 0.7488 (0.8101) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6438 (0.5946) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,709 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4701 (0.3870) Acc D Real: 99.035% 
Loss D Fake: 0.7487 (0.8094) Acc D Fake: 0.000% 
Loss D: 1.219 
Loss G: 0.6437 (0.5952) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,718 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4402 (0.3876) Acc D Real: 99.037% 
Loss D Fake: 0.7489 (0.8087) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6433 (0.5957) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,726 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4783 (0.3886) Acc D Real: 99.036% 
Loss D Fake: 0.7495 (0.8081) Acc D Fake: 0.000% 
Loss D: 1.228 
Loss G: 0.6426 (0.5963) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,735 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4601 (0.3894) Acc D Real: 99.038% 
Loss D Fake: 0.7504 (0.8074) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.6418 (0.5968) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,743 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.4040 (0.3895) Acc D Real: 99.040% 
Loss D Fake: 0.7513 (0.8068) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6411 (0.5972) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,752 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3560 (0.3892) Acc D Real: 99.037% 
Loss D Fake: 0.7518 (0.8062) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6409 (0.5977) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,761 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4407 (0.3897) Acc D Real: 99.039% 
Loss D Fake: 0.7519 (0.8056) Acc D Fake: 0.000% 
Loss D: 1.193 
Loss G: 0.6407 (0.5982) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,770 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4278 (0.3901) Acc D Real: 99.038% 
Loss D Fake: 0.7523 (0.8051) Acc D Fake: 0.000% 
Loss D: 1.180 
Loss G: 0.6403 (0.5986) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,777 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4056 (0.3903) Acc D Real: 99.041% 
Loss D Fake: 0.7527 (0.8045) Acc D Fake: 0.000% 
Loss D: 1.158 
Loss G: 0.6399 (0.5990) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,785 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3678 (0.3900) Acc D Real: 99.046% 
Loss D Fake: 0.7530 (0.8040) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6399 (0.5995) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,794 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4015 (0.3902) Acc D Real: 99.049% 
Loss D Fake: 0.7529 (0.8035) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6400 (0.5999) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,802 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3491 (0.3897) Acc D Real: 99.053% 
Loss D Fake: 0.7526 (0.8030) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6404 (0.6003) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,809 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3708 (0.3895) Acc D Real: 99.052% 
Loss D Fake: 0.7520 (0.8025) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6411 (0.6007) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,816 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3291 (0.3890) Acc D Real: 99.054% 
Loss D Fake: 0.7511 (0.8020) Acc D Fake: 0.000% 
Loss D: 1.080 
Loss G: 0.6421 (0.6011) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,824 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4281 (0.3893) Acc D Real: 99.054% 
Loss D Fake: 0.7499 (0.8014) Acc D Fake: 0.000% 
Loss D: 1.178 
Loss G: 0.6430 (0.6015) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,831 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.4030 (0.3895) Acc D Real: 99.050% 
Loss D Fake: 0.7491 (0.8009) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6437 (0.6019) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,839 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.4276 (0.3898) Acc D Real: 99.052% 
Loss D Fake: 0.7484 (0.8004) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6442 (0.6023) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,846 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3375 (0.3893) Acc D Real: 99.057% 
Loss D Fake: 0.7477 (0.7999) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6451 (0.6027) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,854 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4170 (0.3896) Acc D Real: 99.060% 
Loss D Fake: 0.7468 (0.7994) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6458 (0.6032) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,861 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3783 (0.3895) Acc D Real: 99.064% 
Loss D Fake: 0.7460 (0.7989) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6466 (0.6036) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,869 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4624 (0.3902) Acc D Real: 99.063% 
Loss D Fake: 0.7453 (0.7984) Acc D Fake: 0.000% 
Loss D: 1.208 
Loss G: 0.6469 (0.6040) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,876 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3523 (0.3898) Acc D Real: 99.068% 
Loss D Fake: 0.7449 (0.7979) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6475 (0.6044) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,883 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4710 (0.3906) Acc D Real: 99.069% 
Loss D Fake: 0.7443 (0.7975) Acc D Fake: 0.000% 
Loss D: 1.215 
Loss G: 0.6477 (0.6048) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,891 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4035 (0.3907) Acc D Real: 99.070% 
Loss D Fake: 0.7442 (0.7970) Acc D Fake: 0.000% 
Loss D: 1.148 
Loss G: 0.6479 (0.6051) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,898 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4662 (0.3913) Acc D Real: 99.070% 
Loss D Fake: 0.7441 (0.7965) Acc D Fake: 0.000% 
Loss D: 1.210 
Loss G: 0.6477 (0.6055) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,906 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4180 (0.3916) Acc D Real: 99.070% 
Loss D Fake: 0.7445 (0.7960) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6474 (0.6059) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,913 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3960 (0.3916) Acc D Real: 99.072% 
Loss D Fake: 0.7448 (0.7956) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6472 (0.6063) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,921 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.4016 (0.3917) Acc D Real: 99.073% 
Loss D Fake: 0.7449 (0.7951) Acc D Fake: 0.000% 
Loss D: 1.146 
Loss G: 0.6472 (0.6066) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,929 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3550 (0.3914) Acc D Real: 99.076% 
Loss D Fake: 0.7448 (0.7947) Acc D Fake: 0.000% 
Loss D: 1.100 
Loss G: 0.6475 (0.6070) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,936 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3524 (0.3911) Acc D Real: 99.075% 
Loss D Fake: 0.7442 (0.7943) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6481 (0.6073) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,944 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4341 (0.3914) Acc D Real: 99.076% 
Loss D Fake: 0.7434 (0.7939) Acc D Fake: 0.000% 
Loss D: 1.178 
Loss G: 0.6487 (0.6077) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,951 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3954 (0.3915) Acc D Real: 99.078% 
Loss D Fake: 0.7429 (0.7934) Acc D Fake: 0.000% 
Loss D: 1.138 
Loss G: 0.6492 (0.6080) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,958 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3156 (0.3908) Acc D Real: 99.082% 
Loss D Fake: 0.7422 (0.7930) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6501 (0.6084) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,966 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3578 (0.3906) Acc D Real: 99.085% 
Loss D Fake: 0.7411 (0.7926) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6511 (0.6087) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,973 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3930 (0.3906) Acc D Real: 99.087% 
Loss D Fake: 0.7400 (0.7921) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6521 (0.6091) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,981 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3963 (0.3906) Acc D Real: 99.088% 
Loss D Fake: 0.7390 (0.7917) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6529 (0.6094) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,988 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4172 (0.3908) Acc D Real: 99.091% 
Loss D Fake: 0.7383 (0.7913) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6535 (0.6098) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:33,997 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3415 (0.3904) Acc D Real: 99.094% 
Loss D Fake: 0.7376 (0.7908) Acc D Fake: 0.000% 
Loss D: 1.079 
Loss G: 0.6542 (0.6101) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,006 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4150 (0.3906) Acc D Real: 99.096% 
Loss D Fake: 0.7367 (0.7904) Acc D Fake: 0.000% 
Loss D: 1.152 
Loss G: 0.6549 (0.6105) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,015 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.4446 (0.3911) Acc D Real: 99.095% 
Loss D Fake: 0.7361 (0.7900) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6553 (0.6108) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,025 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4369 (0.3914) Acc D Real: 99.097% 
Loss D Fake: 0.7359 (0.7896) Acc D Fake: 0.000% 
Loss D: 1.173 
Loss G: 0.6554 (0.6112) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,032 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4668 (0.3920) Acc D Real: 99.100% 
Loss D Fake: 0.7360 (0.7891) Acc D Fake: 0.000% 
Loss D: 1.203 
Loss G: 0.6550 (0.6115) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,040 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4164 (0.3922) Acc D Real: 99.102% 
Loss D Fake: 0.7366 (0.7887) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6545 (0.6119) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,048 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3729 (0.3920) Acc D Real: 99.104% 
Loss D Fake: 0.7370 (0.7884) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6544 (0.6122) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,055 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3533 (0.3917) Acc D Real: 99.109% 
Loss D Fake: 0.7369 (0.7880) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6546 (0.6125) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,063 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.4328 (0.3921) Acc D Real: 99.111% 
Loss D Fake: 0.7366 (0.7876) Acc D Fake: 0.000% 
Loss D: 1.169 
Loss G: 0.6547 (0.6128) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,071 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4028 (0.3921) Acc D Real: 99.113% 
Loss D Fake: 0.7366 (0.7872) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6548 (0.6131) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,078 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3795 (0.3920) Acc D Real: 99.113% 
Loss D Fake: 0.7365 (0.7868) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6550 (0.6135) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,086 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4287 (0.3923) Acc D Real: 99.112% 
Loss D Fake: 0.7363 (0.7864) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6550 (0.6138) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,093 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3863 (0.3923) Acc D Real: 99.113% 
Loss D Fake: 0.7362 (0.7861) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6551 (0.6141) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,100 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4505 (0.3927) Acc D Real: 99.115% 
Loss D Fake: 0.7362 (0.7857) Acc D Fake: 0.000% 
Loss D: 1.187 
Loss G: 0.6550 (0.6144) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,108 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.4795 (0.3933) Acc D Real: 99.115% 
Loss D Fake: 0.7367 (0.7854) Acc D Fake: 0.000% 
Loss D: 1.216 
Loss G: 0.6543 (0.6146) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,115 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4832 (0.3940) Acc D Real: 99.118% 
Loss D Fake: 0.7376 (0.7850) Acc D Fake: 0.000% 
Loss D: 1.221 
Loss G: 0.6532 (0.6149) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,122 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3422 (0.3936) Acc D Real: 99.119% 
Loss D Fake: 0.7387 (0.7847) Acc D Fake: 0.000% 
Loss D: 1.081 
Loss G: 0.6526 (0.6152) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,130 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4513 (0.3940) Acc D Real: 99.122% 
Loss D Fake: 0.7392 (0.7844) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6520 (0.6154) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,137 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4199 (0.3942) Acc D Real: 99.125% 
Loss D Fake: 0.7399 (0.7841) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6513 (0.6157) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,144 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3112 (0.3936) Acc D Real: 99.128% 
Loss D Fake: 0.7404 (0.7838) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6513 (0.6159) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,152 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4256 (0.3938) Acc D Real: 99.130% 
Loss D Fake: 0.7403 (0.7835) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6513 (0.6162) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,159 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4143 (0.3940) Acc D Real: 99.132% 
Loss D Fake: 0.7403 (0.7832) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6512 (0.6164) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,167 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3968 (0.3940) Acc D Real: 99.134% 
Loss D Fake: 0.7404 (0.7829) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6511 (0.6167) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,174 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3703 (0.3938) Acc D Real: 99.137% 
Loss D Fake: 0.7405 (0.7826) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.6512 (0.6169) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,181 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3152 (0.3933) Acc D Real: 99.140% 
Loss D Fake: 0.7402 (0.7823) Acc D Fake: 0.000% 
Loss D: 1.055 
Loss G: 0.6518 (0.6171) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,189 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3174 (0.3928) Acc D Real: 99.145% 
Loss D Fake: 0.7392 (0.7820) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.6529 (0.6174) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,196 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3825 (0.3927) Acc D Real: 99.148% 
Loss D Fake: 0.7379 (0.7817) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6541 (0.6176) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,203 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4335 (0.3930) Acc D Real: 99.149% 
Loss D Fake: 0.7368 (0.7814) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.6549 (0.6179) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,211 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4612 (0.3934) Acc D Real: 99.146% 
Loss D Fake: 0.7361 (0.7811) Acc D Fake: 0.000% 
Loss D: 1.197 
Loss G: 0.6553 (0.6181) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,218 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4175 (0.3936) Acc D Real: 99.149% 
Loss D Fake: 0.7359 (0.7808) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6554 (0.6183) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,225 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3840 (0.3935) Acc D Real: 99.152% 
Loss D Fake: 0.7358 (0.7805) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6555 (0.6186) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,233 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4321 (0.3938) Acc D Real: 99.154% 
Loss D Fake: 0.7357 (0.7803) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6555 (0.6188) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,240 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4015 (0.3938) Acc D Real: 99.157% 
Loss D Fake: 0.7358 (0.7800) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6554 (0.6191) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,247 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.3044 (0.3933) Acc D Real: 99.157% 
Loss D Fake: 0.7356 (0.7797) Acc D Fake: 0.000% 
Loss D: 1.040 
Loss G: 0.6560 (0.6193) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:34,474 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.854 | Generator Loss: 0.656 | Avg: 1.510 
2023-03-02 02:00:34,496 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.944 | Generator Loss: 0.656 | Avg: 1.600 
2023-03-02 02:00:34,520 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.920 | Generator Loss: 0.656 | Avg: 1.576 
2023-03-02 02:00:34,547 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.896 | Generator Loss: 0.656 | Avg: 1.552 
2023-03-02 02:00:34,574 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.895 | Generator Loss: 0.656 | Avg: 1.551 
2023-03-02 02:00:34,600 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.938 | Generator Loss: 0.656 | Avg: 1.594 
2023-03-02 02:00:34,627 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.965 | Generator Loss: 0.656 | Avg: 1.621 
2023-03-02 02:00:34,652 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.004 | Generator Loss: 0.656 | Avg: 1.660 
2023-03-02 02:00:34,679 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.026 | Generator Loss: 0.656 | Avg: 1.682 
2023-03-02 02:00:34,705 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.062 | Generator Loss: 0.656 | Avg: 1.718 
2023-03-02 02:00:34,733 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.089 | Generator Loss: 0.656 | Avg: 1.745 
2023-03-02 02:00:34,760 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.114 | Generator Loss: 0.656 | Avg: 1.770 
2023-03-02 02:00:34,786 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.134 | Generator Loss: 0.656 | Avg: 1.790 
2023-03-02 02:00:34,812 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.150 | Generator Loss: 0.656 | Avg: 1.806 
2023-03-02 02:00:34,839 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.138 | Generator Loss: 0.656 | Avg: 1.794 
2023-03-02 02:00:34,864 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.125 | Generator Loss: 0.656 | Avg: 1.781 
2023-03-02 02:00:34,890 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.112 | Generator Loss: 0.656 | Avg: 1.768 
2023-03-02 02:00:34,923 -                train: [    INFO] - 
Epoch: 17/20
2023-03-02 02:00:35,090 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4032 (0.4052) Acc D Real: 99.323% 
Loss D Fake: 0.7342 (0.7345) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6572 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,098 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.5374 (0.4493) Acc D Real: 99.253% 
Loss D Fake: 0.7339 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.271 
Loss G: 0.6569 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,107 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3301 (0.4195) Acc D Real: 99.362% 
Loss D Fake: 0.7344 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6568 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,124 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4379 (0.4232) Acc D Real: 99.417% 
Loss D Fake: 0.7344 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6566 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,131 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3655 (0.4136) Acc D Real: 99.427% 
Loss D Fake: 0.7346 (0.7344) Acc D Fake: 0.000% 
Loss D: 1.100 
Loss G: 0.6566 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,138 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3264 (0.4011) Acc D Real: 99.479% 
Loss D Fake: 0.7343 (0.7344) Acc D Fake: 0.000% 
Loss D: 1.061 
Loss G: 0.6571 (0.6568) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,145 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3871 (0.3993) Acc D Real: 99.486% 
Loss D Fake: 0.7337 (0.7343) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6577 (0.6569) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,153 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4065 (0.4001) Acc D Real: 99.479% 
Loss D Fake: 0.7331 (0.7342) Acc D Fake: 0.000% 
Loss D: 1.140 
Loss G: 0.6582 (0.6571) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,160 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3886 (0.3990) Acc D Real: 99.479% 
Loss D Fake: 0.7325 (0.7340) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6588 (0.6573) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,167 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.3324 (0.3929) Acc D Real: 99.498% 
Loss D Fake: 0.7318 (0.7338) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6596 (0.6575) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,174 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3968 (0.3933) Acc D Real: 99.510% 
Loss D Fake: 0.7309 (0.7336) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6604 (0.6577) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,181 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4691 (0.3991) Acc D Real: 99.479% 
Loss D Fake: 0.7302 (0.7333) Acc D Fake: 0.000% 
Loss D: 1.199 
Loss G: 0.6607 (0.6579) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,188 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.4188 (0.4005) Acc D Real: 99.479% 
Loss D Fake: 0.7302 (0.7331) Acc D Fake: 0.000% 
Loss D: 1.149 
Loss G: 0.6607 (0.6581) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,196 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3590 (0.3977) Acc D Real: 99.476% 
Loss D Fake: 0.7302 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6608 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,203 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.4113 (0.3986) Acc D Real: 99.473% 
Loss D Fake: 0.7300 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6608 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,212 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4343 (0.4007) Acc D Real: 99.479% 
Loss D Fake: 0.7301 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.164 
Loss G: 0.6607 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,219 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3870 (0.3999) Acc D Real: 99.479% 
Loss D Fake: 0.7303 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6605 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,226 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3799 (0.3989) Acc D Real: 99.490% 
Loss D Fake: 0.7304 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6605 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,233 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.4637 (0.4021) Acc D Real: 99.477% 
Loss D Fake: 0.7305 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6601 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,240 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.4193 (0.4029) Acc D Real: 99.484% 
Loss D Fake: 0.7310 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6596 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,248 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.4381 (0.4045) Acc D Real: 99.496% 
Loss D Fake: 0.7316 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.170 
Loss G: 0.6589 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,255 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.4485 (0.4064) Acc D Real: 99.500% 
Loss D Fake: 0.7325 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6580 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,262 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3638 (0.4047) Acc D Real: 99.507% 
Loss D Fake: 0.7333 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6575 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,269 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3818 (0.4037) Acc D Real: 99.513% 
Loss D Fake: 0.7338 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6572 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,276 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3976 (0.4035) Acc D Real: 99.517% 
Loss D Fake: 0.7340 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6570 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,283 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.4029 (0.4035) Acc D Real: 99.516% 
Loss D Fake: 0.7341 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6569 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,291 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3819 (0.4027) Acc D Real: 99.513% 
Loss D Fake: 0.7342 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6569 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,300 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3757 (0.4018) Acc D Real: 99.519% 
Loss D Fake: 0.7341 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6570 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,307 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.4320 (0.4028) Acc D Real: 99.523% 
Loss D Fake: 0.7340 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6570 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,315 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3831 (0.4022) Acc D Real: 99.530% 
Loss D Fake: 0.7341 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6570 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,322 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.4713 (0.4043) Acc D Real: 99.528% 
Loss D Fake: 0.7342 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.205 
Loss G: 0.6566 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,330 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3816 (0.4036) Acc D Real: 99.528% 
Loss D Fake: 0.7346 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6564 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,337 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3501 (0.4021) Acc D Real: 99.534% 
Loss D Fake: 0.7347 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6565 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,345 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3882 (0.4017) Acc D Real: 99.540% 
Loss D Fake: 0.7345 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6568 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,352 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3826 (0.4011) Acc D Real: 99.544% 
Loss D Fake: 0.7341 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6571 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,360 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3284 (0.3992) Acc D Real: 99.552% 
Loss D Fake: 0.7336 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.062 
Loss G: 0.6578 (0.6581) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,367 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3941 (0.3990) Acc D Real: 99.559% 
Loss D Fake: 0.7328 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6584 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,375 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3255 (0.3971) Acc D Real: 99.561% 
Loss D Fake: 0.7321 (0.7329) Acc D Fake: 0.000% 
Loss D: 1.058 
Loss G: 0.6594 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,382 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4767 (0.3991) Acc D Real: 99.556% 
Loss D Fake: 0.7312 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.208 
Loss G: 0.6598 (0.6582) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,390 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4414 (0.4002) Acc D Real: 99.559% 
Loss D Fake: 0.7311 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.173 
Loss G: 0.6597 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,397 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4397 (0.4011) Acc D Real: 99.559% 
Loss D Fake: 0.7313 (0.7328) Acc D Fake: 0.000% 
Loss D: 1.171 
Loss G: 0.6593 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,405 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3813 (0.4006) Acc D Real: 99.560% 
Loss D Fake: 0.7317 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6590 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,412 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3447 (0.3994) Acc D Real: 99.564% 
Loss D Fake: 0.7319 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.077 
Loss G: 0.6591 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,420 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3748 (0.3988) Acc D Real: 99.565% 
Loss D Fake: 0.7316 (0.7327) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6595 (0.6583) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,427 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.2894 (0.3965) Acc D Real: 99.570% 
Loss D Fake: 0.7310 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.6604 (0.6584) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,434 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.4298 (0.3972) Acc D Real: 99.573% 
Loss D Fake: 0.7299 (0.7326) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6611 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,442 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.4354 (0.3980) Acc D Real: 99.574% 
Loss D Fake: 0.7294 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6614 (0.6585) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,449 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.4521 (0.3991) Acc D Real: 99.575% 
Loss D Fake: 0.7293 (0.7325) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6613 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,457 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3774 (0.3986) Acc D Real: 99.576% 
Loss D Fake: 0.7295 (0.7324) Acc D Fake: 0.000% 
Loss D: 1.107 
Loss G: 0.6612 (0.6586) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,464 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.4032 (0.3987) Acc D Real: 99.580% 
Loss D Fake: 0.7295 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.133 
Loss G: 0.6612 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,472 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4113 (0.3990) Acc D Real: 99.582% 
Loss D Fake: 0.7296 (0.7323) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6610 (0.6587) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,479 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4469 (0.3999) Acc D Real: 99.586% 
Loss D Fake: 0.7299 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6606 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,487 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3430 (0.3988) Acc D Real: 99.586% 
Loss D Fake: 0.7303 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.073 
Loss G: 0.6606 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,494 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3854 (0.3986) Acc D Real: 99.587% 
Loss D Fake: 0.7302 (0.7322) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6607 (0.6588) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,502 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.4166 (0.3989) Acc D Real: 99.589% 
Loss D Fake: 0.7301 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6607 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,509 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4766 (0.4003) Acc D Real: 99.587% 
Loss D Fake: 0.7303 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.6602 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,517 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3926 (0.4001) Acc D Real: 99.591% 
Loss D Fake: 0.7309 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6597 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,524 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3332 (0.3990) Acc D Real: 99.596% 
Loss D Fake: 0.7312 (0.7321) Acc D Fake: 0.000% 
Loss D: 1.064 
Loss G: 0.6597 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,531 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3618 (0.3984) Acc D Real: 99.599% 
Loss D Fake: 0.7310 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6600 (0.6589) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,539 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3092 (0.3969) Acc D Real: 99.601% 
Loss D Fake: 0.7305 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.040 
Loss G: 0.6608 (0.6590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,547 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3895 (0.3968) Acc D Real: 99.603% 
Loss D Fake: 0.7295 (0.7320) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6616 (0.6590) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,554 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3688 (0.3963) Acc D Real: 99.605% 
Loss D Fake: 0.7286 (0.7319) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6625 (0.6591) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,562 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3836 (0.3961) Acc D Real: 99.605% 
Loss D Fake: 0.7277 (0.7319) Acc D Fake: 0.000% 
Loss D: 1.111 
Loss G: 0.6634 (0.6591) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,570 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4125 (0.3964) Acc D Real: 99.607% 
Loss D Fake: 0.7269 (0.7318) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6639 (0.6592) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,579 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4015 (0.3965) Acc D Real: 99.607% 
Loss D Fake: 0.7264 (0.7317) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6643 (0.6593) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,588 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3678 (0.3960) Acc D Real: 99.609% 
Loss D Fake: 0.7260 (0.7316) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6648 (0.6594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,598 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4104 (0.3963) Acc D Real: 99.611% 
Loss D Fake: 0.7255 (0.7315) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6651 (0.6594) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,605 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3996 (0.3963) Acc D Real: 99.611% 
Loss D Fake: 0.7252 (0.7314) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6654 (0.6595) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,613 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4281 (0.3968) Acc D Real: 99.613% 
Loss D Fake: 0.7250 (0.7313) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6654 (0.6596) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,621 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4501 (0.3975) Acc D Real: 99.610% 
Loss D Fake: 0.7252 (0.7313) Acc D Fake: 0.000% 
Loss D: 1.175 
Loss G: 0.6650 (0.6597) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,629 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3990 (0.3975) Acc D Real: 99.612% 
Loss D Fake: 0.7257 (0.7312) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6646 (0.6598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,636 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3724 (0.3972) Acc D Real: 99.615% 
Loss D Fake: 0.7261 (0.7311) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6643 (0.6598) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,644 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3766 (0.3969) Acc D Real: 99.617% 
Loss D Fake: 0.7262 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.103 
Loss G: 0.6643 (0.6599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,652 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3802 (0.3967) Acc D Real: 99.619% 
Loss D Fake: 0.7263 (0.7310) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6643 (0.6599) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,659 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4636 (0.3976) Acc D Real: 99.623% 
Loss D Fake: 0.7264 (0.7309) Acc D Fake: 0.000% 
Loss D: 1.190 
Loss G: 0.6639 (0.6600) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,667 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3629 (0.3971) Acc D Real: 99.626% 
Loss D Fake: 0.7268 (0.7309) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6637 (0.6600) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,675 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.2715 (0.3955) Acc D Real: 99.627% 
Loss D Fake: 0.7267 (0.7308) Acc D Fake: 0.000% 
Loss D: 0.998 
Loss G: 0.6642 (0.6601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,683 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4329 (0.3960) Acc D Real: 99.629% 
Loss D Fake: 0.7260 (0.7308) Acc D Fake: 0.000% 
Loss D: 1.159 
Loss G: 0.6646 (0.6601) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,691 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3749 (0.3957) Acc D Real: 99.631% 
Loss D Fake: 0.7257 (0.7307) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6650 (0.6602) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,699 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3799 (0.3955) Acc D Real: 99.633% 
Loss D Fake: 0.7253 (0.7306) Acc D Fake: 0.000% 
Loss D: 1.105 
Loss G: 0.6655 (0.6603) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,707 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4223 (0.3958) Acc D Real: 99.632% 
Loss D Fake: 0.7248 (0.7306) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6657 (0.6603) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,716 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3445 (0.3952) Acc D Real: 99.633% 
Loss D Fake: 0.7245 (0.7305) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6661 (0.6604) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,724 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3289 (0.3944) Acc D Real: 99.635% 
Loss D Fake: 0.7239 (0.7304) Acc D Fake: 0.000% 
Loss D: 1.053 
Loss G: 0.6669 (0.6605) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,731 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3215 (0.3936) Acc D Real: 99.637% 
Loss D Fake: 0.7229 (0.7303) Acc D Fake: 0.000% 
Loss D: 1.044 
Loss G: 0.6681 (0.6606) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,739 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3072 (0.3926) Acc D Real: 99.638% 
Loss D Fake: 0.7215 (0.7302) Acc D Fake: 0.000% 
Loss D: 1.029 
Loss G: 0.6697 (0.6607) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,747 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3840 (0.3925) Acc D Real: 99.639% 
Loss D Fake: 0.7198 (0.7301) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6712 (0.6608) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,755 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.5043 (0.3938) Acc D Real: 99.638% 
Loss D Fake: 0.7186 (0.7300) Acc D Fake: 0.000% 
Loss D: 1.223 
Loss G: 0.6717 (0.6609) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,763 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4008 (0.3938) Acc D Real: 99.640% 
Loss D Fake: 0.7183 (0.7298) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6720 (0.6611) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,771 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4542 (0.3945) Acc D Real: 99.639% 
Loss D Fake: 0.7183 (0.7297) Acc D Fake: 0.000% 
Loss D: 1.172 
Loss G: 0.6717 (0.6612) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,779 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3496 (0.3940) Acc D Real: 99.641% 
Loss D Fake: 0.7185 (0.7296) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6717 (0.6613) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,787 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3899 (0.3940) Acc D Real: 99.641% 
Loss D Fake: 0.7184 (0.7295) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6718 (0.6614) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,795 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4179 (0.3942) Acc D Real: 99.643% 
Loss D Fake: 0.7184 (0.7293) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6717 (0.6615) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,803 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4706 (0.3950) Acc D Real: 99.642% 
Loss D Fake: 0.7187 (0.7292) Acc D Fake: 0.000% 
Loss D: 1.189 
Loss G: 0.6711 (0.6616) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,811 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3885 (0.3950) Acc D Real: 99.643% 
Loss D Fake: 0.7195 (0.7291) Acc D Fake: 0.000% 
Loss D: 1.108 
Loss G: 0.6704 (0.6617) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,819 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.4608 (0.3957) Acc D Real: 99.642% 
Loss D Fake: 0.7202 (0.7290) Acc D Fake: 0.000% 
Loss D: 1.181 
Loss G: 0.6696 (0.6618) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,827 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.5416 (0.3972) Acc D Real: 99.639% 
Loss D Fake: 0.7214 (0.7290) Acc D Fake: 0.000% 
Loss D: 1.263 
Loss G: 0.6679 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,835 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3247 (0.3964) Acc D Real: 99.640% 
Loss D Fake: 0.7232 (0.7289) Acc D Fake: 0.000% 
Loss D: 1.048 
Loss G: 0.6666 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,843 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3850 (0.3963) Acc D Real: 99.642% 
Loss D Fake: 0.7242 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.6659 (0.6619) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,851 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4357 (0.3967) Acc D Real: 99.644% 
Loss D Fake: 0.7249 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.6651 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,858 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.4055 (0.3968) Acc D Real: 99.646% 
Loss D Fake: 0.7258 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.131 
Loss G: 0.6642 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,867 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3603 (0.3964) Acc D Real: 99.647% 
Loss D Fake: 0.7266 (0.7288) Acc D Fake: 0.000% 
Loss D: 1.087 
Loss G: 0.6636 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,876 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2971 (0.3955) Acc D Real: 99.649% 
Loss D Fake: 0.7269 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.024 
Loss G: 0.6638 (0.6620) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,885 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3099 (0.3946) Acc D Real: 99.651% 
Loss D Fake: 0.7264 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.036 
Loss G: 0.6646 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,894 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3764 (0.3945) Acc D Real: 99.653% 
Loss D Fake: 0.7254 (0.7287) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6654 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,902 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3149 (0.3937) Acc D Real: 99.655% 
Loss D Fake: 0.7245 (0.7286) Acc D Fake: 0.000% 
Loss D: 1.039 
Loss G: 0.6666 (0.6621) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,911 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3337 (0.3932) Acc D Real: 99.656% 
Loss D Fake: 0.7230 (0.7286) Acc D Fake: 0.000% 
Loss D: 1.057 
Loss G: 0.6680 (0.6622) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,920 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3972 (0.3932) Acc D Real: 99.656% 
Loss D Fake: 0.7216 (0.7285) Acc D Fake: 0.000% 
Loss D: 1.119 
Loss G: 0.6692 (0.6622) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,928 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.4561 (0.3938) Acc D Real: 99.655% 
Loss D Fake: 0.7206 (0.7285) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6698 (0.6623) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,937 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.4199 (0.3940) Acc D Real: 99.657% 
Loss D Fake: 0.7203 (0.7284) Acc D Fake: 0.000% 
Loss D: 1.140 
Loss G: 0.6700 (0.6624) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,946 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3842 (0.3939) Acc D Real: 99.657% 
Loss D Fake: 0.7201 (0.7283) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6701 (0.6625) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,954 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3060 (0.3931) Acc D Real: 99.660% 
Loss D Fake: 0.7198 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.026 
Loss G: 0.6707 (0.6625) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,963 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.4159 (0.3933) Acc D Real: 99.662% 
Loss D Fake: 0.7192 (0.7282) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6712 (0.6626) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,972 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3986 (0.3934) Acc D Real: 99.664% 
Loss D Fake: 0.7188 (0.7281) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6714 (0.6627) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,980 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3478 (0.3930) Acc D Real: 99.665% 
Loss D Fake: 0.7185 (0.7280) Acc D Fake: 0.000% 
Loss D: 1.066 
Loss G: 0.6719 (0.6628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,987 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3825 (0.3929) Acc D Real: 99.667% 
Loss D Fake: 0.7180 (0.7279) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6723 (0.6628) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:35,995 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4425 (0.3933) Acc D Real: 99.667% 
Loss D Fake: 0.7177 (0.7278) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6724 (0.6629) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:36,003 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3659 (0.3931) Acc D Real: 99.670% 
Loss D Fake: 0.7176 (0.7277) Acc D Fake: 0.000% 
Loss D: 1.084 
Loss G: 0.6725 (0.6630) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:36,011 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3641 (0.3928) Acc D Real: 99.671% 
Loss D Fake: 0.7174 (0.7276) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6728 (0.6631) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:36,018 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3538 (0.3925) Acc D Real: 99.671% 
Loss D Fake: 0.7171 (0.7276) Acc D Fake: 0.000% 
Loss D: 1.071 
Loss G: 0.6733 (0.6632) Acc G: 99.583% 
LR: 2.000e-04 

2023-03-02 02:00:36,025 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3612 (0.3923) Acc D Real: 99.557% 
Loss D Fake: 0.7165 (0.7275) Acc D Fake: 0.413% 
Loss D: 1.078 
Loss G: 0.6739 (0.6633) Acc G: 99.091% 
LR: 2.000e-04 

2023-03-02 02:00:36,033 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3945 (0.3923) Acc D Real: 99.381% 
Loss D Fake: 0.7159 (0.7274) Acc D Fake: 0.902% 
Loss D: 1.110 
Loss G: 0.6744 (0.6634) Acc G: 98.579% 
LR: 2.000e-04 

2023-03-02 02:00:36,040 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4318 (0.3926) Acc D Real: 99.175% 
Loss D Fake: 0.7154 (0.7273) Acc D Fake: 1.396% 
Loss D: 1.147 
Loss G: 0.6747 (0.6634) Acc G: 98.062% 
LR: 2.000e-04 

2023-03-02 02:00:36,048 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3787 (0.3925) Acc D Real: 98.950% 
Loss D Fake: 0.7152 (0.7272) Acc D Fake: 1.895% 
Loss D: 1.094 
Loss G: 0.6750 (0.6635) Acc G: 97.554% 
LR: 2.000e-04 

2023-03-02 02:00:36,056 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3715 (0.3923) Acc D Real: 98.731% 
Loss D Fake: 0.7148 (0.7271) Acc D Fake: 2.400% 
Loss D: 1.086 
Loss G: 0.6754 (0.6636) Acc G: 97.040% 
LR: 2.000e-04 

2023-03-02 02:00:36,063 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3800 (0.3922) Acc D Real: 98.488% 
Loss D Fake: 0.7144 (0.7270) Acc D Fake: 2.910% 
Loss D: 1.094 
Loss G: 0.6757 (0.6637) Acc G: 96.534% 
LR: 2.000e-04 

2023-03-02 02:00:36,071 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3563 (0.3919) Acc D Real: 98.263% 
Loss D Fake: 0.7140 (0.7269) Acc D Fake: 3.412% 
Loss D: 1.070 
Loss G: 0.6762 (0.6638) Acc G: 96.024% 
LR: 2.000e-04 

2023-03-02 02:00:36,079 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3822 (0.3919) Acc D Real: 98.005% 
Loss D Fake: 0.7135 (0.7268) Acc D Fake: 3.919% 
Loss D: 1.096 
Loss G: 0.6766 (0.6639) Acc G: 95.521% 
LR: 2.000e-04 

2023-03-02 02:00:36,086 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.2889 (0.3911) Acc D Real: 97.862% 
Loss D Fake: 0.7129 (0.7267) Acc D Fake: 4.419% 
Loss D: 1.002 
Loss G: 0.6776 (0.6640) Acc G: 95.013% 
LR: 2.000e-04 

2023-03-02 02:00:36,094 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3996 (0.3911) Acc D Real: 97.605% 
Loss D Fake: 0.7118 (0.7265) Acc D Fake: 4.923% 
Loss D: 1.111 
Loss G: 0.6785 (0.6641) Acc G: 94.513% 
LR: 2.000e-04 

2023-03-02 02:00:36,101 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.2938 (0.3904) Acc D Real: 97.435% 
Loss D Fake: 0.7109 (0.7264) Acc D Fake: 5.433% 
Loss D: 1.005 
Loss G: 0.6797 (0.6643) Acc G: 94.008% 
LR: 2.000e-04 

2023-03-02 02:00:36,109 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3642 (0.3902) Acc D Real: 97.204% 
Loss D Fake: 0.7095 (0.7263) Acc D Fake: 5.934% 
Loss D: 1.074 
Loss G: 0.6810 (0.6644) Acc G: 93.497% 
LR: 2.000e-04 

2023-03-02 02:00:36,118 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3527 (0.3899) Acc D Real: 96.974% 
Loss D Fake: 0.7082 (0.7262) Acc D Fake: 6.441% 
Loss D: 1.061 
Loss G: 0.6823 (0.6645) Acc G: 92.995% 
LR: 2.000e-04 

2023-03-02 02:00:36,126 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.3252 (0.3894) Acc D Real: 96.788% 
Loss D Fake: 0.7069 (0.7260) Acc D Fake: 6.940% 
Loss D: 1.032 
Loss G: 0.6838 (0.6647) Acc G: 92.500% 
LR: 2.000e-04 

2023-03-02 02:00:36,134 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4558 (0.3899) Acc D Real: 96.477% 
Loss D Fake: 0.7055 (0.7259) Acc D Fake: 7.432% 
Loss D: 1.161 
Loss G: 0.6847 (0.6648) Acc G: 92.000% 
LR: 2.000e-04 

2023-03-02 02:00:36,141 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4037 (0.3900) Acc D Real: 96.211% 
Loss D Fake: 0.7048 (0.7257) Acc D Fake: 7.929% 
Loss D: 1.108 
Loss G: 0.6853 (0.6650) Acc G: 91.507% 
LR: 2.000e-04 

2023-03-02 02:00:36,149 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4588 (0.3905) Acc D Real: 95.893% 
Loss D Fake: 0.7044 (0.7256) Acc D Fake: 8.418% 
Loss D: 1.163 
Loss G: 0.6853 (0.6651) Acc G: 91.022% 
LR: 2.000e-04 

2023-03-02 02:00:36,156 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3213 (0.3900) Acc D Real: 95.716% 
Loss D Fake: 0.7044 (0.7254) Acc D Fake: 8.901% 
Loss D: 1.026 
Loss G: 0.6856 (0.6653) Acc G: 90.543% 
LR: 2.000e-04 

2023-03-02 02:00:36,164 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3357 (0.3896) Acc D Real: 95.520% 
Loss D Fake: 0.7039 (0.7252) Acc D Fake: 9.376% 
Loss D: 1.040 
Loss G: 0.6863 (0.6654) Acc G: 90.072% 
LR: 2.000e-04 

2023-03-02 02:00:36,171 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3491 (0.3893) Acc D Real: 95.323% 
Loss D Fake: 0.7032 (0.7251) Acc D Fake: 9.845% 
Loss D: 1.052 
Loss G: 0.6871 (0.6656) Acc G: 89.607% 
LR: 2.000e-04 

2023-03-02 02:00:36,178 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3892 (0.3893) Acc D Real: 95.083% 
Loss D Fake: 0.7024 (0.7249) Acc D Fake: 10.307% 
Loss D: 1.092 
Loss G: 0.6878 (0.6657) Acc G: 89.149% 
LR: 2.000e-04 

2023-03-02 02:00:36,185 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3647 (0.3892) Acc D Real: 94.886% 
Loss D Fake: 0.7016 (0.7248) Acc D Fake: 10.763% 
Loss D: 1.066 
Loss G: 0.6886 (0.6659) Acc G: 88.697% 
LR: 2.000e-04 

2023-03-02 02:00:36,193 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3339 (0.3888) Acc D Real: 94.704% 
Loss D Fake: 0.7008 (0.7246) Acc D Fake: 11.212% 
Loss D: 1.035 
Loss G: 0.6896 (0.6661) Acc G: 88.252% 
LR: 2.000e-04 

2023-03-02 02:00:36,200 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.4855 (0.3894) Acc D Real: 94.394% 
Loss D Fake: 0.7000 (0.7244) Acc D Fake: 11.655% 
Loss D: 1.185 
Loss G: 0.6899 (0.6662) Acc G: 87.812% 
LR: 2.000e-04 

2023-03-02 02:00:36,208 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.2813 (0.3887) Acc D Real: 94.265% 
Loss D Fake: 0.6997 (0.7243) Acc D Fake: 12.092% 
Loss D: 0.981 
Loss G: 0.6906 (0.6664) Acc G: 87.379% 
LR: 2.000e-04 

2023-03-02 02:00:36,215 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.4636 (0.3892) Acc D Real: 93.982% 
Loss D Fake: 0.6990 (0.7241) Acc D Fake: 12.523% 
Loss D: 1.163 
Loss G: 0.6909 (0.6666) Acc G: 86.941% 
LR: 2.000e-04 

2023-03-02 02:00:36,223 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3760 (0.3891) Acc D Real: 93.777% 
Loss D Fake: 0.6988 (0.7239) Acc D Fake: 12.948% 
Loss D: 1.075 
Loss G: 0.6911 (0.6667) Acc G: 86.508% 
LR: 2.000e-04 

2023-03-02 02:00:36,231 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.4099 (0.3893) Acc D Real: 93.541% 
Loss D Fake: 0.6987 (0.7237) Acc D Fake: 13.378% 
Loss D: 1.109 
Loss G: 0.6912 (0.6669) Acc G: 86.081% 
LR: 2.000e-04 

2023-03-02 02:00:36,239 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3409 (0.3889) Acc D Real: 93.370% 
Loss D Fake: 0.6986 (0.7236) Acc D Fake: 13.803% 
Loss D: 1.039 
Loss G: 0.6914 (0.6671) Acc G: 85.660% 
LR: 2.000e-04 

2023-03-02 02:00:36,248 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3633 (0.3888) Acc D Real: 93.188% 
Loss D Fake: 0.6982 (0.7234) Acc D Fake: 14.222% 
Loss D: 1.062 
Loss G: 0.6919 (0.6672) Acc G: 85.244% 
LR: 2.000e-04 

2023-03-02 02:00:36,256 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3663 (0.3886) Acc D Real: 93.003% 
Loss D Fake: 0.6977 (0.7232) Acc D Fake: 14.636% 
Loss D: 1.064 
Loss G: 0.6925 (0.6674) Acc G: 84.834% 
LR: 2.000e-04 

2023-03-02 02:00:36,263 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3878 (0.3886) Acc D Real: 92.791% 
Loss D Fake: 0.6971 (0.7231) Acc D Fake: 15.044% 
Loss D: 1.085 
Loss G: 0.6930 (0.6676) Acc G: 84.430% 
LR: 2.000e-04 

2023-03-02 02:00:36,271 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.4084 (0.3887) Acc D Real: 92.567% 
Loss D Fake: 0.6968 (0.7229) Acc D Fake: 15.447% 
Loss D: 1.105 
Loss G: 0.6932 (0.6677) Acc G: 84.031% 
LR: 2.000e-04 

2023-03-02 02:00:36,278 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4856 (0.3894) Acc D Real: 92.276% 
Loss D Fake: 0.6968 (0.7227) Acc D Fake: 15.844% 
Loss D: 1.182 
Loss G: 0.6927 (0.6679) Acc G: 83.636% 
LR: 2.000e-04 

2023-03-02 02:00:36,285 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.4042 (0.3895) Acc D Real: 92.054% 
Loss D Fake: 0.6975 (0.7226) Acc D Fake: 16.237% 
Loss D: 1.102 
Loss G: 0.6919 (0.6680) Acc G: 83.247% 
LR: 2.000e-04 

2023-03-02 02:00:36,293 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.4449 (0.3898) Acc D Real: 91.816% 
Loss D Fake: 0.6984 (0.7224) Acc D Fake: 16.624% 
Loss D: 1.143 
Loss G: 0.6909 (0.6682) Acc G: 82.874% 
LR: 2.000e-04 

2023-03-02 02:00:36,300 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4314 (0.3901) Acc D Real: 91.597% 
Loss D Fake: 0.6995 (0.7223) Acc D Fake: 16.996% 
Loss D: 1.131 
Loss G: 0.6898 (0.6683) Acc G: 82.505% 
LR: 2.000e-04 

2023-03-02 02:00:36,307 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.6905 (0.3920) Acc D Real: 91.554% 
Loss D Fake: 0.7012 (0.7221) Acc D Fake: 17.030% 
Loss D: 1.392 
Loss G: 0.6869 (0.6684) Acc G: 82.471% 
LR: 2.000e-04 

2023-03-02 02:00:36,508 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.797 | Generator Loss: 0.687 | Avg: 1.484 
2023-03-02 02:00:36,531 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.884 | Generator Loss: 0.687 | Avg: 1.571 
2023-03-02 02:00:36,554 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.861 | Generator Loss: 0.687 | Avg: 1.548 
2023-03-02 02:00:36,583 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.838 | Generator Loss: 0.687 | Avg: 1.525 
2023-03-02 02:00:36,610 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.837 | Generator Loss: 0.687 | Avg: 1.524 
2023-03-02 02:00:36,641 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.886 | Generator Loss: 0.687 | Avg: 1.573 
2023-03-02 02:00:36,672 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.910 | Generator Loss: 0.687 | Avg: 1.597 
2023-03-02 02:00:36,700 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.952 | Generator Loss: 0.687 | Avg: 1.639 
2023-03-02 02:00:36,728 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.975 | Generator Loss: 0.687 | Avg: 1.662 
2023-03-02 02:00:36,754 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.015 | Generator Loss: 0.687 | Avg: 1.702 
2023-03-02 02:00:36,779 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.044 | Generator Loss: 0.687 | Avg: 1.731 
2023-03-02 02:00:36,805 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.073 | Generator Loss: 0.687 | Avg: 1.760 
2023-03-02 02:00:36,831 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.096 | Generator Loss: 0.687 | Avg: 1.783 
2023-03-02 02:00:36,857 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.112 | Generator Loss: 0.687 | Avg: 1.799 
2023-03-02 02:00:36,883 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.098 | Generator Loss: 0.687 | Avg: 1.785 
2023-03-02 02:00:36,908 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.083 | Generator Loss: 0.687 | Avg: 1.770 
2023-03-02 02:00:36,934 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.069 | Generator Loss: 0.687 | Avg: 1.756 
2023-03-02 02:00:36,968 -                train: [    INFO] - 
Epoch: 18/20
2023-03-02 02:00:37,126 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4380 (0.3861) Acc D Real: 63.255% 
Loss D Fake: 0.7066 (0.7054) Acc D Fake: 74.167% 
Loss D: 1.145 
Loss G: 0.6821 (0.6833) Acc G: 26.667% 
LR: 2.000e-04 

2023-03-02 02:00:37,136 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3838 (0.3853) Acc D Real: 63.455% 
Loss D Fake: 0.7087 (0.7065) Acc D Fake: 73.333% 
Loss D: 1.092 
Loss G: 0.6802 (0.6822) Acc G: 27.222% 
LR: 2.000e-04 

2023-03-02 02:00:37,144 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3807 (0.3842) Acc D Real: 63.438% 
Loss D Fake: 0.7104 (0.7075) Acc D Fake: 72.917% 
Loss D: 1.091 
Loss G: 0.6787 (0.6814) Acc G: 27.500% 
LR: 2.000e-04 

2023-03-02 02:00:37,175 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3338 (0.3741) Acc D Real: 65.448% 
Loss D Fake: 0.7117 (0.7083) Acc D Fake: 72.333% 
Loss D: 1.046 
Loss G: 0.6778 (0.6807) Acc G: 28.000% 
LR: 2.000e-04 

2023-03-02 02:00:37,183 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.2942 (0.3608) Acc D Real: 67.578% 
Loss D Fake: 0.7122 (0.7090) Acc D Fake: 71.944% 
Loss D: 1.006 
Loss G: 0.6778 (0.6802) Acc G: 28.333% 
LR: 2.000e-04 

2023-03-02 02:00:37,190 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3479 (0.3590) Acc D Real: 67.820% 
Loss D Fake: 0.7119 (0.7094) Acc D Fake: 71.667% 
Loss D: 1.060 
Loss G: 0.6782 (0.6799) Acc G: 28.571% 
LR: 2.000e-04 

2023-03-02 02:00:37,197 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4723 (0.3731) Acc D Real: 66.250% 
Loss D Fake: 0.7117 (0.7097) Acc D Fake: 71.458% 
Loss D: 1.184 
Loss G: 0.6780 (0.6797) Acc G: 28.750% 
LR: 2.000e-04 

2023-03-02 02:00:37,204 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.4026 (0.3764) Acc D Real: 65.642% 
Loss D Fake: 0.7121 (0.7099) Acc D Fake: 71.296% 
Loss D: 1.115 
Loss G: 0.6775 (0.6794) Acc G: 28.889% 
LR: 2.000e-04 

2023-03-02 02:00:37,211 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3834 (0.3771) Acc D Real: 65.531% 
Loss D Fake: 0.7125 (0.7102) Acc D Fake: 71.000% 
Loss D: 1.096 
Loss G: 0.6771 (0.6792) Acc G: 29.000% 
LR: 2.000e-04 

2023-03-02 02:00:37,218 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4125 (0.3803) Acc D Real: 65.128% 
Loss D Fake: 0.7130 (0.7105) Acc D Fake: 70.758% 
Loss D: 1.125 
Loss G: 0.6766 (0.6790) Acc G: 29.242% 
LR: 2.000e-04 

2023-03-02 02:00:37,225 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3822 (0.3805) Acc D Real: 65.642% 
Loss D Fake: 0.7135 (0.7107) Acc D Fake: 70.556% 
Loss D: 1.096 
Loss G: 0.6762 (0.6787) Acc G: 29.444% 
LR: 2.000e-04 

2023-03-02 02:00:37,232 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4784 (0.3880) Acc D Real: 64.900% 
Loss D Fake: 0.7141 (0.7110) Acc D Fake: 70.256% 
Loss D: 1.193 
Loss G: 0.6753 (0.6785) Acc G: 29.744% 
LR: 2.000e-04 

2023-03-02 02:00:37,239 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3857 (0.3878) Acc D Real: 65.320% 
Loss D Fake: 0.7151 (0.7113) Acc D Fake: 69.762% 
Loss D: 1.101 
Loss G: 0.6745 (0.6782) Acc G: 30.238% 
LR: 2.000e-04 

2023-03-02 02:00:37,246 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.2996 (0.3820) Acc D Real: 66.208% 
Loss D Fake: 0.7157 (0.7116) Acc D Fake: 69.111% 
Loss D: 1.015 
Loss G: 0.6742 (0.6779) Acc G: 30.778% 
LR: 2.000e-04 

2023-03-02 02:00:37,253 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3603 (0.3806) Acc D Real: 66.865% 
Loss D Fake: 0.7157 (0.7118) Acc D Fake: 68.542% 
Loss D: 1.076 
Loss G: 0.6743 (0.6777) Acc G: 31.146% 
LR: 2.000e-04 

2023-03-02 02:00:37,261 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4161 (0.3827) Acc D Real: 67.194% 
Loss D Fake: 0.7156 (0.7120) Acc D Fake: 68.137% 
Loss D: 1.132 
Loss G: 0.6743 (0.6775) Acc G: 31.471% 
LR: 2.000e-04 

2023-03-02 02:00:37,268 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3633 (0.3816) Acc D Real: 67.804% 
Loss D Fake: 0.7157 (0.7122) Acc D Fake: 67.778% 
Loss D: 1.079 
Loss G: 0.6743 (0.6773) Acc G: 31.759% 
LR: 2.000e-04 

2023-03-02 02:00:37,275 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.4700 (0.3863) Acc D Real: 67.569% 
Loss D Fake: 0.7158 (0.7124) Acc D Fake: 67.368% 
Loss D: 1.186 
Loss G: 0.6739 (0.6771) Acc G: 32.193% 
LR: 2.000e-04 

2023-03-02 02:00:37,282 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3981 (0.3869) Acc D Real: 68.151% 
Loss D Fake: 0.7164 (0.7126) Acc D Fake: 66.583% 
Loss D: 1.115 
Loss G: 0.6732 (0.6769) Acc G: 33.000% 
LR: 2.000e-04 

2023-03-02 02:00:37,289 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.3620 (0.3857) Acc D Real: 69.187% 
Loss D Fake: 0.7169 (0.7128) Acc D Fake: 63.413% 
Loss D: 1.079 
Loss G: 0.6729 (0.6767) Acc G: 36.190% 
LR: 2.000e-04 

2023-03-02 02:00:37,297 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3463 (0.3839) Acc D Real: 70.530% 
Loss D Fake: 0.7171 (0.7130) Acc D Fake: 60.530% 
Loss D: 1.063 
Loss G: 0.6729 (0.6766) Acc G: 39.091% 
LR: 2.000e-04 

2023-03-02 02:00:37,304 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3866 (0.3840) Acc D Real: 71.517% 
Loss D Fake: 0.7170 (0.7132) Acc D Fake: 57.899% 
Loss D: 1.104 
Loss G: 0.6730 (0.6764) Acc G: 40.290% 
LR: 2.000e-04 

2023-03-02 02:00:37,311 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.4526 (0.3869) Acc D Real: 72.437% 
Loss D Fake: 0.7171 (0.7134) Acc D Fake: 55.486% 
Loss D: 1.170 
Loss G: 0.6727 (0.6763) Acc G: 42.778% 
LR: 2.000e-04 

2023-03-02 02:00:37,318 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.3797 (0.3866) Acc D Real: 73.515% 
Loss D Fake: 0.7175 (0.7135) Acc D Fake: 53.267% 
Loss D: 1.097 
Loss G: 0.6723 (0.6761) Acc G: 45.067% 
LR: 2.000e-04 

2023-03-02 02:00:37,325 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.4185 (0.3878) Acc D Real: 74.519% 
Loss D Fake: 0.7179 (0.7137) Acc D Fake: 51.218% 
Loss D: 1.136 
Loss G: 0.6718 (0.6759) Acc G: 47.179% 
LR: 2.000e-04 

2023-03-02 02:00:37,333 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3851 (0.3877) Acc D Real: 75.455% 
Loss D Fake: 0.7184 (0.7139) Acc D Fake: 49.321% 
Loss D: 1.104 
Loss G: 0.6714 (0.6758) Acc G: 49.136% 
LR: 2.000e-04 

2023-03-02 02:00:37,340 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4185 (0.3888) Acc D Real: 76.328% 
Loss D Fake: 0.7189 (0.7140) Acc D Fake: 47.560% 
Loss D: 1.137 
Loss G: 0.6709 (0.6756) Acc G: 50.952% 
LR: 2.000e-04 

2023-03-02 02:00:37,348 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.3735 (0.3883) Acc D Real: 77.141% 
Loss D Fake: 0.7194 (0.7142) Acc D Fake: 45.920% 
Loss D: 1.093 
Loss G: 0.6705 (0.6754) Acc G: 52.644% 
LR: 2.000e-04 

2023-03-02 02:00:37,356 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.4600 (0.3907) Acc D Real: 77.898% 
Loss D Fake: 0.7198 (0.7144) Acc D Fake: 44.389% 
Loss D: 1.180 
Loss G: 0.6699 (0.6752) Acc G: 54.222% 
LR: 2.000e-04 

2023-03-02 02:00:37,363 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3311 (0.3888) Acc D Real: 78.607% 
Loss D Fake: 0.7204 (0.7146) Acc D Fake: 42.957% 
Loss D: 1.051 
Loss G: 0.6696 (0.6750) Acc G: 55.699% 
LR: 2.000e-04 

2023-03-02 02:00:37,371 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3907 (0.3888) Acc D Real: 79.269% 
Loss D Fake: 0.7206 (0.7148) Acc D Fake: 41.615% 
Loss D: 1.111 
Loss G: 0.6695 (0.6749) Acc G: 57.083% 
LR: 2.000e-04 

2023-03-02 02:00:37,378 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.4636 (0.3911) Acc D Real: 79.894% 
Loss D Fake: 0.7208 (0.7150) Acc D Fake: 40.354% 
Loss D: 1.184 
Loss G: 0.6689 (0.6747) Acc G: 58.384% 
LR: 2.000e-04 

2023-03-02 02:00:37,386 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3030 (0.3885) Acc D Real: 80.481% 
Loss D Fake: 0.7214 (0.7152) Acc D Fake: 39.167% 
Loss D: 1.024 
Loss G: 0.6687 (0.6745) Acc G: 59.608% 
LR: 2.000e-04 

2023-03-02 02:00:37,394 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3879 (0.3885) Acc D Real: 81.036% 
Loss D Fake: 0.7214 (0.7153) Acc D Fake: 38.048% 
Loss D: 1.109 
Loss G: 0.6686 (0.6743) Acc G: 60.762% 
LR: 2.000e-04 

2023-03-02 02:00:37,401 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3586 (0.3876) Acc D Real: 81.560% 
Loss D Fake: 0.7214 (0.7155) Acc D Fake: 36.991% 
Loss D: 1.080 
Loss G: 0.6687 (0.6742) Acc G: 61.852% 
LR: 2.000e-04 

2023-03-02 02:00:37,409 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3426 (0.3864) Acc D Real: 82.058% 
Loss D Fake: 0.7212 (0.7157) Acc D Fake: 35.991% 
Loss D: 1.064 
Loss G: 0.6690 (0.6740) Acc G: 62.883% 
LR: 2.000e-04 

2023-03-02 02:00:37,417 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.3798 (0.3863) Acc D Real: 82.526% 
Loss D Fake: 0.7208 (0.7158) Acc D Fake: 35.044% 
Loss D: 1.101 
Loss G: 0.6694 (0.6739) Acc G: 63.860% 
LR: 2.000e-04 

2023-03-02 02:00:37,424 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.2483 (0.3827) Acc D Real: 82.971% 
Loss D Fake: 0.7202 (0.7159) Acc D Fake: 34.145% 
Loss D: 0.968 
Loss G: 0.6705 (0.6738) Acc G: 64.786% 
LR: 2.000e-04 

2023-03-02 02:00:37,432 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3437 (0.3817) Acc D Real: 83.396% 
Loss D Fake: 0.7188 (0.7160) Acc D Fake: 33.292% 
Loss D: 1.063 
Loss G: 0.6718 (0.6738) Acc G: 65.667% 
LR: 2.000e-04 

2023-03-02 02:00:37,440 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.4155 (0.3826) Acc D Real: 83.793% 
Loss D Fake: 0.7176 (0.7160) Acc D Fake: 32.480% 
Loss D: 1.133 
Loss G: 0.6728 (0.6738) Acc G: 66.504% 
LR: 2.000e-04 

2023-03-02 02:00:37,447 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4351 (0.3838) Acc D Real: 84.017% 
Loss D Fake: 0.7168 (0.7161) Acc D Fake: 31.706% 
Loss D: 1.152 
Loss G: 0.6732 (0.6737) Acc G: 66.071% 
LR: 2.000e-04 

2023-03-02 02:00:37,455 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3968 (0.3841) Acc D Real: 84.090% 
Loss D Fake: 0.7166 (0.7161) Acc D Fake: 31.899% 
Loss D: 1.113 
Loss G: 0.6734 (0.6737) Acc G: 65.581% 
LR: 2.000e-04 

2023-03-02 02:00:37,463 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3174 (0.3826) Acc D Real: 84.272% 
Loss D Fake: 0.7163 (0.7161) Acc D Fake: 32.386% 
Loss D: 1.034 
Loss G: 0.6739 (0.6737) Acc G: 65.000% 
LR: 2.000e-04 

2023-03-02 02:00:37,470 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3962 (0.3829) Acc D Real: 84.119% 
Loss D Fake: 0.7158 (0.7161) Acc D Fake: 33.000% 
Loss D: 1.112 
Loss G: 0.6743 (0.6738) Acc G: 64.370% 
LR: 2.000e-04 

2023-03-02 02:00:37,478 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4225 (0.3838) Acc D Real: 83.899% 
Loss D Fake: 0.7155 (0.7161) Acc D Fake: 33.623% 
Loss D: 1.138 
Loss G: 0.6744 (0.6738) Acc G: 63.768% 
LR: 2.000e-04 

2023-03-02 02:00:37,486 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3750 (0.3836) Acc D Real: 83.757% 
Loss D Fake: 0.7155 (0.7160) Acc D Fake: 34.220% 
Loss D: 1.090 
Loss G: 0.6744 (0.6738) Acc G: 63.191% 
LR: 2.000e-04 

2023-03-02 02:00:37,493 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3842 (0.3836) Acc D Real: 83.600% 
Loss D Fake: 0.7154 (0.7160) Acc D Fake: 34.792% 
Loss D: 1.100 
Loss G: 0.6745 (0.6738) Acc G: 62.639% 
LR: 2.000e-04 

2023-03-02 02:00:37,501 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3614 (0.3831) Acc D Real: 83.476% 
Loss D Fake: 0.7153 (0.7160) Acc D Fake: 35.340% 
Loss D: 1.077 
Loss G: 0.6746 (0.6738) Acc G: 62.109% 
LR: 2.000e-04 

2023-03-02 02:00:37,509 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3836 (0.3831) Acc D Real: 83.281% 
Loss D Fake: 0.7151 (0.7160) Acc D Fake: 35.900% 
Loss D: 1.099 
Loss G: 0.6748 (0.6738) Acc G: 61.567% 
LR: 2.000e-04 

2023-03-02 02:00:37,516 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3955 (0.3834) Acc D Real: 83.086% 
Loss D Fake: 0.7150 (0.7160) Acc D Fake: 36.438% 
Loss D: 1.110 
Loss G: 0.6749 (0.6739) Acc G: 61.046% 
LR: 2.000e-04 

2023-03-02 02:00:37,524 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.4250 (0.3842) Acc D Real: 82.755% 
Loss D Fake: 0.7150 (0.7160) Acc D Fake: 36.955% 
Loss D: 1.140 
Loss G: 0.6747 (0.6739) Acc G: 60.545% 
LR: 2.000e-04 

2023-03-02 02:00:37,531 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3304 (0.3832) Acc D Real: 82.696% 
Loss D Fake: 0.7152 (0.7159) Acc D Fake: 37.453% 
Loss D: 1.046 
Loss G: 0.6748 (0.6739) Acc G: 60.063% 
LR: 2.000e-04 

2023-03-02 02:00:37,539 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3136 (0.3819) Acc D Real: 82.747% 
Loss D Fake: 0.7148 (0.7159) Acc D Fake: 37.963% 
Loss D: 1.028 
Loss G: 0.6754 (0.6739) Acc G: 59.568% 
LR: 2.000e-04 

2023-03-02 02:00:37,547 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3289 (0.3809) Acc D Real: 82.661% 
Loss D Fake: 0.7140 (0.7159) Acc D Fake: 38.485% 
Loss D: 1.043 
Loss G: 0.6763 (0.6740) Acc G: 59.061% 
LR: 2.000e-04 

2023-03-02 02:00:37,554 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3098 (0.3796) Acc D Real: 82.557% 
Loss D Fake: 0.7129 (0.7158) Acc D Fake: 39.018% 
Loss D: 1.023 
Loss G: 0.6776 (0.6740) Acc G: 58.542% 
LR: 2.000e-04 

2023-03-02 02:00:37,563 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3894 (0.3798) Acc D Real: 82.282% 
Loss D Fake: 0.7116 (0.7158) Acc D Fake: 39.561% 
Loss D: 1.101 
Loss G: 0.6787 (0.6741) Acc G: 58.041% 
LR: 2.000e-04 

2023-03-02 02:00:37,570 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3168 (0.3787) Acc D Real: 82.126% 
Loss D Fake: 0.7104 (0.7157) Acc D Fake: 40.115% 
Loss D: 1.027 
Loss G: 0.6800 (0.6742) Acc G: 57.529% 
LR: 2.000e-04 

2023-03-02 02:00:37,578 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3847 (0.3788) Acc D Real: 81.806% 
Loss D Fake: 0.7091 (0.7156) Acc D Fake: 40.650% 
Loss D: 1.094 
Loss G: 0.6812 (0.6743) Acc G: 57.034% 
LR: 2.000e-04 

2023-03-02 02:00:37,585 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3938 (0.3791) Acc D Real: 81.463% 
Loss D Fake: 0.7081 (0.7154) Acc D Fake: 41.167% 
Loss D: 1.102 
Loss G: 0.6820 (0.6745) Acc G: 56.528% 
LR: 2.000e-04 

2023-03-02 02:00:37,593 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.3468 (0.3786) Acc D Real: 81.237% 
Loss D Fake: 0.7073 (0.7153) Acc D Fake: 41.694% 
Loss D: 1.054 
Loss G: 0.6829 (0.6746) Acc G: 56.038% 
LR: 2.000e-04 

2023-03-02 02:00:37,602 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3887 (0.3787) Acc D Real: 80.946% 
Loss D Fake: 0.7064 (0.7152) Acc D Fake: 42.204% 
Loss D: 1.095 
Loss G: 0.6836 (0.6747) Acc G: 55.565% 
LR: 2.000e-04 

2023-03-02 02:00:37,611 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3608 (0.3784) Acc D Real: 80.747% 
Loss D Fake: 0.7057 (0.7150) Acc D Fake: 42.698% 
Loss D: 1.066 
Loss G: 0.6844 (0.6749) Acc G: 55.106% 
LR: 2.000e-04 

2023-03-02 02:00:37,619 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3546 (0.3781) Acc D Real: 80.491% 
Loss D Fake: 0.7049 (0.7148) Acc D Fake: 43.177% 
Loss D: 1.060 
Loss G: 0.6851 (0.6750) Acc G: 54.661% 
LR: 2.000e-04 

2023-03-02 02:00:37,626 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4430 (0.3791) Acc D Real: 80.102% 
Loss D Fake: 0.7043 (0.7147) Acc D Fake: 43.641% 
Loss D: 1.147 
Loss G: 0.6855 (0.6752) Acc G: 54.231% 
LR: 2.000e-04 

2023-03-02 02:00:37,633 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.3937 (0.3793) Acc D Real: 79.857% 
Loss D Fake: 0.7041 (0.7145) Acc D Fake: 44.091% 
Loss D: 1.098 
Loss G: 0.6856 (0.6754) Acc G: 53.813% 
LR: 2.000e-04 

2023-03-02 02:00:37,642 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3490 (0.3788) Acc D Real: 79.661% 
Loss D Fake: 0.7040 (0.7144) Acc D Fake: 44.527% 
Loss D: 1.053 
Loss G: 0.6858 (0.6755) Acc G: 53.408% 
LR: 2.000e-04 

2023-03-02 02:00:37,649 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3653 (0.3786) Acc D Real: 79.435% 
Loss D Fake: 0.7037 (0.7142) Acc D Fake: 44.951% 
Loss D: 1.069 
Loss G: 0.6862 (0.6757) Acc G: 52.999% 
LR: 2.000e-04 

2023-03-02 02:00:37,657 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3507 (0.3782) Acc D Real: 79.253% 
Loss D Fake: 0.7033 (0.7140) Acc D Fake: 45.362% 
Loss D: 1.054 
Loss G: 0.6867 (0.6758) Acc G: 52.594% 
LR: 2.000e-04 

2023-03-02 02:00:37,665 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3851 (0.3783) Acc D Real: 78.996% 
Loss D Fake: 0.7028 (0.7139) Acc D Fake: 45.786% 
Loss D: 1.088 
Loss G: 0.6871 (0.6760) Acc G: 52.199% 
LR: 2.000e-04 

2023-03-02 02:00:37,672 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4011 (0.3786) Acc D Real: 78.724% 
Loss D Fake: 0.7025 (0.7137) Acc D Fake: 46.197% 
Loss D: 1.104 
Loss G: 0.6872 (0.6762) Acc G: 51.816% 
LR: 2.000e-04 

2023-03-02 02:00:37,680 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3357 (0.3781) Acc D Real: 78.586% 
Loss D Fake: 0.7023 (0.7136) Acc D Fake: 46.597% 
Loss D: 1.038 
Loss G: 0.6876 (0.6763) Acc G: 51.444% 
LR: 2.000e-04 

2023-03-02 02:00:37,688 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.4028 (0.3784) Acc D Real: 78.330% 
Loss D Fake: 0.7019 (0.7134) Acc D Fake: 46.986% 
Loss D: 1.105 
Loss G: 0.6879 (0.6765) Acc G: 51.082% 
LR: 2.000e-04 

2023-03-02 02:00:37,696 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.3674 (0.3782) Acc D Real: 78.131% 
Loss D Fake: 0.7017 (0.7132) Acc D Fake: 47.365% 
Loss D: 1.069 
Loss G: 0.6881 (0.6766) Acc G: 50.729% 
LR: 2.000e-04 

2023-03-02 02:00:37,703 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3526 (0.3779) Acc D Real: 77.969% 
Loss D Fake: 0.7014 (0.7131) Acc D Fake: 47.733% 
Loss D: 1.054 
Loss G: 0.6884 (0.6768) Acc G: 50.386% 
LR: 2.000e-04 

2023-03-02 02:00:37,711 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.4241 (0.3785) Acc D Real: 77.684% 
Loss D Fake: 0.7011 (0.7129) Acc D Fake: 48.092% 
Loss D: 1.125 
Loss G: 0.6885 (0.6769) Acc G: 50.052% 
LR: 2.000e-04 

2023-03-02 02:00:37,719 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4243 (0.3791) Acc D Real: 77.427% 
Loss D Fake: 0.7013 (0.7128) Acc D Fake: 48.442% 
Loss D: 1.126 
Loss G: 0.6881 (0.6771) Acc G: 49.727% 
LR: 2.000e-04 

2023-03-02 02:00:37,727 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.4223 (0.3797) Acc D Real: 77.149% 
Loss D Fake: 0.7018 (0.7126) Acc D Fake: 48.782% 
Loss D: 1.124 
Loss G: 0.6875 (0.6772) Acc G: 49.410% 
LR: 2.000e-04 

2023-03-02 02:00:37,735 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4465 (0.3805) Acc D Real: 76.864% 
Loss D Fake: 0.7026 (0.7125) Acc D Fake: 49.114% 
Loss D: 1.149 
Loss G: 0.6865 (0.6773) Acc G: 49.122% 
LR: 2.000e-04 

2023-03-02 02:00:37,743 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3530 (0.3802) Acc D Real: 76.748% 
Loss D Fake: 0.7035 (0.7124) Acc D Fake: 49.417% 
Loss D: 1.057 
Loss G: 0.6858 (0.6774) Acc G: 48.841% 
LR: 2.000e-04 

2023-03-02 02:00:37,751 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.4554 (0.3811) Acc D Real: 76.448% 
Loss D Fake: 0.7042 (0.7123) Acc D Fake: 49.712% 
Loss D: 1.160 
Loss G: 0.6848 (0.6775) Acc G: 48.567% 
LR: 2.000e-04 

2023-03-02 02:00:37,759 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3621 (0.3809) Acc D Real: 76.306% 
Loss D Fake: 0.7052 (0.7122) Acc D Fake: 50.000% 
Loss D: 1.067 
Loss G: 0.6840 (0.6776) Acc G: 48.300% 
LR: 2.000e-04 

2023-03-02 02:00:37,767 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3524 (0.3805) Acc D Real: 76.194% 
Loss D Fake: 0.7059 (0.7121) Acc D Fake: 50.281% 
Loss D: 1.058 
Loss G: 0.6835 (0.6777) Acc G: 48.040% 
LR: 2.000e-04 

2023-03-02 02:00:37,775 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3030 (0.3796) Acc D Real: 76.149% 
Loss D Fake: 0.7062 (0.7121) Acc D Fake: 50.556% 
Loss D: 1.009 
Loss G: 0.6836 (0.6778) Acc G: 47.785% 
LR: 2.000e-04 

2023-03-02 02:00:37,783 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3822 (0.3796) Acc D Real: 75.971% 
Loss D Fake: 0.7059 (0.7120) Acc D Fake: 50.824% 
Loss D: 1.088 
Loss G: 0.6838 (0.6778) Acc G: 47.537% 
LR: 2.000e-04 

2023-03-02 02:00:37,792 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.4433 (0.3804) Acc D Real: 75.733% 
Loss D Fake: 0.7059 (0.7119) Acc D Fake: 51.085% 
Loss D: 1.149 
Loss G: 0.6835 (0.6779) Acc G: 47.294% 
LR: 2.000e-04 

2023-03-02 02:00:37,801 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2599 (0.3790) Acc D Real: 75.768% 
Loss D Fake: 0.7060 (0.7119) Acc D Fake: 51.341% 
Loss D: 0.966 
Loss G: 0.6838 (0.6780) Acc G: 47.057% 
LR: 2.000e-04 

2023-03-02 02:00:37,808 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3317 (0.3784) Acc D Real: 75.732% 
Loss D Fake: 0.7054 (0.7118) Acc D Fake: 51.591% 
Loss D: 1.037 
Loss G: 0.6846 (0.6780) Acc G: 46.825% 
LR: 2.000e-04 

2023-03-02 02:00:37,816 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3320 (0.3779) Acc D Real: 75.653% 
Loss D Fake: 0.7045 (0.7117) Acc D Fake: 51.835% 
Loss D: 1.036 
Loss G: 0.6856 (0.6781) Acc G: 46.599% 
LR: 2.000e-04 

2023-03-02 02:00:37,823 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3861 (0.3780) Acc D Real: 75.498% 
Loss D Fake: 0.7035 (0.7116) Acc D Fake: 52.074% 
Loss D: 1.090 
Loss G: 0.6865 (0.6782) Acc G: 46.377% 
LR: 2.000e-04 

2023-03-02 02:00:37,831 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4812 (0.3791) Acc D Real: 75.228% 
Loss D Fake: 0.7030 (0.7115) Acc D Fake: 52.308% 
Loss D: 1.184 
Loss G: 0.6866 (0.6783) Acc G: 46.161% 
LR: 2.000e-04 

2023-03-02 02:00:37,839 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3632 (0.3790) Acc D Real: 75.093% 
Loss D Fake: 0.7030 (0.7114) Acc D Fake: 52.536% 
Loss D: 1.066 
Loss G: 0.6865 (0.6784) Acc G: 45.949% 
LR: 2.000e-04 

2023-03-02 02:00:37,846 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4971 (0.3802) Acc D Real: 74.796% 
Loss D Fake: 0.7033 (0.7113) Acc D Fake: 52.760% 
Loss D: 1.200 
Loss G: 0.6858 (0.6785) Acc G: 45.741% 
LR: 2.000e-04 

2023-03-02 02:00:37,853 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3892 (0.3803) Acc D Real: 74.648% 
Loss D Fake: 0.7043 (0.7113) Acc D Fake: 52.979% 
Loss D: 1.093 
Loss G: 0.6849 (0.6785) Acc G: 45.539% 
LR: 2.000e-04 

2023-03-02 02:00:37,861 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.3757 (0.3803) Acc D Real: 74.538% 
Loss D Fake: 0.7051 (0.7112) Acc D Fake: 53.193% 
Loss D: 1.081 
Loss G: 0.6842 (0.6786) Acc G: 45.340% 
LR: 2.000e-04 

2023-03-02 02:00:37,868 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3781 (0.3803) Acc D Real: 74.411% 
Loss D Fake: 0.7057 (0.7111) Acc D Fake: 53.403% 
Loss D: 1.084 
Loss G: 0.6837 (0.6787) Acc G: 45.145% 
LR: 2.000e-04 

2023-03-02 02:00:37,876 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3142 (0.3796) Acc D Real: 74.375% 
Loss D Fake: 0.7060 (0.7111) Acc D Fake: 53.608% 
Loss D: 1.020 
Loss G: 0.6836 (0.6787) Acc G: 44.955% 
LR: 2.000e-04 

2023-03-02 02:00:37,884 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.4103 (0.3799) Acc D Real: 74.225% 
Loss D Fake: 0.7061 (0.7110) Acc D Fake: 53.810% 
Loss D: 1.116 
Loss G: 0.6834 (0.6788) Acc G: 44.768% 
LR: 2.000e-04 

2023-03-02 02:00:37,892 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3609 (0.3797) Acc D Real: 74.163% 
Loss D Fake: 0.7062 (0.7110) Acc D Fake: 54.007% 
Loss D: 1.067 
Loss G: 0.6833 (0.6788) Acc G: 44.585% 
LR: 2.000e-04 

2023-03-02 02:00:37,900 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.4391 (0.3803) Acc D Real: 73.987% 
Loss D Fake: 0.7064 (0.7109) Acc D Fake: 54.200% 
Loss D: 1.145 
Loss G: 0.6829 (0.6788) Acc G: 44.406% 
LR: 2.000e-04 

2023-03-02 02:00:37,908 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3424 (0.3799) Acc D Real: 73.922% 
Loss D Fake: 0.7069 (0.7109) Acc D Fake: 54.373% 
Loss D: 1.049 
Loss G: 0.6826 (0.6789) Acc G: 44.231% 
LR: 2.000e-04 

2023-03-02 02:00:37,916 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.3796 (0.3799) Acc D Real: 73.806% 
Loss D Fake: 0.7071 (0.7109) Acc D Fake: 54.542% 
Loss D: 1.087 
Loss G: 0.6824 (0.6789) Acc G: 44.075% 
LR: 2.000e-04 

2023-03-02 02:00:37,924 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3476 (0.3796) Acc D Real: 73.750% 
Loss D Fake: 0.7072 (0.7108) Acc D Fake: 54.709% 
Loss D: 1.055 
Loss G: 0.6824 (0.6789) Acc G: 43.922% 
LR: 2.000e-04 

2023-03-02 02:00:37,932 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3520 (0.3793) Acc D Real: 73.682% 
Loss D Fake: 0.7071 (0.7108) Acc D Fake: 54.872% 
Loss D: 1.059 
Loss G: 0.6826 (0.6790) Acc G: 43.772% 
LR: 2.000e-04 

2023-03-02 02:00:37,940 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3985 (0.3795) Acc D Real: 73.578% 
Loss D Fake: 0.7069 (0.7108) Acc D Fake: 55.032% 
Loss D: 1.105 
Loss G: 0.6827 (0.6790) Acc G: 43.609% 
LR: 2.000e-04 

2023-03-02 02:00:37,947 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.4952 (0.3806) Acc D Real: 73.342% 
Loss D Fake: 0.7071 (0.7107) Acc D Fake: 55.189% 
Loss D: 1.202 
Loss G: 0.6821 (0.6790) Acc G: 43.465% 
LR: 2.000e-04 

2023-03-02 02:00:37,955 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3582 (0.3804) Acc D Real: 73.265% 
Loss D Fake: 0.7079 (0.7107) Acc D Fake: 55.343% 
Loss D: 1.066 
Loss G: 0.6814 (0.6791) Acc G: 43.324% 
LR: 2.000e-04 

2023-03-02 02:00:37,963 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.2971 (0.3796) Acc D Real: 73.267% 
Loss D Fake: 0.7083 (0.7107) Acc D Fake: 55.494% 
Loss D: 1.005 
Loss G: 0.6814 (0.6791) Acc G: 43.185% 
LR: 2.000e-04 

2023-03-02 02:00:37,971 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3801 (0.3796) Acc D Real: 73.179% 
Loss D Fake: 0.7081 (0.7106) Acc D Fake: 55.642% 
Loss D: 1.088 
Loss G: 0.6815 (0.6791) Acc G: 43.049% 
LR: 2.000e-04 

2023-03-02 02:00:37,979 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3488 (0.3794) Acc D Real: 73.124% 
Loss D Fake: 0.7079 (0.7106) Acc D Fake: 55.788% 
Loss D: 1.057 
Loss G: 0.6818 (0.6791) Acc G: 42.915% 
LR: 2.000e-04 

2023-03-02 02:00:37,986 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4372 (0.3799) Acc D Real: 72.972% 
Loss D Fake: 0.7078 (0.7106) Acc D Fake: 55.931% 
Loss D: 1.145 
Loss G: 0.6817 (0.6792) Acc G: 42.783% 
LR: 2.000e-04 

2023-03-02 02:00:37,994 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.4027 (0.3801) Acc D Real: 72.879% 
Loss D Fake: 0.7080 (0.7106) Acc D Fake: 56.071% 
Loss D: 1.111 
Loss G: 0.6814 (0.6792) Acc G: 42.654% 
LR: 2.000e-04 

2023-03-02 02:00:38,002 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3776 (0.3801) Acc D Real: 72.791% 
Loss D Fake: 0.7084 (0.7106) Acc D Fake: 56.209% 
Loss D: 1.086 
Loss G: 0.6810 (0.6792) Acc G: 42.528% 
LR: 2.000e-04 

2023-03-02 02:00:38,009 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3411 (0.3797) Acc D Real: 72.745% 
Loss D Fake: 0.7086 (0.7105) Acc D Fake: 56.345% 
Loss D: 1.050 
Loss G: 0.6810 (0.6792) Acc G: 42.403% 
LR: 2.000e-04 

2023-03-02 02:00:38,017 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3569 (0.3795) Acc D Real: 72.697% 
Loss D Fake: 0.7085 (0.7105) Acc D Fake: 56.478% 
Loss D: 1.065 
Loss G: 0.6811 (0.6792) Acc G: 42.281% 
LR: 2.000e-04 

2023-03-02 02:00:38,024 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3273 (0.3791) Acc D Real: 72.668% 
Loss D Fake: 0.7082 (0.7105) Acc D Fake: 56.609% 
Loss D: 1.036 
Loss G: 0.6816 (0.6792) Acc G: 42.161% 
LR: 2.000e-04 

2023-03-02 02:00:38,032 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.3697 (0.3790) Acc D Real: 72.605% 
Loss D Fake: 0.7077 (0.7105) Acc D Fake: 56.738% 
Loss D: 1.077 
Loss G: 0.6821 (0.6793) Acc G: 42.042% 
LR: 2.000e-04 

2023-03-02 02:00:38,040 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.4246 (0.3794) Acc D Real: 72.471% 
Loss D Fake: 0.7073 (0.7104) Acc D Fake: 56.864% 
Loss D: 1.132 
Loss G: 0.6823 (0.6793) Acc G: 41.926% 
LR: 2.000e-04 

2023-03-02 02:00:38,047 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3454 (0.3791) Acc D Real: 72.423% 
Loss D Fake: 0.7072 (0.7104) Acc D Fake: 56.989% 
Loss D: 1.053 
Loss G: 0.6825 (0.6793) Acc G: 41.812% 
LR: 2.000e-04 

2023-03-02 02:00:38,055 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3364 (0.3787) Acc D Real: 72.389% 
Loss D Fake: 0.7069 (0.7104) Acc D Fake: 57.111% 
Loss D: 1.043 
Loss G: 0.6829 (0.6794) Acc G: 41.700% 
LR: 2.000e-04 

2023-03-02 02:00:38,063 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3299 (0.3783) Acc D Real: 72.368% 
Loss D Fake: 0.7063 (0.7104) Acc D Fake: 57.231% 
Loss D: 1.036 
Loss G: 0.6836 (0.6794) Acc G: 41.575% 
LR: 2.000e-04 

2023-03-02 02:00:38,071 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3771 (0.3783) Acc D Real: 72.286% 
Loss D Fake: 0.7056 (0.7103) Acc D Fake: 57.363% 
Loss D: 1.083 
Loss G: 0.6842 (0.6794) Acc G: 41.453% 
LR: 2.000e-04 

2023-03-02 02:00:38,078 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3233 (0.3779) Acc D Real: 72.262% 
Loss D Fake: 0.7049 (0.7103) Acc D Fake: 57.493% 
Loss D: 1.028 
Loss G: 0.6850 (0.6795) Acc G: 41.333% 
LR: 2.000e-04 

2023-03-02 02:00:38,086 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3694 (0.3778) Acc D Real: 72.195% 
Loss D Fake: 0.7041 (0.7102) Acc D Fake: 57.621% 
Loss D: 1.074 
Loss G: 0.6858 (0.6795) Acc G: 41.215% 
LR: 2.000e-04 

2023-03-02 02:00:38,094 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3236 (0.3774) Acc D Real: 72.175% 
Loss D Fake: 0.7033 (0.7102) Acc D Fake: 57.747% 
Loss D: 1.027 
Loss G: 0.6867 (0.6796) Acc G: 41.098% 
LR: 2.000e-04 

2023-03-02 02:00:38,101 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4437 (0.3779) Acc D Real: 72.027% 
Loss D Fake: 0.7025 (0.7101) Acc D Fake: 57.870% 
Loss D: 1.146 
Loss G: 0.6871 (0.6796) Acc G: 40.984% 
LR: 2.000e-04 

2023-03-02 02:00:38,109 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3759 (0.3779) Acc D Real: 71.970% 
Loss D Fake: 0.7023 (0.7100) Acc D Fake: 57.992% 
Loss D: 1.078 
Loss G: 0.6873 (0.6797) Acc G: 40.871% 
LR: 2.000e-04 

2023-03-02 02:00:38,116 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.4238 (0.3782) Acc D Real: 71.851% 
Loss D Fake: 0.7021 (0.7100) Acc D Fake: 58.112% 
Loss D: 1.126 
Loss G: 0.6873 (0.6798) Acc G: 40.760% 
LR: 2.000e-04 

2023-03-02 02:00:38,124 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.4212 (0.3786) Acc D Real: 71.738% 
Loss D Fake: 0.7024 (0.7099) Acc D Fake: 58.230% 
Loss D: 1.124 
Loss G: 0.6868 (0.6798) Acc G: 40.651% 
LR: 2.000e-04 

2023-03-02 02:00:38,132 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.3626 (0.3785) Acc D Real: 71.674% 
Loss D Fake: 0.7029 (0.7099) Acc D Fake: 58.346% 
Loss D: 1.065 
Loss G: 0.6864 (0.6799) Acc G: 40.543% 
LR: 2.000e-04 

2023-03-02 02:00:38,139 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3955 (0.3786) Acc D Real: 71.598% 
Loss D Fake: 0.7033 (0.7098) Acc D Fake: 58.461% 
Loss D: 1.099 
Loss G: 0.6859 (0.6799) Acc G: 40.437% 
LR: 2.000e-04 

2023-03-02 02:00:38,147 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.3633 (0.3785) Acc D Real: 71.551% 
Loss D Fake: 0.7037 (0.7098) Acc D Fake: 58.573% 
Loss D: 1.067 
Loss G: 0.6856 (0.6800) Acc G: 40.333% 
LR: 2.000e-04 

2023-03-02 02:00:38,154 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3675 (0.3784) Acc D Real: 71.500% 
Loss D Fake: 0.7040 (0.7097) Acc D Fake: 58.684% 
Loss D: 1.072 
Loss G: 0.6853 (0.6800) Acc G: 40.230% 
LR: 2.000e-04 

2023-03-02 02:00:38,162 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4055 (0.3786) Acc D Real: 71.417% 
Loss D Fake: 0.7043 (0.7097) Acc D Fake: 58.794% 
Loss D: 1.110 
Loss G: 0.6850 (0.6800) Acc G: 40.129% 
LR: 2.000e-04 

2023-03-02 02:00:38,170 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4002 (0.3787) Acc D Real: 71.330% 
Loss D Fake: 0.7047 (0.7097) Acc D Fake: 58.901% 
Loss D: 1.105 
Loss G: 0.6846 (0.6801) Acc G: 40.029% 
LR: 2.000e-04 

2023-03-02 02:00:38,177 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.4188 (0.3790) Acc D Real: 71.219% 
Loss D Fake: 0.7052 (0.7096) Acc D Fake: 58.995% 
Loss D: 1.124 
Loss G: 0.6838 (0.6801) Acc G: 39.943% 
LR: 2.000e-04 

2023-03-02 02:00:38,185 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4581 (0.3796) Acc D Real: 71.085% 
Loss D Fake: 0.7062 (0.7096) Acc D Fake: 59.088% 
Loss D: 1.164 
Loss G: 0.6827 (0.6801) Acc G: 39.859% 
LR: 2.000e-04 

2023-03-02 02:00:38,192 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4025 (0.3798) Acc D Real: 71.015% 
Loss D Fake: 0.7074 (0.7096) Acc D Fake: 59.179% 
Loss D: 1.110 
Loss G: 0.6814 (0.6801) Acc G: 39.775% 
LR: 2.000e-04 

2023-03-02 02:00:38,200 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3880 (0.3798) Acc D Real: 70.966% 
Loss D Fake: 0.7087 (0.7096) Acc D Fake: 59.269% 
Loss D: 1.097 
Loss G: 0.6803 (0.6801) Acc G: 39.705% 
LR: 2.000e-04 

2023-03-02 02:00:38,208 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4183 (0.3801) Acc D Real: 70.887% 
Loss D Fake: 0.7098 (0.7096) Acc D Fake: 59.345% 
Loss D: 1.128 
Loss G: 0.6791 (0.6801) Acc G: 39.635% 
LR: 2.000e-04 

2023-03-02 02:00:38,217 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.2889 (0.3795) Acc D Real: 70.909% 
Loss D Fake: 0.7108 (0.7096) Acc D Fake: 59.421% 
Loss D: 1.000 
Loss G: 0.6785 (0.6801) Acc G: 39.567% 
LR: 2.000e-04 

2023-03-02 02:00:38,225 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4046 (0.3797) Acc D Real: 70.838% 
Loss D Fake: 0.7112 (0.7096) Acc D Fake: 59.484% 
Loss D: 1.116 
Loss G: 0.6780 (0.6801) Acc G: 39.500% 
LR: 2.000e-04 

2023-03-02 02:00:38,234 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3676 (0.3796) Acc D Real: 70.815% 
Loss D Fake: 0.7117 (0.7096) Acc D Fake: 59.545% 
Loss D: 1.079 
Loss G: 0.6777 (0.6801) Acc G: 39.445% 
LR: 2.000e-04 

2023-03-02 02:00:38,242 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3454 (0.3793) Acc D Real: 70.802% 
Loss D Fake: 0.7119 (0.7096) Acc D Fake: 59.606% 
Loss D: 1.057 
Loss G: 0.6776 (0.6801) Acc G: 39.391% 
LR: 2.000e-04 

2023-03-02 02:00:38,251 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.3571 (0.3792) Acc D Real: 70.795% 
Loss D Fake: 0.7119 (0.7096) Acc D Fake: 59.667% 
Loss D: 1.069 
Loss G: 0.6777 (0.6800) Acc G: 39.338% 
LR: 2.000e-04 

2023-03-02 02:00:38,258 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3851 (0.3792) Acc D Real: 70.754% 
Loss D Fake: 0.7118 (0.7097) Acc D Fake: 59.726% 
Loss D: 1.097 
Loss G: 0.6777 (0.6800) Acc G: 39.285% 
LR: 2.000e-04 

2023-03-02 02:00:38,266 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3980 (0.3793) Acc D Real: 70.692% 
Loss D Fake: 0.7118 (0.7097) Acc D Fake: 59.785% 
Loss D: 1.110 
Loss G: 0.6776 (0.6800) Acc G: 39.233% 
LR: 2.000e-04 

2023-03-02 02:00:38,273 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3777 (0.3793) Acc D Real: 70.664% 
Loss D Fake: 0.7120 (0.7097) Acc D Fake: 59.842% 
Loss D: 1.090 
Loss G: 0.6774 (0.6800) Acc G: 39.182% 
LR: 2.000e-04 

2023-03-02 02:00:38,281 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3067 (0.3788) Acc D Real: 70.677% 
Loss D Fake: 0.7121 (0.7097) Acc D Fake: 59.899% 
Loss D: 1.019 
Loss G: 0.6776 (0.6800) Acc G: 39.132% 
LR: 2.000e-04 

2023-03-02 02:00:38,288 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3372 (0.3786) Acc D Real: 70.665% 
Loss D Fake: 0.7117 (0.7097) Acc D Fake: 59.956% 
Loss D: 1.049 
Loss G: 0.6780 (0.6800) Acc G: 39.082% 
LR: 2.000e-04 

2023-03-02 02:00:38,296 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3842 (0.3786) Acc D Real: 70.623% 
Loss D Fake: 0.7112 (0.7097) Acc D Fake: 60.011% 
Loss D: 1.095 
Loss G: 0.6784 (0.6800) Acc G: 39.022% 
LR: 2.000e-04 

2023-03-02 02:00:38,304 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4748 (0.3792) Acc D Real: 70.494% 
Loss D Fake: 0.7111 (0.7097) Acc D Fake: 60.066% 
Loss D: 1.186 
Loss G: 0.6782 (0.6799) Acc G: 38.973% 
LR: 2.000e-04 

2023-03-02 02:00:38,311 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3695 (0.3792) Acc D Real: 70.474% 
Loss D Fake: 0.7115 (0.7097) Acc D Fake: 60.120% 
Loss D: 1.081 
Loss G: 0.6778 (0.6799) Acc G: 38.926% 
LR: 2.000e-04 

2023-03-02 02:00:38,318 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3792 (0.3792) Acc D Real: 70.430% 
Loss D Fake: 0.7118 (0.7098) Acc D Fake: 60.173% 
Loss D: 1.091 
Loss G: 0.6775 (0.6799) Acc G: 38.879% 
LR: 2.000e-04 

2023-03-02 02:00:38,326 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.2625 (0.3784) Acc D Real: 70.475% 
Loss D Fake: 0.7119 (0.7098) Acc D Fake: 60.226% 
Loss D: 0.974 
Loss G: 0.6779 (0.6799) Acc G: 38.832% 
LR: 2.000e-04 

2023-03-02 02:00:38,333 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2650 (0.3777) Acc D Real: 70.516% 
Loss D Fake: 0.7111 (0.7098) Acc D Fake: 60.278% 
Loss D: 0.976 
Loss G: 0.6789 (0.6799) Acc G: 38.775% 
LR: 2.000e-04 

2023-03-02 02:00:38,341 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.3269 (0.3774) Acc D Real: 70.520% 
Loss D Fake: 0.7099 (0.7098) Acc D Fake: 60.340% 
Loss D: 1.037 
Loss G: 0.6802 (0.6799) Acc G: 38.719% 
LR: 2.000e-04 

2023-03-02 02:00:38,348 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.4849 (0.3781) Acc D Real: 70.506% 
Loss D Fake: 0.7088 (0.7098) Acc D Fake: 60.345% 
Loss D: 1.194 
Loss G: 0.6807 (0.6799) Acc G: 38.714% 
LR: 2.000e-04 

2023-03-02 02:00:38,591 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.788 | Generator Loss: 0.681 | Avg: 1.468 
2023-03-02 02:00:38,613 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.869 | Generator Loss: 0.681 | Avg: 1.550 
2023-03-02 02:00:38,636 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.848 | Generator Loss: 0.681 | Avg: 1.528 
2023-03-02 02:00:38,663 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.827 | Generator Loss: 0.681 | Avg: 1.508 
2023-03-02 02:00:38,690 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.825 | Generator Loss: 0.681 | Avg: 1.506 
2023-03-02 02:00:38,717 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.876 | Generator Loss: 0.681 | Avg: 1.556 
2023-03-02 02:00:38,743 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.896 | Generator Loss: 0.681 | Avg: 1.577 
2023-03-02 02:00:38,770 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.938 | Generator Loss: 0.681 | Avg: 1.619 
2023-03-02 02:00:38,796 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.960 | Generator Loss: 0.681 | Avg: 1.641 
2023-03-02 02:00:38,822 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.002 | Generator Loss: 0.681 | Avg: 1.682 
2023-03-02 02:00:38,848 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.030 | Generator Loss: 0.681 | Avg: 1.711 
2023-03-02 02:00:38,874 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.060 | Generator Loss: 0.681 | Avg: 1.741 
2023-03-02 02:00:38,899 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.084 | Generator Loss: 0.681 | Avg: 1.765 
2023-03-02 02:00:38,925 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.100 | Generator Loss: 0.681 | Avg: 1.780 
2023-03-02 02:00:38,952 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.085 | Generator Loss: 0.681 | Avg: 1.766 
2023-03-02 02:00:38,978 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.070 | Generator Loss: 0.681 | Avg: 1.751 
2023-03-02 02:00:39,003 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.056 | Generator Loss: 0.681 | Avg: 1.737 
2023-03-02 02:00:39,035 -                train: [    INFO] - 
Epoch: 19/20
2023-03-02 02:00:39,223 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.4214 (0.4035) Acc D Real: 61.745% 
Loss D Fake: 0.7088 (0.7087) Acc D Fake: 70.000% 
Loss D: 1.130 
Loss G: 0.6804 (0.6806) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 02:00:39,233 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.4547 (0.4205) Acc D Real: 58.594% 
Loss D Fake: 0.7093 (0.7089) Acc D Fake: 70.000% 
Loss D: 1.164 
Loss G: 0.6796 (0.6803) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 02:00:39,241 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.3826 (0.4111) Acc D Real: 59.688% 
Loss D Fake: 0.7102 (0.7092) Acc D Fake: 70.000% 
Loss D: 1.093 
Loss G: 0.6787 (0.6799) Acc G: 30.000% 
LR: 2.000e-04 

2023-03-02 02:00:39,258 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.4452 (0.4179) Acc D Real: 59.260% 
Loss D Fake: 0.7112 (0.7096) Acc D Fake: 69.667% 
Loss D: 1.156 
Loss G: 0.6776 (0.6794) Acc G: 30.333% 
LR: 2.000e-04 

2023-03-02 02:00:39,265 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3416 (0.4052) Acc D Real: 60.547% 
Loss D Fake: 0.7123 (0.7101) Acc D Fake: 69.444% 
Loss D: 1.054 
Loss G: 0.6767 (0.6790) Acc G: 30.833% 
LR: 2.000e-04 

2023-03-02 02:00:39,273 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3599 (0.3987) Acc D Real: 61.406% 
Loss D Fake: 0.7130 (0.7105) Acc D Fake: 69.048% 
Loss D: 1.073 
Loss G: 0.6761 (0.6786) Acc G: 31.190% 
LR: 2.000e-04 

2023-03-02 02:00:39,281 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.3194 (0.3888) Acc D Real: 62.513% 
Loss D Fake: 0.7134 (0.7109) Acc D Fake: 68.750% 
Loss D: 1.033 
Loss G: 0.6759 (0.6782) Acc G: 31.458% 
LR: 2.000e-04 

2023-03-02 02:00:39,289 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3819 (0.3880) Acc D Real: 62.691% 
Loss D Fake: 0.7136 (0.7112) Acc D Fake: 68.333% 
Loss D: 1.096 
Loss G: 0.6758 (0.6780) Acc G: 31.667% 
LR: 2.000e-04 

2023-03-02 02:00:39,297 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.3846 (0.3877) Acc D Real: 62.990% 
Loss D Fake: 0.7138 (0.7114) Acc D Fake: 68.000% 
Loss D: 1.098 
Loss G: 0.6756 (0.6777) Acc G: 32.000% 
LR: 2.000e-04 

2023-03-02 02:00:39,304 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.2453 (0.3747) Acc D Real: 64.564% 
Loss D Fake: 0.7137 (0.7116) Acc D Fake: 67.727% 
Loss D: 0.959 
Loss G: 0.6761 (0.6776) Acc G: 32.121% 
LR: 2.000e-04 

2023-03-02 02:00:39,311 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.3704 (0.3744) Acc D Real: 65.113% 
Loss D Fake: 0.7130 (0.7117) Acc D Fake: 67.639% 
Loss D: 1.083 
Loss G: 0.6767 (0.6775) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 02:00:39,318 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4018 (0.3765) Acc D Real: 64.996% 
Loss D Fake: 0.7125 (0.7118) Acc D Fake: 67.564% 
Loss D: 1.114 
Loss G: 0.6770 (0.6775) Acc G: 32.179% 
LR: 2.000e-04 

2023-03-02 02:00:39,325 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3669 (0.3758) Acc D Real: 65.160% 
Loss D Fake: 0.7122 (0.7118) Acc D Fake: 67.500% 
Loss D: 1.079 
Loss G: 0.6773 (0.6775) Acc G: 32.143% 
LR: 2.000e-04 

2023-03-02 02:00:39,332 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.4427 (0.3803) Acc D Real: 64.660% 
Loss D Fake: 0.7121 (0.7118) Acc D Fake: 67.444% 
Loss D: 1.155 
Loss G: 0.6771 (0.6774) Acc G: 32.111% 
LR: 2.000e-04 

2023-03-02 02:00:39,340 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3465 (0.3782) Acc D Real: 65.020% 
Loss D Fake: 0.7123 (0.7119) Acc D Fake: 67.396% 
Loss D: 1.059 
Loss G: 0.6770 (0.6774) Acc G: 32.083% 
LR: 2.000e-04 

2023-03-02 02:00:39,348 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.4217 (0.3807) Acc D Real: 64.770% 
Loss D Fake: 0.7124 (0.7119) Acc D Fake: 67.353% 
Loss D: 1.134 
Loss G: 0.6767 (0.6774) Acc G: 32.157% 
LR: 2.000e-04 

2023-03-02 02:00:39,355 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3701 (0.3801) Acc D Real: 64.933% 
Loss D Fake: 0.7128 (0.7120) Acc D Fake: 67.315% 
Loss D: 1.083 
Loss G: 0.6764 (0.6773) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 02:00:39,363 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3469 (0.3784) Acc D Real: 65.197% 
Loss D Fake: 0.7130 (0.7120) Acc D Fake: 67.281% 
Loss D: 1.060 
Loss G: 0.6763 (0.6773) Acc G: 32.281% 
LR: 2.000e-04 

2023-03-02 02:00:39,370 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3970 (0.3793) Acc D Real: 65.344% 
Loss D Fake: 0.7131 (0.7121) Acc D Fake: 67.250% 
Loss D: 1.110 
Loss G: 0.6762 (0.6772) Acc G: 32.333% 
LR: 2.000e-04 

2023-03-02 02:00:39,378 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2992 (0.3755) Acc D Real: 65.801% 
Loss D Fake: 0.7131 (0.7121) Acc D Fake: 67.222% 
Loss D: 1.012 
Loss G: 0.6764 (0.6772) Acc G: 32.381% 
LR: 2.000e-04 

2023-03-02 02:00:39,385 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.3041 (0.3723) Acc D Real: 66.233% 
Loss D Fake: 0.7127 (0.7121) Acc D Fake: 67.197% 
Loss D: 1.017 
Loss G: 0.6770 (0.6772) Acc G: 32.424% 
LR: 2.000e-04 

2023-03-02 02:00:39,392 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3072 (0.3694) Acc D Real: 66.558% 
Loss D Fake: 0.7118 (0.7121) Acc D Fake: 67.246% 
Loss D: 1.019 
Loss G: 0.6780 (0.6772) Acc G: 32.391% 
LR: 2.000e-04 

2023-03-02 02:00:39,399 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3916 (0.3704) Acc D Real: 66.382% 
Loss D Fake: 0.7108 (0.7121) Acc D Fake: 67.292% 
Loss D: 1.102 
Loss G: 0.6788 (0.6773) Acc G: 32.361% 
LR: 2.000e-04 

2023-03-02 02:00:39,406 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4397 (0.3731) Acc D Real: 66.094% 
Loss D Fake: 0.7103 (0.7120) Acc D Fake: 67.333% 
Loss D: 1.150 
Loss G: 0.6791 (0.6773) Acc G: 32.333% 
LR: 2.000e-04 

2023-03-02 02:00:39,413 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3595 (0.3726) Acc D Real: 66.030% 
Loss D Fake: 0.7102 (0.7119) Acc D Fake: 67.372% 
Loss D: 1.070 
Loss G: 0.6792 (0.6774) Acc G: 32.308% 
LR: 2.000e-04 

2023-03-02 02:00:39,421 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3082 (0.3702) Acc D Real: 66.360% 
Loss D Fake: 0.7099 (0.7119) Acc D Fake: 67.407% 
Loss D: 1.018 
Loss G: 0.6796 (0.6775) Acc G: 32.222% 
LR: 2.000e-04 

2023-03-02 02:00:39,428 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.3889 (0.3709) Acc D Real: 66.326% 
Loss D Fake: 0.7094 (0.7118) Acc D Fake: 67.500% 
Loss D: 1.098 
Loss G: 0.6801 (0.6776) Acc G: 32.143% 
LR: 2.000e-04 

2023-03-02 02:00:39,435 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.4041 (0.3720) Acc D Real: 66.097% 
Loss D Fake: 0.7091 (0.7117) Acc D Fake: 67.586% 
Loss D: 1.113 
Loss G: 0.6802 (0.6777) Acc G: 32.069% 
LR: 2.000e-04 

2023-03-02 02:00:39,444 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3116 (0.3700) Acc D Real: 66.278% 
Loss D Fake: 0.7089 (0.7116) Acc D Fake: 67.667% 
Loss D: 1.021 
Loss G: 0.6806 (0.6778) Acc G: 32.000% 
LR: 2.000e-04 

2023-03-02 02:00:39,451 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.3829 (0.3704) Acc D Real: 66.240% 
Loss D Fake: 0.7084 (0.7115) Acc D Fake: 67.742% 
Loss D: 1.091 
Loss G: 0.6810 (0.6779) Acc G: 31.935% 
LR: 2.000e-04 

2023-03-02 02:00:39,459 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3844 (0.3709) Acc D Real: 66.180% 
Loss D Fake: 0.7081 (0.7114) Acc D Fake: 67.812% 
Loss D: 1.093 
Loss G: 0.6812 (0.6780) Acc G: 31.875% 
LR: 2.000e-04 

2023-03-02 02:00:39,466 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3927 (0.3715) Acc D Real: 66.053% 
Loss D Fake: 0.7080 (0.7113) Acc D Fake: 67.879% 
Loss D: 1.101 
Loss G: 0.6812 (0.6781) Acc G: 31.818% 
LR: 2.000e-04 

2023-03-02 02:00:39,474 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.4282 (0.3732) Acc D Real: 65.780% 
Loss D Fake: 0.7081 (0.7112) Acc D Fake: 67.941% 
Loss D: 1.136 
Loss G: 0.6809 (0.6782) Acc G: 31.765% 
LR: 2.000e-04 

2023-03-02 02:00:39,481 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.3078 (0.3713) Acc D Real: 65.988% 
Loss D Fake: 0.7084 (0.7111) Acc D Fake: 68.000% 
Loss D: 1.016 
Loss G: 0.6809 (0.6782) Acc G: 31.714% 
LR: 2.000e-04 

2023-03-02 02:00:39,489 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3422 (0.3705) Acc D Real: 66.068% 
Loss D Fake: 0.7082 (0.7110) Acc D Fake: 68.056% 
Loss D: 1.050 
Loss G: 0.6811 (0.6783) Acc G: 31.667% 
LR: 2.000e-04 

2023-03-02 02:00:39,496 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.3167 (0.3691) Acc D Real: 66.223% 
Loss D Fake: 0.7078 (0.7109) Acc D Fake: 68.108% 
Loss D: 1.025 
Loss G: 0.6817 (0.6784) Acc G: 31.622% 
LR: 2.000e-04 

2023-03-02 02:00:39,504 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.2973 (0.3672) Acc D Real: 66.357% 
Loss D Fake: 0.7070 (0.7108) Acc D Fake: 68.158% 
Loss D: 1.004 
Loss G: 0.6827 (0.6785) Acc G: 31.579% 
LR: 2.000e-04 

2023-03-02 02:00:39,512 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3254 (0.3661) Acc D Real: 66.433% 
Loss D Fake: 0.7060 (0.7107) Acc D Fake: 68.205% 
Loss D: 1.031 
Loss G: 0.6838 (0.6787) Acc G: 31.496% 
LR: 2.000e-04 

2023-03-02 02:00:39,520 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.3212 (0.3650) Acc D Real: 66.509% 
Loss D Fake: 0.7048 (0.7106) Acc D Fake: 68.292% 
Loss D: 1.026 
Loss G: 0.6850 (0.6788) Acc G: 31.417% 
LR: 2.000e-04 

2023-03-02 02:00:39,528 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3991 (0.3658) Acc D Real: 66.331% 
Loss D Fake: 0.7036 (0.7104) Acc D Fake: 68.374% 
Loss D: 1.103 
Loss G: 0.6859 (0.6790) Acc G: 31.341% 
LR: 2.000e-04 

2023-03-02 02:00:39,536 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.4027 (0.3667) Acc D Real: 66.166% 
Loss D Fake: 0.7030 (0.7102) Acc D Fake: 68.452% 
Loss D: 1.106 
Loss G: 0.6864 (0.6792) Acc G: 31.270% 
LR: 2.000e-04 

2023-03-02 02:00:39,543 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.3112 (0.3654) Acc D Real: 66.271% 
Loss D Fake: 0.7025 (0.7100) Acc D Fake: 68.527% 
Loss D: 1.014 
Loss G: 0.6870 (0.6793) Acc G: 31.202% 
LR: 2.000e-04 

2023-03-02 02:00:39,551 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.4296 (0.3669) Acc D Real: 66.050% 
Loss D Fake: 0.7019 (0.7099) Acc D Fake: 68.598% 
Loss D: 1.132 
Loss G: 0.6873 (0.6795) Acc G: 31.136% 
LR: 2.000e-04 

2023-03-02 02:00:39,558 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3947 (0.3675) Acc D Real: 65.918% 
Loss D Fake: 0.7018 (0.7097) Acc D Fake: 68.667% 
Loss D: 1.097 
Loss G: 0.6873 (0.6797) Acc G: 31.074% 
LR: 2.000e-04 

2023-03-02 02:00:39,566 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.3620 (0.3674) Acc D Real: 65.851% 
Loss D Fake: 0.7019 (0.7095) Acc D Fake: 68.732% 
Loss D: 1.064 
Loss G: 0.6872 (0.6799) Acc G: 31.014% 
LR: 2.000e-04 

2023-03-02 02:00:39,574 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3796 (0.3676) Acc D Real: 65.769% 
Loss D Fake: 0.7020 (0.7093) Acc D Fake: 68.794% 
Loss D: 1.082 
Loss G: 0.6871 (0.6800) Acc G: 30.957% 
LR: 2.000e-04 

2023-03-02 02:00:39,581 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3412 (0.3671) Acc D Real: 65.789% 
Loss D Fake: 0.7021 (0.7092) Acc D Fake: 68.854% 
Loss D: 1.043 
Loss G: 0.6870 (0.6802) Acc G: 30.903% 
LR: 2.000e-04 

2023-03-02 02:00:39,589 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.3824 (0.3674) Acc D Real: 65.757% 
Loss D Fake: 0.7022 (0.7091) Acc D Fake: 68.912% 
Loss D: 1.085 
Loss G: 0.6868 (0.6803) Acc G: 30.850% 
LR: 2.000e-04 

2023-03-02 02:00:39,597 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3225 (0.3665) Acc D Real: 65.838% 
Loss D Fake: 0.7023 (0.7089) Acc D Fake: 68.967% 
Loss D: 1.025 
Loss G: 0.6869 (0.6804) Acc G: 30.800% 
LR: 2.000e-04 

2023-03-02 02:00:39,605 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.3597 (0.3664) Acc D Real: 65.843% 
Loss D Fake: 0.7021 (0.7088) Acc D Fake: 69.020% 
Loss D: 1.062 
Loss G: 0.6871 (0.6806) Acc G: 30.752% 
LR: 2.000e-04 

2023-03-02 02:00:39,612 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3766 (0.3665) Acc D Real: 65.744% 
Loss D Fake: 0.7020 (0.7087) Acc D Fake: 69.071% 
Loss D: 1.079 
Loss G: 0.6871 (0.6807) Acc G: 30.705% 
LR: 2.000e-04 

2023-03-02 02:00:39,620 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.3414 (0.3661) Acc D Real: 65.792% 
Loss D Fake: 0.7019 (0.7085) Acc D Fake: 69.117% 
Loss D: 1.043 
Loss G: 0.6873 (0.6808) Acc G: 30.660% 
LR: 2.000e-04 

2023-03-02 02:00:39,627 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3885 (0.3665) Acc D Real: 65.715% 
Loss D Fake: 0.7018 (0.7084) Acc D Fake: 69.133% 
Loss D: 1.090 
Loss G: 0.6873 (0.6809) Acc G: 30.648% 
LR: 2.000e-04 

2023-03-02 02:00:39,634 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.4621 (0.3682) Acc D Real: 65.444% 
Loss D Fake: 0.7019 (0.7083) Acc D Fake: 69.149% 
Loss D: 1.164 
Loss G: 0.6868 (0.6810) Acc G: 30.636% 
LR: 2.000e-04 

2023-03-02 02:00:39,642 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3630 (0.3681) Acc D Real: 65.425% 
Loss D Fake: 0.7026 (0.7082) Acc D Fake: 69.164% 
Loss D: 1.066 
Loss G: 0.6862 (0.6811) Acc G: 30.625% 
LR: 2.000e-04 

2023-03-02 02:00:39,649 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.3267 (0.3674) Acc D Real: 65.458% 
Loss D Fake: 0.7031 (0.7081) Acc D Fake: 69.179% 
Loss D: 1.030 
Loss G: 0.6858 (0.6812) Acc G: 30.614% 
LR: 2.000e-04 

2023-03-02 02:00:39,656 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3171 (0.3665) Acc D Real: 65.514% 
Loss D Fake: 0.7033 (0.7080) Acc D Fake: 69.193% 
Loss D: 1.020 
Loss G: 0.6858 (0.6813) Acc G: 30.603% 
LR: 2.000e-04 

2023-03-02 02:00:39,664 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.4005 (0.3671) Acc D Real: 65.400% 
Loss D Fake: 0.7033 (0.7079) Acc D Fake: 69.206% 
Loss D: 1.104 
Loss G: 0.6856 (0.6814) Acc G: 30.593% 
LR: 2.000e-04 

2023-03-02 02:00:39,672 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.3672 (0.3671) Acc D Real: 65.387% 
Loss D Fake: 0.7035 (0.7079) Acc D Fake: 69.220% 
Loss D: 1.071 
Loss G: 0.6854 (0.6814) Acc G: 30.583% 
LR: 2.000e-04 

2023-03-02 02:00:39,681 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.4786 (0.3689) Acc D Real: 65.098% 
Loss D Fake: 0.7040 (0.7078) Acc D Fake: 69.232% 
Loss D: 1.183 
Loss G: 0.6844 (0.6815) Acc G: 30.574% 
LR: 2.000e-04 

2023-03-02 02:00:39,688 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.2578 (0.3672) Acc D Real: 65.299% 
Loss D Fake: 0.7049 (0.7077) Acc D Fake: 69.218% 
Loss D: 0.963 
Loss G: 0.6840 (0.6815) Acc G: 30.565% 
LR: 2.000e-04 

2023-03-02 02:00:39,697 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.2671 (0.3656) Acc D Real: 65.460% 
Loss D Fake: 0.7049 (0.7077) Acc D Fake: 69.204% 
Loss D: 0.972 
Loss G: 0.6843 (0.6816) Acc G: 30.556% 
LR: 2.000e-04 

2023-03-02 02:00:39,705 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3507 (0.3653) Acc D Real: 65.467% 
Loss D Fake: 0.7044 (0.7077) Acc D Fake: 69.190% 
Loss D: 1.055 
Loss G: 0.6848 (0.6816) Acc G: 30.547% 
LR: 2.000e-04 

2023-03-02 02:00:39,713 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.4390 (0.3665) Acc D Real: 65.284% 
Loss D Fake: 0.7041 (0.7076) Acc D Fake: 69.177% 
Loss D: 1.143 
Loss G: 0.6847 (0.6817) Acc G: 30.544% 
LR: 2.000e-04 

2023-03-02 02:00:39,720 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.2978 (0.3654) Acc D Real: 65.415% 
Loss D Fake: 0.7042 (0.7075) Acc D Fake: 69.164% 
Loss D: 1.002 
Loss G: 0.6849 (0.6817) Acc G: 30.561% 
LR: 2.000e-04 

2023-03-02 02:00:39,728 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.3665 (0.3654) Acc D Real: 65.410% 
Loss D Fake: 0.7039 (0.7075) Acc D Fake: 69.152% 
Loss D: 1.070 
Loss G: 0.6852 (0.6818) Acc G: 30.578% 
LR: 2.000e-04 

2023-03-02 02:00:39,736 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.4229 (0.3663) Acc D Real: 65.276% 
Loss D Fake: 0.7038 (0.7074) Acc D Fake: 69.140% 
Loss D: 1.127 
Loss G: 0.6850 (0.6818) Acc G: 30.594% 
LR: 2.000e-04 

2023-03-02 02:00:39,743 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3041 (0.3654) Acc D Real: 65.347% 
Loss D Fake: 0.7040 (0.7074) Acc D Fake: 69.128% 
Loss D: 1.008 
Loss G: 0.6850 (0.6819) Acc G: 30.609% 
LR: 2.000e-04 

2023-03-02 02:00:39,751 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.4395 (0.3664) Acc D Real: 65.193% 
Loss D Fake: 0.7040 (0.7073) Acc D Fake: 69.117% 
Loss D: 1.144 
Loss G: 0.6847 (0.6819) Acc G: 30.624% 
LR: 2.000e-04 

2023-03-02 02:00:39,759 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.3658 (0.3664) Acc D Real: 65.187% 
Loss D Fake: 0.7045 (0.7073) Acc D Fake: 69.106% 
Loss D: 1.070 
Loss G: 0.6842 (0.6819) Acc G: 30.639% 
LR: 2.000e-04 

2023-03-02 02:00:39,768 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3888 (0.3667) Acc D Real: 65.129% 
Loss D Fake: 0.7050 (0.7073) Acc D Fake: 69.095% 
Loss D: 1.094 
Loss G: 0.6837 (0.6820) Acc G: 30.653% 
LR: 2.000e-04 

2023-03-02 02:00:39,776 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3066 (0.3659) Acc D Real: 65.233% 
Loss D Fake: 0.7054 (0.7072) Acc D Fake: 69.085% 
Loss D: 1.012 
Loss G: 0.6835 (0.6820) Acc G: 30.667% 
LR: 2.000e-04 

2023-03-02 02:00:39,784 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.4218 (0.3667) Acc D Real: 65.110% 
Loss D Fake: 0.7056 (0.7072) Acc D Fake: 69.074% 
Loss D: 1.127 
Loss G: 0.6830 (0.6820) Acc G: 30.681% 
LR: 2.000e-04 

2023-03-02 02:00:39,791 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3572 (0.3666) Acc D Real: 65.097% 
Loss D Fake: 0.7061 (0.7072) Acc D Fake: 69.065% 
Loss D: 1.063 
Loss G: 0.6825 (0.6820) Acc G: 30.694% 
LR: 2.000e-04 

2023-03-02 02:00:39,799 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.2868 (0.3655) Acc D Real: 65.221% 
Loss D Fake: 0.7065 (0.7072) Acc D Fake: 69.033% 
Loss D: 0.993 
Loss G: 0.6824 (0.6820) Acc G: 30.728% 
LR: 2.000e-04 

2023-03-02 02:00:39,807 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.3703 (0.3656) Acc D Real: 65.212% 
Loss D Fake: 0.7064 (0.7072) Acc D Fake: 69.002% 
Loss D: 1.077 
Loss G: 0.6825 (0.6820) Acc G: 30.762% 
LR: 2.000e-04 

2023-03-02 02:00:39,815 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3056 (0.3648) Acc D Real: 65.317% 
Loss D Fake: 0.7062 (0.7072) Acc D Fake: 68.972% 
Loss D: 1.012 
Loss G: 0.6828 (0.6820) Acc G: 30.795% 
LR: 2.000e-04 

2023-03-02 02:00:39,823 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.3955 (0.3652) Acc D Real: 65.251% 
Loss D Fake: 0.7059 (0.7072) Acc D Fake: 68.943% 
Loss D: 1.101 
Loss G: 0.6829 (0.6820) Acc G: 30.827% 
LR: 2.000e-04 

2023-03-02 02:00:39,831 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.3643 (0.3652) Acc D Real: 65.247% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.915% 
Loss D: 1.070 
Loss G: 0.6829 (0.6820) Acc G: 30.858% 
LR: 2.000e-04 

2023-03-02 02:00:39,838 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.3740 (0.3653) Acc D Real: 65.218% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.887% 
Loss D: 1.080 
Loss G: 0.6828 (0.6820) Acc G: 30.889% 
LR: 2.000e-04 

2023-03-02 02:00:39,846 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.3147 (0.3647) Acc D Real: 65.259% 
Loss D Fake: 0.7059 (0.7071) Acc D Fake: 68.860% 
Loss D: 1.021 
Loss G: 0.6829 (0.6821) Acc G: 30.918% 
LR: 2.000e-04 

2023-03-02 02:00:39,854 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.2878 (0.3637) Acc D Real: 65.352% 
Loss D Fake: 0.7057 (0.7071) Acc D Fake: 68.833% 
Loss D: 0.993 
Loss G: 0.6834 (0.6821) Acc G: 30.948% 
LR: 2.000e-04 

2023-03-02 02:00:39,861 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.4018 (0.3642) Acc D Real: 65.297% 
Loss D Fake: 0.7052 (0.7071) Acc D Fake: 68.808% 
Loss D: 1.107 
Loss G: 0.6836 (0.6821) Acc G: 30.976% 
LR: 2.000e-04 

2023-03-02 02:00:39,869 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.4424 (0.3651) Acc D Real: 65.159% 
Loss D Fake: 0.7053 (0.7071) Acc D Fake: 68.782% 
Loss D: 1.148 
Loss G: 0.6832 (0.6821) Acc G: 31.004% 
LR: 2.000e-04 

2023-03-02 02:00:39,876 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3801 (0.3653) Acc D Real: 65.137% 
Loss D Fake: 0.7059 (0.7070) Acc D Fake: 68.758% 
Loss D: 1.086 
Loss G: 0.6825 (0.6821) Acc G: 31.031% 
LR: 2.000e-04 

2023-03-02 02:00:39,885 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.2716 (0.3642) Acc D Real: 65.284% 
Loss D Fake: 0.7064 (0.7070) Acc D Fake: 68.734% 
Loss D: 0.978 
Loss G: 0.6824 (0.6821) Acc G: 31.057% 
LR: 2.000e-04 

2023-03-02 02:00:39,893 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.4799 (0.3655) Acc D Real: 65.115% 
Loss D Fake: 0.7065 (0.7070) Acc D Fake: 68.710% 
Loss D: 1.186 
Loss G: 0.6818 (0.6821) Acc G: 31.096% 
LR: 2.000e-04 

2023-03-02 02:00:39,900 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.3312 (0.3651) Acc D Real: 65.156% 
Loss D Fake: 0.7072 (0.7070) Acc D Fake: 68.669% 
Loss D: 1.038 
Loss G: 0.6812 (0.6821) Acc G: 31.140% 
LR: 2.000e-04 

2023-03-02 02:00:39,908 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.3656 (0.3651) Acc D Real: 65.157% 
Loss D Fake: 0.7077 (0.7070) Acc D Fake: 68.628% 
Loss D: 1.073 
Loss G: 0.6808 (0.6821) Acc G: 31.183% 
LR: 2.000e-04 

2023-03-02 02:00:39,916 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.3087 (0.3645) Acc D Real: 65.221% 
Loss D Fake: 0.7080 (0.7070) Acc D Fake: 68.588% 
Loss D: 1.017 
Loss G: 0.6806 (0.6821) Acc G: 31.225% 
LR: 2.000e-04 

2023-03-02 02:00:39,924 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3757 (0.3646) Acc D Real: 65.183% 
Loss D Fake: 0.7081 (0.7071) Acc D Fake: 68.549% 
Loss D: 1.084 
Loss G: 0.6804 (0.6821) Acc G: 31.266% 
LR: 2.000e-04 

2023-03-02 02:00:39,932 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.3256 (0.3642) Acc D Real: 65.223% 
Loss D Fake: 0.7083 (0.7071) Acc D Fake: 68.511% 
Loss D: 1.034 
Loss G: 0.6804 (0.6820) Acc G: 31.306% 
LR: 2.000e-04 

2023-03-02 02:00:39,940 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.3024 (0.3636) Acc D Real: 65.288% 
Loss D Fake: 0.7082 (0.7071) Acc D Fake: 68.456% 
Loss D: 1.011 
Loss G: 0.6806 (0.6820) Acc G: 31.345% 
LR: 2.000e-04 

2023-03-02 02:00:39,947 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.4619 (0.3646) Acc D Real: 65.156% 
Loss D Fake: 0.7081 (0.7071) Acc D Fake: 68.402% 
Loss D: 1.170 
Loss G: 0.6802 (0.6820) Acc G: 31.401% 
LR: 2.000e-04 

2023-03-02 02:00:39,954 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3852 (0.3648) Acc D Real: 65.117% 
Loss D Fake: 0.7087 (0.7071) Acc D Fake: 68.349% 
Loss D: 1.094 
Loss G: 0.6796 (0.6820) Acc G: 31.456% 
LR: 2.000e-04 

2023-03-02 02:00:39,962 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3873 (0.3651) Acc D Real: 65.082% 
Loss D Fake: 0.7094 (0.7071) Acc D Fake: 68.297% 
Loss D: 1.097 
Loss G: 0.6788 (0.6819) Acc G: 31.510% 
LR: 2.000e-04 

2023-03-02 02:00:39,971 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3445 (0.3648) Acc D Real: 65.113% 
Loss D Fake: 0.7102 (0.7072) Acc D Fake: 68.230% 
Loss D: 1.055 
Loss G: 0.6781 (0.6819) Acc G: 31.580% 
LR: 2.000e-04 

2023-03-02 02:00:39,978 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.3623 (0.3648) Acc D Real: 65.132% 
Loss D Fake: 0.7108 (0.7072) Acc D Fake: 68.163% 
Loss D: 1.073 
Loss G: 0.6775 (0.6819) Acc G: 31.648% 
LR: 2.000e-04 

2023-03-02 02:00:39,986 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.2706 (0.3639) Acc D Real: 65.247% 
Loss D Fake: 0.7112 (0.7072) Acc D Fake: 68.082% 
Loss D: 0.982 
Loss G: 0.6774 (0.6818) Acc G: 31.730% 
LR: 2.000e-04 

2023-03-02 02:00:39,994 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3468 (0.3637) Acc D Real: 65.275% 
Loss D Fake: 0.7112 (0.7073) Acc D Fake: 68.002% 
Loss D: 1.058 
Loss G: 0.6774 (0.6818) Acc G: 31.812% 
LR: 2.000e-04 

2023-03-02 02:00:40,003 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.4185 (0.3642) Acc D Real: 65.210% 
Loss D Fake: 0.7112 (0.7073) Acc D Fake: 67.923% 
Loss D: 1.130 
Loss G: 0.6771 (0.6817) Acc G: 31.892% 
LR: 2.000e-04 

2023-03-02 02:00:40,011 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.2456 (0.3631) Acc D Real: 65.345% 
Loss D Fake: 0.7115 (0.7074) Acc D Fake: 67.846% 
Loss D: 0.957 
Loss G: 0.6771 (0.6817) Acc G: 31.971% 
LR: 2.000e-04 

2023-03-02 02:00:40,022 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3571 (0.3630) Acc D Real: 65.344% 
Loss D Fake: 0.7113 (0.7074) Acc D Fake: 67.755% 
Loss D: 1.068 
Loss G: 0.6772 (0.6816) Acc G: 32.048% 
LR: 2.000e-04 

2023-03-02 02:00:40,029 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3756 (0.3632) Acc D Real: 65.327% 
Loss D Fake: 0.7113 (0.7074) Acc D Fake: 67.665% 
Loss D: 1.087 
Loss G: 0.6770 (0.6816) Acc G: 32.139% 
LR: 2.000e-04 

2023-03-02 02:00:40,037 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.3934 (0.3634) Acc D Real: 65.294% 
Loss D Fake: 0.7116 (0.7075) Acc D Fake: 67.561% 
Loss D: 1.105 
Loss G: 0.6765 (0.6815) Acc G: 32.245% 
LR: 2.000e-04 

2023-03-02 02:00:40,044 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3208 (0.3630) Acc D Real: 65.339% 
Loss D Fake: 0.7122 (0.7075) Acc D Fake: 67.460% 
Loss D: 1.033 
Loss G: 0.6760 (0.6815) Acc G: 32.349% 
LR: 2.000e-04 

2023-03-02 02:00:40,051 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.4191 (0.3636) Acc D Real: 65.271% 
Loss D Fake: 0.7129 (0.7076) Acc D Fake: 67.344% 
Loss D: 1.132 
Loss G: 0.6751 (0.6814) Acc G: 32.481% 
LR: 2.000e-04 

2023-03-02 02:00:40,059 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3132 (0.3631) Acc D Real: 65.313% 
Loss D Fake: 0.7139 (0.7076) Acc D Fake: 67.200% 
Loss D: 1.027 
Loss G: 0.6742 (0.6814) Acc G: 32.627% 
LR: 2.000e-04 

2023-03-02 02:00:40,066 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3917 (0.3634) Acc D Real: 65.342% 
Loss D Fake: 0.7150 (0.7077) Acc D Fake: 67.029% 
Loss D: 1.107 
Loss G: 0.6729 (0.6813) Acc G: 32.815% 
LR: 2.000e-04 

2023-03-02 02:00:40,073 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.3740 (0.3635) Acc D Real: 65.362% 
Loss D Fake: 0.7169 (0.7078) Acc D Fake: 66.770% 
Loss D: 1.091 
Loss G: 0.6711 (0.6812) Acc G: 33.138% 
LR: 2.000e-04 

2023-03-02 02:00:40,080 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3909 (0.3637) Acc D Real: 65.436% 
Loss D Fake: 2.5470 (0.7242) Acc D Fake: 66.174% 
Loss D: 2.938 
Loss G: 0.6717 (0.6811) Acc G: 33.735% 
LR: 2.000e-04 

2023-03-02 02:00:40,088 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3927 (0.3640) Acc D Real: 65.724% 
Loss D Fake: 0.7174 (0.7241) Acc D Fake: 65.589% 
Loss D: 1.110 
Loss G: 0.6702 (0.6810) Acc G: 34.322% 
LR: 2.000e-04 

2023-03-02 02:00:40,095 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.3001 (0.3634) Acc D Real: 66.023% 
Loss D Fake: 0.7193 (0.7241) Acc D Fake: 65.013% 
Loss D: 1.019 
Loss G: 0.6685 (0.6809) Acc G: 34.898% 
LR: 2.000e-04 

2023-03-02 02:00:40,102 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3778 (0.3635) Acc D Real: 66.318% 
Loss D Fake: 0.7209 (0.7241) Acc D Fake: 64.448% 
Loss D: 1.099 
Loss G: 0.6671 (0.6808) Acc G: 35.464% 
LR: 2.000e-04 

2023-03-02 02:00:40,110 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.2963 (0.3629) Acc D Real: 66.608% 
Loss D Fake: 0.7223 (0.7240) Acc D Fake: 63.892% 
Loss D: 1.019 
Loss G: 0.6661 (0.6807) Acc G: 36.020% 
LR: 2.000e-04 

2023-03-02 02:00:40,117 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.5230 (0.3643) Acc D Real: 66.892% 
Loss D Fake: 0.7235 (0.7240) Acc D Fake: 63.346% 
Loss D: 1.246 
Loss G: 0.6645 (0.6805) Acc G: 36.567% 
LR: 2.000e-04 

2023-03-02 02:00:40,125 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3331 (0.3640) Acc D Real: 67.172% 
Loss D Fake: 0.7252 (0.7241) Acc D Fake: 62.809% 
Loss D: 1.058 
Loss G: 0.6631 (0.6804) Acc G: 37.105% 
LR: 2.000e-04 

2023-03-02 02:00:40,132 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.2969 (0.3635) Acc D Real: 67.447% 
Loss D Fake: 0.7263 (0.7241) Acc D Fake: 62.282% 
Loss D: 1.023 
Loss G: 0.6624 (0.6802) Acc G: 37.633% 
LR: 2.000e-04 

2023-03-02 02:00:40,140 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.4635 (0.3643) Acc D Real: 67.717% 
Loss D Fake: 0.7271 (0.7241) Acc D Fake: 61.763% 
Loss D: 1.191 
Loss G: 0.6615 (0.6801) Acc G: 38.153% 
LR: 2.000e-04 

2023-03-02 02:00:40,148 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.4423 (0.3650) Acc D Real: 67.984% 
Loss D Fake: 0.7283 (0.7241) Acc D Fake: 61.252% 
Loss D: 1.171 
Loss G: 0.6602 (0.6799) Acc G: 38.664% 
LR: 2.000e-04 

2023-03-02 02:00:40,155 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.4241 (0.3654) Acc D Real: 68.246% 
Loss D Fake: 0.7298 (0.7242) Acc D Fake: 60.750% 
Loss D: 1.154 
Loss G: 0.6587 (0.6797) Acc G: 39.167% 
LR: 2.000e-04 

2023-03-02 02:00:40,163 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.4036 (0.3658) Acc D Real: 68.504% 
Loss D Fake: 0.7314 (0.7242) Acc D Fake: 60.256% 
Loss D: 1.135 
Loss G: 0.6573 (0.6795) Acc G: 39.661% 
LR: 2.000e-04 

2023-03-02 02:00:40,171 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.4259 (0.3662) Acc D Real: 68.758% 
Loss D Fake: 0.7329 (0.7243) Acc D Fake: 59.770% 
Loss D: 1.159 
Loss G: 0.6557 (0.6794) Acc G: 40.148% 
LR: 2.000e-04 

2023-03-02 02:00:40,179 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.2710 (0.3655) Acc D Real: 69.007% 
Loss D Fake: 0.7344 (0.7244) Acc D Fake: 59.292% 
Loss D: 1.005 
Loss G: 0.6549 (0.6792) Acc G: 40.627% 
LR: 2.000e-04 

2023-03-02 02:00:40,187 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.4405 (0.3661) Acc D Real: 69.252% 
Loss D Fake: 0.7351 (0.7245) Acc D Fake: 58.822% 
Loss D: 1.176 
Loss G: 0.6541 (0.6790) Acc G: 41.098% 
LR: 2.000e-04 

2023-03-02 02:00:40,195 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3944 (0.3663) Acc D Real: 69.494% 
Loss D Fake: 0.7361 (0.7246) Acc D Fake: 58.358% 
Loss D: 1.130 
Loss G: 0.6531 (0.6788) Acc G: 41.562% 
LR: 2.000e-04 

2023-03-02 02:00:40,203 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3845 (0.3664) Acc D Real: 69.732% 
Loss D Fake: 0.7370 (0.7247) Acc D Fake: 57.902% 
Loss D: 1.122 
Loss G: 0.6523 (0.6785) Acc G: 42.018% 
LR: 2.000e-04 

2023-03-02 02:00:40,210 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3971 (0.3667) Acc D Real: 69.967% 
Loss D Fake: 0.7379 (0.7248) Acc D Fake: 57.454% 
Loss D: 1.135 
Loss G: 0.6514 (0.6783) Acc G: 42.468% 
LR: 2.000e-04 

2023-03-02 02:00:40,218 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4729 (0.3675) Acc D Real: 70.198% 
Loss D Fake: 0.7390 (0.7249) Acc D Fake: 57.012% 
Loss D: 1.212 
Loss G: 0.6502 (0.6781) Acc G: 42.910% 
LR: 2.000e-04 

2023-03-02 02:00:40,226 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3599 (0.3674) Acc D Real: 70.425% 
Loss D Fake: 0.7404 (0.7250) Acc D Fake: 56.576% 
Loss D: 1.100 
Loss G: 0.6491 (0.6779) Acc G: 43.346% 
LR: 2.000e-04 

2023-03-02 02:00:40,234 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4333 (0.3679) Acc D Real: 70.648% 
Loss D Fake: 0.7416 (0.7251) Acc D Fake: 56.148% 
Loss D: 1.175 
Loss G: 0.6479 (0.6777) Acc G: 43.775% 
LR: 2.000e-04 

2023-03-02 02:00:40,242 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3744 (0.3680) Acc D Real: 70.869% 
Loss D Fake: 0.7429 (0.7253) Acc D Fake: 55.726% 
Loss D: 1.117 
Loss G: 0.6468 (0.6774) Acc G: 44.198% 
LR: 2.000e-04 

2023-03-02 02:00:40,250 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.4682 (0.3687) Acc D Real: 71.086% 
Loss D Fake: 0.7442 (0.7254) Acc D Fake: 55.310% 
Loss D: 1.212 
Loss G: 0.6453 (0.6772) Acc G: 44.614% 
LR: 2.000e-04 

2023-03-02 02:00:40,258 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.3969 (0.3689) Acc D Real: 71.299% 
Loss D Fake: 0.7459 (0.7255) Acc D Fake: 54.900% 
Loss D: 1.143 
Loss G: 0.6439 (0.6770) Acc G: 45.025% 
LR: 2.000e-04 

2023-03-02 02:00:40,265 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.3403 (0.3687) Acc D Real: 71.510% 
Loss D Fake: 0.7473 (0.7257) Acc D Fake: 54.496% 
Loss D: 1.088 
Loss G: 0.6428 (0.6767) Acc G: 45.429% 
LR: 2.000e-04 

2023-03-02 02:00:40,273 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.4033 (0.3690) Acc D Real: 71.718% 
Loss D Fake: 0.7484 (0.7259) Acc D Fake: 54.099% 
Loss D: 1.152 
Loss G: 0.6417 (0.6764) Acc G: 45.827% 
LR: 2.000e-04 

2023-03-02 02:00:40,280 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.4644 (0.3697) Acc D Real: 71.923% 
Loss D Fake: 0.7497 (0.7260) Acc D Fake: 53.707% 
Loss D: 1.214 
Loss G: 0.6404 (0.6762) Acc G: 46.220% 
LR: 2.000e-04 

2023-03-02 02:00:40,288 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3912 (0.3698) Acc D Real: 72.124% 
Loss D Fake: 0.7513 (0.7262) Acc D Fake: 53.320% 
Loss D: 1.143 
Loss G: 0.6390 (0.6759) Acc G: 46.607% 
LR: 2.000e-04 

2023-03-02 02:00:40,295 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.3177 (0.3695) Acc D Real: 72.323% 
Loss D Fake: 0.7525 (0.7264) Acc D Fake: 52.939% 
Loss D: 1.070 
Loss G: 0.6383 (0.6756) Acc G: 46.988% 
LR: 2.000e-04 

2023-03-02 02:00:40,303 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3207 (0.3691) Acc D Real: 72.520% 
Loss D Fake: 0.7531 (0.7266) Acc D Fake: 52.564% 
Loss D: 1.074 
Loss G: 0.6381 (0.6754) Acc G: 47.364% 
LR: 2.000e-04 

2023-03-02 02:00:40,310 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.4252 (0.3695) Acc D Real: 72.713% 
Loss D Fake: 0.7532 (0.7268) Acc D Fake: 52.194% 
Loss D: 1.178 
Loss G: 0.6378 (0.6751) Acc G: 47.735% 
LR: 2.000e-04 

2023-03-02 02:00:40,317 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.3191 (0.3692) Acc D Real: 72.904% 
Loss D Fake: 0.7536 (0.7270) Acc D Fake: 51.829% 
Loss D: 1.073 
Loss G: 0.6377 (0.6749) Acc G: 48.100% 
LR: 2.000e-04 

2023-03-02 02:00:40,325 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3319 (0.3689) Acc D Real: 73.092% 
Loss D Fake: 0.7534 (0.7272) Acc D Fake: 51.469% 
Loss D: 1.085 
Loss G: 0.6379 (0.6746) Acc G: 48.461% 
LR: 2.000e-04 

2023-03-02 02:00:40,332 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4068 (0.3692) Acc D Real: 73.277% 
Loss D Fake: 0.7532 (0.7273) Acc D Fake: 51.114% 
Loss D: 1.160 
Loss G: 0.6380 (0.6743) Acc G: 48.816% 
LR: 2.000e-04 

2023-03-02 02:00:40,340 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.2995 (0.3687) Acc D Real: 73.460% 
Loss D Fake: 0.7530 (0.7275) Acc D Fake: 50.764% 
Loss D: 1.052 
Loss G: 0.6384 (0.6741) Acc G: 49.167% 
LR: 2.000e-04 

2023-03-02 02:00:40,347 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3490 (0.3685) Acc D Real: 73.640% 
Loss D Fake: 0.7524 (0.7277) Acc D Fake: 50.418% 
Loss D: 1.101 
Loss G: 0.6390 (0.6739) Acc G: 49.512% 
LR: 2.000e-04 

2023-03-02 02:00:40,355 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3934 (0.3687) Acc D Real: 73.818% 
Loss D Fake: 0.7517 (0.7278) Acc D Fake: 50.078% 
Loss D: 1.145 
Loss G: 0.6395 (0.6736) Acc G: 49.854% 
LR: 2.000e-04 

2023-03-02 02:00:40,362 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.4072 (0.3690) Acc D Real: 73.993% 
Loss D Fake: 0.7514 (0.7280) Acc D Fake: 49.742% 
Loss D: 1.159 
Loss G: 0.6396 (0.6734) Acc G: 50.190% 
LR: 2.000e-04 

2023-03-02 02:00:40,370 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3365 (0.3688) Acc D Real: 74.166% 
Loss D Fake: 0.7513 (0.7282) Acc D Fake: 49.410% 
Loss D: 1.088 
Loss G: 0.6398 (0.6732) Acc G: 50.522% 
LR: 2.000e-04 

2023-03-02 02:00:40,378 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.3425 (0.3686) Acc D Real: 74.337% 
Loss D Fake: 0.7510 (0.7283) Acc D Fake: 49.083% 
Loss D: 1.093 
Loss G: 0.6402 (0.6730) Acc G: 50.850% 
LR: 2.000e-04 

2023-03-02 02:00:40,386 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.4121 (0.3689) Acc D Real: 74.506% 
Loss D Fake: 0.7506 (0.7285) Acc D Fake: 48.760% 
Loss D: 1.163 
Loss G: 0.6404 (0.6727) Acc G: 51.173% 
LR: 2.000e-04 

2023-03-02 02:00:40,393 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3647 (0.3688) Acc D Real: 74.673% 
Loss D Fake: 0.7504 (0.7286) Acc D Fake: 48.441% 
Loss D: 1.115 
Loss G: 0.6405 (0.6725) Acc G: 51.492% 
LR: 2.000e-04 

2023-03-02 02:00:40,402 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.4102 (0.3691) Acc D Real: 74.836% 
Loss D Fake: 0.7503 (0.7287) Acc D Fake: 48.127% 
Loss D: 1.160 
Loss G: 0.6405 (0.6723) Acc G: 51.807% 
LR: 2.000e-04 

2023-03-02 02:00:40,409 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3715 (0.3691) Acc D Real: 74.998% 
Loss D Fake: 0.7504 (0.7289) Acc D Fake: 47.816% 
Loss D: 1.122 
Loss G: 0.6404 (0.6721) Acc G: 52.118% 
LR: 2.000e-04 

2023-03-02 02:00:40,416 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.3760 (0.3692) Acc D Real: 75.159% 
Loss D Fake: 0.7505 (0.7290) Acc D Fake: 47.510% 
Loss D: 1.127 
Loss G: 0.6403 (0.6719) Acc G: 52.425% 
LR: 2.000e-04 

2023-03-02 02:00:40,424 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4957 (0.3700) Acc D Real: 75.317% 
Loss D Fake: 0.7509 (0.7292) Acc D Fake: 47.207% 
Loss D: 1.247 
Loss G: 0.6395 (0.6717) Acc G: 52.728% 
LR: 2.000e-04 

2023-03-02 02:00:40,432 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2512 (0.3692) Acc D Real: 75.332% 
Loss D Fake: 0.7516 (0.7293) Acc D Fake: 47.179% 
Loss D: 1.003 
Loss G: 0.6393 (0.6715) Acc G: 52.756% 
LR: 2.000e-04 

2023-03-02 02:00:40,655 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.845 | Generator Loss: 0.639 | Avg: 1.485 
2023-03-02 02:00:40,676 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.939 | Generator Loss: 0.639 | Avg: 1.579 
2023-03-02 02:00:40,698 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.914 | Generator Loss: 0.639 | Avg: 1.553 
2023-03-02 02:00:40,724 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.889 | Generator Loss: 0.639 | Avg: 1.528 
2023-03-02 02:00:40,750 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.890 | Generator Loss: 0.639 | Avg: 1.529 
2023-03-02 02:00:40,775 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.933 | Generator Loss: 0.639 | Avg: 1.572 
2023-03-02 02:00:40,800 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.962 | Generator Loss: 0.639 | Avg: 1.602 
2023-03-02 02:00:40,826 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 1.003 | Generator Loss: 0.639 | Avg: 1.642 
2023-03-02 02:00:40,852 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 1.027 | Generator Loss: 0.639 | Avg: 1.666 
2023-03-02 02:00:40,877 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.062 | Generator Loss: 0.639 | Avg: 1.702 
2023-03-02 02:00:40,902 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.090 | Generator Loss: 0.639 | Avg: 1.730 
2023-03-02 02:00:40,928 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.115 | Generator Loss: 0.639 | Avg: 1.754 
2023-03-02 02:00:40,953 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.136 | Generator Loss: 0.639 | Avg: 1.775 
2023-03-02 02:00:40,979 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.152 | Generator Loss: 0.639 | Avg: 1.791 
2023-03-02 02:00:41,004 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.140 | Generator Loss: 0.639 | Avg: 1.779 
2023-03-02 02:00:41,029 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.127 | Generator Loss: 0.639 | Avg: 1.766 
2023-03-02 02:00:41,055 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.113 | Generator Loss: 0.639 | Avg: 1.753 
2023-03-02 02:00:41,087 -                train: [    INFO] - 
Epoch: 20/20
2023-03-02 02:00:41,277 -                train: [    INFO] - TRAIN Iteration: [   2/158] 
Loss D Real: 0.3939 (0.3941) Acc D Real: 100.000% 
Loss D Fake: 0.7518 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.146 
Loss G: 0.6391 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,285 -                train: [    INFO] - TRAIN Iteration: [   3/158] 
Loss D Real: 0.3385 (0.3756) Acc D Real: 100.000% 
Loss D Fake: 0.7520 (0.7518) Acc D Fake: 0.000% 
Loss D: 1.090 
Loss G: 0.6391 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,292 -                train: [    INFO] - TRAIN Iteration: [   4/158] 
Loss D Real: 0.2320 (0.3397) Acc D Real: 100.000% 
Loss D Fake: 0.7516 (0.7517) Acc D Fake: 0.000% 
Loss D: 0.984 
Loss G: 0.6399 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,311 -                train: [    INFO] - TRAIN Iteration: [   5/158] 
Loss D Real: 0.3918 (0.3501) Acc D Real: 100.000% 
Loss D Fake: 0.7505 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.142 
Loss G: 0.6407 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,319 -                train: [    INFO] - TRAIN Iteration: [   6/158] 
Loss D Real: 0.3524 (0.3505) Acc D Real: 100.000% 
Loss D Fake: 0.7497 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6414 (0.6399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,332 -                train: [    INFO] - TRAIN Iteration: [   7/158] 
Loss D Real: 0.3867 (0.3556) Acc D Real: 100.000% 
Loss D Fake: 0.7489 (0.7509) Acc D Fake: 0.000% 
Loss D: 1.136 
Loss G: 0.6420 (0.6402) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,340 -                train: [    INFO] - TRAIN Iteration: [   8/158] 
Loss D Real: 0.4078 (0.3622) Acc D Real: 99.993% 
Loss D Fake: 0.7485 (0.7506) Acc D Fake: 0.000% 
Loss D: 1.156 
Loss G: 0.6422 (0.6405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,349 -                train: [    INFO] - TRAIN Iteration: [   9/158] 
Loss D Real: 0.3208 (0.3576) Acc D Real: 99.994% 
Loss D Fake: 0.7483 (0.7503) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6426 (0.6407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,357 -                train: [    INFO] - TRAIN Iteration: [  10/158] 
Loss D Real: 0.4903 (0.3708) Acc D Real: 99.995% 
Loss D Fake: 0.7480 (0.7501) Acc D Fake: 0.000% 
Loss D: 1.238 
Loss G: 0.6423 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,366 -                train: [    INFO] - TRAIN Iteration: [  11/158] 
Loss D Real: 0.4605 (0.3790) Acc D Real: 99.991% 
Loss D Fake: 0.7487 (0.7500) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.6414 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,373 -                train: [    INFO] - TRAIN Iteration: [  12/158] 
Loss D Real: 0.4104 (0.3816) Acc D Real: 99.991% 
Loss D Fake: 0.7499 (0.7500) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6403 (0.6409) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,380 -                train: [    INFO] - TRAIN Iteration: [  13/158] 
Loss D Real: 0.4430 (0.3863) Acc D Real: 99.988% 
Loss D Fake: 0.7512 (0.7501) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6390 (0.6407) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,387 -                train: [    INFO] - TRAIN Iteration: [  14/158] 
Loss D Real: 0.3360 (0.3827) Acc D Real: 99.985% 
Loss D Fake: 0.7527 (0.7502) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6379 (0.6405) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,395 -                train: [    INFO] - TRAIN Iteration: [  15/158] 
Loss D Real: 0.3492 (0.3805) Acc D Real: 99.986% 
Loss D Fake: 0.7536 (0.7505) Acc D Fake: 0.000% 
Loss D: 1.103 
Loss G: 0.6372 (0.6403) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,402 -                train: [    INFO] - TRAIN Iteration: [  16/158] 
Loss D Real: 0.3224 (0.3769) Acc D Real: 99.987% 
Loss D Fake: 0.7542 (0.7507) Acc D Fake: 0.000% 
Loss D: 1.077 
Loss G: 0.6370 (0.6401) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,413 -                train: [    INFO] - TRAIN Iteration: [  17/158] 
Loss D Real: 0.3893 (0.3776) Acc D Real: 99.985% 
Loss D Fake: 0.7544 (0.7509) Acc D Fake: 0.000% 
Loss D: 1.144 
Loss G: 0.6367 (0.6399) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,420 -                train: [    INFO] - TRAIN Iteration: [  18/158] 
Loss D Real: 0.3618 (0.3767) Acc D Real: 99.980% 
Loss D Fake: 0.7546 (0.7511) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6366 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,427 -                train: [    INFO] - TRAIN Iteration: [  19/158] 
Loss D Real: 0.3721 (0.3765) Acc D Real: 99.978% 
Loss D Fake: 0.7547 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6365 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,434 -                train: [    INFO] - TRAIN Iteration: [  20/158] 
Loss D Real: 0.3679 (0.3761) Acc D Real: 99.979% 
Loss D Fake: 0.7548 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6364 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,441 -                train: [    INFO] - TRAIN Iteration: [  21/158] 
Loss D Real: 0.2969 (0.3723) Acc D Real: 99.980% 
Loss D Fake: 0.7547 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6368 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,448 -                train: [    INFO] - TRAIN Iteration: [  22/158] 
Loss D Real: 0.2707 (0.3677) Acc D Real: 99.979% 
Loss D Fake: 0.7538 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.025 
Loss G: 0.6379 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,455 -                train: [    INFO] - TRAIN Iteration: [  23/158] 
Loss D Real: 0.3290 (0.3660) Acc D Real: 99.980% 
Loss D Fake: 0.7525 (0.7518) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6391 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,462 -                train: [    INFO] - TRAIN Iteration: [  24/158] 
Loss D Real: 0.3629 (0.3659) Acc D Real: 99.980% 
Loss D Fake: 0.7511 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.114 
Loss G: 0.6403 (0.6392) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,469 -                train: [    INFO] - TRAIN Iteration: [  25/158] 
Loss D Real: 0.4516 (0.3693) Acc D Real: 99.981% 
Loss D Fake: 0.7501 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.202 
Loss G: 0.6408 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,477 -                train: [    INFO] - TRAIN Iteration: [  26/158] 
Loss D Real: 0.3346 (0.3680) Acc D Real: 99.980% 
Loss D Fake: 0.7496 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.084 
Loss G: 0.6413 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,484 -                train: [    INFO] - TRAIN Iteration: [  27/158] 
Loss D Real: 0.3527 (0.3674) Acc D Real: 99.981% 
Loss D Fake: 0.7490 (0.7515) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6419 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,491 -                train: [    INFO] - TRAIN Iteration: [  28/158] 
Loss D Real: 0.4125 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7486 (0.7514) Acc D Fake: 0.000% 
Loss D: 1.161 
Loss G: 0.6421 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,499 -                train: [    INFO] - TRAIN Iteration: [  29/158] 
Loss D Real: 0.4289 (0.3711) Acc D Real: 99.980% 
Loss D Fake: 0.7486 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.177 
Loss G: 0.6419 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,506 -                train: [    INFO] - TRAIN Iteration: [  30/158] 
Loss D Real: 0.3981 (0.3720) Acc D Real: 99.977% 
Loss D Fake: 0.7490 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6414 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,514 -                train: [    INFO] - TRAIN Iteration: [  31/158] 
Loss D Real: 0.4575 (0.3747) Acc D Real: 99.978% 
Loss D Fake: 0.7496 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.207 
Loss G: 0.6405 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,522 -                train: [    INFO] - TRAIN Iteration: [  32/158] 
Loss D Real: 0.3668 (0.3745) Acc D Real: 99.977% 
Loss D Fake: 0.7507 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.118 
Loss G: 0.6396 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,529 -                train: [    INFO] - TRAIN Iteration: [  33/158] 
Loss D Real: 0.3583 (0.3740) Acc D Real: 99.978% 
Loss D Fake: 0.7516 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.110 
Loss G: 0.6389 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,537 -                train: [    INFO] - TRAIN Iteration: [  34/158] 
Loss D Real: 0.3980 (0.3747) Acc D Real: 99.977% 
Loss D Fake: 0.7523 (0.7512) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6383 (0.6397) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,545 -                train: [    INFO] - TRAIN Iteration: [  35/158] 
Loss D Real: 0.4085 (0.3757) Acc D Real: 99.978% 
Loss D Fake: 0.7531 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.162 
Loss G: 0.6375 (0.6396) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,553 -                train: [    INFO] - TRAIN Iteration: [  36/158] 
Loss D Real: 0.3907 (0.3761) Acc D Real: 99.975% 
Loss D Fake: 0.7540 (0.7513) Acc D Fake: 0.000% 
Loss D: 1.145 
Loss G: 0.6367 (0.6395) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,561 -                train: [    INFO] - TRAIN Iteration: [  37/158] 
Loss D Real: 0.4047 (0.3769) Acc D Real: 99.976% 
Loss D Fake: 0.7550 (0.7514) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6358 (0.6394) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,569 -                train: [    INFO] - TRAIN Iteration: [  38/158] 
Loss D Real: 0.4622 (0.3791) Acc D Real: 99.977% 
Loss D Fake: 0.7562 (0.7516) Acc D Fake: 0.000% 
Loss D: 1.218 
Loss G: 0.6345 (0.6393) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,578 -                train: [    INFO] - TRAIN Iteration: [  39/158] 
Loss D Real: 0.3317 (0.3779) Acc D Real: 99.976% 
Loss D Fake: 0.7575 (0.7517) Acc D Fake: 0.000% 
Loss D: 1.089 
Loss G: 0.6336 (0.6391) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,587 -                train: [    INFO] - TRAIN Iteration: [  40/158] 
Loss D Real: 0.4429 (0.3795) Acc D Real: 99.977% 
Loss D Fake: 0.7585 (0.7519) Acc D Fake: 0.000% 
Loss D: 1.201 
Loss G: 0.6325 (0.6390) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,595 -                train: [    INFO] - TRAIN Iteration: [  41/158] 
Loss D Real: 0.3491 (0.3788) Acc D Real: 99.977% 
Loss D Fake: 0.7597 (0.7521) Acc D Fake: 0.000% 
Loss D: 1.109 
Loss G: 0.6317 (0.6388) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,603 -                train: [    INFO] - TRAIN Iteration: [  42/158] 
Loss D Real: 0.3664 (0.3785) Acc D Real: 99.976% 
Loss D Fake: 0.7606 (0.7523) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6310 (0.6386) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,610 -                train: [    INFO] - TRAIN Iteration: [  43/158] 
Loss D Real: 0.4235 (0.3795) Acc D Real: 99.977% 
Loss D Fake: 0.7613 (0.7525) Acc D Fake: 0.000% 
Loss D: 1.185 
Loss G: 0.6304 (0.6384) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,618 -                train: [    INFO] - TRAIN Iteration: [  44/158] 
Loss D Real: 0.3194 (0.3782) Acc D Real: 99.978% 
Loss D Fake: 0.7619 (0.7527) Acc D Fake: 0.000% 
Loss D: 1.081 
Loss G: 0.6300 (0.6382) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,626 -                train: [    INFO] - TRAIN Iteration: [  45/158] 
Loss D Real: 0.3824 (0.3783) Acc D Real: 99.978% 
Loss D Fake: 0.7622 (0.7529) Acc D Fake: 0.000% 
Loss D: 1.145 
Loss G: 0.6298 (0.6380) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,634 -                train: [    INFO] - TRAIN Iteration: [  46/158] 
Loss D Real: 0.4692 (0.3802) Acc D Real: 99.978% 
Loss D Fake: 0.7627 (0.7531) Acc D Fake: 0.000% 
Loss D: 1.232 
Loss G: 0.6290 (0.6379) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,643 -                train: [    INFO] - TRAIN Iteration: [  47/158] 
Loss D Real: 0.3105 (0.3787) Acc D Real: 99.979% 
Loss D Fake: 0.7635 (0.7533) Acc D Fake: 0.000% 
Loss D: 1.074 
Loss G: 0.6286 (0.6377) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,651 -                train: [    INFO] - TRAIN Iteration: [  48/158] 
Loss D Real: 0.3568 (0.3783) Acc D Real: 99.979% 
Loss D Fake: 0.7638 (0.7536) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6284 (0.6375) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,658 -                train: [    INFO] - TRAIN Iteration: [  49/158] 
Loss D Real: 0.2958 (0.3766) Acc D Real: 99.980% 
Loss D Fake: 0.7637 (0.7538) Acc D Fake: 0.000% 
Loss D: 1.059 
Loss G: 0.6288 (0.6373) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,667 -                train: [    INFO] - TRAIN Iteration: [  50/158] 
Loss D Real: 0.3871 (0.3768) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7540) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6291 (0.6371) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,675 -                train: [    INFO] - TRAIN Iteration: [  51/158] 
Loss D Real: 0.2932 (0.3752) Acc D Real: 99.981% 
Loss D Fake: 0.7627 (0.7541) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.6298 (0.6370) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,683 -                train: [    INFO] - TRAIN Iteration: [  52/158] 
Loss D Real: 0.3644 (0.3750) Acc D Real: 99.981% 
Loss D Fake: 0.7619 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.126 
Loss G: 0.6304 (0.6369) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,692 -                train: [    INFO] - TRAIN Iteration: [  53/158] 
Loss D Real: 0.4719 (0.3768) Acc D Real: 99.981% 
Loss D Fake: 0.7614 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.233 
Loss G: 0.6305 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,700 -                train: [    INFO] - TRAIN Iteration: [  54/158] 
Loss D Real: 0.3731 (0.3767) Acc D Real: 99.982% 
Loss D Fake: 0.7616 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.135 
Loss G: 0.6304 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,707 -                train: [    INFO] - TRAIN Iteration: [  55/158] 
Loss D Real: 0.3352 (0.3760) Acc D Real: 99.979% 
Loss D Fake: 0.7616 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.097 
Loss G: 0.6305 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,715 -                train: [    INFO] - TRAIN Iteration: [  56/158] 
Loss D Real: 0.3319 (0.3752) Acc D Real: 99.980% 
Loss D Fake: 0.7613 (0.7548) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6308 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,722 -                train: [    INFO] - TRAIN Iteration: [  57/158] 
Loss D Real: 0.4385 (0.3763) Acc D Real: 99.980% 
Loss D Fake: 0.7610 (0.7549) Acc D Fake: 0.000% 
Loss D: 1.200 
Loss G: 0.6308 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,730 -                train: [    INFO] - TRAIN Iteration: [  58/158] 
Loss D Real: 0.3985 (0.3767) Acc D Real: 99.980% 
Loss D Fake: 0.7613 (0.7550) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6305 (0.6362) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,738 -                train: [    INFO] - TRAIN Iteration: [  59/158] 
Loss D Real: 0.3447 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7616 (0.7551) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6303 (0.6361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,746 -                train: [    INFO] - TRAIN Iteration: [  60/158] 
Loss D Real: 0.5042 (0.3783) Acc D Real: 99.981% 
Loss D Fake: 0.7621 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.266 
Loss G: 0.6295 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,755 -                train: [    INFO] - TRAIN Iteration: [  61/158] 
Loss D Real: 0.2959 (0.3769) Acc D Real: 99.980% 
Loss D Fake: 0.7630 (0.7554) Acc D Fake: 0.000% 
Loss D: 1.059 
Loss G: 0.6290 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,764 -                train: [    INFO] - TRAIN Iteration: [  62/158] 
Loss D Real: 0.3311 (0.3762) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6290 (0.6358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,772 -                train: [    INFO] - TRAIN Iteration: [  63/158] 
Loss D Real: 0.3972 (0.3765) Acc D Real: 99.980% 
Loss D Fake: 0.7632 (0.7556) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6289 (0.6357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,781 -                train: [    INFO] - TRAIN Iteration: [  64/158] 
Loss D Real: 0.3419 (0.3760) Acc D Real: 99.980% 
Loss D Fake: 0.7634 (0.7557) Acc D Fake: 0.000% 
Loss D: 1.105 
Loss G: 0.6289 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,789 -                train: [    INFO] - TRAIN Iteration: [  65/158] 
Loss D Real: 0.2930 (0.3747) Acc D Real: 99.980% 
Loss D Fake: 0.7631 (0.7559) Acc D Fake: 0.000% 
Loss D: 1.056 
Loss G: 0.6293 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,797 -                train: [    INFO] - TRAIN Iteration: [  66/158] 
Loss D Real: 0.4356 (0.3756) Acc D Real: 99.980% 
Loss D Fake: 0.7627 (0.7560) Acc D Fake: 0.000% 
Loss D: 1.198 
Loss G: 0.6294 (0.6354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,805 -                train: [    INFO] - TRAIN Iteration: [  67/158] 
Loss D Real: 0.4165 (0.3762) Acc D Real: 99.981% 
Loss D Fake: 0.7628 (0.7561) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6292 (0.6353) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,812 -                train: [    INFO] - TRAIN Iteration: [  68/158] 
Loss D Real: 0.3876 (0.3764) Acc D Real: 99.981% 
Loss D Fake: 0.7631 (0.7562) Acc D Fake: 0.000% 
Loss D: 1.151 
Loss G: 0.6289 (0.6352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,820 -                train: [    INFO] - TRAIN Iteration: [  69/158] 
Loss D Real: 0.3353 (0.3758) Acc D Real: 99.981% 
Loss D Fake: 0.7633 (0.7563) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6288 (0.6351) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,828 -                train: [    INFO] - TRAIN Iteration: [  70/158] 
Loss D Real: 0.3965 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7634 (0.7564) Acc D Fake: 0.000% 
Loss D: 1.160 
Loss G: 0.6287 (0.6350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,835 -                train: [    INFO] - TRAIN Iteration: [  71/158] 
Loss D Real: 0.4041 (0.3765) Acc D Real: 99.982% 
Loss D Fake: 0.7637 (0.7565) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6284 (0.6349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,843 -                train: [    INFO] - TRAIN Iteration: [  72/158] 
Loss D Real: 0.3896 (0.3767) Acc D Real: 99.981% 
Loss D Fake: 0.7641 (0.7566) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6280 (0.6348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,851 -                train: [    INFO] - TRAIN Iteration: [  73/158] 
Loss D Real: 0.3344 (0.3761) Acc D Real: 99.981% 
Loss D Fake: 0.7645 (0.7567) Acc D Fake: 0.000% 
Loss D: 1.099 
Loss G: 0.6278 (0.6347) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,858 -                train: [    INFO] - TRAIN Iteration: [  74/158] 
Loss D Real: 0.2557 (0.3745) Acc D Real: 99.982% 
Loss D Fake: 0.7643 (0.7568) Acc D Fake: 0.000% 
Loss D: 1.020 
Loss G: 0.6284 (0.6346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,866 -                train: [    INFO] - TRAIN Iteration: [  75/158] 
Loss D Real: 0.3646 (0.3743) Acc D Real: 99.982% 
Loss D Fake: 0.7634 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6291 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,873 -                train: [    INFO] - TRAIN Iteration: [  76/158] 
Loss D Real: 0.3311 (0.3738) Acc D Real: 99.982% 
Loss D Fake: 0.7626 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6298 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,881 -                train: [    INFO] - TRAIN Iteration: [  77/158] 
Loss D Real: 0.4028 (0.3741) Acc D Real: 99.982% 
Loss D Fake: 0.7618 (0.7570) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6304 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,889 -                train: [    INFO] - TRAIN Iteration: [  78/158] 
Loss D Real: 0.3246 (0.3735) Acc D Real: 99.983% 
Loss D Fake: 0.7612 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6311 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,896 -                train: [    INFO] - TRAIN Iteration: [  79/158] 
Loss D Real: 0.4335 (0.3743) Acc D Real: 99.983% 
Loss D Fake: 0.7605 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.194 
Loss G: 0.6314 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,904 -                train: [    INFO] - TRAIN Iteration: [  80/158] 
Loss D Real: 0.4486 (0.3752) Acc D Real: 99.983% 
Loss D Fake: 0.7605 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.209 
Loss G: 0.6311 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,912 -                train: [    INFO] - TRAIN Iteration: [  81/158] 
Loss D Real: 0.2420 (0.3736) Acc D Real: 99.983% 
Loss D Fake: 0.7606 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.003 
Loss G: 0.6315 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,920 -                train: [    INFO] - TRAIN Iteration: [  82/158] 
Loss D Real: 0.4192 (0.3741) Acc D Real: 99.983% 
Loss D Fake: 0.7600 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.179 
Loss G: 0.6318 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,927 -                train: [    INFO] - TRAIN Iteration: [  83/158] 
Loss D Real: 0.3532 (0.3739) Acc D Real: 99.984% 
Loss D Fake: 0.7598 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6320 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,935 -                train: [    INFO] - TRAIN Iteration: [  84/158] 
Loss D Real: 0.3875 (0.3740) Acc D Real: 99.984% 
Loss D Fake: 0.7596 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.147 
Loss G: 0.6321 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,943 -                train: [    INFO] - TRAIN Iteration: [  85/158] 
Loss D Real: 0.3641 (0.3739) Acc D Real: 99.984% 
Loss D Fake: 0.7595 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6322 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,951 -                train: [    INFO] - TRAIN Iteration: [  86/158] 
Loss D Real: 0.3427 (0.3735) Acc D Real: 99.984% 
Loss D Fake: 0.7593 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.102 
Loss G: 0.6324 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,958 -                train: [    INFO] - TRAIN Iteration: [  87/158] 
Loss D Real: 0.3998 (0.3738) Acc D Real: 99.983% 
Loss D Fake: 0.7592 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.159 
Loss G: 0.6324 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,965 -                train: [    INFO] - TRAIN Iteration: [  88/158] 
Loss D Real: 0.3634 (0.3737) Acc D Real: 99.983% 
Loss D Fake: 0.7592 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6324 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,973 -                train: [    INFO] - TRAIN Iteration: [  89/158] 
Loss D Real: 0.4360 (0.3744) Acc D Real: 99.984% 
Loss D Fake: 0.7594 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.195 
Loss G: 0.6320 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,981 -                train: [    INFO] - TRAIN Iteration: [  90/158] 
Loss D Real: 0.4063 (0.3748) Acc D Real: 99.984% 
Loss D Fake: 0.7599 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.166 
Loss G: 0.6315 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,989 -                train: [    INFO] - TRAIN Iteration: [  91/158] 
Loss D Real: 0.4416 (0.3755) Acc D Real: 99.984% 
Loss D Fake: 0.7607 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.202 
Loss G: 0.6306 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:41,997 -                train: [    INFO] - TRAIN Iteration: [  92/158] 
Loss D Real: 0.3884 (0.3757) Acc D Real: 99.984% 
Loss D Fake: 0.7618 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.150 
Loss G: 0.6297 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,005 -                train: [    INFO] - TRAIN Iteration: [  93/158] 
Loss D Real: 0.4021 (0.3759) Acc D Real: 99.984% 
Loss D Fake: 0.7628 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.165 
Loss G: 0.6288 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,013 -                train: [    INFO] - TRAIN Iteration: [  94/158] 
Loss D Real: 0.4045 (0.3762) Acc D Real: 99.984% 
Loss D Fake: 0.7639 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.168 
Loss G: 0.6278 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,021 -                train: [    INFO] - TRAIN Iteration: [  95/158] 
Loss D Real: 0.2886 (0.3753) Acc D Real: 99.984% 
Loss D Fake: 0.7648 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.053 
Loss G: 0.6274 (0.6338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,028 -                train: [    INFO] - TRAIN Iteration: [  96/158] 
Loss D Real: 0.3285 (0.3748) Acc D Real: 99.984% 
Loss D Fake: 0.7650 (0.7578) Acc D Fake: 0.000% 
Loss D: 1.093 
Loss G: 0.6274 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,036 -                train: [    INFO] - TRAIN Iteration: [  97/158] 
Loss D Real: 0.3144 (0.3742) Acc D Real: 99.984% 
Loss D Fake: 0.7648 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.079 
Loss G: 0.6278 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,047 -                train: [    INFO] - TRAIN Iteration: [  98/158] 
Loss D Real: 0.3239 (0.3737) Acc D Real: 99.984% 
Loss D Fake: 0.7641 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.088 
Loss G: 0.6285 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,055 -                train: [    INFO] - TRAIN Iteration: [  99/158] 
Loss D Real: 0.4194 (0.3742) Acc D Real: 99.984% 
Loss D Fake: 0.7633 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.183 
Loss G: 0.6289 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,063 -                train: [    INFO] - TRAIN Iteration: [ 100/158] 
Loss D Real: 0.3647 (0.3741) Acc D Real: 99.984% 
Loss D Fake: 0.7630 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.128 
Loss G: 0.6292 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,071 -                train: [    INFO] - TRAIN Iteration: [ 101/158] 
Loss D Real: 0.3351 (0.3737) Acc D Real: 99.985% 
Loss D Fake: 0.7626 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.6297 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,078 -                train: [    INFO] - TRAIN Iteration: [ 102/158] 
Loss D Real: 0.2379 (0.3723) Acc D Real: 99.985% 
Loss D Fake: 0.7617 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.000 
Loss G: 0.6309 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,086 -                train: [    INFO] - TRAIN Iteration: [ 103/158] 
Loss D Real: 0.3938 (0.3726) Acc D Real: 99.985% 
Loss D Fake: 0.7602 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.154 
Loss G: 0.6320 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,093 -                train: [    INFO] - TRAIN Iteration: [ 104/158] 
Loss D Real: 0.3324 (0.3722) Acc D Real: 99.984% 
Loss D Fake: 0.7590 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.091 
Loss G: 0.6330 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,102 -                train: [    INFO] - TRAIN Iteration: [ 105/158] 
Loss D Real: 0.3303 (0.3718) Acc D Real: 99.985% 
Loss D Fake: 0.7578 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.088 
Loss G: 0.6341 (0.6334) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,110 -                train: [    INFO] - TRAIN Iteration: [ 106/158] 
Loss D Real: 0.2930 (0.3710) Acc D Real: 99.985% 
Loss D Fake: 0.7565 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.049 
Loss G: 0.6354 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,118 -                train: [    INFO] - TRAIN Iteration: [ 107/158] 
Loss D Real: 0.3672 (0.3710) Acc D Real: 99.984% 
Loss D Fake: 0.7550 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.122 
Loss G: 0.6366 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,126 -                train: [    INFO] - TRAIN Iteration: [ 108/158] 
Loss D Real: 0.3693 (0.3710) Acc D Real: 99.984% 
Loss D Fake: 0.7538 (0.7581) Acc D Fake: 0.000% 
Loss D: 1.123 
Loss G: 0.6376 (0.6335) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,134 -                train: [    INFO] - TRAIN Iteration: [ 109/158] 
Loss D Real: 0.3126 (0.3704) Acc D Real: 99.984% 
Loss D Fake: 0.7527 (0.7580) Acc D Fake: 0.000% 
Loss D: 1.065 
Loss G: 0.6387 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,141 -                train: [    INFO] - TRAIN Iteration: [ 110/158] 
Loss D Real: 0.3695 (0.3704) Acc D Real: 99.984% 
Loss D Fake: 0.7515 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.121 
Loss G: 0.6397 (0.6336) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,149 -                train: [    INFO] - TRAIN Iteration: [ 111/158] 
Loss D Real: 0.4455 (0.3711) Acc D Real: 99.985% 
Loss D Fake: 0.7506 (0.7579) Acc D Fake: 0.000% 
Loss D: 1.196 
Loss G: 0.6401 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,156 -                train: [    INFO] - TRAIN Iteration: [ 112/158] 
Loss D Real: 0.3178 (0.3706) Acc D Real: 99.984% 
Loss D Fake: 0.7502 (0.7578) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6406 (0.6337) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,164 -                train: [    INFO] - TRAIN Iteration: [ 113/158] 
Loss D Real: 0.3891 (0.3708) Acc D Real: 99.984% 
Loss D Fake: 0.7496 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.139 
Loss G: 0.6410 (0.6338) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,171 -                train: [    INFO] - TRAIN Iteration: [ 114/158] 
Loss D Real: 0.2840 (0.3700) Acc D Real: 99.984% 
Loss D Fake: 0.7490 (0.7577) Acc D Fake: 0.000% 
Loss D: 1.033 
Loss G: 0.6418 (0.6339) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,179 -                train: [    INFO] - TRAIN Iteration: [ 115/158] 
Loss D Real: 0.3039 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7480 (0.7576) Acc D Fake: 0.000% 
Loss D: 1.052 
Loss G: 0.6429 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,186 -                train: [    INFO] - TRAIN Iteration: [ 116/158] 
Loss D Real: 0.3381 (0.3692) Acc D Real: 99.983% 
Loss D Fake: 0.7467 (0.7575) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6440 (0.6340) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,194 -                train: [    INFO] - TRAIN Iteration: [ 117/158] 
Loss D Real: 0.4388 (0.3698) Acc D Real: 99.983% 
Loss D Fake: 0.7457 (0.7574) Acc D Fake: 0.000% 
Loss D: 1.185 
Loss G: 0.6446 (0.6341) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,201 -                train: [    INFO] - TRAIN Iteration: [ 118/158] 
Loss D Real: 0.3771 (0.3698) Acc D Real: 99.983% 
Loss D Fake: 0.7453 (0.7573) Acc D Fake: 0.000% 
Loss D: 1.122 
Loss G: 0.6449 (0.6342) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,210 -                train: [    INFO] - TRAIN Iteration: [ 119/158] 
Loss D Real: 0.3958 (0.3701) Acc D Real: 99.983% 
Loss D Fake: 0.7451 (0.7572) Acc D Fake: 0.000% 
Loss D: 1.141 
Loss G: 0.6449 (0.6343) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,218 -                train: [    INFO] - TRAIN Iteration: [ 120/158] 
Loss D Real: 0.3235 (0.3697) Acc D Real: 99.983% 
Loss D Fake: 0.7450 (0.7571) Acc D Fake: 0.000% 
Loss D: 1.068 
Loss G: 0.6452 (0.6344) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,225 -                train: [    INFO] - TRAIN Iteration: [ 121/158] 
Loss D Real: 0.3429 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7446 (0.7570) Acc D Fake: 0.000% 
Loss D: 1.087 
Loss G: 0.6456 (0.6345) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,233 -                train: [    INFO] - TRAIN Iteration: [ 122/158] 
Loss D Real: 0.3715 (0.3695) Acc D Real: 99.983% 
Loss D Fake: 0.7441 (0.7569) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6459 (0.6346) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,240 -                train: [    INFO] - TRAIN Iteration: [ 123/158] 
Loss D Real: 0.3884 (0.3696) Acc D Real: 99.982% 
Loss D Fake: 0.7439 (0.7568) Acc D Fake: 0.000% 
Loss D: 1.132 
Loss G: 0.6460 (0.6347) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,248 -                train: [    INFO] - TRAIN Iteration: [ 124/158] 
Loss D Real: 0.3426 (0.3694) Acc D Real: 99.982% 
Loss D Fake: 0.7438 (0.7567) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6462 (0.6348) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,255 -                train: [    INFO] - TRAIN Iteration: [ 125/158] 
Loss D Real: 0.3255 (0.3691) Acc D Real: 99.983% 
Loss D Fake: 0.7434 (0.7565) Acc D Fake: 0.000% 
Loss D: 1.069 
Loss G: 0.6466 (0.6349) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,263 -                train: [    INFO] - TRAIN Iteration: [ 126/158] 
Loss D Real: 0.3727 (0.3691) Acc D Real: 99.983% 
Loss D Fake: 0.7429 (0.7564) Acc D Fake: 0.000% 
Loss D: 1.116 
Loss G: 0.6470 (0.6350) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,271 -                train: [    INFO] - TRAIN Iteration: [ 127/158] 
Loss D Real: 0.3586 (0.3690) Acc D Real: 99.983% 
Loss D Fake: 0.7426 (0.7563) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6473 (0.6351) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,278 -                train: [    INFO] - TRAIN Iteration: [ 128/158] 
Loss D Real: 0.3393 (0.3688) Acc D Real: 99.983% 
Loss D Fake: 0.7422 (0.7562) Acc D Fake: 0.000% 
Loss D: 1.082 
Loss G: 0.6477 (0.6352) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,286 -                train: [    INFO] - TRAIN Iteration: [ 129/158] 
Loss D Real: 0.3702 (0.3688) Acc D Real: 99.983% 
Loss D Fake: 0.7418 (0.7561) Acc D Fake: 0.000% 
Loss D: 1.112 
Loss G: 0.6480 (0.6353) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,293 -                train: [    INFO] - TRAIN Iteration: [ 130/158] 
Loss D Real: 0.4137 (0.3691) Acc D Real: 99.982% 
Loss D Fake: 0.7416 (0.7560) Acc D Fake: 0.000% 
Loss D: 1.155 
Loss G: 0.6479 (0.6354) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,301 -                train: [    INFO] - TRAIN Iteration: [ 131/158] 
Loss D Real: 0.3787 (0.3692) Acc D Real: 99.982% 
Loss D Fake: 0.7418 (0.7559) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6477 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,308 -                train: [    INFO] - TRAIN Iteration: [ 132/158] 
Loss D Real: 0.4794 (0.3700) Acc D Real: 99.982% 
Loss D Fake: 0.7423 (0.7558) Acc D Fake: 0.000% 
Loss D: 1.222 
Loss G: 0.6468 (0.6355) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,316 -                train: [    INFO] - TRAIN Iteration: [ 133/158] 
Loss D Real: 0.3006 (0.3695) Acc D Real: 99.982% 
Loss D Fake: 0.7433 (0.7557) Acc D Fake: 0.000% 
Loss D: 1.044 
Loss G: 0.6462 (0.6356) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,324 -                train: [    INFO] - TRAIN Iteration: [ 134/158] 
Loss D Real: 0.2601 (0.3687) Acc D Real: 99.982% 
Loss D Fake: 0.7436 (0.7556) Acc D Fake: 0.000% 
Loss D: 1.004 
Loss G: 0.6464 (0.6357) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,331 -                train: [    INFO] - TRAIN Iteration: [ 135/158] 
Loss D Real: 0.4238 (0.3691) Acc D Real: 99.982% 
Loss D Fake: 0.7433 (0.7555) Acc D Fake: 0.000% 
Loss D: 1.167 
Loss G: 0.6464 (0.6358) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,338 -                train: [    INFO] - TRAIN Iteration: [ 136/158] 
Loss D Real: 0.2940 (0.3686) Acc D Real: 99.982% 
Loss D Fake: 0.7434 (0.7554) Acc D Fake: 0.000% 
Loss D: 1.037 
Loss G: 0.6465 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,346 -                train: [    INFO] - TRAIN Iteration: [ 137/158] 
Loss D Real: 0.3820 (0.3686) Acc D Real: 99.981% 
Loss D Fake: 0.7431 (0.7553) Acc D Fake: 0.000% 
Loss D: 1.125 
Loss G: 0.6467 (0.6359) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,353 -                train: [    INFO] - TRAIN Iteration: [ 138/158] 
Loss D Real: 0.3809 (0.3687) Acc D Real: 99.980% 
Loss D Fake: 0.7429 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.124 
Loss G: 0.6468 (0.6360) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,360 -                train: [    INFO] - TRAIN Iteration: [ 139/158] 
Loss D Real: 0.3516 (0.3686) Acc D Real: 99.981% 
Loss D Fake: 0.7428 (0.7552) Acc D Fake: 0.000% 
Loss D: 1.094 
Loss G: 0.6470 (0.6361) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,368 -                train: [    INFO] - TRAIN Iteration: [ 140/158] 
Loss D Real: 0.4329 (0.3691) Acc D Real: 99.980% 
Loss D Fake: 0.7427 (0.7551) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6468 (0.6362) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,375 -                train: [    INFO] - TRAIN Iteration: [ 141/158] 
Loss D Real: 0.3546 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7431 (0.7550) Acc D Fake: 0.000% 
Loss D: 1.098 
Loss G: 0.6466 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,382 -                train: [    INFO] - TRAIN Iteration: [ 142/158] 
Loss D Real: 0.3695 (0.3690) Acc D Real: 99.980% 
Loss D Fake: 0.7433 (0.7549) Acc D Fake: 0.000% 
Loss D: 1.113 
Loss G: 0.6463 (0.6363) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,389 -                train: [    INFO] - TRAIN Iteration: [ 143/158] 
Loss D Real: 0.4819 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7438 (0.7548) Acc D Fake: 0.000% 
Loss D: 1.226 
Loss G: 0.6455 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,397 -                train: [    INFO] - TRAIN Iteration: [ 144/158] 
Loss D Real: 0.3567 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7448 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.101 
Loss G: 0.6447 (0.6364) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,404 -                train: [    INFO] - TRAIN Iteration: [ 145/158] 
Loss D Real: 0.4427 (0.3702) Acc D Real: 99.980% 
Loss D Fake: 0.7457 (0.7547) Acc D Fake: 0.000% 
Loss D: 1.188 
Loss G: 0.6437 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,411 -                train: [    INFO] - TRAIN Iteration: [ 146/158] 
Loss D Real: 0.3703 (0.3702) Acc D Real: 99.980% 
Loss D Fake: 0.7470 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.117 
Loss G: 0.6426 (0.6365) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,419 -                train: [    INFO] - TRAIN Iteration: [ 147/158] 
Loss D Real: 0.3562 (0.3701) Acc D Real: 99.979% 
Loss D Fake: 0.7481 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.104 
Loss G: 0.6418 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,426 -                train: [    INFO] - TRAIN Iteration: [ 148/158] 
Loss D Real: 0.3310 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7488 (0.7546) Acc D Fake: 0.000% 
Loss D: 1.080 
Loss G: 0.6412 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,434 -                train: [    INFO] - TRAIN Iteration: [ 149/158] 
Loss D Real: 0.3565 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7493 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.106 
Loss G: 0.6409 (0.6366) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,441 -                train: [    INFO] - TRAIN Iteration: [ 150/158] 
Loss D Real: 0.3363 (0.3695) Acc D Real: 99.980% 
Loss D Fake: 0.7495 (0.7545) Acc D Fake: 0.000% 
Loss D: 1.086 
Loss G: 0.6408 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,448 -                train: [    INFO] - TRAIN Iteration: [ 151/158] 
Loss D Real: 0.4031 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7496 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.153 
Loss G: 0.6406 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,456 -                train: [    INFO] - TRAIN Iteration: [ 152/158] 
Loss D Real: 0.3871 (0.3698) Acc D Real: 99.980% 
Loss D Fake: 0.7500 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.137 
Loss G: 0.6402 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,463 -                train: [    INFO] - TRAIN Iteration: [ 153/158] 
Loss D Real: 0.3348 (0.3696) Acc D Real: 99.980% 
Loss D Fake: 0.7504 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.085 
Loss G: 0.6400 (0.6367) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,471 -                train: [    INFO] - TRAIN Iteration: [ 154/158] 
Loss D Real: 0.3699 (0.3696) Acc D Real: 99.980% 
Loss D Fake: 0.7505 (0.7544) Acc D Fake: 0.000% 
Loss D: 1.120 
Loss G: 0.6399 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,478 -                train: [    INFO] - TRAIN Iteration: [ 155/158] 
Loss D Real: 0.3759 (0.3697) Acc D Real: 99.980% 
Loss D Fake: 0.7507 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.127 
Loss G: 0.6397 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,485 -                train: [    INFO] - TRAIN Iteration: [ 156/158] 
Loss D Real: 0.2994 (0.3692) Acc D Real: 99.979% 
Loss D Fake: 0.7508 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.050 
Loss G: 0.6398 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,492 -                train: [    INFO] - TRAIN Iteration: [ 157/158] 
Loss D Real: 0.4256 (0.3696) Acc D Real: 99.979% 
Loss D Fake: 0.7506 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.176 
Loss G: 0.6398 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,500 -                train: [    INFO] - TRAIN Iteration: [ 158/158] 
Loss D Real: 0.2504 (0.3688) Acc D Real: 99.979% 
Loss D Fake: 0.7505 (0.7543) Acc D Fake: 0.000% 
Loss D: 1.001 
Loss G: 0.6403 (0.6368) Acc G: 100.000% 
LR: 2.000e-04 

2023-03-02 02:00:42,728 -                train: [    INFO] - TEST [11/180]: Discriminator Loss: 0.825 | Generator Loss: 0.640 | Avg: 1.465 
2023-03-02 02:00:42,751 -                train: [    INFO] - TEST [21/180]: Discriminator Loss: 0.912 | Generator Loss: 0.640 | Avg: 1.552 
2023-03-02 02:00:42,774 -                train: [    INFO] - TEST [31/180]: Discriminator Loss: 0.889 | Generator Loss: 0.640 | Avg: 1.529 
2023-03-02 02:00:42,800 -                train: [    INFO] - TEST [41/180]: Discriminator Loss: 0.867 | Generator Loss: 0.640 | Avg: 1.507 
2023-03-02 02:00:42,825 -                train: [    INFO] - TEST [51/180]: Discriminator Loss: 0.865 | Generator Loss: 0.640 | Avg: 1.506 
2023-03-02 02:00:42,850 -                train: [    INFO] - TEST [61/180]: Discriminator Loss: 0.911 | Generator Loss: 0.640 | Avg: 1.551 
2023-03-02 02:00:42,876 -                train: [    INFO] - TEST [71/180]: Discriminator Loss: 0.935 | Generator Loss: 0.640 | Avg: 1.575 
2023-03-02 02:00:42,902 -                train: [    INFO] - TEST [81/180]: Discriminator Loss: 0.975 | Generator Loss: 0.640 | Avg: 1.615 
2023-03-02 02:00:42,929 -                train: [    INFO] - TEST [91/180]: Discriminator Loss: 0.997 | Generator Loss: 0.640 | Avg: 1.638 
2023-03-02 02:00:42,955 -                train: [    INFO] - TEST [101/180]: Discriminator Loss: 1.035 | Generator Loss: 0.640 | Avg: 1.675 
2023-03-02 02:00:42,981 -                train: [    INFO] - TEST [111/180]: Discriminator Loss: 1.062 | Generator Loss: 0.640 | Avg: 1.703 
2023-03-02 02:00:43,006 -                train: [    INFO] - TEST [121/180]: Discriminator Loss: 1.089 | Generator Loss: 0.640 | Avg: 1.729 
2023-03-02 02:00:43,037 -                train: [    INFO] - TEST [131/180]: Discriminator Loss: 1.110 | Generator Loss: 0.640 | Avg: 1.751 
2023-03-02 02:00:43,067 -                train: [    INFO] - TEST [141/180]: Discriminator Loss: 1.126 | Generator Loss: 0.640 | Avg: 1.766 
2023-03-02 02:00:43,094 -                train: [    INFO] - TEST [151/180]: Discriminator Loss: 1.113 | Generator Loss: 0.640 | Avg: 1.753 
2023-03-02 02:00:43,120 -                train: [    INFO] - TEST [161/180]: Discriminator Loss: 1.099 | Generator Loss: 0.640 | Avg: 1.740 
2023-03-02 02:00:43,147 -                train: [    INFO] - TEST [171/180]: Discriminator Loss: 1.086 | Generator Loss: 0.640 | Avg: 1.726 
2023-03-02 02:00:43,182 -                train: [    INFO] - Best Metric: At 2 Epoch Gen 0.849 Dis
2023-03-02 02:00:43,183 -                train: [    INFO] - MODEL TRAINING COMPLETED. 
 BEST RESULT SAVED
2023-03-02 02:00:43,635 -                train: [    INFO] - TEST [11/44]: Discriminator Loss: 0.619 | Generator Loss: 0.849 | Avg: 1.468 
2023-03-02 02:00:43,658 -                train: [    INFO] - TEST [21/44]: Discriminator Loss: 0.617 | Generator Loss: 0.849 | Avg: 1.465 
2023-03-02 02:00:43,681 -                train: [    INFO] - TEST [31/44]: Discriminator Loss: 0.615 | Generator Loss: 0.849 | Avg: 1.464 
2023-03-02 02:00:43,706 -                train: [    INFO] - TEST [41/44]: Discriminator Loss: 0.616 | Generator Loss: 0.849 | Avg: 1.464 
2023-03-02 02:00:44,107 -         Optimization: [    INFO] - 
 Batch: 1/121
2023-03-02 02:00:46,268 -         Optimization: [    INFO] - Batch [1/121]: Anomaly Score: 466.257 label: 0.0
2023-03-02 02:00:46,271 -         Optimization: [    INFO] - 
 Batch: 2/121
2023-03-02 02:00:48,385 -         Optimization: [    INFO] - Batch [2/121]: Anomaly Score: 529.977 label: 0.0
2023-03-02 02:00:48,385 -         Optimization: [    INFO] - 
 Batch: 3/121
2023-03-02 02:00:50,500 -         Optimization: [    INFO] - Batch [3/121]: Anomaly Score: 504.801 label: 0.0
2023-03-02 02:00:50,501 -         Optimization: [    INFO] - 
 Batch: 4/121
2023-03-02 02:00:52,616 -         Optimization: [    INFO] - Batch [4/121]: Anomaly Score: 443.575 label: 0.0
2023-03-02 02:00:52,626 -         Optimization: [    INFO] - 
 Batch: 5/121
2023-03-02 02:00:54,741 -         Optimization: [    INFO] - Batch [5/121]: Anomaly Score: 504.627 label: 0.0
2023-03-02 02:00:54,742 -         Optimization: [    INFO] - 
 Batch: 6/121
2023-03-02 02:00:56,856 -         Optimization: [    INFO] - Batch [6/121]: Anomaly Score: 389.892 label: 0.0
2023-03-02 02:00:56,856 -         Optimization: [    INFO] - 
 Batch: 7/121
2023-03-02 02:00:58,969 -         Optimization: [    INFO] - Batch [7/121]: Anomaly Score: 413.785 label: 0.0
2023-03-02 02:00:58,969 -         Optimization: [    INFO] - 
 Batch: 8/121
2023-03-02 02:01:01,076 -         Optimization: [    INFO] - Batch [8/121]: Anomaly Score: 510.531 label: 0.0
2023-03-02 02:01:01,077 -         Optimization: [    INFO] - 
 Batch: 9/121
2023-03-02 02:01:03,193 -         Optimization: [    INFO] - Batch [9/121]: Anomaly Score: 463.740 label: 0.0
2023-03-02 02:01:03,194 -         Optimization: [    INFO] - 
 Batch: 10/121
2023-03-02 02:01:05,304 -         Optimization: [    INFO] - Batch [10/121]: Anomaly Score: 358.456 label: 0.0
2023-03-02 02:01:05,305 -         Optimization: [    INFO] - 
 Batch: 11/121
2023-03-02 02:01:07,416 -         Optimization: [    INFO] - Batch [11/121]: Anomaly Score: 389.799 label: 0.0
2023-03-02 02:01:07,417 -         Optimization: [    INFO] - 
 Batch: 12/121
2023-03-02 02:01:09,538 -         Optimization: [    INFO] - Batch [12/121]: Anomaly Score: 351.639 label: 0.0
2023-03-02 02:01:09,539 -         Optimization: [    INFO] - 
 Batch: 13/121
2023-03-02 02:01:11,650 -         Optimization: [    INFO] - Batch [13/121]: Anomaly Score: 499.466 label: 0.0
2023-03-02 02:01:11,651 -         Optimization: [    INFO] - 
 Batch: 14/121
2023-03-02 02:01:13,766 -         Optimization: [    INFO] - Batch [14/121]: Anomaly Score: 420.870 label: 0.0
2023-03-02 02:01:13,767 -         Optimization: [    INFO] - 
 Batch: 15/121
2023-03-02 02:01:15,876 -         Optimization: [    INFO] - Batch [15/121]: Anomaly Score: 481.751 label: 0.0
2023-03-02 02:01:15,877 -         Optimization: [    INFO] - 
 Batch: 16/121
2023-03-02 02:01:18,000 -         Optimization: [    INFO] - Batch [16/121]: Anomaly Score: 524.861 label: 0.0
2023-03-02 02:01:18,002 -         Optimization: [    INFO] - 
 Batch: 17/121
2023-03-02 02:01:20,114 -         Optimization: [    INFO] - Batch [17/121]: Anomaly Score: 478.398 label: 0.0
2023-03-02 02:01:20,115 -         Optimization: [    INFO] - 
 Batch: 18/121
2023-03-02 02:01:22,226 -         Optimization: [    INFO] - Batch [18/121]: Anomaly Score: 463.407 label: 0.0
2023-03-02 02:01:22,227 -         Optimization: [    INFO] - 
 Batch: 19/121
2023-03-02 02:01:24,342 -         Optimization: [    INFO] - Batch [19/121]: Anomaly Score: 512.908 label: 0.0
2023-03-02 02:01:24,343 -         Optimization: [    INFO] - 
 Batch: 20/121
2023-03-02 02:01:26,453 -         Optimization: [    INFO] - Batch [20/121]: Anomaly Score: 487.421 label: 0.0
2023-03-02 02:01:26,454 -         Optimization: [    INFO] - 
 Batch: 21/121
2023-03-02 02:01:28,569 -         Optimization: [    INFO] - Batch [21/121]: Anomaly Score: 536.308 label: 0.0
2023-03-02 02:01:28,570 -         Optimization: [    INFO] - 
 Batch: 22/121
2023-03-02 02:01:30,709 -         Optimization: [    INFO] - Batch [22/121]: Anomaly Score: 502.256 label: 0.0
2023-03-02 02:01:30,710 -         Optimization: [    INFO] - 
 Batch: 23/121
2023-03-02 02:01:32,850 -         Optimization: [    INFO] - Batch [23/121]: Anomaly Score: 516.259 label: 0.0
2023-03-02 02:01:32,851 -         Optimization: [    INFO] - 
 Batch: 24/121
2023-03-02 02:01:34,981 -         Optimization: [    INFO] - Batch [24/121]: Anomaly Score: 482.294 label: 0.0
2023-03-02 02:01:34,981 -         Optimization: [    INFO] - 
 Batch: 25/121
2023-03-02 02:01:37,115 -         Optimization: [    INFO] - Batch [25/121]: Anomaly Score: 444.781 label: 0.0
2023-03-02 02:01:37,117 -         Optimization: [    INFO] - 
 Batch: 26/121
2023-03-02 02:01:39,287 -         Optimization: [    INFO] - Batch [26/121]: Anomaly Score: 450.907 label: 0.0
2023-03-02 02:01:39,289 -         Optimization: [    INFO] - 
 Batch: 27/121
2023-03-02 02:01:41,474 -         Optimization: [    INFO] - Batch [27/121]: Anomaly Score: 398.580 label: 0.0
2023-03-02 02:01:41,475 -         Optimization: [    INFO] - 
 Batch: 28/121
2023-03-02 02:01:43,649 -         Optimization: [    INFO] - Batch [28/121]: Anomaly Score: 500.339 label: 0.0
2023-03-02 02:01:43,650 -         Optimization: [    INFO] - 
 Batch: 29/121
2023-03-02 02:01:45,794 -         Optimization: [    INFO] - Batch [29/121]: Anomaly Score: 470.945 label: 0.0
2023-03-02 02:01:45,795 -         Optimization: [    INFO] - 
 Batch: 30/121
2023-03-02 02:01:47,973 -         Optimization: [    INFO] - Batch [30/121]: Anomaly Score: 401.075 label: 0.0
2023-03-02 02:01:47,974 -         Optimization: [    INFO] - 
 Batch: 31/121
2023-03-02 02:01:50,122 -         Optimization: [    INFO] - Batch [31/121]: Anomaly Score: 277.443 label: 0.0
2023-03-02 02:01:50,123 -         Optimization: [    INFO] - 
 Batch: 32/121
2023-03-02 02:01:52,266 -         Optimization: [    INFO] - Batch [32/121]: Anomaly Score: 250.701 label: 0.0
2023-03-02 02:01:52,268 -         Optimization: [    INFO] - 
 Batch: 33/121
2023-03-02 02:01:54,423 -         Optimization: [    INFO] - Batch [33/121]: Anomaly Score: 278.422 label: 0.0
2023-03-02 02:01:54,424 -         Optimization: [    INFO] - 
 Batch: 34/121
2023-03-02 02:01:56,569 -         Optimization: [    INFO] - Batch [34/121]: Anomaly Score: 351.372 label: 0.0
2023-03-02 02:01:56,571 -         Optimization: [    INFO] - 
 Batch: 35/121
2023-03-02 02:01:58,735 -         Optimization: [    INFO] - Batch [35/121]: Anomaly Score: 351.406 label: 0.0
2023-03-02 02:01:58,737 -         Optimization: [    INFO] - 
 Batch: 36/121
2023-03-02 02:02:00,882 -         Optimization: [    INFO] - Batch [36/121]: Anomaly Score: 409.445 label: 0.0
2023-03-02 02:02:00,885 -         Optimization: [    INFO] - 
 Batch: 37/121
2023-03-02 02:02:03,077 -         Optimization: [    INFO] - Batch [37/121]: Anomaly Score: 388.781 label: 0.0
2023-03-02 02:02:03,079 -         Optimization: [    INFO] - 
 Batch: 38/121
2023-03-02 02:02:05,223 -         Optimization: [    INFO] - Batch [38/121]: Anomaly Score: 387.812 label: 0.0
2023-03-02 02:02:05,225 -         Optimization: [    INFO] - 
 Batch: 39/121
2023-03-02 02:02:07,365 -         Optimization: [    INFO] - Batch [39/121]: Anomaly Score: 263.026 label: 0.0
2023-03-02 02:02:07,366 -         Optimization: [    INFO] - 
 Batch: 40/121
2023-03-02 02:02:09,614 -         Optimization: [    INFO] - Batch [40/121]: Anomaly Score: 414.920 label: 0.0
2023-03-02 02:02:09,615 -         Optimization: [    INFO] - 
 Batch: 41/121
2023-03-02 02:02:11,784 -         Optimization: [    INFO] - Batch [41/121]: Anomaly Score: 356.912 label: 0.0
2023-03-02 02:02:11,786 -         Optimization: [    INFO] - 
 Batch: 42/121
2023-03-02 02:02:13,951 -         Optimization: [    INFO] - Batch [42/121]: Anomaly Score: 233.457 label: 0.0
2023-03-02 02:02:13,952 -         Optimization: [    INFO] - 
 Batch: 43/121
2023-03-02 02:02:16,134 -         Optimization: [    INFO] - Batch [43/121]: Anomaly Score: 259.992 label: 0.0
2023-03-02 02:02:16,135 -         Optimization: [    INFO] - 
 Batch: 44/121
2023-03-02 02:02:18,312 -         Optimization: [    INFO] - Batch [44/121]: Anomaly Score: 225.597 label: 0.0
2023-03-02 02:02:18,314 -         Optimization: [    INFO] - 
 Batch: 45/121
2023-03-02 02:02:20,532 -         Optimization: [    INFO] - Batch [45/121]: Anomaly Score: 271.384 label: 0.0
2023-03-02 02:02:20,534 -         Optimization: [    INFO] - 
 Batch: 46/121
2023-03-02 02:02:22,704 -         Optimization: [    INFO] - Batch [46/121]: Anomaly Score: 377.532 label: 0.0
2023-03-02 02:02:22,706 -         Optimization: [    INFO] - 
 Batch: 47/121
2023-03-02 02:02:24,884 -         Optimization: [    INFO] - Batch [47/121]: Anomaly Score: 324.280 label: 0.0
2023-03-02 02:02:24,886 -         Optimization: [    INFO] - 
 Batch: 48/121
2023-03-02 02:02:27,053 -         Optimization: [    INFO] - Batch [48/121]: Anomaly Score: 402.056 label: 0.0
2023-03-02 02:02:27,055 -         Optimization: [    INFO] - 
 Batch: 49/121
2023-03-02 02:02:29,287 -         Optimization: [    INFO] - Batch [49/121]: Anomaly Score: 414.724 label: 0.0
2023-03-02 02:02:29,289 -         Optimization: [    INFO] - 
 Batch: 50/121
2023-03-02 02:02:31,445 -         Optimization: [    INFO] - Batch [50/121]: Anomaly Score: 48.807 label: 0.0
2023-03-02 02:02:31,446 -         Optimization: [    INFO] - 
 Batch: 51/121
2023-03-02 02:02:33,665 -         Optimization: [    INFO] - Batch [51/121]: Anomaly Score: 306.523 label: 0.0
2023-03-02 02:02:33,666 -         Optimization: [    INFO] - 
 Batch: 52/121
2023-03-02 02:02:35,884 -         Optimization: [    INFO] - Batch [52/121]: Anomaly Score: 18.860 label: 0.0
2023-03-02 02:02:35,886 -         Optimization: [    INFO] - 
 Batch: 53/121
2023-03-02 02:02:38,056 -         Optimization: [    INFO] - Batch [53/121]: Anomaly Score: 280.499 label: 0.0
2023-03-02 02:02:38,057 -         Optimization: [    INFO] - 
 Batch: 54/121
2023-03-02 02:02:40,265 -         Optimization: [    INFO] - Batch [54/121]: Anomaly Score: 278.947 label: 0.0
2023-03-02 02:02:40,267 -         Optimization: [    INFO] - 
 Batch: 55/121
2023-03-02 02:02:42,524 -         Optimization: [    INFO] - Batch [55/121]: Anomaly Score: 293.226 label: 0.0
2023-03-02 02:02:42,526 -         Optimization: [    INFO] - 
 Batch: 56/121
2023-03-02 02:02:44,784 -         Optimization: [    INFO] - Batch [56/121]: Anomaly Score: 262.013 label: 0.0
2023-03-02 02:02:44,786 -         Optimization: [    INFO] - 
 Batch: 57/121
2023-03-02 02:02:46,939 -         Optimization: [    INFO] - Batch [57/121]: Anomaly Score: 271.355 label: 0.0
2023-03-02 02:02:46,940 -         Optimization: [    INFO] - 
 Batch: 58/121
2023-03-02 02:02:49,096 -         Optimization: [    INFO] - Batch [58/121]: Anomaly Score: 367.570 label: 0.0
2023-03-02 02:02:49,097 -         Optimization: [    INFO] - 
 Batch: 59/121
2023-03-02 02:02:51,245 -         Optimization: [    INFO] - Batch [59/121]: Anomaly Score: 259.245 label: 0.0
2023-03-02 02:02:51,246 -         Optimization: [    INFO] - 
 Batch: 60/121
2023-03-02 02:02:53,426 -         Optimization: [    INFO] - Batch [60/121]: Anomaly Score: 275.737 label: 1.0
2023-03-02 02:02:53,427 -         Optimization: [    INFO] - 
 Batch: 61/121
2023-03-02 02:02:55,645 -         Optimization: [    INFO] - Batch [61/121]: Anomaly Score: 267.439 label: 1.0
2023-03-02 02:02:55,647 -         Optimization: [    INFO] - 
 Batch: 62/121
2023-03-02 02:02:57,854 -         Optimization: [    INFO] - Batch [62/121]: Anomaly Score: 296.469 label: 1.0
2023-03-02 02:02:57,856 -         Optimization: [    INFO] - 
 Batch: 63/121
2023-03-02 02:03:00,103 -         Optimization: [    INFO] - Batch [63/121]: Anomaly Score: 346.789 label: 1.0
2023-03-02 02:03:00,104 -         Optimization: [    INFO] - 
 Batch: 64/121
2023-03-02 02:03:02,308 -         Optimization: [    INFO] - Batch [64/121]: Anomaly Score: 281.667 label: 1.0
2023-03-02 02:03:02,309 -         Optimization: [    INFO] - 
 Batch: 65/121
2023-03-02 02:03:04,469 -         Optimization: [    INFO] - Batch [65/121]: Anomaly Score: 255.648 label: 1.0
2023-03-02 02:03:04,471 -         Optimization: [    INFO] - 
 Batch: 66/121
2023-03-02 02:03:06,612 -         Optimization: [    INFO] - Batch [66/121]: Anomaly Score: 258.009 label: 1.0
2023-03-02 02:03:06,614 -         Optimization: [    INFO] - 
 Batch: 67/121
2023-03-02 02:03:08,751 -         Optimization: [    INFO] - Batch [67/121]: Anomaly Score: 278.039 label: 0.0
2023-03-02 02:03:08,753 -         Optimization: [    INFO] - 
 Batch: 68/121
2023-03-02 02:03:10,896 -         Optimization: [    INFO] - Batch [68/121]: Anomaly Score: 360.976 label: 0.0
2023-03-02 02:03:10,897 -         Optimization: [    INFO] - 
 Batch: 69/121
2023-03-02 02:03:13,036 -         Optimization: [    INFO] - Batch [69/121]: Anomaly Score: 295.035 label: 0.0
2023-03-02 02:03:13,037 -         Optimization: [    INFO] - 
 Batch: 70/121
2023-03-02 02:03:15,196 -         Optimization: [    INFO] - Batch [70/121]: Anomaly Score: 216.804 label: 0.0
2023-03-02 02:03:15,198 -         Optimization: [    INFO] - 
 Batch: 71/121
2023-03-02 02:03:17,328 -         Optimization: [    INFO] - Batch [71/121]: Anomaly Score: 291.379 label: 0.0
2023-03-02 02:03:17,330 -         Optimization: [    INFO] - 
 Batch: 72/121
2023-03-02 02:03:19,482 -         Optimization: [    INFO] - Batch [72/121]: Anomaly Score: 272.524 label: 0.0
2023-03-02 02:03:19,483 -         Optimization: [    INFO] - 
 Batch: 73/121
2023-03-02 02:03:21,658 -         Optimization: [    INFO] - Batch [73/121]: Anomaly Score: 333.014 label: 0.0
2023-03-02 02:03:21,659 -         Optimization: [    INFO] - 
 Batch: 74/121
2023-03-02 02:03:23,870 -         Optimization: [    INFO] - Batch [74/121]: Anomaly Score: 364.602 label: 0.0
2023-03-02 02:03:23,871 -         Optimization: [    INFO] - 
 Batch: 75/121
2023-03-02 02:03:26,109 -         Optimization: [    INFO] - Batch [75/121]: Anomaly Score: 350.230 label: 0.0
2023-03-02 02:03:26,110 -         Optimization: [    INFO] - 
 Batch: 76/121
2023-03-02 02:03:28,342 -         Optimization: [    INFO] - Batch [76/121]: Anomaly Score: 364.371 label: 0.0
2023-03-02 02:03:28,343 -         Optimization: [    INFO] - 
 Batch: 77/121
2023-03-02 02:03:30,544 -         Optimization: [    INFO] - Batch [77/121]: Anomaly Score: 414.719 label: 0.0
2023-03-02 02:03:30,546 -         Optimization: [    INFO] - 
 Batch: 78/121
2023-03-02 02:03:32,704 -         Optimization: [    INFO] - Batch [78/121]: Anomaly Score: 463.004 label: 0.0
2023-03-02 02:03:32,705 -         Optimization: [    INFO] - 
 Batch: 79/121
2023-03-02 02:03:34,846 -         Optimization: [    INFO] - Batch [79/121]: Anomaly Score: 390.393 label: 0.0
2023-03-02 02:03:34,848 -         Optimization: [    INFO] - 
 Batch: 80/121
2023-03-02 02:03:37,022 -         Optimization: [    INFO] - Batch [80/121]: Anomaly Score: 423.657 label: 0.0
2023-03-02 02:03:37,023 -         Optimization: [    INFO] - 
 Batch: 81/121
2023-03-02 02:03:39,183 -         Optimization: [    INFO] - Batch [81/121]: Anomaly Score: 394.235 label: 0.0
2023-03-02 02:03:39,184 -         Optimization: [    INFO] - 
 Batch: 82/121
2023-03-02 02:03:41,313 -         Optimization: [    INFO] - Batch [82/121]: Anomaly Score: 416.885 label: 0.0
2023-03-02 02:03:41,314 -         Optimization: [    INFO] - 
 Batch: 83/121
2023-03-02 02:03:43,538 -         Optimization: [    INFO] - Batch [83/121]: Anomaly Score: 386.050 label: 0.0
2023-03-02 02:03:43,539 -         Optimization: [    INFO] - 
 Batch: 84/121
2023-03-02 02:03:45,680 -         Optimization: [    INFO] - Batch [84/121]: Anomaly Score: 423.716 label: 0.0
2023-03-02 02:03:45,682 -         Optimization: [    INFO] - 
 Batch: 85/121
2023-03-02 02:03:47,939 -         Optimization: [    INFO] - Batch [85/121]: Anomaly Score: 513.518 label: 0.0
2023-03-02 02:03:47,940 -         Optimization: [    INFO] - 
 Batch: 86/121
2023-03-02 02:03:50,082 -         Optimization: [    INFO] - Batch [86/121]: Anomaly Score: 483.231 label: 0.0
2023-03-02 02:03:50,084 -         Optimization: [    INFO] - 
 Batch: 87/121
2023-03-02 02:03:52,225 -         Optimization: [    INFO] - Batch [87/121]: Anomaly Score: 409.316 label: 0.0
2023-03-02 02:03:52,226 -         Optimization: [    INFO] - 
 Batch: 88/121
2023-03-02 02:03:54,382 -         Optimization: [    INFO] - Batch [88/121]: Anomaly Score: 455.813 label: 0.0
2023-03-02 02:03:54,384 -         Optimization: [    INFO] - 
 Batch: 89/121
2023-03-02 02:03:56,516 -         Optimization: [    INFO] - Batch [89/121]: Anomaly Score: 374.110 label: 0.0
2023-03-02 02:03:56,518 -         Optimization: [    INFO] - 
 Batch: 90/121
2023-03-02 02:03:58,741 -         Optimization: [    INFO] - Batch [90/121]: Anomaly Score: 470.483 label: 0.0
2023-03-02 02:03:58,742 -         Optimization: [    INFO] - 
 Batch: 91/121
2023-03-02 02:04:00,902 -         Optimization: [    INFO] - Batch [91/121]: Anomaly Score: 502.853 label: 0.0
2023-03-02 02:04:00,903 -         Optimization: [    INFO] - 
 Batch: 92/121
2023-03-02 02:04:03,042 -         Optimization: [    INFO] - Batch [92/121]: Anomaly Score: 535.836 label: 0.0
2023-03-02 02:04:03,043 -         Optimization: [    INFO] - 
 Batch: 93/121
2023-03-02 02:04:05,190 -         Optimization: [    INFO] - Batch [93/121]: Anomaly Score: 543.372 label: 0.0
2023-03-02 02:04:05,193 -         Optimization: [    INFO] - 
 Batch: 94/121
2023-03-02 02:04:07,381 -         Optimization: [    INFO] - Batch [94/121]: Anomaly Score: 453.029 label: 0.0
2023-03-02 02:04:07,383 -         Optimization: [    INFO] - 
 Batch: 95/121
2023-03-02 02:04:09,551 -         Optimization: [    INFO] - Batch [95/121]: Anomaly Score: 504.012 label: 0.0
2023-03-02 02:04:09,553 -         Optimization: [    INFO] - 
 Batch: 96/121
2023-03-02 02:04:11,701 -         Optimization: [    INFO] - Batch [96/121]: Anomaly Score: 531.092 label: 0.0
2023-03-02 02:04:11,703 -         Optimization: [    INFO] - 
 Batch: 97/121
2023-03-02 02:04:13,827 -         Optimization: [    INFO] - Batch [97/121]: Anomaly Score: 500.644 label: 0.0
2023-03-02 02:04:13,829 -         Optimization: [    INFO] - 
 Batch: 98/121
2023-03-02 02:04:15,971 -         Optimization: [    INFO] - Batch [98/121]: Anomaly Score: 502.354 label: 0.0
2023-03-02 02:04:15,973 -         Optimization: [    INFO] - 
 Batch: 99/121
2023-03-02 02:04:18,111 -         Optimization: [    INFO] - Batch [99/121]: Anomaly Score: 512.126 label: 0.0
2023-03-02 02:04:18,112 -         Optimization: [    INFO] - 
 Batch: 100/121
2023-03-02 02:04:20,268 -         Optimization: [    INFO] - Batch [100/121]: Anomaly Score: 503.317 label: 1.0
2023-03-02 02:04:20,270 -         Optimization: [    INFO] - 
 Batch: 101/121
2023-03-02 02:04:22,391 -         Optimization: [    INFO] - Batch [101/121]: Anomaly Score: 534.251 label: 1.0
2023-03-02 02:04:22,392 -         Optimization: [    INFO] - 
 Batch: 102/121
2023-03-02 02:04:24,508 -         Optimization: [    INFO] - Batch [102/121]: Anomaly Score: 510.888 label: 1.0
2023-03-02 02:04:24,510 -         Optimization: [    INFO] - 
 Batch: 103/121
2023-03-02 02:04:26,624 -         Optimization: [    INFO] - Batch [103/121]: Anomaly Score: 545.569 label: 1.0
2023-03-02 02:04:26,626 -         Optimization: [    INFO] - 
 Batch: 104/121
2023-03-02 02:04:28,766 -         Optimization: [    INFO] - Batch [104/121]: Anomaly Score: 568.716 label: 1.0
2023-03-02 02:04:28,768 -         Optimization: [    INFO] - 
 Batch: 105/121
2023-03-02 02:04:30,917 -         Optimization: [    INFO] - Batch [105/121]: Anomaly Score: 518.923 label: 1.0
2023-03-02 02:04:30,919 -         Optimization: [    INFO] - 
 Batch: 106/121
2023-03-02 02:04:33,059 -         Optimization: [    INFO] - Batch [106/121]: Anomaly Score: 538.571 label: 1.0
2023-03-02 02:04:33,060 -         Optimization: [    INFO] - 
 Batch: 107/121
2023-03-02 02:04:35,190 -         Optimization: [    INFO] - Batch [107/121]: Anomaly Score: 546.892 label: 1.0
2023-03-02 02:04:35,192 -         Optimization: [    INFO] - 
 Batch: 108/121
2023-03-02 02:04:37,367 -         Optimization: [    INFO] - Batch [108/121]: Anomaly Score: 495.599 label: 0.0
2023-03-02 02:04:37,369 -         Optimization: [    INFO] - 
 Batch: 109/121
2023-03-02 02:04:39,497 -         Optimization: [    INFO] - Batch [109/121]: Anomaly Score: 527.425 label: 0.0
2023-03-02 02:04:39,498 -         Optimization: [    INFO] - 
 Batch: 110/121
2023-03-02 02:04:41,639 -         Optimization: [    INFO] - Batch [110/121]: Anomaly Score: 536.287 label: 0.0
2023-03-02 02:04:41,641 -         Optimization: [    INFO] - 
 Batch: 111/121
2023-03-02 02:04:43,779 -         Optimization: [    INFO] - Batch [111/121]: Anomaly Score: 534.817 label: 0.0
2023-03-02 02:04:43,781 -         Optimization: [    INFO] - 
 Batch: 112/121
2023-03-02 02:04:45,917 -         Optimization: [    INFO] - Batch [112/121]: Anomaly Score: 561.760 label: 0.0
2023-03-02 02:04:45,918 -         Optimization: [    INFO] - 
 Batch: 113/121
2023-03-02 02:04:48,057 -         Optimization: [    INFO] - Batch [113/121]: Anomaly Score: 537.073 label: 0.0
2023-03-02 02:04:48,059 -         Optimization: [    INFO] - 
 Batch: 114/121
2023-03-02 02:04:50,203 -         Optimization: [    INFO] - Batch [114/121]: Anomaly Score: 491.927 label: 0.0
2023-03-02 02:04:50,205 -         Optimization: [    INFO] - 
 Batch: 115/121
2023-03-02 02:04:52,341 -         Optimization: [    INFO] - Batch [115/121]: Anomaly Score: 550.126 label: 0.0
2023-03-02 02:04:52,343 -         Optimization: [    INFO] - 
 Batch: 116/121
2023-03-02 02:04:54,529 -         Optimization: [    INFO] - Batch [116/121]: Anomaly Score: 500.297 label: 0.0
2023-03-02 02:04:54,530 -         Optimization: [    INFO] - 
 Batch: 117/121
2023-03-02 02:04:56,780 -         Optimization: [    INFO] - Batch [117/121]: Anomaly Score: 514.858 label: 0.0
2023-03-02 02:04:56,781 -         Optimization: [    INFO] - 
 Batch: 118/121
2023-03-02 02:04:59,004 -         Optimization: [    INFO] - Batch [118/121]: Anomaly Score: 565.347 label: 0.0
2023-03-02 02:04:59,005 -         Optimization: [    INFO] - 
 Batch: 119/121
2023-03-02 02:05:01,230 -         Optimization: [    INFO] - Batch [119/121]: Anomaly Score: 504.058 label: 0.0
2023-03-02 02:05:01,232 -         Optimization: [    INFO] - 
 Batch: 120/121
2023-03-02 02:05:03,484 -         Optimization: [    INFO] - Batch [120/121]: Anomaly Score: 506.758 label: 0.0
2023-03-02 02:05:03,487 -         Optimization: [    INFO] - 
 Batch: 121/121
2023-03-02 02:05:05,753 -         Optimization: [    INFO] - Batch [121/121]: Anomaly Score: 530.517 label: 0.0
