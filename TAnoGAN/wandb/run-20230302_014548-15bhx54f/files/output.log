Epoch: 1/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.6834 (0.6845) Acc D Real: 72.734%
Loss D Fake: 0.7040 (0.7026) Acc D Fake: 0.000%
Loss D: 1.387
Loss G: 0.6813 (0.6826) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.6771 (0.6820) Acc D Real: 77.240%
Loss D Fake: 0.7066 (0.7039) Acc D Fake: 0.000%
Loss D: 1.384
Loss G: 0.6788 (0.6814) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.6741 (0.6800) Acc D Real: 79.505%
Loss D Fake: 0.7092 (0.7053) Acc D Fake: 0.000%
Loss D: 1.383
Loss G: 0.6763 (0.6801) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.6717 (0.6784) Acc D Real: 82.240%
Loss D Fake: 0.7118 (0.7066) Acc D Fake: 0.000%
Loss D: 1.383
Loss G: 0.6738 (0.6788) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.6789 (0.6784) Acc D Real: 81.502%
Loss D Fake: 0.7143 (0.7079) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.6713 (0.6776) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.6672 (0.6768) Acc D Real: 82.805%
Loss D Fake: 0.7169 (0.7091) Acc D Fake: 0.000%
Loss D: 1.384
Loss G: 0.6689 (0.6763) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.6680 (0.6757) Acc D Real: 84.316%
Loss D Fake: 0.7195 (0.7104) Acc D Fake: 0.000%
Loss D: 1.388
Loss G: 0.6664 (0.6751) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.6687 (0.6749) Acc D Real: 85.556%
Loss D Fake: 0.7221 (0.7117) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6640 (0.6739) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.6642 (0.6739) Acc D Real: 86.115%
Loss D Fake: 0.7247 (0.7130) Acc D Fake: 0.000%
Loss D: 1.389
Loss G: 0.6615 (0.6726) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.6637 (0.6729) Acc D Real: 87.131%
Loss D Fake: 0.7273 (0.7143) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6591 (0.6714) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.6582 (0.6717) Acc D Real: 88.147%
Loss D Fake: 0.7299 (0.7156) Acc D Fake: 0.000%
Loss D: 1.388
Loss G: 0.6566 (0.6702) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.6530 (0.6703) Acc D Real: 89.058%
Loss D Fake: 0.7326 (0.7169) Acc D Fake: 0.000%
Loss D: 1.386
Loss G: 0.6541 (0.6689) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.6559 (0.6693) Acc D Real: 89.833%
Loss D Fake: 0.7353 (0.7182) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6516 (0.6677) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.6528 (0.6682) Acc D Real: 90.503%
Loss D Fake: 0.7380 (0.7196) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6491 (0.6665) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.6522 (0.6672) Acc D Real: 91.097%
Loss D Fake: 0.7408 (0.7209) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.6466 (0.6652) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.6484 (0.6661) Acc D Real: 91.621%
Loss D Fake: 0.7435 (0.7222) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6441 (0.6640) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.6440 (0.6648) Acc D Real: 92.086%
Loss D Fake: 0.7463 (0.7236) Acc D Fake: 0.000%
Loss D: 1.390
Loss G: 0.6416 (0.6627) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.6441 (0.6637) Acc D Real: 92.503%
Loss D Fake: 0.7492 (0.7249) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.6390 (0.6615) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.6372 (0.6624) Acc D Real: 92.878%
Loss D Fake: 0.7521 (0.7263) Acc D Fake: 0.000%
Loss D: 1.389
Loss G: 0.6364 (0.6602) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.6373 (0.6612) Acc D Real: 93.217%
Loss D Fake: 0.7550 (0.7276) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6338 (0.6590) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.6340 (0.6600) Acc D Real: 93.525%
Loss D Fake: 0.7580 (0.7290) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6311 (0.6577) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.6343 (0.6589) Acc D Real: 93.807%
Loss D Fake: 0.7610 (0.7304) Acc D Fake: 0.000%
Loss D: 1.395
Loss G: 0.6285 (0.6564) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.6282 (0.6576) Acc D Real: 94.065%
Loss D Fake: 0.7640 (0.7318) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6259 (0.6552) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.6236 (0.6562) Acc D Real: 94.302%
Loss D Fake: 0.7671 (0.7332) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6231 (0.6539) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.6238 (0.6550) Acc D Real: 94.521%
Loss D Fake: 0.7704 (0.7346) Acc D Fake: 0.000%
Loss D: 1.394
Loss G: 0.6203 (0.6526) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.6175 (0.6536) Acc D Real: 94.724%
Loss D Fake: 0.7737 (0.7361) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.6174 (0.6513) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.6157 (0.6522) Acc D Real: 94.913%
Loss D Fake: 0.7771 (0.7376) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.6145 (0.6500) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.6122 (0.6509) Acc D Real: 95.088%
Loss D Fake: 0.7806 (0.7390) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.6115 (0.6486) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.6081 (0.6494) Acc D Real: 95.252%
Loss D Fake: 0.7843 (0.7406) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6083 (0.6473) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.6034 (0.6479) Acc D Real: 95.405%
Loss D Fake: 0.7882 (0.7421) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6050 (0.6459) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.5998 (0.6464) Acc D Real: 95.549%
Loss D Fake: 0.7923 (0.7437) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.6016 (0.6445) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.5968 (0.6449) Acc D Real: 95.683%
Loss D Fake: 0.7965 (0.7453) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.5980 (0.6431) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.5908 (0.6433) Acc D Real: 95.810%
Loss D Fake: 0.8011 (0.7469) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.5942 (0.6417) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.5866 (0.6417) Acc D Real: 95.930%
Loss D Fake: 0.8059 (0.7486) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.5902 (0.6402) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.5827 (0.6401) Acc D Real: 96.043%
Loss D Fake: 0.8110 (0.7503) Acc D Fake: 0.000%
Loss D: 1.394
Loss G: 0.5860 (0.6387) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.5768 (0.6384) Acc D Real: 96.150%
Loss D Fake: 0.8165 (0.7521) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.5817 (0.6372) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.5727 (0.6366) Acc D Real: 96.251%
Loss D Fake: 0.8222 (0.7540) Acc D Fake: 0.000%
Loss D: 1.395
Loss G: 0.5770 (0.6356) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.5631 (0.6348) Acc D Real: 96.347%
Loss D Fake: 0.8284 (0.7559) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.5721 (0.6340) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.5597 (0.6329) Acc D Real: 96.439%
Loss D Fake: 0.8353 (0.7578) Acc D Fake: 0.000%
Loss D: 1.395
Loss G: 0.5667 (0.6323) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.5434 (0.6307) Acc D Real: 96.526%
Loss D Fake: 0.8428 (0.7599) Acc D Fake: 0.000%
Loss D: 1.386
Loss G: 0.5608 (0.6305) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.5333 (0.6284) Acc D Real: 96.608%
Loss D Fake: 0.8512 (0.7621) Acc D Fake: 0.000%
Loss D: 1.385
Loss G: 0.5543 (0.6287) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.5300 (0.6261) Acc D Real: 96.687%
Loss D Fake: 0.8608 (0.7644) Acc D Fake: 0.000%
Loss D: 1.391
Loss G: 0.5470 (0.6268) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.5200 (0.6237) Acc D Real: 96.763%
Loss D Fake: 0.8720 (0.7668) Acc D Fake: 0.000%
Loss D: 1.392
Loss G: 0.5386 (0.6248) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.5076 (0.6211) Acc D Real: 96.834%
Loss D Fake: 0.8851 (0.7695) Acc D Fake: 0.000%
Loss D: 1.393
Loss G: 0.5289 (0.6227) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.5038 (0.6185) Acc D Real: 96.903%
Loss D Fake: 0.9009 (0.7723) Acc D Fake: 0.000%
Loss D: 1.405
Loss G: 0.5176 (0.6204) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4583 (0.6151) Acc D Real: 96.969%
Loss D Fake: 0.9198 (0.7755) Acc D Fake: 0.000%
Loss D: 1.378
Loss G: 0.5043 (0.6179) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.4580 (0.6119) Acc D Real: 97.032%
Loss D Fake: 0.9438 (0.7790) Acc D Fake: 0.000%
Loss D: 1.402
Loss G: 0.4886 (0.6152) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.4260 (0.6081) Acc D Real: 97.093%
Loss D Fake: 0.9732 (0.7829) Acc D Fake: 0.000%
Loss D: 1.399
Loss G: 0.4715 (0.6123) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.4241 (0.6044) Acc D Real: 97.151%
Loss D Fake: 1.0056 (0.7874) Acc D Fake: 0.000%
Loss D: 1.430
Loss G: 0.4556 (0.6092) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3987 (0.6004) Acc D Real: 97.207%
Loss D Fake: 1.0331 (0.7922) Acc D Fake: 0.000%
Loss D: 1.432
Loss G: 0.4459 (0.6060) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4169 (0.5968) Acc D Real: 97.261%
Loss D Fake: 1.0420 (0.7970) Acc D Fake: 0.000%
Loss D: 1.459
Loss G: 0.4469 (0.6029) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3664 (0.5925) Acc D Real: 97.312%
Loss D Fake: 1.0317 (0.8014) Acc D Fake: 0.000%
Loss D: 1.398
Loss G: 0.4523 (0.6001) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3761 (0.5885) Acc D Real: 97.362%
Loss D Fake: 1.0190 (0.8055) Acc D Fake: 0.000%
Loss D: 1.395
Loss G: 0.4585 (0.5974) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.3991 (0.5850) Acc D Real: 97.410%
Loss D Fake: 1.0057 (0.8091) Acc D Fake: 0.000%
Loss D: 1.405
Loss G: 0.4657 (0.5951) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3715 (0.5812) Acc D Real: 97.456%
Loss D Fake: 0.9923 (0.8124) Acc D Fake: 0.000%
Loss D: 1.364
Loss G: 0.4718 (0.5929) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.4232 (0.5784) Acc D Real: 97.501%
Loss D Fake: 0.9822 (0.8154) Acc D Fake: 0.000%
Loss D: 1.405
Loss G: 0.4768 (0.5908) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3921 (0.5752) Acc D Real: 97.544%
Loss D Fake: 0.9739 (0.8181) Acc D Fake: 0.000%
Loss D: 1.366
Loss G: 0.4807 (0.5889) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3988 (0.5722) Acc D Real: 97.586%
Loss D Fake: 0.9681 (0.8206) Acc D Fake: 0.000%
Loss D: 1.367
Loss G: 0.4835 (0.5871) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.4198 (0.5697) Acc D Real: 97.626%
Loss D Fake: 0.9638 (0.8230) Acc D Fake: 0.000%
Loss D: 1.384
Loss G: 0.4858 (0.5854) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.4475 (0.5677) Acc D Real: 97.665%
Loss D Fake: 0.9601 (0.8253) Acc D Fake: 0.000%
Loss D: 1.408
Loss G: 0.4876 (0.5838) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.4519 (0.5658) Acc D Real: 97.702%
Loss D Fake: 0.9570 (0.8274) Acc D Fake: 0.000%
Loss D: 1.409
Loss G: 0.4898 (0.5823) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.4347 (0.5637) Acc D Real: 97.739%
Loss D Fake: 0.9530 (0.8294) Acc D Fake: 0.000%
Loss D: 1.388
Loss G: 0.4919 (0.5809) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3710 (0.5607) Acc D Real: 97.774%
Loss D Fake: 0.9496 (0.8313) Acc D Fake: 0.000%
Loss D: 1.321
Loss G: 0.4936 (0.5795) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3920 (0.5581) Acc D Real: 97.808%
Loss D Fake: 0.9471 (0.8330) Acc D Fake: 0.000%
Loss D: 1.339
Loss G: 0.4950 (0.5782) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3633 (0.5552) Acc D Real: 97.842%
Loss D Fake: 0.9457 (0.8348) Acc D Fake: 0.000%
Loss D: 1.309
Loss G: 0.4948 (0.5770) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.4542 (0.5537) Acc D Real: 97.874%
Loss D Fake: 0.9471 (0.8364) Acc D Fake: 0.000%
Loss D: 1.401
Loss G: 0.4940 (0.5757) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4074 (0.5515) Acc D Real: 97.905%
Loss D Fake: 0.9477 (0.8381) Acc D Fake: 0.000%
Loss D: 1.355
Loss G: 0.4944 (0.5745) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.3775 (0.5490) Acc D Real: 97.936%
Loss D Fake: 0.9481 (0.8397) Acc D Fake: 0.000%
Loss D: 1.326
Loss G: 0.4925 (0.5733) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3235 (0.5458) Acc D Real: 97.965%
Loss D Fake: 0.9527 (0.8413) Acc D Fake: 0.000%
Loss D: 1.276
Loss G: 0.4903 (0.5721) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.4567 (0.5445) Acc D Real: 97.994%
Loss D Fake: 0.9550 (0.8429) Acc D Fake: 0.000%
Loss D: 1.412
Loss G: 0.4906 (0.5710) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3839 (0.5423) Acc D Real: 98.022%
Loss D Fake: 0.9530 (0.8444) Acc D Fake: 0.000%
Loss D: 1.337
Loss G: 0.4923 (0.5699) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3520 (0.5397) Acc D Real: 98.049%
Loss D Fake: 0.9499 (0.8459) Acc D Fake: 0.000%
Loss D: 1.302
Loss G: 0.4939 (0.5689) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3912 (0.5377) Acc D Real: 98.075%
Loss D Fake: 0.9467 (0.8472) Acc D Fake: 0.000%
Loss D: 1.338
Loss G: 0.4965 (0.5679) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3975 (0.5358) Acc D Real: 98.101%
Loss D Fake: 0.9419 (0.8485) Acc D Fake: 0.000%
Loss D: 1.339
Loss G: 0.4991 (0.5670) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4274 (0.5344) Acc D Real: 98.126%
Loss D Fake: 0.9375 (0.8496) Acc D Fake: 0.000%
Loss D: 1.365
Loss G: 0.5017 (0.5661) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.4029 (0.5327) Acc D Real: 98.150%
Loss D Fake: 0.9333 (0.8507) Acc D Fake: 0.000%
Loss D: 1.336
Loss G: 0.5043 (0.5653) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3829 (0.5308) Acc D Real: 98.174%
Loss D Fake: 0.9295 (0.8517) Acc D Fake: 0.000%
Loss D: 1.312
Loss G: 0.5062 (0.5646) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.4286 (0.5295) Acc D Real: 98.197%
Loss D Fake: 0.9267 (0.8527) Acc D Fake: 0.000%
Loss D: 1.355
Loss G: 0.5078 (0.5638) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3303 (0.5270) Acc D Real: 98.219%
Loss D Fake: 0.9246 (0.8536) Acc D Fake: 0.000%
Loss D: 1.255
Loss G: 0.5087 (0.5631) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3777 (0.5251) Acc D Real: 98.241%
Loss D Fake: 0.9236 (0.8545) Acc D Fake: 0.000%
Loss D: 1.301
Loss G: 0.5091 (0.5625) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.4028 (0.5237) Acc D Real: 98.263%
Loss D Fake: 0.9230 (0.8553) Acc D Fake: 0.000%
Loss D: 1.326
Loss G: 0.5096 (0.5618) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.4031 (0.5222) Acc D Real: 98.284%
Loss D Fake: 0.9222 (0.8561) Acc D Fake: 0.000%
Loss D: 1.325
Loss G: 0.5100 (0.5612) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.4177 (0.5210) Acc D Real: 98.304%
Loss D Fake: 0.9214 (0.8569) Acc D Fake: 0.000%
Loss D: 1.339
Loss G: 0.5106 (0.5606) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3845 (0.5194) Acc D Real: 98.324%
Loss D Fake: 0.9208 (0.8576) Acc D Fake: 0.000%
Loss D: 1.305
Loss G: 0.5105 (0.5600) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.3701 (0.5176) Acc D Real: 98.344%
Loss D Fake: 0.9217 (0.8584) Acc D Fake: 0.000%
Loss D: 1.292
Loss G: 0.5091 (0.5594) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3263 (0.5154) Acc D Real: 98.363%
Loss D Fake: 0.9245 (0.8591) Acc D Fake: 0.000%
Loss D: 1.251
Loss G: 0.5070 (0.5588) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3495 (0.5135) Acc D Real: 98.381%
Loss D Fake: 0.9281 (0.8599) Acc D Fake: 0.000%
Loss D: 1.278
Loss G: 0.5048 (0.5582) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3143 (0.5113) Acc D Real: 98.399%
Loss D Fake: 0.9318 (0.8607) Acc D Fake: 0.000%
Loss D: 1.246
Loss G: 0.5022 (0.5576) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3416 (0.5094) Acc D Real: 98.417%
Loss D Fake: 0.9362 (0.8616) Acc D Fake: 0.000%
Loss D: 1.278
Loss G: 0.4988 (0.5569) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.2795 (0.5069) Acc D Real: 98.435%
Loss D Fake: 0.9420 (0.8624) Acc D Fake: 0.000%
Loss D: 1.222
Loss G: 0.4947 (0.5562) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3113 (0.5048) Acc D Real: 98.452%
Loss D Fake: 0.9491 (0.8634) Acc D Fake: 0.000%
Loss D: 1.260
Loss G: 0.4896 (0.5555) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2631 (0.5022) Acc D Real: 98.468%
Loss D Fake: 0.9569 (0.8644) Acc D Fake: 0.000%
Loss D: 1.220
Loss G: 0.4861 (0.5548) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.2399 (0.4994) Acc D Real: 98.485%
Loss D Fake: 0.9596 (0.8654) Acc D Fake: 0.000%
Loss D: 1.199
Loss G: 0.4863 (0.5540) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.2254 (0.4965) Acc D Real: 98.501%
Loss D Fake: 0.9572 (0.8664) Acc D Fake: 0.000%
Loss D: 1.183
Loss G: 0.4886 (0.5534) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2204 (0.4936) Acc D Real: 98.516%
Loss D Fake: 0.9573 (0.8673) Acc D Fake: 0.000%
Loss D: 1.178
Loss G: 0.4881 (0.5527) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.1917 (0.4905) Acc D Real: 98.531%
Loss D Fake: 1.0054 (0.8687) Acc D Fake: 0.000%
Loss D: 1.197
Loss G: 0.4937 (0.5521) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2132 (0.4877) Acc D Real: 98.546%
Loss D Fake: 0.9441 (0.8695) Acc D Fake: 0.000%
Loss D: 1.157
Loss G: 0.4902 (0.5514) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.1767 (0.4845) Acc D Real: 98.561%
Loss D Fake: 0.9867 (0.8707) Acc D Fake: 0.000%
Loss D: 1.163
Loss G: 0.4897 (0.5508) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.1921 (0.4816) Acc D Real: 98.576%
Loss D Fake: 0.9478 (0.8715) Acc D Fake: 0.000%
Loss D: 1.140
Loss G: 0.5107 (0.5504) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.1985 (0.4788) Acc D Real: 98.590%
Loss D Fake: 0.9107 (0.8719) Acc D Fake: 0.000%
Loss D: 1.109
Loss G: 0.5250 (0.5502) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.2101 (0.4762) Acc D Real: 98.603%
Loss D Fake: 0.8919 (0.8720) Acc D Fake: 0.000%
Loss D: 1.102
Loss G: 0.5343 (0.5500) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.1774 (0.4733) Acc D Real: 98.617%
Loss D Fake: 0.8799 (0.8721) Acc D Fake: 0.000%
Loss D: 1.057
Loss G: 0.5413 (0.5499) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2398 (0.4710) Acc D Real: 98.630%
Loss D Fake: 0.8710 (0.8721) Acc D Fake: 0.000%
Loss D: 1.111
Loss G: 0.5466 (0.5499) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.2274 (0.4687) Acc D Real: 98.643%
Loss D Fake: 0.8644 (0.8720) Acc D Fake: 0.000%
Loss D: 1.092
Loss G: 0.5506 (0.5499) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.2023 (0.4662) Acc D Real: 98.656%
Loss D Fake: 0.8595 (0.8719) Acc D Fake: 0.000%
Loss D: 1.062
Loss G: 0.5535 (0.5499) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2510 (0.4642) Acc D Real: 98.669%
Loss D Fake: 0.8562 (0.8718) Acc D Fake: 0.000%
Loss D: 1.107
Loss G: 0.5555 (0.5500) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2253 (0.4620) Acc D Real: 98.681%
Loss D Fake: 0.8541 (0.8716) Acc D Fake: 0.000%
Loss D: 1.079
Loss G: 0.5568 (0.5500) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.2036 (0.4596) Acc D Real: 98.693%
Loss D Fake: 0.8527 (0.8714) Acc D Fake: 0.000%
Loss D: 1.056
Loss G: 0.5576 (0.5501) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.2181 (0.4574) Acc D Real: 98.705%
Loss D Fake: 0.8520 (0.8713) Acc D Fake: 0.000%
Loss D: 1.070
Loss G: 0.5579 (0.5502) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.1910 (0.4550) Acc D Real: 98.717%
Loss D Fake: 0.8521 (0.8711) Acc D Fake: 0.000%
Loss D: 1.043
Loss G: 0.5578 (0.5502) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.2227 (0.4529) Acc D Real: 98.728%
Loss D Fake: 0.8525 (0.8709) Acc D Fake: 0.000%
Loss D: 1.075
Loss G: 0.5576 (0.5503) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.1896 (0.4506) Acc D Real: 98.739%
Loss D Fake: 0.8531 (0.8708) Acc D Fake: 0.000%
Loss D: 1.043
Loss G: 0.5572 (0.5504) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.2167 (0.4485) Acc D Real: 98.750%
Loss D Fake: 0.8540 (0.8706) Acc D Fake: 0.000%
Loss D: 1.071
Loss G: 0.5568 (0.5504) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.2025 (0.4464) Acc D Real: 98.761%
Loss D Fake: 0.8545 (0.8705) Acc D Fake: 0.000%
Loss D: 1.057
Loss G: 0.5569 (0.5505) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.1508 (0.4438) Acc D Real: 98.772%
Loss D Fake: 0.8540 (0.8703) Acc D Fake: 0.000%
Loss D: 1.005
Loss G: 0.5576 (0.5506) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.1599 (0.4414) Acc D Real: 98.782%
Loss D Fake: 0.8526 (0.8702) Acc D Fake: 0.000%
Loss D: 1.012
Loss G: 0.5589 (0.5506) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.1911 (0.4393) Acc D Real: 98.793%
Loss D Fake: 0.8504 (0.8700) Acc D Fake: 0.000%
Loss D: 1.042
Loss G: 0.5606 (0.5507) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.1779 (0.4371) Acc D Real: 98.803%
Loss D Fake: 0.8478 (0.8698) Acc D Fake: 0.000%
Loss D: 1.026
Loss G: 0.5622 (0.5508) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.1703 (0.4349) Acc D Real: 98.813%
Loss D Fake: 0.8455 (0.8696) Acc D Fake: 0.000%
Loss D: 1.016
Loss G: 0.5637 (0.5509) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.1990 (0.4329) Acc D Real: 98.823%
Loss D Fake: 0.8435 (0.8694) Acc D Fake: 0.000%
Loss D: 1.042
Loss G: 0.5650 (0.5510) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.1543 (0.4306) Acc D Real: 98.832%
Loss D Fake: 0.8416 (0.8692) Acc D Fake: 0.000%
Loss D: 0.996
Loss G: 0.5663 (0.5512) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.1693 (0.4285) Acc D Real: 98.842%
Loss D Fake: 0.8397 (0.8689) Acc D Fake: 0.000%
Loss D: 1.009
Loss G: 0.5676 (0.5513) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.1473 (0.4263) Acc D Real: 98.851%
Loss D Fake: 0.8378 (0.8687) Acc D Fake: 0.000%
Loss D: 0.985
Loss G: 0.5689 (0.5514) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.1338 (0.4239) Acc D Real: 98.860%
Loss D Fake: 0.8359 (0.8684) Acc D Fake: 0.000%
Loss D: 0.970
Loss G: 0.5703 (0.5516) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.1793 (0.4220) Acc D Real: 98.869%
Loss D Fake: 0.8341 (0.8682) Acc D Fake: 0.000%
Loss D: 1.013
Loss G: 0.5715 (0.5517) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.1430 (0.4198) Acc D Real: 98.878%
Loss D Fake: 0.8324 (0.8679) Acc D Fake: 0.000%
Loss D: 0.975
Loss G: 0.5729 (0.5519) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.1737 (0.4179) Acc D Real: 98.887%
Loss D Fake: 0.8307 (0.8676) Acc D Fake: 0.000%
Loss D: 1.004
Loss G: 0.5740 (0.5521) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.1369 (0.4157) Acc D Real: 98.896%
Loss D Fake: 0.8291 (0.8673) Acc D Fake: 0.000%
Loss D: 0.966
Loss G: 0.5752 (0.5523) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.1803 (0.4139) Acc D Real: 98.904%
Loss D Fake: 0.8276 (0.8670) Acc D Fake: 0.000%
Loss D: 1.008
Loss G: 0.5763 (0.5524) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.1845 (0.4121) Acc D Real: 98.913%
Loss D Fake: 0.8263 (0.8667) Acc D Fake: 0.000%
Loss D: 1.011
Loss G: 0.5773 (0.5526) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.1876 (0.4104) Acc D Real: 98.921%
Loss D Fake: 0.8251 (0.8664) Acc D Fake: 0.000%
Loss D: 1.013
Loss G: 0.5780 (0.5528) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.1658 (0.4086) Acc D Real: 98.929%
Loss D Fake: 0.8242 (0.8660) Acc D Fake: 0.000%
Loss D: 0.990
Loss G: 0.5789 (0.5530) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.1310 (0.4065) Acc D Real: 98.937%
Loss D Fake: 0.8229 (0.8657) Acc D Fake: 0.000%
Loss D: 0.954
Loss G: 0.5800 (0.5532) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.1282 (0.4044) Acc D Real: 98.945%
Loss D Fake: 0.8215 (0.8654) Acc D Fake: 0.000%
Loss D: 0.950
Loss G: 0.5812 (0.5534) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.1794 (0.4028) Acc D Real: 98.953%
Loss D Fake: 0.8199 (0.8651) Acc D Fake: 0.000%
Loss D: 0.999
Loss G: 0.5823 (0.5536) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.1582 (0.4010) Acc D Real: 98.960%
Loss D Fake: 0.8186 (0.8647) Acc D Fake: 0.000%
Loss D: 0.977
Loss G: 0.5833 (0.5539) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.1451 (0.3991) Acc D Real: 98.968%
Loss D Fake: 0.8174 (0.8644) Acc D Fake: 0.000%
Loss D: 0.962
Loss G: 0.5841 (0.5541) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.1427 (0.3973) Acc D Real: 98.975%
Loss D Fake: 0.8164 (0.8640) Acc D Fake: 0.000%
Loss D: 0.959
Loss G: 0.5851 (0.5543) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.1370 (0.3954) Acc D Real: 98.982%
Loss D Fake: 0.8150 (0.8637) Acc D Fake: 0.000%
Loss D: 0.952
Loss G: 0.5864 (0.5545) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.1346 (0.3936) Acc D Real: 98.989%
Loss D Fake: 0.8133 (0.8633) Acc D Fake: 0.000%
Loss D: 0.948
Loss G: 0.5878 (0.5548) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.1661 (0.3920) Acc D Real: 98.996%
Loss D Fake: 0.8115 (0.8630) Acc D Fake: 0.000%
Loss D: 0.978
Loss G: 0.5892 (0.5550) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.1695 (0.3904) Acc D Real: 99.003%
Loss D Fake: 0.8098 (0.8626) Acc D Fake: 0.000%
Loss D: 0.979
Loss G: 0.5906 (0.5553) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.1417 (0.3887) Acc D Real: 99.010%
Loss D Fake: 0.8078 (0.8622) Acc D Fake: 0.000%
Loss D: 0.949
Loss G: 0.5922 (0.5555) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2106 (0.3875) Acc D Real: 99.017%
Loss D Fake: 0.8059 (0.8618) Acc D Fake: 0.000%
Loss D: 1.017
Loss G: 0.5934 (0.5558) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.1769 (0.3860) Acc D Real: 99.023%
Loss D Fake: 0.8051 (0.8614) Acc D Fake: 0.000%
Loss D: 0.982
Loss G: 0.5936 (0.5560) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.1577 (0.3845) Acc D Real: 99.029%
Loss D Fake: 0.8055 (0.8610) Acc D Fake: 0.000%
Loss D: 0.963
Loss G: 0.5935 (0.5563) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.1374 (0.3828) Acc D Real: 99.035%
Loss D Fake: 0.8057 (0.8607) Acc D Fake: 0.000%
Loss D: 0.943
Loss G: 0.5938 (0.5565) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.1525 (0.3813) Acc D Real: 99.041%
Loss D Fake: 0.8051 (0.8603) Acc D Fake: 0.000%
Loss D: 0.958
Loss G: 0.5949 (0.5568) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.1339 (0.3796) Acc D Real: 99.047%
Loss D Fake: 0.8041 (0.8599) Acc D Fake: 0.000%
Loss D: 0.938
Loss G: 0.5952 (0.5571) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.1374 (0.3780) Acc D Real: 99.053%
Loss D Fake: 0.8042 (0.8596) Acc D Fake: 0.000%
Loss D: 0.942
Loss G: 0.5959 (0.5573) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.1316 (0.3764) Acc D Real: 99.059%
Loss D Fake: 0.8029 (0.8592) Acc D Fake: 0.000%
Loss D: 0.935
Loss G: 0.5974 (0.5576) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.1661 (0.3750) Acc D Real: 99.065%
Loss D Fake: 0.8008 (0.8588) Acc D Fake: 0.000%
Loss D: 0.967
Loss G: 0.5993 (0.5578) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.1280 (0.3734) Acc D Real: 99.071%
Loss D Fake: 0.7980 (0.8584) Acc D Fake: 0.000%
Loss D: 0.926
Loss G: 0.6018 (0.5581) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.1362 (0.3719) Acc D Real: 99.076%
Loss D Fake: 0.7946 (0.8580) Acc D Fake: 0.000%
Loss D: 0.931
Loss G: 0.6044 (0.5584) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.1484 (0.3705) Acc D Real: 99.082%
Loss D Fake: 0.7913 (0.8576) Acc D Fake: 0.000%
Loss D: 0.940
Loss G: 0.6068 (0.5587) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.1253 (0.3689) Acc D Real: 99.087%
Loss D Fake: 0.7883 (0.8571) Acc D Fake: 0.000%
Loss D: 0.914
Loss G: 0.6090 (0.5591) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.1600 (0.3676) Acc D Real: 99.092%
Loss D Fake: 0.7856 (0.8567) Acc D Fake: 0.000%
Loss D: 0.946
Loss G: 0.6112 (0.5594) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.1795 (0.3664) Acc D Real: 99.096%
Loss D Fake: 0.7831 (0.8562) Acc D Fake: 0.000%
Loss D: 0.963
Loss G: 0.6131 (0.5597) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.1381 (0.3650) Acc D Real: 99.101%
Loss D Fake: 0.7808 (0.8557) Acc D Fake: 0.000%
Loss D: 0.919
Loss G: 0.6150 (0.5601) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.1536 (0.3636) Acc D Real: 99.105%
Loss D Fake: 0.7791 (0.8553) Acc D Fake: 0.000%
Loss D: 0.933
Loss G: 0.6155 (0.5604) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.1613 (0.3624) Acc D Real: 99.109%
Loss D Fake: 0.7794 (0.8548) Acc D Fake: 0.000%
Loss D: 0.941
Loss G: 0.6158 (0.5608) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.1665 (0.3612) Acc D Real: 99.113%
Loss D Fake: 0.7793 (0.8543) Acc D Fake: 0.000%
Loss D: 0.946
Loss G: 0.6159 (0.5611) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.1523 (0.3599) Acc D Real: 99.117%
Loss D Fake: 0.7799 (0.8539) Acc D Fake: 0.000%
Loss D: 0.932
Loss G: 0.6159 (0.5614) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1647 (0.3587) Acc D Real: 99.122%
Loss D Fake: 0.7810 (0.8534) Acc D Fake: 0.000%
Loss D: 0.946
Loss G: 0.6153 (0.5618) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.1434 (0.3574) Acc D Real: 99.126%
Loss D Fake: 0.7827 (0.8530) Acc D Fake: 0.000%
Loss D: 0.926
Loss G: 0.6157 (0.5621) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.1624 (0.3563) Acc D Real: 99.131%
Loss D Fake: 0.7821 (0.8526) Acc D Fake: 0.000%
Loss D: 0.944
Loss G: 0.6171 (0.5624) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.1386 (0.3550) Acc D Real: 99.134%
Loss D Fake: 0.7803 (0.8522) Acc D Fake: 0.000%
Loss D: 0.919
Loss G: 0.6194 (0.5628) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.1555 (0.3538) Acc D Real: 99.138%
Loss D Fake: 0.7768 (0.8517) Acc D Fake: 0.000%
Loss D: 0.932
Loss G: 0.6228 (0.5631) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.1565 (0.3526) Acc D Real: 99.142%
Loss D Fake: 0.7723 (0.8512) Acc D Fake: 0.000%
Loss D: 0.929
Loss G: 0.6259 (0.5635) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.1579 (0.3515) Acc D Real: 99.146%
Loss D Fake: 0.7686 (0.8508) Acc D Fake: 0.000%
Loss D: 0.927
Loss G: 0.6284 (0.5639) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.1620 (0.3504) Acc D Real: 99.150%
Loss D Fake: 0.7659 (0.8503) Acc D Fake: 0.000%
Loss D: 0.928
Loss G: 0.6307 (0.5642) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.1347 (0.3492) Acc D Real: 99.153%
Loss D Fake: 0.7633 (0.8498) Acc D Fake: 0.000%
Loss D: 0.898
Loss G: 0.6329 (0.5646) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1212 (0.3478) Acc D Real: 99.155%
Loss D Fake: 0.7611 (0.8493) Acc D Fake: 0.000%
Loss D: 0.882
Loss G: 0.6345 (0.5650) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.1427 (0.3467) Acc D Real: 99.159%
Loss D Fake: 0.7598 (0.8487) Acc D Fake: 0.000%
Loss D: 0.903
Loss G: 0.6360 (0.5655) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.1808 (0.3457) Acc D Real: 99.163%
Loss D Fake: 0.7591 (0.8482) Acc D Fake: 0.000%
Loss D: 0.940
Loss G: 0.6363 (0.5659) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.1562 (0.3447) Acc D Real: 99.165%
Loss D Fake: 0.7608 (0.8477) Acc D Fake: 0.000%
Loss D: 0.917
Loss G: 0.6365 (0.5663) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.1308 (0.3435) Acc D Real: 99.169%
Loss D Fake: 0.7624 (0.8473) Acc D Fake: 0.000%
Loss D: 0.893
Loss G: 0.6369 (0.5666) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.1847 (0.3426) Acc D Real: 99.172%
Loss D Fake: 0.7667 (0.8468) Acc D Fake: 0.000%
Loss D: 0.951
Loss G: 0.6350 (0.5670) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.1383 (0.3414) Acc D Real: 99.175%
Loss D Fake: 0.7812 (0.8464) Acc D Fake: 0.000%
Loss D: 0.919
Loss G: 0.6346 (0.5674) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.1579 (0.3404) Acc D Real: 99.178%
Loss D Fake: 0.7829 (0.8461) Acc D Fake: 0.000%
Loss D: 0.941
Loss G: 0.6390 (0.5678) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.1651 (0.3395) Acc D Real: 99.182%
Loss D Fake: 0.7674 (0.8457) Acc D Fake: 0.000%
Loss D: 0.932
Loss G: 0.6453 (0.5682) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.1351 (0.3383) Acc D Real: 99.184%
Loss D Fake: 0.7552 (0.8452) Acc D Fake: 0.000%
Loss D: 0.890
Loss G: 0.6503 (0.5687) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.1167 (0.3371) Acc D Real: 99.185%
Loss D Fake: 0.7486 (0.8446) Acc D Fake: 0.000%
Loss D: 0.865
Loss G: 0.6536 (0.5691) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.1944 (0.3364) Acc D Real: 99.189%
Loss D Fake: 0.7464 (0.8441) Acc D Fake: 0.000%
Loss D: 0.941
Loss G: 0.6556 (0.5696) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1416 (0.3353) Acc D Real: 99.192%
Loss D Fake: 0.7483 (0.8436) Acc D Fake: 0.000%
Loss D: 0.890
Loss G: 0.6560 (0.5701) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.1693 (0.3344) Acc D Real: 99.195%
Loss D Fake: 0.7632 (0.8432) Acc D Fake: 0.000%
Loss D: 0.933
Loss G: 0.6507 (0.5705) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.1964 (0.3337) Acc D Real: 99.197%
Loss D Fake: 0.8211 (0.8431) Acc D Fake: 0.000%
Loss D: 1.018
Loss G: 0.6529 (0.5709) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.1268 (0.3326) Acc D Real: 99.200%
Loss D Fake: 0.8370 (0.8430) Acc D Fake: 0.000%
Loss D: 0.964
Loss G: 0.6407 (0.5713) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.1490 (0.3316) Acc D Real: 99.201%
Loss D Fake: 0.8552 (0.8431) Acc D Fake: 0.000%
Loss D: 1.004
Loss G: 0.6579 (0.5718) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.1400 (0.3306) Acc D Real: 99.203%
Loss D Fake: 0.7254 (0.8425) Acc D Fake: 0.000%
Loss D: 0.865
Loss G: 0.6614 (0.5722) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.1741 (0.3298) Acc D Real: 99.202%
Loss D Fake: 1.5518 (0.8462) Acc D Fake: 0.000%
Loss D: 1.726
Loss G: 0.6729 (0.5728) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.1667 (0.3290) Acc D Real: 99.200%
Loss D Fake: 0.7213 (0.8455) Acc D Fake: 0.000%
Loss D: 0.888
Loss G: 0.6554 (0.5732) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.1691 (0.3281) Acc D Real: 99.193%
Loss D Fake: 0.7846 (0.8452) Acc D Fake: 0.000%
Loss D: 0.954
Loss G: 0.6003 (0.5733) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.1676 (0.3273) Acc D Real: 99.191%
Loss D Fake: 1.0120 (0.8461) Acc D Fake: 0.000%
Loss D: 1.180
Loss G: 0.6165 (0.5735) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.1707 (0.3265) Acc D Real: 99.184%
Loss D Fake: 0.7509 (0.8456) Acc D Fake: 0.000%
Loss D: 0.922
Loss G: 0.6612 (0.5740) Acc G: 100.000%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.1642 (0.3257) Acc D Real: 99.174%
Loss D Fake: 0.7165 (0.8449) Acc D Fake: 0.000%
Loss D: 0.881
Loss G: 0.6796 (0.5745) Acc G: 99.958%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.1789 (0.3250) Acc D Real: 99.156%
Loss D Fake: 0.7013 (0.8442) Acc D Fake: 0.084%
Loss D: 0.880
Loss G: 0.6910 (0.5751) Acc G: 99.747%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3253 (0.3250) Acc D Real: 99.070%
Loss D Fake: 0.6914 (0.8434) Acc D Fake: 0.494%
Loss D: 1.017
Loss G: 0.6991 (0.5757) Acc G: 99.330%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2187 (0.3244) Acc D Real: 99.033%
Loss D Fake: 0.6845 (0.8426) Acc D Fake: 0.908%
Loss D: 0.903
Loss G: 0.7051 (0.5764) Acc G: 98.917%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2044 (0.3238) Acc D Real: 98.995%
Loss D Fake: 0.6793 (0.8418) Acc D Fake: 1.318%
Loss D: 0.884
Loss G: 0.7099 (0.5771) Acc G: 98.499%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.2441 (0.3234) Acc D Real: 98.933%
Loss D Fake: 0.6751 (0.8410) Acc D Fake: 1.733%
Loss D: 0.919
Loss G: 0.7138 (0.5777) Acc G: 98.086%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2033 (0.3228) Acc D Real: 98.896%
Loss D Fake: 0.6716 (0.8402) Acc D Fake: 2.143%
Loss D: 0.875
Loss G: 0.7172 (0.5784) Acc G: 97.677%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.2208 (0.3223) Acc D Real: 98.862%
Loss D Fake: 0.6686 (0.8393) Acc D Fake: 2.549%
Loss D: 0.889
Loss G: 0.7202 (0.5791) Acc G: 97.263%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2125 (0.3218) Acc D Real: 98.813%
Loss D Fake: 0.6660 (0.8385) Acc D Fake: 2.959%
Loss D: 0.879
Loss G: 0.7227 (0.5798) Acc G: 96.854%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3122 (0.3218) Acc D Real: 98.715%
Loss D Fake: 0.6638 (0.8376) Acc D Fake: 3.366%
Loss D: 0.976
Loss G: 0.7249 (0.5805) Acc G: 96.448%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.3068 (0.3217) Acc D Real: 98.619%
Loss D Fake: 0.6620 (0.8368) Acc D Fake: 3.768%
Loss D: 0.969
Loss G: 0.7267 (0.5812) Acc G: 96.047%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2742 (0.3215) Acc D Real: 98.539%
Loss D Fake: 0.6605 (0.8359) Acc D Fake: 4.167%
Loss D: 0.935
Loss G: 0.7281 (0.5819) Acc G: 95.649%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.3452 (0.3216) Acc D Real: 98.433%
Loss D Fake: 0.6593 (0.8351) Acc D Fake: 4.561%
Loss D: 1.005
Loss G: 0.7292 (0.5826) Acc G: 95.255%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3192 (0.3216) Acc D Real: 98.335%
Loss D Fake: 0.6584 (0.8342) Acc D Fake: 4.952%
Loss D: 0.978
Loss G: 0.7302 (0.5833) Acc G: 94.865%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3003 (0.3215) Acc D Real: 98.244%
Loss D Fake: 0.6575 (0.8334) Acc D Fake: 5.340%
Loss D: 0.958
Loss G: 0.7311 (0.5840) Acc G: 94.479%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3113 (0.3214) Acc D Real: 98.162%
Loss D Fake: 0.6568 (0.8326) Acc D Fake: 5.723%
Loss D: 0.968
Loss G: 0.7318 (0.5847) Acc G: 94.096%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2714 (0.3212) Acc D Real: 98.089%
Loss D Fake: 0.6562 (0.8317) Acc D Fake: 6.103%
Loss D: 0.928
Loss G: 0.7323 (0.5854) Acc G: 93.717%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3413 (0.3213) Acc D Real: 97.986%
Loss D Fake: 0.6557 (0.8309) Acc D Fake: 6.480%
Loss D: 0.997
Loss G: 0.7328 (0.5861) Acc G: 93.341%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3206 (0.3213) Acc D Real: 97.896%
Loss D Fake: 0.6554 (0.8301) Acc D Fake: 6.853%
Loss D: 0.976
Loss G: 0.7331 (0.5868) Acc G: 92.969%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2204 (0.3208) Acc D Real: 97.846%
Loss D Fake: 0.6552 (0.8293) Acc D Fake: 7.222%
Loss D: 0.876
Loss G: 0.7333 (0.5875) Acc G: 92.600%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3598 (0.3210) Acc D Real: 97.737%
Loss D Fake: 0.6550 (0.8285) Acc D Fake: 7.588%
Loss D: 1.015
Loss G: 0.7335 (0.5881) Acc G: 92.235%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2751 (0.3208) Acc D Real: 97.671%
Loss D Fake: 0.6549 (0.8277) Acc D Fake: 7.951%
Loss D: 0.930
Loss G: 0.7336 (0.5888) Acc G: 91.873%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2724 (0.3206) Acc D Real: 97.607%
Loss D Fake: 0.6547 (0.8269) Acc D Fake: 8.311%
Loss D: 0.927
Loss G: 0.7339 (0.5895) Acc G: 91.514%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2603 (0.3203) Acc D Real: 97.544%
Loss D Fake: 0.6544 (0.8261) Acc D Fake: 8.667%
Loss D: 0.915
Loss G: 0.7343 (0.5901) Acc G: 91.159%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2045 (0.3198) Acc D Real: 97.512%
Loss D Fake: 0.6539 (0.8253) Acc D Fake: 9.020%
Loss D: 0.858
Loss G: 0.7348 (0.5908) Acc G: 90.807%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.1949 (0.3192) Acc D Real: 97.478%
Loss D Fake: 0.6534 (0.8246) Acc D Fake: 9.369%
Loss D: 0.848
Loss G: 0.7355 (0.5914) Acc G: 90.450%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.1978 (0.3186) Acc D Real: 97.447%
Loss D Fake: 0.6528 (0.8238) Acc D Fake: 9.723%
Loss D: 0.851
Loss G: 0.7363 (0.5921) Acc G: 90.097%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2491 (0.3183) Acc D Real: 97.395%
Loss D Fake: 0.6520 (0.8230) Acc D Fake: 10.074%
Loss D: 0.901
Loss G: 0.7370 (0.5927) Acc G: 89.747%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.2723 (0.3181) Acc D Real: 97.318%
Loss D Fake: 0.6514 (0.8223) Acc D Fake: 10.422%
Loss D: 0.924
Loss G: 0.7377 (0.5934) Acc G: 89.400%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.1748 (0.3175) Acc D Real: 97.314%
Loss D Fake: 0.6508 (0.8215) Acc D Fake: 10.509%
Loss D: 0.826
Loss G: 0.7384 (0.5940) Acc G: 89.314%
LR: 2.000e-04
Best Loss 100000000000.000 to 0.867
Epoch: 2/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2926 (0.2538) Acc D Real: 85.651%
Loss D Fake: 0.6511 (0.6508) Acc D Fake: 88.333%
Loss D: 0.944
Loss G: 0.7374 (0.7377) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.2771 (0.2616) Acc D Real: 84.705%
Loss D Fake: 0.6516 (0.6511) Acc D Fake: 88.333%
Loss D: 0.929
Loss G: 0.7369 (0.7374) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2535 (0.2595) Acc D Real: 84.961%
Loss D Fake: 0.6520 (0.6513) Acc D Fake: 88.333%
Loss D: 0.906
Loss G: 0.7366 (0.7372) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2350 (0.2546) Acc D Real: 85.438%
Loss D Fake: 0.6522 (0.6515) Acc D Fake: 88.333%
Loss D: 0.887
Loss G: 0.7364 (0.7370) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2772 (0.2584) Acc D Real: 84.557%
Loss D Fake: 0.6523 (0.6516) Acc D Fake: 88.333%
Loss D: 0.930
Loss G: 0.7363 (0.7369) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.1999 (0.2500) Acc D Real: 85.952%
Loss D Fake: 0.6524 (0.6517) Acc D Fake: 88.333%
Loss D: 0.852
Loss G: 0.7364 (0.7369) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.2225 (0.2466) Acc D Real: 85.983%
Loss D Fake: 0.6522 (0.6518) Acc D Fake: 88.333%
Loss D: 0.875
Loss G: 0.7367 (0.7368) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2235 (0.2440) Acc D Real: 85.943%
Loss D Fake: 0.6519 (0.6518) Acc D Fake: 88.333%
Loss D: 0.875
Loss G: 0.7371 (0.7369) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2122 (0.2409) Acc D Real: 86.458%
Loss D Fake: 0.6514 (0.6518) Acc D Fake: 88.333%
Loss D: 0.864
Loss G: 0.7378 (0.7370) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.2122 (0.2383) Acc D Real: 86.591%
Loss D Fake: 0.6506 (0.6517) Acc D Fake: 88.333%
Loss D: 0.863
Loss G: 0.7387 (0.7371) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2442 (0.2388) Acc D Real: 86.458%
Loss D Fake: 0.6497 (0.6515) Acc D Fake: 88.333%
Loss D: 0.894
Loss G: 0.7398 (0.7373) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2120 (0.2367) Acc D Real: 86.514%
Loss D Fake: 0.6487 (0.6513) Acc D Fake: 88.333%
Loss D: 0.861
Loss G: 0.7410 (0.7376) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2442 (0.2372) Acc D Real: 86.555%
Loss D Fake: 0.6476 (0.6510) Acc D Fake: 88.333%
Loss D: 0.892
Loss G: 0.7421 (0.7379) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2349 (0.2371) Acc D Real: 86.611%
Loss D Fake: 0.6467 (0.6507) Acc D Fake: 88.333%
Loss D: 0.882
Loss G: 0.7430 (0.7383) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2349 (0.2369) Acc D Real: 86.725%
Loss D Fake: 0.6460 (0.6505) Acc D Fake: 88.333%
Loss D: 0.881
Loss G: 0.7437 (0.7386) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.2146 (0.2356) Acc D Real: 86.777%
Loss D Fake: 0.6455 (0.6502) Acc D Fake: 88.333%
Loss D: 0.860
Loss G: 0.7443 (0.7390) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2191 (0.2347) Acc D Real: 86.863%
Loss D Fake: 0.6449 (0.6499) Acc D Fake: 88.333%
Loss D: 0.864
Loss G: 0.7450 (0.7393) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.2193 (0.2339) Acc D Real: 87.050%
Loss D Fake: 0.6445 (0.6496) Acc D Fake: 88.333%
Loss D: 0.864
Loss G: 0.7451 (0.7396) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.3084 (0.2376) Acc D Real: 86.578%
Loss D Fake: 0.6447 (0.6493) Acc D Fake: 88.333%
Loss D: 0.953
Loss G: 0.7448 (0.7399) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.2135 (0.2365) Acc D Real: 86.709%
Loss D Fake: 0.6450 (0.6491) Acc D Fake: 88.333%
Loss D: 0.858
Loss G: 0.7444 (0.7401) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1803 (0.2339) Acc D Real: 86.927%
Loss D Fake: 0.6452 (0.6490) Acc D Fake: 88.333%
Loss D: 0.826
Loss G: 0.7443 (0.7403) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1791 (0.2315) Acc D Real: 87.144%
Loss D Fake: 0.6451 (0.6488) Acc D Fake: 88.333%
Loss D: 0.824
Loss G: 0.7446 (0.7405) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.2599 (0.2327) Acc D Real: 86.973%
Loss D Fake: 0.6447 (0.6486) Acc D Fake: 88.333%
Loss D: 0.905
Loss G: 0.7452 (0.7406) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2050 (0.2316) Acc D Real: 87.102%
Loss D Fake: 0.6441 (0.6484) Acc D Fake: 88.333%
Loss D: 0.849
Loss G: 0.7460 (0.7409) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2363 (0.2318) Acc D Real: 87.057%
Loss D Fake: 0.6434 (0.6482) Acc D Fake: 88.333%
Loss D: 0.880
Loss G: 0.7468 (0.7411) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.2998 (0.2343) Acc D Real: 86.763%
Loss D Fake: 0.6426 (0.6480) Acc D Fake: 88.333%
Loss D: 0.942
Loss G: 0.7477 (0.7413) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.2198 (0.2338) Acc D Real: 86.760%
Loss D Fake: 0.6418 (0.6478) Acc D Fake: 88.333%
Loss D: 0.862
Loss G: 0.7487 (0.7416) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.2541 (0.2345) Acc D Real: 86.618%
Loss D Fake: 0.6408 (0.6476) Acc D Fake: 88.333%
Loss D: 0.895
Loss G: 0.7498 (0.7419) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.2256 (0.2342) Acc D Real: 86.547%
Loss D Fake: 0.6400 (0.6473) Acc D Fake: 88.333%
Loss D: 0.866
Loss G: 0.7505 (0.7422) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2065 (0.2333) Acc D Real: 86.662%
Loss D Fake: 0.6395 (0.6471) Acc D Fake: 88.333%
Loss D: 0.846
Loss G: 0.7512 (0.7425) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.2132 (0.2327) Acc D Real: 86.681%
Loss D Fake: 0.6388 (0.6468) Acc D Fake: 88.333%
Loss D: 0.852
Loss G: 0.7520 (0.7428) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2639 (0.2336) Acc D Real: 86.679%
Loss D Fake: 0.6389 (0.6466) Acc D Fake: 88.333%
Loss D: 0.903
Loss G: 0.7498 (0.7430) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.2048 (0.2328) Acc D Real: 86.711%
Loss D Fake: 0.6417 (0.6464) Acc D Fake: 88.333%
Loss D: 0.846
Loss G: 0.7470 (0.7431) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.1315 (0.2299) Acc D Real: 86.933%
Loss D Fake: 0.6440 (0.6464) Acc D Fake: 88.333%
Loss D: 0.775
Loss G: 0.7448 (0.7431) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.2388 (0.2301) Acc D Real: 86.889%
Loss D Fake: 0.6461 (0.6463) Acc D Fake: 88.333%
Loss D: 0.885
Loss G: 0.7416 (0.7431) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2885 (0.2317) Acc D Real: 86.700%
Loss D Fake: 0.6492 (0.6464) Acc D Fake: 88.333%
Loss D: 0.938
Loss G: 0.7385 (0.7430) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.1693 (0.2301) Acc D Real: 86.852%
Loss D Fake: 0.6516 (0.6466) Acc D Fake: 88.333%
Loss D: 0.821
Loss G: 0.7361 (0.7428) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.1853 (0.2289) Acc D Real: 86.981%
Loss D Fake: 0.6538 (0.6467) Acc D Fake: 88.332%
Loss D: 0.839
Loss G: 0.7337 (0.7426) Acc G: 11.709%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2015 (0.2282) Acc D Real: 87.049%
Loss D Fake: 0.6557 (0.6470) Acc D Fake: 88.290%
Loss D: 0.857
Loss G: 0.7323 (0.7423) Acc G: 11.750%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.1785 (0.2270) Acc D Real: 87.139%
Loss D Fake: 0.6563 (0.6472) Acc D Fake: 88.251%
Loss D: 0.835
Loss G: 0.7323 (0.7421) Acc G: 11.789%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.1920 (0.2262) Acc D Real: 87.250%
Loss D Fake: 0.6559 (0.6474) Acc D Fake: 88.213%
Loss D: 0.848
Loss G: 0.7332 (0.7418) Acc G: 11.786%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2417 (0.2265) Acc D Real: 87.146%
Loss D Fake: 0.6546 (0.6476) Acc D Fake: 88.216%
Loss D: 0.896
Loss G: 0.7352 (0.7417) Acc G: 11.783%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.1784 (0.2254) Acc D Real: 87.251%
Loss D Fake: 0.6522 (0.6477) Acc D Fake: 88.219%
Loss D: 0.831
Loss G: 0.7385 (0.7416) Acc G: 11.780%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.2381 (0.2257) Acc D Real: 87.194%
Loss D Fake: 0.6490 (0.6477) Acc D Fake: 88.221%
Loss D: 0.887
Loss G: 0.7420 (0.7416) Acc G: 11.778%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.2121 (0.2254) Acc D Real: 87.198%
Loss D Fake: 0.6458 (0.6477) Acc D Fake: 88.224%
Loss D: 0.858
Loss G: 0.7455 (0.7417) Acc G: 11.775%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.1389 (0.2236) Acc D Real: 87.394%
Loss D Fake: 0.6428 (0.6476) Acc D Fake: 88.226%
Loss D: 0.782
Loss G: 0.7486 (0.7419) Acc G: 11.773%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.1766 (0.2226) Acc D Real: 87.472%
Loss D Fake: 0.6403 (0.6474) Acc D Fake: 88.228%
Loss D: 0.817
Loss G: 0.7512 (0.7421) Acc G: 11.771%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.1698 (0.2215) Acc D Real: 87.549%
Loss D Fake: 0.6381 (0.6472) Acc D Fake: 88.230%
Loss D: 0.808
Loss G: 0.7539 (0.7423) Acc G: 11.769%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.1407 (0.2199) Acc D Real: 87.710%
Loss D Fake: 0.6357 (0.6470) Acc D Fake: 88.232%
Loss D: 0.776
Loss G: 0.7566 (0.7426) Acc G: 11.767%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.2039 (0.2196) Acc D Real: 87.750%
Loss D Fake: 0.6333 (0.6467) Acc D Fake: 88.234%
Loss D: 0.837
Loss G: 0.7594 (0.7429) Acc G: 11.765%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.1567 (0.2184) Acc D Real: 87.883%
Loss D Fake: 0.6308 (0.6464) Acc D Fake: 88.236%
Loss D: 0.788
Loss G: 0.7624 (0.7433) Acc G: 11.763%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.1540 (0.2172) Acc D Real: 87.962%
Loss D Fake: 0.6283 (0.6461) Acc D Fake: 88.238%
Loss D: 0.782
Loss G: 0.7652 (0.7437) Acc G: 11.761%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.1711 (0.2163) Acc D Real: 88.035%
Loss D Fake: 0.6259 (0.6457) Acc D Fake: 88.240%
Loss D: 0.797
Loss G: 0.7679 (0.7441) Acc G: 11.728%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.1849 (0.2158) Acc D Real: 88.079%
Loss D Fake: 0.6238 (0.6453) Acc D Fake: 88.272%
Loss D: 0.809
Loss G: 0.7701 (0.7446) Acc G: 11.697%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.1539 (0.2147) Acc D Real: 88.186%
Loss D Fake: 0.6220 (0.6449) Acc D Fake: 88.303%
Loss D: 0.776
Loss G: 0.7722 (0.7451) Acc G: 11.667%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2146 (0.2147) Acc D Real: 88.151%
Loss D Fake: 0.6203 (0.6445) Acc D Fake: 88.332%
Loss D: 0.835
Loss G: 0.7742 (0.7456) Acc G: 11.637%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.1437 (0.2134) Acc D Real: 88.270%
Loss D Fake: 0.6186 (0.6440) Acc D Fake: 88.361%
Loss D: 0.762
Loss G: 0.7763 (0.7462) Acc G: 11.609%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.1755 (0.2128) Acc D Real: 88.290%
Loss D Fake: 0.6169 (0.6436) Acc D Fake: 88.389%
Loss D: 0.792
Loss G: 0.7782 (0.7467) Acc G: 11.582%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.1427 (0.2116) Acc D Real: 88.362%
Loss D Fake: 0.6153 (0.6431) Acc D Fake: 88.416%
Loss D: 0.758
Loss G: 0.7801 (0.7473) Acc G: 11.556%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.2078 (0.2116) Acc D Real: 88.353%
Loss D Fake: 0.6143 (0.6426) Acc D Fake: 88.442%
Loss D: 0.822
Loss G: 0.7802 (0.7478) Acc G: 11.530%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.1566 (0.2107) Acc D Real: 88.438%
Loss D Fake: 0.6149 (0.6422) Acc D Fake: 88.467%
Loss D: 0.772
Loss G: 0.7791 (0.7483) Acc G: 11.505%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.1563 (0.2098) Acc D Real: 88.508%
Loss D Fake: 0.6158 (0.6417) Acc D Fake: 88.491%
Loss D: 0.772
Loss G: 0.7784 (0.7488) Acc G: 11.481%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.1983 (0.2096) Acc D Real: 88.512%
Loss D Fake: 0.6162 (0.6413) Acc D Fake: 88.515%
Loss D: 0.814
Loss G: 0.7781 (0.7492) Acc G: 11.458%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.1470 (0.2087) Acc D Real: 88.588%
Loss D Fake: 0.6161 (0.6410) Acc D Fake: 88.538%
Loss D: 0.763
Loss G: 0.7786 (0.7497) Acc G: 11.436%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.1453 (0.2077) Acc D Real: 88.678%
Loss D Fake: 0.6155 (0.6406) Acc D Fake: 88.560%
Loss D: 0.761
Loss G: 0.7795 (0.7501) Acc G: 11.414%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.2069 (0.2077) Acc D Real: 88.657%
Loss D Fake: 0.6147 (0.6402) Acc D Fake: 88.581%
Loss D: 0.822
Loss G: 0.7807 (0.7506) Acc G: 11.393%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.1568 (0.2069) Acc D Real: 88.729%
Loss D Fake: 0.6136 (0.6398) Acc D Fake: 88.602%
Loss D: 0.770
Loss G: 0.7817 (0.7511) Acc G: 11.373%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.1667 (0.2064) Acc D Real: 88.764%
Loss D Fake: 0.6128 (0.6394) Acc D Fake: 88.622%
Loss D: 0.779
Loss G: 0.7830 (0.7515) Acc G: 11.353%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.2074 (0.2064) Acc D Real: 88.766%
Loss D Fake: 0.6116 (0.6390) Acc D Fake: 88.642%
Loss D: 0.819
Loss G: 0.7847 (0.7520) Acc G: 11.333%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2097 (0.2064) Acc D Real: 88.758%
Loss D Fake: 0.6106 (0.6386) Acc D Fake: 88.661%
Loss D: 0.820
Loss G: 0.7850 (0.7525) Acc G: 11.315%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.1812 (0.2061) Acc D Real: 88.787%
Loss D Fake: 0.6105 (0.6382) Acc D Fake: 88.680%
Loss D: 0.792
Loss G: 0.7854 (0.7529) Acc G: 11.296%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.1713 (0.2056) Acc D Real: 88.834%
Loss D Fake: 0.6099 (0.6378) Acc D Fake: 88.698%
Loss D: 0.781
Loss G: 0.7865 (0.7534) Acc G: 11.279%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.1726 (0.2051) Acc D Real: 88.880%
Loss D Fake: 0.6088 (0.6374) Acc D Fake: 88.716%
Loss D: 0.781
Loss G: 0.7881 (0.7538) Acc G: 11.261%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.1746 (0.2047) Acc D Real: 88.900%
Loss D Fake: 0.6074 (0.6370) Acc D Fake: 88.733%
Loss D: 0.782
Loss G: 0.7902 (0.7543) Acc G: 11.244%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.2005 (0.2047) Acc D Real: 88.884%
Loss D Fake: 0.6055 (0.6366) Acc D Fake: 88.749%
Loss D: 0.806
Loss G: 0.7925 (0.7548) Acc G: 11.228%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2111 (0.2048) Acc D Real: 88.872%
Loss D Fake: 0.6055 (0.6362) Acc D Fake: 88.766%
Loss D: 0.817
Loss G: 0.7877 (0.7553) Acc G: 11.212%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.1281 (0.2038) Acc D Real: 88.974%
Loss D Fake: 0.6119 (0.6359) Acc D Fake: 88.781%
Loss D: 0.740
Loss G: 0.7794 (0.7556) Acc G: 11.197%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.1116 (0.2026) Acc D Real: 89.074%
Loss D Fake: 0.6200 (0.6357) Acc D Fake: 88.797%
Loss D: 0.732
Loss G: 0.7705 (0.7558) Acc G: 11.181%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.1031 (0.2014) Acc D Real: 89.186%
Loss D Fake: 0.6273 (0.6356) Acc D Fake: 88.812%
Loss D: 0.730
Loss G: 0.7740 (0.7560) Acc G: 11.167%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.1210 (0.2004) Acc D Real: 89.278%
Loss D Fake: 0.6153 (0.6353) Acc D Fake: 88.827%
Loss D: 0.736
Loss G: 0.7867 (0.7564) Acc G: 11.152%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.2026 (0.2004) Acc D Real: 89.279%
Loss D Fake: 0.6059 (0.6350) Acc D Fake: 88.841%
Loss D: 0.809
Loss G: 0.7954 (0.7568) Acc G: 11.138%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.1438 (0.1997) Acc D Real: 89.337%
Loss D Fake: 0.5997 (0.6346) Acc D Fake: 88.855%
Loss D: 0.743
Loss G: 0.8024 (0.7574) Acc G: 11.124%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.1482 (0.1991) Acc D Real: 89.391%
Loss D Fake: 0.5945 (0.6341) Acc D Fake: 88.868%
Loss D: 0.743
Loss G: 0.8083 (0.7580) Acc G: 11.111%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.1655 (0.1987) Acc D Real: 89.408%
Loss D Fake: 0.5903 (0.6336) Acc D Fake: 88.882%
Loss D: 0.756
Loss G: 0.8131 (0.7586) Acc G: 11.098%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.1763 (0.1985) Acc D Real: 89.428%
Loss D Fake: 0.5870 (0.6330) Acc D Fake: 88.895%
Loss D: 0.763
Loss G: 0.8170 (0.7593) Acc G: 11.085%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.1507 (0.1979) Acc D Real: 89.460%
Loss D Fake: 0.5841 (0.6325) Acc D Fake: 88.907%
Loss D: 0.735
Loss G: 0.8205 (0.7600) Acc G: 11.073%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.1734 (0.1976) Acc D Real: 89.480%
Loss D Fake: 0.5815 (0.6319) Acc D Fake: 88.939%
Loss D: 0.755
Loss G: 0.8238 (0.7607) Acc G: 11.042%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.1370 (0.1969) Acc D Real: 89.535%
Loss D Fake: 0.5791 (0.6313) Acc D Fake: 88.969%
Loss D: 0.716
Loss G: 0.8267 (0.7615) Acc G: 11.011%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.1270 (0.1962) Acc D Real: 89.600%
Loss D Fake: 0.5770 (0.6307) Acc D Fake: 88.999%
Loss D: 0.704
Loss G: 0.8294 (0.7622) Acc G: 10.981%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.1117 (0.1952) Acc D Real: 89.690%
Loss D Fake: 0.5751 (0.6301) Acc D Fake: 89.029%
Loss D: 0.687
Loss G: 0.8319 (0.7630) Acc G: 10.952%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.1083 (0.1943) Acc D Real: 89.772%
Loss D Fake: 0.5731 (0.6295) Acc D Fake: 89.057%
Loss D: 0.681
Loss G: 0.8346 (0.7638) Acc G: 10.924%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.1274 (0.1936) Acc D Real: 89.834%
Loss D Fake: 0.5711 (0.6288) Acc D Fake: 89.085%
Loss D: 0.698
Loss G: 0.8373 (0.7646) Acc G: 10.896%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.1504 (0.1931) Acc D Real: 89.880%
Loss D Fake: 0.5692 (0.6282) Acc D Fake: 89.113%
Loss D: 0.720
Loss G: 0.8396 (0.7654) Acc G: 10.869%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.1217 (0.1924) Acc D Real: 89.935%
Loss D Fake: 0.5676 (0.6276) Acc D Fake: 89.140%
Loss D: 0.689
Loss G: 0.8419 (0.7662) Acc G: 10.842%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.1805 (0.1922) Acc D Real: 89.929%
Loss D Fake: 0.5658 (0.6269) Acc D Fake: 89.166%
Loss D: 0.746
Loss G: 0.8444 (0.7670) Acc G: 10.816%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.1700 (0.1920) Acc D Real: 89.945%
Loss D Fake: 0.5640 (0.6263) Acc D Fake: 89.192%
Loss D: 0.734
Loss G: 0.8468 (0.7678) Acc G: 10.790%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2078 (0.1922) Acc D Real: 89.908%
Loss D Fake: 0.5623 (0.6256) Acc D Fake: 89.217%
Loss D: 0.770
Loss G: 0.8492 (0.7686) Acc G: 10.765%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.1589 (0.1918) Acc D Real: 89.922%
Loss D Fake: 0.5606 (0.6250) Acc D Fake: 89.242%
Loss D: 0.719
Loss G: 0.8515 (0.7695) Acc G: 10.741%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.1373 (0.1913) Acc D Real: 89.955%
Loss D Fake: 0.5588 (0.6243) Acc D Fake: 89.283%
Loss D: 0.696
Loss G: 0.8541 (0.7703) Acc G: 10.700%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.2630 (0.1920) Acc D Real: 89.838%
Loss D Fake: 0.5569 (0.6236) Acc D Fake: 89.323%
Loss D: 0.820
Loss G: 0.8566 (0.7712) Acc G: 10.660%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.1614 (0.1917) Acc D Real: 89.854%
Loss D Fake: 0.5552 (0.6230) Acc D Fake: 89.362%
Loss D: 0.717
Loss G: 0.8589 (0.7720) Acc G: 10.621%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.1527 (0.1913) Acc D Real: 89.859%
Loss D Fake: 0.5536 (0.6223) Acc D Fake: 89.401%
Loss D: 0.706
Loss G: 0.8612 (0.7729) Acc G: 10.566%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.1528 (0.1910) Acc D Real: 89.874%
Loss D Fake: 0.5519 (0.6216) Acc D Fake: 89.455%
Loss D: 0.705
Loss G: 0.8636 (0.7738) Acc G: 10.513%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.1342 (0.1904) Acc D Real: 89.911%
Loss D Fake: 0.5502 (0.6209) Acc D Fake: 89.507%
Loss D: 0.684
Loss G: 0.8660 (0.7747) Acc G: 10.460%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.1816 (0.1903) Acc D Real: 89.886%
Loss D Fake: 0.5485 (0.6202) Acc D Fake: 89.559%
Loss D: 0.730
Loss G: 0.8683 (0.7755) Acc G: 10.409%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.0947 (0.1894) Acc D Real: 89.958%
Loss D Fake: 0.5469 (0.6196) Acc D Fake: 89.610%
Loss D: 0.642
Loss G: 0.8707 (0.7764) Acc G: 10.358%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.1471 (0.1890) Acc D Real: 89.964%
Loss D Fake: 0.5451 (0.6189) Acc D Fake: 89.660%
Loss D: 0.692
Loss G: 0.8734 (0.7773) Acc G: 10.309%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.1954 (0.1891) Acc D Real: 89.931%
Loss D Fake: 0.5434 (0.6182) Acc D Fake: 89.709%
Loss D: 0.739
Loss G: 0.8755 (0.7782) Acc G: 10.260%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.1317 (0.1886) Acc D Real: 89.954%
Loss D Fake: 0.5420 (0.6175) Acc D Fake: 89.757%
Loss D: 0.674
Loss G: 0.8775 (0.7791) Acc G: 10.212%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.1602 (0.1883) Acc D Real: 89.960%
Loss D Fake: 0.5405 (0.6168) Acc D Fake: 89.804%
Loss D: 0.701
Loss G: 0.8797 (0.7800) Acc G: 10.165%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.1297 (0.1878) Acc D Real: 89.989%
Loss D Fake: 0.5390 (0.6161) Acc D Fake: 89.851%
Loss D: 0.669
Loss G: 0.8820 (0.7810) Acc G: 10.119%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.1914 (0.1878) Acc D Real: 89.960%
Loss D Fake: 0.5374 (0.6154) Acc D Fake: 89.896%
Loss D: 0.729
Loss G: 0.8844 (0.7819) Acc G: 10.074%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.1741 (0.1877) Acc D Real: 89.950%
Loss D Fake: 0.5358 (0.6147) Acc D Fake: 89.941%
Loss D: 0.710
Loss G: 0.8866 (0.7828) Acc G: 10.029%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.1121 (0.1871) Acc D Real: 90.002%
Loss D Fake: 0.5343 (0.6140) Acc D Fake: 89.985%
Loss D: 0.646
Loss G: 0.8891 (0.7837) Acc G: 9.986%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.1179 (0.1865) Acc D Real: 90.035%
Loss D Fake: 0.5325 (0.6133) Acc D Fake: 90.028%
Loss D: 0.650
Loss G: 0.8917 (0.7846) Acc G: 9.943%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.1572 (0.1862) Acc D Real: 90.036%
Loss D Fake: 0.5307 (0.6126) Acc D Fake: 90.071%
Loss D: 0.688
Loss G: 0.8943 (0.7856) Acc G: 9.900%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.1894 (0.1862) Acc D Real: 90.004%
Loss D Fake: 0.5291 (0.6119) Acc D Fake: 90.113%
Loss D: 0.719
Loss G: 0.8967 (0.7865) Acc G: 9.845%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.1768 (0.1862) Acc D Real: 89.979%
Loss D Fake: 0.5275 (0.6112) Acc D Fake: 90.168%
Loss D: 0.704
Loss G: 0.8990 (0.7875) Acc G: 9.790%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.1380 (0.1858) Acc D Real: 89.997%
Loss D Fake: 0.5260 (0.6105) Acc D Fake: 90.222%
Loss D: 0.664
Loss G: 0.9012 (0.7884) Acc G: 9.736%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.1421 (0.1854) Acc D Real: 90.012%
Loss D Fake: 0.5248 (0.6098) Acc D Fake: 90.275%
Loss D: 0.667
Loss G: 0.9023 (0.7894) Acc G: 9.683%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.1356 (0.1850) Acc D Real: 90.018%
Loss D Fake: 0.5243 (0.6091) Acc D Fake: 90.327%
Loss D: 0.660
Loss G: 0.9032 (0.7903) Acc G: 9.631%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.1805 (0.1850) Acc D Real: 89.993%
Loss D Fake: 0.5239 (0.6084) Acc D Fake: 90.379%
Loss D: 0.704
Loss G: 0.9036 (0.7912) Acc G: 9.580%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.2231 (0.1853) Acc D Real: 89.931%
Loss D Fake: 0.5237 (0.6077) Acc D Fake: 90.430%
Loss D: 0.747
Loss G: 0.9040 (0.7921) Acc G: 9.530%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.1272 (0.1848) Acc D Real: 89.948%
Loss D Fake: 0.5234 (0.6070) Acc D Fake: 90.480%
Loss D: 0.651
Loss G: 0.9051 (0.7930) Acc G: 9.480%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.1342 (0.1844) Acc D Real: 89.962%
Loss D Fake: 0.5223 (0.6063) Acc D Fake: 90.529%
Loss D: 0.656
Loss G: 0.9076 (0.7939) Acc G: 9.431%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.1807 (0.1844) Acc D Real: 89.927%
Loss D Fake: 0.5203 (0.6057) Acc D Fake: 90.577%
Loss D: 0.701
Loss G: 0.9110 (0.7949) Acc G: 9.383%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.1771 (0.1843) Acc D Real: 89.908%
Loss D Fake: 0.5179 (0.6050) Acc D Fake: 90.625%
Loss D: 0.695
Loss G: 0.9148 (0.7958) Acc G: 9.336%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.1838 (0.1843) Acc D Real: 89.876%
Loss D Fake: 0.5154 (0.6043) Acc D Fake: 90.671%
Loss D: 0.699
Loss G: 0.9190 (0.7967) Acc G: 9.289%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.1976 (0.1844) Acc D Real: 89.825%
Loss D Fake: 0.5128 (0.6036) Acc D Fake: 90.718%
Loss D: 0.710
Loss G: 0.9230 (0.7977) Acc G: 9.244%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.1441 (0.1841) Acc D Real: 89.822%
Loss D Fake: 0.5103 (0.6029) Acc D Fake: 90.763%
Loss D: 0.654
Loss G: 0.9271 (0.7987) Acc G: 9.198%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.1752 (0.1840) Acc D Real: 89.804%
Loss D Fake: 0.5078 (0.6021) Acc D Fake: 90.808%
Loss D: 0.683
Loss G: 0.9309 (0.7997) Acc G: 9.154%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.1246 (0.1836) Acc D Real: 89.818%
Loss D Fake: 0.5054 (0.6014) Acc D Fake: 90.852%
Loss D: 0.630
Loss G: 0.9350 (0.8007) Acc G: 9.110%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.1724 (0.1835) Acc D Real: 89.808%
Loss D Fake: 0.5029 (0.6007) Acc D Fake: 90.895%
Loss D: 0.675
Loss G: 0.9390 (0.8018) Acc G: 9.067%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.1375 (0.1832) Acc D Real: 89.814%
Loss D Fake: 0.5005 (0.5999) Acc D Fake: 90.938%
Loss D: 0.638
Loss G: 0.9428 (0.8028) Acc G: 9.025%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.1208 (0.1827) Acc D Real: 89.835%
Loss D Fake: 0.4982 (0.5992) Acc D Fake: 90.980%
Loss D: 0.619
Loss G: 0.9469 (0.8039) Acc G: 8.983%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.1365 (0.1824) Acc D Real: 89.839%
Loss D Fake: 0.4958 (0.5984) Acc D Fake: 91.022%
Loss D: 0.632
Loss G: 0.9509 (0.8049) Acc G: 8.942%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.1747 (0.1823) Acc D Real: 89.816%
Loss D Fake: 0.4935 (0.5977) Acc D Fake: 91.062%
Loss D: 0.668
Loss G: 0.9547 (0.8060) Acc G: 8.901%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.1531 (0.1821) Acc D Real: 89.818%
Loss D Fake: 0.4912 (0.5969) Acc D Fake: 91.103%
Loss D: 0.644
Loss G: 0.9588 (0.8071) Acc G: 8.861%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2096 (0.1823) Acc D Real: 89.766%
Loss D Fake: 0.4889 (0.5961) Acc D Fake: 91.142%
Loss D: 0.699
Loss G: 0.9623 (0.8082) Acc G: 8.821%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.1296 (0.1819) Acc D Real: 89.774%
Loss D Fake: 0.4870 (0.5954) Acc D Fake: 91.182%
Loss D: 0.617
Loss G: 0.9660 (0.8093) Acc G: 8.783%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.1403 (0.1816) Acc D Real: 89.783%
Loss D Fake: 0.4847 (0.5946) Acc D Fake: 91.220%
Loss D: 0.625
Loss G: 0.9702 (0.8105) Acc G: 8.744%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.1281 (0.1813) Acc D Real: 89.798%
Loss D Fake: 0.4822 (0.5938) Acc D Fake: 91.258%
Loss D: 0.610
Loss G: 0.9750 (0.8116) Acc G: 8.706%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.1490 (0.1810) Acc D Real: 89.803%
Loss D Fake: 0.4793 (0.5930) Acc D Fake: 91.296%
Loss D: 0.628
Loss G: 0.9805 (0.8128) Acc G: 8.657%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.1474 (0.1808) Acc D Real: 89.812%
Loss D Fake: 0.4760 (0.5922) Acc D Fake: 91.344%
Loss D: 0.623
Loss G: 0.9864 (0.8140) Acc G: 8.609%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.0987 (0.1802) Acc D Real: 89.846%
Loss D Fake: 0.4724 (0.5914) Acc D Fake: 91.404%
Loss D: 0.571
Loss G: 0.9931 (0.8152) Acc G: 8.550%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.1511 (0.1800) Acc D Real: 89.842%
Loss D Fake: 0.4686 (0.5905) Acc D Fake: 91.462%
Loss D: 0.620
Loss G: 1.0002 (0.8165) Acc G: 8.492%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.1319 (0.1797) Acc D Real: 89.846%
Loss D Fake: 0.4646 (0.5897) Acc D Fake: 91.520%
Loss D: 0.597
Loss G: 1.0075 (0.8178) Acc G: 8.435%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.1300 (0.1794) Acc D Real: 89.858%
Loss D Fake: 0.4606 (0.5888) Acc D Fake: 91.577%
Loss D: 0.591
Loss G: 1.0152 (0.8191) Acc G: 8.378%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.1123 (0.1789) Acc D Real: 89.884%
Loss D Fake: 0.4563 (0.5879) Acc D Fake: 91.633%
Loss D: 0.569
Loss G: 1.0235 (0.8205) Acc G: 8.322%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.1741 (0.1789) Acc D Real: 89.861%
Loss D Fake: 0.4520 (0.5870) Acc D Fake: 91.688%
Loss D: 0.626
Loss G: 1.0314 (0.8219) Acc G: 8.267%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.1232 (0.1785) Acc D Real: 89.876%
Loss D Fake: 0.4479 (0.5861) Acc D Fake: 91.743%
Loss D: 0.571
Loss G: 1.0395 (0.8233) Acc G: 8.213%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2039 (0.1787) Acc D Real: 89.837%
Loss D Fake: 0.4438 (0.5852) Acc D Fake: 91.797%
Loss D: 0.648
Loss G: 1.0471 (0.8247) Acc G: 8.159%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.1283 (0.1784) Acc D Real: 89.846%
Loss D Fake: 0.4399 (0.5843) Acc D Fake: 91.850%
Loss D: 0.568
Loss G: 1.0550 (0.8262) Acc G: 8.106%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.1543 (0.1782) Acc D Real: 89.837%
Loss D Fake: 0.4359 (0.5833) Acc D Fake: 91.903%
Loss D: 0.590
Loss G: 1.0634 (0.8278) Acc G: 8.054%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.1959 (0.1783) Acc D Real: 89.804%
Loss D Fake: 0.4318 (0.5823) Acc D Fake: 91.955%
Loss D: 0.628
Loss G: 1.0716 (0.8293) Acc G: 8.002%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.1635 (0.1782) Acc D Real: 89.785%
Loss D Fake: 0.4279 (0.5813) Acc D Fake: 92.006%
Loss D: 0.591
Loss G: 1.0801 (0.8309) Acc G: 7.951%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.1843 (0.1783) Acc D Real: 89.751%
Loss D Fake: 0.4239 (0.5803) Acc D Fake: 92.057%
Loss D: 0.608
Loss G: 1.0884 (0.8326) Acc G: 7.901%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.1832 (0.1783) Acc D Real: 89.726%
Loss D Fake: 0.4201 (0.5793) Acc D Fake: 92.107%
Loss D: 0.603
Loss G: 1.0965 (0.8342) Acc G: 7.851%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.1574 (0.1782) Acc D Real: 89.724%
Loss D Fake: 0.4164 (0.5783) Acc D Fake: 92.156%
Loss D: 0.574
Loss G: 1.1048 (0.8359) Acc G: 7.802%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.1020 (0.1777) Acc D Real: 89.753%
Loss D Fake: 0.4125 (0.5773) Acc D Fake: 92.205%
Loss D: 0.514
Loss G: 1.1148 (0.8376) Acc G: 7.754%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.1831 (0.1777) Acc D Real: 89.725%
Loss D Fake: 0.4079 (0.5762) Acc D Fake: 92.253%
Loss D: 0.591
Loss G: 1.1253 (0.8394) Acc G: 7.706%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.1388 (0.1775) Acc D Real: 89.723%
Loss D Fake: 0.4029 (0.5752) Acc D Fake: 92.300%
Loss D: 0.542
Loss G: 1.1377 (0.8413) Acc G: 7.658%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.0985 (0.1770) Acc D Real: 89.751%
Loss D Fake: 0.3969 (0.5741) Acc D Fake: 92.347%
Loss D: 0.495
Loss G: 1.1520 (0.8431) Acc G: 7.612%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1612 (0.1769) Acc D Real: 89.738%
Loss D Fake: 0.3911 (0.5730) Acc D Fake: 92.394%
Loss D: 0.552
Loss G: 1.1628 (0.8451) Acc G: 7.566%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.1881 (0.1770) Acc D Real: 89.707%
Loss D Fake: 0.3873 (0.5719) Acc D Fake: 92.439%
Loss D: 0.575
Loss G: 1.1727 (0.8471) Acc G: 7.520%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.1349 (0.1767) Acc D Real: 89.711%
Loss D Fake: 0.3831 (0.5707) Acc D Fake: 92.485%
Loss D: 0.518
Loss G: 1.1844 (0.8491) Acc G: 7.475%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.1557 (0.1766) Acc D Real: 89.704%
Loss D Fake: 0.3784 (0.5696) Acc D Fake: 92.529%
Loss D: 0.534
Loss G: 1.1967 (0.8511) Acc G: 7.431%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.1592 (0.1765) Acc D Real: 89.697%
Loss D Fake: 0.3737 (0.5684) Acc D Fake: 92.574%
Loss D: 0.533
Loss G: 1.2093 (0.8533) Acc G: 7.387%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.2331 (0.1768) Acc D Real: 89.648%
Loss D Fake: 0.3692 (0.5673) Acc D Fake: 92.617%
Loss D: 0.602
Loss G: 1.2208 (0.8554) Acc G: 7.343%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.1833 (0.1769) Acc D Real: 89.626%
Loss D Fake: 0.3652 (0.5661) Acc D Fake: 92.661%
Loss D: 0.549
Loss G: 1.2316 (0.8576) Acc G: 7.300%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.1616 (0.1768) Acc D Real: 89.615%
Loss D Fake: 0.3614 (0.5649) Acc D Fake: 92.703%
Loss D: 0.523
Loss G: 1.2423 (0.8599) Acc G: 7.258%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.1134 (0.1764) Acc D Real: 89.631%
Loss D Fake: 0.3575 (0.5637) Acc D Fake: 92.745%
Loss D: 0.471
Loss G: 1.2542 (0.8621) Acc G: 7.216%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1106 (0.1760) Acc D Real: 89.651%
Loss D Fake: 0.3530 (0.5625) Acc D Fake: 92.787%
Loss D: 0.464
Loss G: 1.2690 (0.8645) Acc G: 7.174%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.2694 (0.1766) Acc D Real: 89.597%
Loss D Fake: 0.3481 (0.5613) Acc D Fake: 92.828%
Loss D: 0.617
Loss G: 1.2818 (0.8669) Acc G: 7.133%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.1644 (0.1765) Acc D Real: 89.588%
Loss D Fake: 0.3446 (0.5600) Acc D Fake: 92.869%
Loss D: 0.509
Loss G: 1.2918 (0.8693) Acc G: 7.093%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.0973 (0.1761) Acc D Real: 89.613%
Loss D Fake: 0.3415 (0.5588) Acc D Fake: 92.909%
Loss D: 0.439
Loss G: 1.3046 (0.8717) Acc G: 7.053%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.1229 (0.1758) Acc D Real: 89.625%
Loss D Fake: 0.3372 (0.5575) Acc D Fake: 92.949%
Loss D: 0.460
Loss G: 1.3205 (0.8743) Acc G: 7.013%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.1385 (0.1755) Acc D Real: 89.628%
Loss D Fake: 0.3322 (0.5563) Acc D Fake: 92.989%
Loss D: 0.471
Loss G: 1.3383 (0.8769) Acc G: 6.974%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.1805 (0.1756) Acc D Real: 89.615%
Loss D Fake: 0.3286 (0.5550) Acc D Fake: 93.027%
Loss D: 0.509
Loss G: 1.3369 (0.8794) Acc G: 6.935%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.2109 (0.1758) Acc D Real: 89.589%
Loss D Fake: 0.3324 (0.5538) Acc D Fake: 93.066%
Loss D: 0.543
Loss G: 1.3135 (0.8818) Acc G: 6.897%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.1486 (0.1756) Acc D Real: 89.592%
Loss D Fake: 0.3449 (0.5526) Acc D Fake: 93.104%
Loss D: 0.494
Loss G: 1.2770 (0.8840) Acc G: 6.859%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.0941 (0.1752) Acc D Real: 89.623%
Loss D Fake: 0.3607 (0.5516) Acc D Fake: 93.142%
Loss D: 0.455
Loss G: 1.3221 (0.8864) Acc G: 6.821%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.1298 (0.1749) Acc D Real: 89.638%
Loss D Fake: 0.3237 (0.5504) Acc D Fake: 93.179%
Loss D: 0.453
Loss G: 1.3865 (0.8891) Acc G: 6.784%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.1263 (0.1747) Acc D Real: 89.649%
Loss D Fake: 0.3091 (0.5490) Acc D Fake: 93.216%
Loss D: 0.435
Loss G: 1.4305 (0.8920) Acc G: 6.748%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1726 (0.1747) Acc D Real: 89.638%
Loss D Fake: 0.2987 (0.5477) Acc D Fake: 93.252%
Loss D: 0.471
Loss G: 1.4670 (0.8951) Acc G: 6.711%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.1427 (0.1745) Acc D Real: 89.648%
Loss D Fake: 0.2900 (0.5463) Acc D Fake: 93.288%
Loss D: 0.433
Loss G: 1.4998 (0.8983) Acc G: 6.676%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.1349 (0.1743) Acc D Real: 89.650%
Loss D Fake: 0.2826 (0.5449) Acc D Fake: 93.324%
Loss D: 0.418
Loss G: 1.5279 (0.9017) Acc G: 6.640%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.1547 (0.1742) Acc D Real: 89.657%
Loss D Fake: 0.2766 (0.5435) Acc D Fake: 93.360%
Loss D: 0.431
Loss G: 1.5515 (0.9051) Acc G: 6.605%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.1444 (0.1740) Acc D Real: 89.663%
Loss D Fake: 0.2716 (0.5421) Acc D Fake: 93.394%
Loss D: 0.416
Loss G: 1.5735 (0.9086) Acc G: 6.570%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.1485 (0.1739) Acc D Real: 89.673%
Loss D Fake: 0.2669 (0.5406) Acc D Fake: 93.429%
Loss D: 0.415
Loss G: 1.5943 (0.9122) Acc G: 6.536%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.1487 (0.1738) Acc D Real: 89.673%
Loss D Fake: 0.2633 (0.5392) Acc D Fake: 93.463%
Loss D: 0.412
Loss G: 1.6033 (0.9158) Acc G: 6.502%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.1418 (0.1736) Acc D Real: 89.683%
Loss D Fake: 0.2619 (0.5377) Acc D Fake: 93.497%
Loss D: 0.404
Loss G: 1.6100 (0.9194) Acc G: 6.468%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2070 (0.1738) Acc D Real: 89.669%
Loss D Fake: 0.2607 (0.5363) Acc D Fake: 93.531%
Loss D: 0.468
Loss G: 1.6128 (0.9230) Acc G: 6.435%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.0927 (0.1733) Acc D Real: 89.694%
Loss D Fake: 0.2605 (0.5349) Acc D Fake: 93.564%
Loss D: 0.353
Loss G: 1.6189 (0.9266) Acc G: 6.402%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.0862 (0.1729) Acc D Real: 89.726%
Loss D Fake: 0.2591 (0.5335) Acc D Fake: 93.597%
Loss D: 0.345
Loss G: 1.6308 (0.9302) Acc G: 6.369%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.1120 (0.1726) Acc D Real: 89.745%
Loss D Fake: 0.2567 (0.5321) Acc D Fake: 93.629%
Loss D: 0.369
Loss G: 1.6457 (0.9338) Acc G: 6.337%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.1713 (0.1726) Acc D Real: 89.741%
Loss D Fake: 0.2543 (0.5307) Acc D Fake: 93.661%
Loss D: 0.426
Loss G: 1.6570 (0.9375) Acc G: 6.305%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.1939 (0.1727) Acc D Real: 89.727%
Loss D Fake: 0.2530 (0.5293) Acc D Fake: 93.693%
Loss D: 0.447
Loss G: 1.6590 (0.9411) Acc G: 6.273%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.1309 (0.1725) Acc D Real: 89.734%
Loss D Fake: 0.2533 (0.5279) Acc D Fake: 93.725%
Loss D: 0.384
Loss G: 1.6559 (0.9447) Acc G: 6.242%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2981 (0.1731) Acc D Real: 89.682%
Loss D Fake: 0.2568 (0.5266) Acc D Fake: 93.756%
Loss D: 0.555
Loss G: 1.5938 (0.9479) Acc G: 6.211%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1607 (0.1730) Acc D Real: 89.678%
Loss D Fake: 0.3525 (0.5257) Acc D Fake: 93.762%
Loss D: 0.513
Loss G: 1.6486 (0.9514) Acc G: 6.180%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.1120 (0.1727) Acc D Real: 89.698%
Loss D Fake: 0.2454 (0.5243) Acc D Fake: 93.793%
Loss D: 0.357
Loss G: 1.7322 (0.9552) Acc G: 6.149%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.1817 (0.1728) Acc D Real: 89.692%
Loss D Fake: 0.2349 (0.5229) Acc D Fake: 93.823%
Loss D: 0.417
Loss G: 1.7795 (0.9592) Acc G: 6.119%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2571 (0.1732) Acc D Real: 89.664%
Loss D Fake: 0.2285 (0.5215) Acc D Fake: 93.853%
Loss D: 0.486
Loss G: 1.8112 (0.9634) Acc G: 6.089%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.1245 (0.1730) Acc D Real: 89.685%
Loss D Fake: 0.2241 (0.5200) Acc D Fake: 93.883%
Loss D: 0.349
Loss G: 1.8360 (0.9676) Acc G: 6.060%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.1872 (0.1730) Acc D Real: 89.688%
Loss D Fake: 0.2206 (0.5186) Acc D Fake: 93.913%
Loss D: 0.408
Loss G: 1.8555 (0.9719) Acc G: 6.031%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2369 (0.1733) Acc D Real: 89.670%
Loss D Fake: 0.2186 (0.5171) Acc D Fake: 93.942%
Loss D: 0.456
Loss G: 1.8634 (0.9762) Acc G: 6.002%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1224 (0.1731) Acc D Real: 89.694%
Loss D Fake: 0.2190 (0.5157) Acc D Fake: 93.971%
Loss D: 0.341
Loss G: 1.8672 (0.9805) Acc G: 5.973%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.2493 (0.1735) Acc D Real: 89.676%
Loss D Fake: 0.2198 (0.5143) Acc D Fake: 94.000%
Loss D: 0.469
Loss G: 1.8670 (0.9847) Acc G: 5.944%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.1758 (0.1735) Acc D Real: 89.667%
Loss D Fake: 0.2218 (0.5129) Acc D Fake: 94.028%
Loss D: 0.398
Loss G: 1.8566 (0.9888) Acc G: 5.916%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3428 (0.1743) Acc D Real: 89.625%
Loss D Fake: 0.2255 (0.5116) Acc D Fake: 94.056%
Loss D: 0.568
Loss G: 1.8399 (0.9928) Acc G: 5.888%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2997 (0.1749) Acc D Real: 89.585%
Loss D Fake: 0.2304 (0.5102) Acc D Fake: 94.084%
Loss D: 0.530
Loss G: 1.8121 (0.9967) Acc G: 5.861%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.1433 (0.1747) Acc D Real: 89.591%
Loss D Fake: 0.2372 (0.5090) Acc D Fake: 94.112%
Loss D: 0.380
Loss G: 1.7840 (1.0004) Acc G: 5.833%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.1441 (0.1746) Acc D Real: 89.602%
Loss D Fake: 0.2433 (0.5077) Acc D Fake: 94.139%
Loss D: 0.387
Loss G: 1.7615 (1.0039) Acc G: 5.814%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.0878 (0.1742) Acc D Real: 89.632%
Loss D Fake: 0.2482 (0.5065) Acc D Fake: 94.159%
Loss D: 0.336
Loss G: 1.7461 (1.0073) Acc G: 5.795%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.1480 (0.1740) Acc D Real: 89.641%
Loss D Fake: 0.2519 (0.5054) Acc D Fake: 94.178%
Loss D: 0.400
Loss G: 1.7338 (1.0107) Acc G: 5.776%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.1461 (0.1739) Acc D Real: 89.648%
Loss D Fake: 0.2579 (0.5042) Acc D Fake: 94.197%
Loss D: 0.404
Loss G: 1.6922 (1.0138) Acc G: 5.765%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.1072 (0.1736) Acc D Real: 89.663%
Loss D Fake: 0.2703 (0.5031) Acc D Fake: 94.208%
Loss D: 0.377
Loss G: 1.6502 (1.0167) Acc G: 5.761%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2930 (0.1742) Acc D Real: 89.635%
Loss D Fake: 0.2818 (0.5021) Acc D Fake: 94.212%
Loss D: 0.575
Loss G: 1.6080 (1.0194) Acc G: 5.758%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2457 (0.1745) Acc D Real: 89.617%
Loss D Fake: 0.2942 (0.5012) Acc D Fake: 94.215%
Loss D: 0.540
Loss G: 1.5807 (1.0219) Acc G: 5.754%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.1491 (0.1744) Acc D Real: 89.627%
Loss D Fake: 0.2966 (0.5003) Acc D Fake: 94.219%
Loss D: 0.446
Loss G: 1.5758 (1.0244) Acc G: 5.751%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.1267 (0.1741) Acc D Real: 89.641%
Loss D Fake: 0.2914 (0.4993) Acc D Fake: 94.222%
Loss D: 0.418
Loss G: 1.5829 (1.0269) Acc G: 5.747%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2384 (0.1744) Acc D Real: 89.625%
Loss D Fake: 0.2853 (0.4984) Acc D Fake: 94.233%
Loss D: 0.524
Loss G: 1.5905 (1.0295) Acc G: 5.737%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.1396 (0.1743) Acc D Real: 89.625%
Loss D Fake: 0.2805 (0.4974) Acc D Fake: 94.244%
Loss D: 0.420
Loss G: 1.5979 (1.0320) Acc G: 5.726%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.0850 (0.1739) Acc D Real: 89.630%
Loss D Fake: 0.2763 (0.4964) Acc D Fake: 94.249%
Loss D: 0.361
Loss G: 1.6074 (1.0345) Acc G: 5.721%
LR: 2.000e-04
Best Loss 0.867 to 0.852
Epoch: 3/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.0889 (0.0872) Acc D Real: 95.703%
Loss D Fake: 0.2695 (0.2708) Acc D Fake: 99.167%
Loss D: 0.358
Loss G: 1.6196 (1.6192) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.1588 (0.1110) Acc D Real: 93.681%
Loss D Fake: 0.2714 (0.2710) Acc D Fake: 99.444%
Loss D: 0.430
Loss G: 1.6158 (1.6181) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.1127 (0.1115) Acc D Real: 93.867%
Loss D Fake: 0.2737 (0.2717) Acc D Fake: 99.583%
Loss D: 0.386
Loss G: 1.6130 (1.6168) Acc G: 0.417%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.1442 (0.1180) Acc D Real: 93.594%
Loss D Fake: 0.2760 (0.2725) Acc D Fake: 99.333%
Loss D: 0.420
Loss G: 1.6110 (1.6156) Acc G: 0.667%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.0801 (0.1117) Acc D Real: 94.149%
Loss D Fake: 0.2776 (0.2734) Acc D Fake: 99.167%
Loss D: 0.358
Loss G: 1.6149 (1.6155) Acc G: 0.833%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.1286 (0.1141) Acc D Real: 93.951%
Loss D Fake: 0.2772 (0.2739) Acc D Fake: 99.286%
Loss D: 0.406
Loss G: 1.6211 (1.6163) Acc G: 0.714%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.1257 (0.1156) Acc D Real: 93.854%
Loss D Fake: 0.2760 (0.2742) Acc D Fake: 99.375%
Loss D: 0.402
Loss G: 1.6293 (1.6179) Acc G: 0.625%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.1278 (0.1169) Acc D Real: 93.628%
Loss D Fake: 0.2733 (0.2741) Acc D Fake: 99.444%
Loss D: 0.401
Loss G: 1.6426 (1.6207) Acc G: 0.556%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2126 (0.1265) Acc D Real: 92.979%
Loss D Fake: 0.2692 (0.2736) Acc D Fake: 99.500%
Loss D: 0.482
Loss G: 1.6563 (1.6242) Acc G: 0.500%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1564 (0.1292) Acc D Real: 92.794%
Loss D Fake: 0.2651 (0.2728) Acc D Fake: 99.545%
Loss D: 0.421
Loss G: 1.6708 (1.6285) Acc G: 0.455%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.1372 (0.1299) Acc D Real: 92.595%
Loss D Fake: 0.2607 (0.2718) Acc D Fake: 99.583%
Loss D: 0.398
Loss G: 1.6860 (1.6333) Acc G: 0.417%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.1476 (0.1312) Acc D Real: 92.556%
Loss D Fake: 0.2563 (0.2706) Acc D Fake: 99.615%
Loss D: 0.404
Loss G: 1.7008 (1.6385) Acc G: 0.385%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.0708 (0.1269) Acc D Real: 92.920%
Loss D Fake: 0.2521 (0.2693) Acc D Fake: 99.643%
Loss D: 0.323
Loss G: 1.7170 (1.6441) Acc G: 0.357%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.0839 (0.1240) Acc D Real: 93.104%
Loss D Fake: 0.2478 (0.2679) Acc D Fake: 99.667%
Loss D: 0.332
Loss G: 1.7341 (1.6501) Acc G: 0.333%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2106 (0.1295) Acc D Real: 92.669%
Loss D Fake: 0.2444 (0.2664) Acc D Fake: 99.688%
Loss D: 0.455
Loss G: 1.7362 (1.6555) Acc G: 0.312%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.0871 (0.1270) Acc D Real: 92.822%
Loss D Fake: 0.2432 (0.2650) Acc D Fake: 99.706%
Loss D: 0.330
Loss G: 1.7353 (1.6602) Acc G: 0.294%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.1672 (0.1292) Acc D Real: 92.523%
Loss D Fake: 0.2421 (0.2638) Acc D Fake: 99.722%
Loss D: 0.409
Loss G: 1.7322 (1.6642) Acc G: 0.278%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.1640 (0.1310) Acc D Real: 92.349%
Loss D Fake: 0.2418 (0.2626) Acc D Fake: 99.737%
Loss D: 0.406
Loss G: 1.7277 (1.6675) Acc G: 0.263%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1957 (0.1343) Acc D Real: 92.060%
Loss D Fake: 0.2419 (0.2616) Acc D Fake: 99.750%
Loss D: 0.438
Loss G: 1.7185 (1.6701) Acc G: 0.250%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.1797 (0.1364) Acc D Real: 91.801%
Loss D Fake: 0.2430 (0.2607) Acc D Fake: 99.762%
Loss D: 0.423
Loss G: 1.7075 (1.6718) Acc G: 0.238%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1660 (0.1378) Acc D Real: 91.662%
Loss D Fake: 0.2443 (0.2599) Acc D Fake: 99.773%
Loss D: 0.410
Loss G: 1.6985 (1.6730) Acc G: 0.227%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1226 (0.1371) Acc D Real: 91.617%
Loss D Fake: 0.2450 (0.2593) Acc D Fake: 99.783%
Loss D: 0.368
Loss G: 1.6964 (1.6741) Acc G: 0.217%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.0838 (0.1349) Acc D Real: 91.780%
Loss D Fake: 0.2440 (0.2586) Acc D Fake: 99.792%
Loss D: 0.328
Loss G: 1.7080 (1.6755) Acc G: 0.208%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.1118 (0.1340) Acc D Real: 91.862%
Loss D Fake: 0.2408 (0.2579) Acc D Fake: 99.800%
Loss D: 0.353
Loss G: 1.7276 (1.6776) Acc G: 0.200%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.0896 (0.1323) Acc D Real: 91.967%
Loss D Fake: 0.2371 (0.2571) Acc D Fake: 99.808%
Loss D: 0.327
Loss G: 1.7532 (1.6805) Acc G: 0.192%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.1296 (0.1322) Acc D Real: 91.916%
Loss D Fake: 0.2328 (0.2562) Acc D Fake: 99.815%
Loss D: 0.362
Loss G: 1.7794 (1.6841) Acc G: 0.185%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.1405 (0.1325) Acc D Real: 91.931%
Loss D Fake: 0.2287 (0.2552) Acc D Fake: 99.821%
Loss D: 0.369
Loss G: 1.8057 (1.6885) Acc G: 0.179%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.1872 (0.1344) Acc D Real: 91.778%
Loss D Fake: 0.2254 (0.2542) Acc D Fake: 99.828%
Loss D: 0.413
Loss G: 1.8180 (1.6929) Acc G: 0.172%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.0843 (0.1327) Acc D Real: 91.905%
Loss D Fake: 0.2237 (0.2532) Acc D Fake: 99.833%
Loss D: 0.308
Loss G: 1.8304 (1.6975) Acc G: 0.167%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2183 (0.1354) Acc D Real: 91.778%
Loss D Fake: 0.2216 (0.2522) Acc D Fake: 99.839%
Loss D: 0.440
Loss G: 1.8430 (1.7022) Acc G: 0.161%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1660 (0.1364) Acc D Real: 91.733%
Loss D Fake: 0.2197 (0.2512) Acc D Fake: 99.844%
Loss D: 0.386
Loss G: 1.8540 (1.7070) Acc G: 0.156%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.1650 (0.1373) Acc D Real: 91.649%
Loss D Fake: 0.2184 (0.2502) Acc D Fake: 99.848%
Loss D: 0.383
Loss G: 1.8595 (1.7116) Acc G: 0.152%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.1170 (0.1367) Acc D Real: 91.650%
Loss D Fake: 0.2174 (0.2492) Acc D Fake: 99.853%
Loss D: 0.334
Loss G: 1.8663 (1.7161) Acc G: 0.147%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3007 (0.1414) Acc D Real: 91.399%
Loss D Fake: 0.2165 (0.2483) Acc D Fake: 99.857%
Loss D: 0.517
Loss G: 1.8645 (1.7204) Acc G: 0.143%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3486 (0.1471) Acc D Real: 91.082%
Loss D Fake: 0.2178 (0.2474) Acc D Fake: 99.861%
Loss D: 0.566
Loss G: 1.8506 (1.7240) Acc G: 0.139%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.1534 (0.1473) Acc D Real: 91.077%
Loss D Fake: 0.2207 (0.2467) Acc D Fake: 99.865%
Loss D: 0.374
Loss G: 1.8396 (1.7271) Acc G: 0.135%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.1385 (0.1471) Acc D Real: 91.133%
Loss D Fake: 0.2220 (0.2461) Acc D Fake: 99.868%
Loss D: 0.361
Loss G: 1.8440 (1.7302) Acc G: 0.132%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.0818 (0.1454) Acc D Real: 91.239%
Loss D Fake: 0.2206 (0.2454) Acc D Fake: 99.872%
Loss D: 0.302
Loss G: 1.8629 (1.7336) Acc G: 0.128%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.0887 (0.1440) Acc D Real: 91.329%
Loss D Fake: 0.2176 (0.2447) Acc D Fake: 99.875%
Loss D: 0.306
Loss G: 1.8892 (1.7375) Acc G: 0.125%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.1417 (0.1439) Acc D Real: 91.317%
Loss D Fake: 0.2145 (0.2440) Acc D Fake: 99.878%
Loss D: 0.356
Loss G: 1.9081 (1.7416) Acc G: 0.122%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.1252 (0.1435) Acc D Real: 91.308%
Loss D Fake: 0.2126 (0.2432) Acc D Fake: 99.881%
Loss D: 0.338
Loss G: 1.9244 (1.7460) Acc G: 0.119%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2663 (0.1463) Acc D Real: 91.164%
Loss D Fake: 0.2113 (0.2425) Acc D Fake: 99.884%
Loss D: 0.478
Loss G: 1.9278 (1.7502) Acc G: 0.116%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.1550 (0.1465) Acc D Real: 91.177%
Loss D Fake: 0.2115 (0.2418) Acc D Fake: 99.886%
Loss D: 0.366
Loss G: 1.9301 (1.7543) Acc G: 0.114%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.1036 (0.1456) Acc D Real: 91.227%
Loss D Fake: 0.2111 (0.2411) Acc D Fake: 99.889%
Loss D: 0.315
Loss G: 1.9379 (1.7584) Acc G: 0.111%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.1086 (0.1448) Acc D Real: 91.278%
Loss D Fake: 0.2099 (0.2404) Acc D Fake: 99.891%
Loss D: 0.318
Loss G: 1.9498 (1.7626) Acc G: 0.109%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.1439 (0.1447) Acc D Real: 91.307%
Loss D Fake: 0.2082 (0.2397) Acc D Fake: 99.894%
Loss D: 0.352
Loss G: 1.9628 (1.7668) Acc G: 0.106%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.2580 (0.1471) Acc D Real: 91.127%
Loss D Fake: 0.2079 (0.2391) Acc D Fake: 99.896%
Loss D: 0.466
Loss G: 1.9284 (1.7702) Acc G: 0.104%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.1891 (0.1480) Acc D Real: 91.083%
Loss D Fake: 0.2172 (0.2386) Acc D Fake: 99.898%
Loss D: 0.406
Loss G: 1.8230 (1.7713) Acc G: 0.102%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.0985 (0.1470) Acc D Real: 91.126%
Loss D Fake: 0.4048 (0.2419) Acc D Fake: 99.767%
Loss D: 0.503
Loss G: 2.0837 (1.7775) Acc G: 0.100%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.1984 (0.1480) Acc D Real: 91.103%
Loss D Fake: 0.1855 (0.2408) Acc D Fake: 99.771%
Loss D: 0.384
Loss G: 2.1886 (1.7856) Acc G: 0.098%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.2822 (0.1506) Acc D Real: 90.997%
Loss D Fake: 0.1809 (0.2397) Acc D Fake: 99.776%
Loss D: 0.463
Loss G: 2.2321 (1.7942) Acc G: 0.096%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3350 (0.1540) Acc D Real: 90.867%
Loss D Fake: 0.1831 (0.2386) Acc D Fake: 99.780%
Loss D: 0.518
Loss G: 2.2390 (1.8025) Acc G: 0.094%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.4614 (0.1597) Acc D Real: 90.592%
Loss D Fake: 0.1927 (0.2378) Acc D Fake: 99.784%
Loss D: 0.654
Loss G: 2.1875 (1.8097) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2981 (0.1622) Acc D Real: 90.496%
Loss D Fake: 3.3432 (0.2942) Acc D Fake: 98.487%
Loss D: 3.641
Loss G: 2.3204 (1.8190) Acc G: 0.091%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.4151 (0.1668) Acc D Real: 90.321%
Loss D Fake: 0.1609 (0.2919) Acc D Fake: 98.514%
Loss D: 0.576
Loss G: 2.4129 (1.8296) Acc G: 0.089%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.4422 (0.1716) Acc D Real: 90.119%
Loss D Fake: 0.1482 (0.2893) Acc D Fake: 98.540%
Loss D: 0.590
Loss G: 2.4583 (1.8406) Acc G: 0.088%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.9359 (0.1848) Acc D Real: 89.599%
Loss D Fake: 0.1414 (0.2868) Acc D Fake: 98.565%
Loss D: 1.077
Loss G: 2.4830 (1.8517) Acc G: 0.086%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.9898 (0.1984) Acc D Real: 89.034%
Loss D Fake: 0.1375 (0.2843) Acc D Fake: 98.589%
Loss D: 1.127
Loss G: 2.4935 (1.8626) Acc G: 0.085%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.7314 (0.2073) Acc D Real: 88.676%
Loss D Fake: 0.1354 (0.2818) Acc D Fake: 98.613%
Loss D: 0.867
Loss G: 2.4956 (1.8731) Acc G: 0.083%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.6120 (0.2139) Acc D Real: 88.419%
Loss D Fake: 0.1342 (0.2794) Acc D Fake: 98.636%
Loss D: 0.746
Loss G: 2.4927 (1.8833) Acc G: 0.082%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.7527 (0.2226) Acc D Real: 88.070%
Loss D Fake: 0.1337 (0.2770) Acc D Fake: 98.658%
Loss D: 0.886
Loss G: 2.4853 (1.8930) Acc G: 0.081%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 1.0647 (0.2360) Acc D Real: 87.519%
Loss D Fake: 0.1339 (0.2747) Acc D Fake: 98.679%
Loss D: 1.199
Loss G: 2.4718 (1.9022) Acc G: 0.079%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.6377 (0.2423) Acc D Real: 87.268%
Loss D Fake: 0.1347 (0.2725) Acc D Fake: 98.700%
Loss D: 0.772
Loss G: 2.4562 (1.9108) Acc G: 0.078%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.8115 (0.2510) Acc D Real: 86.913%
Loss D Fake: 0.1358 (0.2704) Acc D Fake: 98.720%
Loss D: 0.947
Loss G: 2.4388 (1.9189) Acc G: 0.077%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.7021 (0.2579) Acc D Real: 86.652%
Loss D Fake: 0.1371 (0.2684) Acc D Fake: 98.739%
Loss D: 0.839
Loss G: 2.4206 (1.9265) Acc G: 0.076%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.7748 (0.2656) Acc D Real: 86.339%
Loss D Fake: 0.1385 (0.2665) Acc D Fake: 98.758%
Loss D: 0.913
Loss G: 2.4016 (1.9336) Acc G: 0.075%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.8222 (0.2738) Acc D Real: 86.006%
Loss D Fake: 0.1401 (0.2646) Acc D Fake: 98.776%
Loss D: 0.962
Loss G: 2.3817 (1.9402) Acc G: 0.074%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.6072 (0.2786) Acc D Real: 85.822%
Loss D Fake: 0.1418 (0.2628) Acc D Fake: 98.794%
Loss D: 0.749
Loss G: 2.3621 (1.9463) Acc G: 0.072%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.8136 (0.2862) Acc D Real: 85.490%
Loss D Fake: 0.1435 (0.2611) Acc D Fake: 98.811%
Loss D: 0.957
Loss G: 2.3419 (1.9520) Acc G: 0.071%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.6204 (0.2909) Acc D Real: 85.294%
Loss D Fake: 0.1454 (0.2595) Acc D Fake: 98.828%
Loss D: 0.766
Loss G: 2.3221 (1.9572) Acc G: 0.070%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 1.0336 (0.3013) Acc D Real: 84.841%
Loss D Fake: 0.1473 (0.2580) Acc D Fake: 98.844%
Loss D: 1.181
Loss G: 2.3008 (1.9620) Acc G: 0.069%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.9244 (0.3098) Acc D Real: 84.473%
Loss D Fake: 0.1495 (0.2565) Acc D Fake: 98.860%
Loss D: 1.074
Loss G: 2.2782 (1.9663) Acc G: 0.068%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.7037 (0.3151) Acc D Real: 84.246%
Loss D Fake: 0.1517 (0.2550) Acc D Fake: 98.875%
Loss D: 0.855
Loss G: 2.2561 (1.9702) Acc G: 0.068%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.6100 (0.3190) Acc D Real: 84.062%
Loss D Fake: 0.1540 (0.2537) Acc D Fake: 98.890%
Loss D: 0.764
Loss G: 2.2350 (1.9737) Acc G: 0.067%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.6616 (0.3236) Acc D Real: 83.851%
Loss D Fake: 0.1562 (0.2524) Acc D Fake: 98.905%
Loss D: 0.818
Loss G: 2.2149 (1.9769) Acc G: 0.066%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.9993 (0.3323) Acc D Real: 83.435%
Loss D Fake: 0.1584 (0.2512) Acc D Fake: 98.919%
Loss D: 1.158
Loss G: 2.1937 (1.9797) Acc G: 0.065%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.6583 (0.3365) Acc D Real: 83.243%
Loss D Fake: 0.1608 (0.2500) Acc D Fake: 98.933%
Loss D: 0.819
Loss G: 2.1729 (1.9822) Acc G: 0.064%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.6327 (0.3403) Acc D Real: 83.058%
Loss D Fake: 0.1632 (0.2489) Acc D Fake: 98.946%
Loss D: 0.796
Loss G: 2.1528 (1.9844) Acc G: 0.063%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.9949 (0.3484) Acc D Real: 82.667%
Loss D Fake: 0.1656 (0.2479) Acc D Fake: 98.960%
Loss D: 1.161
Loss G: 2.1318 (1.9862) Acc G: 0.062%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.8930 (0.3552) Acc D Real: 82.349%
Loss D Fake: 0.1682 (0.2469) Acc D Fake: 98.972%
Loss D: 1.061
Loss G: 2.1104 (1.9877) Acc G: 0.062%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.9123 (0.3620) Acc D Real: 82.019%
Loss D Fake: 0.1709 (0.2460) Acc D Fake: 98.985%
Loss D: 1.083
Loss G: 2.0889 (1.9890) Acc G: 0.061%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.8842 (0.3683) Acc D Real: 81.702%
Loss D Fake: 0.1737 (0.2451) Acc D Fake: 98.997%
Loss D: 1.058
Loss G: 2.0673 (1.9899) Acc G: 0.060%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.4962 (0.3698) Acc D Real: 81.621%
Loss D Fake: 0.1764 (0.2443) Acc D Fake: 99.009%
Loss D: 0.673
Loss G: 2.0476 (1.9906) Acc G: 0.060%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.7112 (0.3738) Acc D Real: 81.403%
Loss D Fake: 0.1790 (0.2435) Acc D Fake: 99.021%
Loss D: 0.890
Loss G: 2.0283 (1.9911) Acc G: 0.059%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.7671 (0.3784) Acc D Real: 81.154%
Loss D Fake: 0.1825 (0.2428) Acc D Fake: 99.032%
Loss D: 0.950
Loss G: 1.9961 (1.9911) Acc G: 0.058%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.4812 (0.3795) Acc D Real: 81.087%
Loss D Fake: 0.1880 (0.2422) Acc D Fake: 99.043%
Loss D: 0.669
Loss G: 1.9639 (1.9908) Acc G: 0.057%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.5898 (0.3819) Acc D Real: 80.953%
Loss D Fake: 0.1930 (0.2416) Acc D Fake: 99.054%
Loss D: 0.783
Loss G: 1.9351 (1.9902) Acc G: 0.057%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.7182 (0.3857) Acc D Real: 80.742%
Loss D Fake: 0.1976 (0.2411) Acc D Fake: 99.065%
Loss D: 0.916
Loss G: 1.9090 (1.9893) Acc G: 0.056%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.5585 (0.3876) Acc D Real: 80.627%
Loss D Fake: 0.2019 (0.2407) Acc D Fake: 99.075%
Loss D: 0.760
Loss G: 1.8856 (1.9881) Acc G: 0.056%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.6161 (0.3901) Acc D Real: 80.465%
Loss D Fake: 0.2058 (0.2403) Acc D Fake: 99.085%
Loss D: 0.822
Loss G: 1.8641 (1.9867) Acc G: 0.055%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.4397 (0.3907) Acc D Real: 80.417%
Loss D Fake: 0.2095 (0.2400) Acc D Fake: 99.095%
Loss D: 0.649
Loss G: 1.8452 (1.9852) Acc G: 0.054%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.5404 (0.3923) Acc D Real: 80.314%
Loss D Fake: 0.2128 (0.2397) Acc D Fake: 99.105%
Loss D: 0.753
Loss G: 1.8281 (1.9835) Acc G: 0.054%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.5170 (0.3936) Acc D Real: 80.211%
Loss D Fake: 0.2158 (0.2394) Acc D Fake: 99.097%
Loss D: 0.733
Loss G: 1.8121 (1.9817) Acc G: 0.071%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.6399 (0.3962) Acc D Real: 80.043%
Loss D Fake: 0.2189 (0.2392) Acc D Fake: 99.089%
Loss D: 0.859
Loss G: 1.7961 (1.9797) Acc G: 0.105%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.5120 (0.3974) Acc D Real: 79.953%
Loss D Fake: 0.2219 (0.2390) Acc D Fake: 99.064%
Loss D: 0.734
Loss G: 1.7812 (1.9777) Acc G: 0.139%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.5029 (0.3985) Acc D Real: 79.876%
Loss D Fake: 0.2247 (0.2389) Acc D Fake: 99.039%
Loss D: 0.728
Loss G: 1.7674 (1.9755) Acc G: 0.172%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.5838 (0.4004) Acc D Real: 79.749%
Loss D Fake: 0.2274 (0.2388) Acc D Fake: 99.015%
Loss D: 0.811
Loss G: 1.7540 (1.9732) Acc G: 0.204%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3431 (0.3998) Acc D Real: 79.766%
Loss D Fake: 0.2300 (0.2387) Acc D Fake: 98.991%
Loss D: 0.573
Loss G: 1.7421 (1.9709) Acc G: 0.236%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.5557 (0.4014) Acc D Real: 79.665%
Loss D Fake: 0.2324 (0.2386) Acc D Fake: 98.968%
Loss D: 0.788
Loss G: 1.7305 (1.9685) Acc G: 0.267%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.6893 (0.4042) Acc D Real: 79.478%
Loss D Fake: 0.2349 (0.2386) Acc D Fake: 98.945%
Loss D: 0.924
Loss G: 1.7177 (1.9660) Acc G: 0.297%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.4644 (0.4048) Acc D Real: 79.416%
Loss D Fake: 0.2377 (0.2386) Acc D Fake: 98.923%
Loss D: 0.702
Loss G: 1.7053 (1.9635) Acc G: 0.327%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3612 (0.4044) Acc D Real: 79.429%
Loss D Fake: 0.2404 (0.2386) Acc D Fake: 98.901%
Loss D: 0.602
Loss G: 1.6941 (1.9608) Acc G: 0.356%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2153 (0.4026) Acc D Real: 79.530%
Loss D Fake: 0.2427 (0.2386) Acc D Fake: 98.879%
Loss D: 0.458
Loss G: 1.6848 (1.9582) Acc G: 0.385%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.2637 (0.4013) Acc D Real: 79.597%
Loss D Fake: 0.2447 (0.2387) Acc D Fake: 98.858%
Loss D: 0.508
Loss G: 1.6771 (1.9555) Acc G: 0.413%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3217 (0.4005) Acc D Real: 79.631%
Loss D Fake: 0.2463 (0.2388) Acc D Fake: 98.837%
Loss D: 0.568
Loss G: 1.6703 (1.9528) Acc G: 0.456%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4809 (0.4013) Acc D Real: 79.567%
Loss D Fake: 0.2479 (0.2388) Acc D Fake: 98.802%
Loss D: 0.729
Loss G: 1.6632 (1.9501) Acc G: 0.498%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2698 (0.4000) Acc D Real: 79.633%
Loss D Fake: 0.2496 (0.2389) Acc D Fake: 98.766%
Loss D: 0.519
Loss G: 1.6566 (1.9474) Acc G: 0.540%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.2685 (0.3988) Acc D Real: 79.689%
Loss D Fake: 0.2511 (0.2391) Acc D Fake: 98.732%
Loss D: 0.520
Loss G: 1.6508 (1.9447) Acc G: 0.581%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.2295 (0.3973) Acc D Real: 79.779%
Loss D Fake: 0.2524 (0.2392) Acc D Fake: 98.698%
Loss D: 0.482
Loss G: 1.6460 (1.9420) Acc G: 0.621%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.2597 (0.3961) Acc D Real: 79.847%
Loss D Fake: 0.2535 (0.2393) Acc D Fake: 98.665%
Loss D: 0.513
Loss G: 1.6420 (1.9393) Acc G: 0.661%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3066 (0.3953) Acc D Real: 79.891%
Loss D Fake: 0.2544 (0.2394) Acc D Fake: 98.632%
Loss D: 0.561
Loss G: 1.6384 (1.9366) Acc G: 0.699%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4402 (0.3957) Acc D Real: 79.835%
Loss D Fake: 0.2553 (0.2396) Acc D Fake: 98.600%
Loss D: 0.696
Loss G: 1.6344 (1.9339) Acc G: 0.737%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.2474 (0.3944) Acc D Real: 79.910%
Loss D Fake: 0.2563 (0.2397) Acc D Fake: 98.568%
Loss D: 0.504
Loss G: 1.6306 (1.9312) Acc G: 0.775%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3051 (0.3936) Acc D Real: 79.942%
Loss D Fake: 0.2571 (0.2399) Acc D Fake: 98.537%
Loss D: 0.562
Loss G: 1.6272 (1.9286) Acc G: 0.812%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4576 (0.3941) Acc D Real: 79.889%
Loss D Fake: 0.2580 (0.2400) Acc D Fake: 98.507%
Loss D: 0.716
Loss G: 1.6233 (1.9260) Acc G: 0.848%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.2456 (0.3929) Acc D Real: 79.956%
Loss D Fake: 0.2589 (0.2402) Acc D Fake: 98.477%
Loss D: 0.505
Loss G: 1.6196 (1.9233) Acc G: 0.883%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3991 (0.3929) Acc D Real: 79.943%
Loss D Fake: 0.2598 (0.2404) Acc D Fake: 98.447%
Loss D: 0.659
Loss G: 1.6158 (1.9207) Acc G: 0.918%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3654 (0.3927) Acc D Real: 79.948%
Loss D Fake: 0.2608 (0.2405) Acc D Fake: 98.418%
Loss D: 0.626
Loss G: 1.6119 (1.9181) Acc G: 0.952%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2873 (0.3918) Acc D Real: 79.993%
Loss D Fake: 0.2618 (0.2407) Acc D Fake: 98.390%
Loss D: 0.549
Loss G: 1.6083 (1.9156) Acc G: 0.986%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.1750 (0.3900) Acc D Real: 80.093%
Loss D Fake: 0.2626 (0.2409) Acc D Fake: 98.362%
Loss D: 0.438
Loss G: 1.6057 (1.9130) Acc G: 1.019%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4166 (0.3902) Acc D Real: 80.064%
Loss D Fake: 0.2634 (0.2411) Acc D Fake: 98.334%
Loss D: 0.680
Loss G: 1.6014 (1.9104) Acc G: 1.052%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3447 (0.3899) Acc D Real: 80.068%
Loss D Fake: 0.2649 (0.2413) Acc D Fake: 98.307%
Loss D: 0.610
Loss G: 1.5959 (1.9079) Acc G: 1.084%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.4016 (0.3900) Acc D Real: 80.059%
Loss D Fake: 0.2666 (0.2415) Acc D Fake: 98.280%
Loss D: 0.668
Loss G: 1.5901 (1.9053) Acc G: 1.116%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.2401 (0.3888) Acc D Real: 80.124%
Loss D Fake: 0.2684 (0.2417) Acc D Fake: 98.254%
Loss D: 0.509
Loss G: 1.5839 (1.9028) Acc G: 1.147%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.4720 (0.3894) Acc D Real: 80.064%
Loss D Fake: 0.2704 (0.2419) Acc D Fake: 98.228%
Loss D: 0.742
Loss G: 1.5772 (1.9002) Acc G: 1.177%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3408 (0.3890) Acc D Real: 80.073%
Loss D Fake: 0.2725 (0.2422) Acc D Fake: 98.203%
Loss D: 0.613
Loss G: 1.5707 (1.8976) Acc G: 1.207%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.2866 (0.3882) Acc D Real: 80.113%
Loss D Fake: 0.2744 (0.2424) Acc D Fake: 98.178%
Loss D: 0.561
Loss G: 1.5649 (1.8950) Acc G: 1.250%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.5245 (0.3893) Acc D Real: 80.036%
Loss D Fake: 0.2761 (0.2427) Acc D Fake: 98.140%
Loss D: 0.801
Loss G: 1.5590 (1.8924) Acc G: 1.292%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3840 (0.3892) Acc D Real: 80.026%
Loss D Fake: 0.2778 (0.2429) Acc D Fake: 98.103%
Loss D: 0.662
Loss G: 1.5533 (1.8898) Acc G: 1.333%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.1663 (0.3875) Acc D Real: 80.127%
Loss D Fake: 0.2793 (0.2432) Acc D Fake: 98.067%
Loss D: 0.446
Loss G: 1.5488 (1.8872) Acc G: 1.374%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3279 (0.3871) Acc D Real: 80.140%
Loss D Fake: 0.2805 (0.2435) Acc D Fake: 98.031%
Loss D: 0.608
Loss G: 1.5444 (1.8846) Acc G: 1.414%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.4154 (0.3873) Acc D Real: 80.114%
Loss D Fake: 0.2818 (0.2438) Acc D Fake: 97.996%
Loss D: 0.697
Loss G: 1.5399 (1.8820) Acc G: 1.454%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2497 (0.3863) Acc D Real: 80.167%
Loss D Fake: 0.2833 (0.2441) Acc D Fake: 97.961%
Loss D: 0.533
Loss G: 1.5342 (1.8794) Acc G: 1.493%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3456 (0.3860) Acc D Real: 80.175%
Loss D Fake: 0.2852 (0.2444) Acc D Fake: 97.927%
Loss D: 0.631
Loss G: 1.5286 (1.8768) Acc G: 1.531%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3303 (0.3856) Acc D Real: 80.187%
Loss D Fake: 0.2870 (0.2447) Acc D Fake: 97.893%
Loss D: 0.617
Loss G: 1.5235 (1.8742) Acc G: 1.569%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3116 (0.3850) Acc D Real: 80.208%
Loss D Fake: 0.2885 (0.2450) Acc D Fake: 97.860%
Loss D: 0.600
Loss G: 1.5188 (1.8716) Acc G: 1.606%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.1738 (0.3835) Acc D Real: 80.292%
Loss D Fake: 0.2906 (0.2454) Acc D Fake: 97.827%
Loss D: 0.464
Loss G: 1.5103 (1.8690) Acc G: 1.643%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.2378 (0.3824) Acc D Real: 80.344%
Loss D Fake: 0.2945 (0.2457) Acc D Fake: 97.795%
Loss D: 0.532
Loss G: 1.5011 (1.8663) Acc G: 1.679%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3230 (0.3820) Acc D Real: 80.348%
Loss D Fake: 0.2982 (0.2461) Acc D Fake: 97.763%
Loss D: 0.621
Loss G: 1.4920 (1.8637) Acc G: 1.714%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.2035 (0.3808) Acc D Real: 80.418%
Loss D Fake: 0.3017 (0.2465) Acc D Fake: 97.731%
Loss D: 0.505
Loss G: 1.4835 (1.8610) Acc G: 1.749%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.2014 (0.3795) Acc D Real: 80.487%
Loss D Fake: 0.3049 (0.2469) Acc D Fake: 97.700%
Loss D: 0.506
Loss G: 1.4767 (1.8582) Acc G: 1.784%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2479 (0.3786) Acc D Real: 80.537%
Loss D Fake: 0.3074 (0.2473) Acc D Fake: 97.670%
Loss D: 0.555
Loss G: 1.4715 (1.8555) Acc G: 1.818%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3372 (0.3783) Acc D Real: 80.541%
Loss D Fake: 0.3093 (0.2477) Acc D Fake: 97.640%
Loss D: 0.646
Loss G: 1.4679 (1.8529) Acc G: 1.852%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2562 (0.3774) Acc D Real: 80.579%
Loss D Fake: 0.3104 (0.2482) Acc D Fake: 97.610%
Loss D: 0.567
Loss G: 1.4660 (1.8502) Acc G: 1.885%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.2536 (0.3766) Acc D Real: 80.615%
Loss D Fake: 0.3107 (0.2486) Acc D Fake: 97.581%
Loss D: 0.564
Loss G: 1.4657 (1.8476) Acc G: 1.918%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.2927 (0.3760) Acc D Real: 80.635%
Loss D Fake: 0.3106 (0.2490) Acc D Fake: 97.552%
Loss D: 0.603
Loss G: 1.4658 (1.8450) Acc G: 1.950%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.1781 (0.3747) Acc D Real: 80.711%
Loss D Fake: 0.3101 (0.2494) Acc D Fake: 97.523%
Loss D: 0.488
Loss G: 1.4672 (1.8424) Acc G: 1.982%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.4012 (0.3749) Acc D Real: 80.692%
Loss D Fake: 0.3092 (0.2498) Acc D Fake: 97.495%
Loss D: 0.710
Loss G: 1.4686 (1.8399) Acc G: 2.013%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.2321 (0.3739) Acc D Real: 80.734%
Loss D Fake: 0.3084 (0.2502) Acc D Fake: 97.467%
Loss D: 0.541
Loss G: 1.4703 (1.8374) Acc G: 2.044%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.2040 (0.3728) Acc D Real: 80.794%
Loss D Fake: 0.3074 (0.2506) Acc D Fake: 97.440%
Loss D: 0.511
Loss G: 1.4725 (1.8350) Acc G: 2.075%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3849 (0.3729) Acc D Real: 80.779%
Loss D Fake: 0.3066 (0.2510) Acc D Fake: 97.413%
Loss D: 0.691
Loss G: 1.4724 (1.8326) Acc G: 2.105%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2997 (0.3724) Acc D Real: 80.793%
Loss D Fake: 0.3077 (0.2513) Acc D Fake: 97.386%
Loss D: 0.607
Loss G: 1.4653 (1.8302) Acc G: 2.135%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.2267 (0.3714) Acc D Real: 80.843%
Loss D Fake: 0.3115 (0.2517) Acc D Fake: 97.349%
Loss D: 0.538
Loss G: 1.4571 (1.8278) Acc G: 2.175%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3302 (0.3712) Acc D Real: 80.843%
Loss D Fake: 0.3147 (0.2521) Acc D Fake: 97.312%
Loss D: 0.645
Loss G: 1.4510 (1.8254) Acc G: 2.215%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.1999 (0.3701) Acc D Real: 80.898%
Loss D Fake: 0.3170 (0.2526) Acc D Fake: 97.276%
Loss D: 0.517
Loss G: 1.4457 (1.8229) Acc G: 2.254%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.2795 (0.3695) Acc D Real: 80.927%
Loss D Fake: 0.3186 (0.2530) Acc D Fake: 97.241%
Loss D: 0.598
Loss G: 1.4429 (1.8205) Acc G: 2.293%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3208 (0.3692) Acc D Real: 80.934%
Loss D Fake: 0.3188 (0.2534) Acc D Fake: 97.205%
Loss D: 0.640
Loss G: 1.4424 (1.8181) Acc G: 2.331%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.1941 (0.3681) Acc D Real: 80.999%
Loss D Fake: 0.3181 (0.2538) Acc D Fake: 97.170%
Loss D: 0.512
Loss G: 1.4441 (1.8158) Acc G: 2.369%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.1236 (0.3666) Acc D Real: 81.087%
Loss D Fake: 0.3164 (0.2542) Acc D Fake: 97.136%
Loss D: 0.440
Loss G: 1.4477 (1.8135) Acc G: 2.406%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.1548 (0.3653) Acc D Real: 81.166%
Loss D Fake: 0.3144 (0.2546) Acc D Fake: 97.102%
Loss D: 0.469
Loss G: 1.4517 (1.8112) Acc G: 2.443%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.1558 (0.3640) Acc D Real: 81.235%
Loss D Fake: 0.3124 (0.2549) Acc D Fake: 97.069%
Loss D: 0.468
Loss G: 1.4559 (1.8090) Acc G: 2.479%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.2178 (0.3631) Acc D Real: 81.270%
Loss D Fake: 0.3106 (0.2553) Acc D Fake: 97.035%
Loss D: 0.528
Loss G: 1.4594 (1.8069) Acc G: 2.515%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3031 (0.3627) Acc D Real: 81.291%
Loss D Fake: 0.3090 (0.2556) Acc D Fake: 97.003%
Loss D: 0.612
Loss G: 1.4624 (1.8048) Acc G: 2.551%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1973 (0.3617) Acc D Real: 81.340%
Loss D Fake: 0.3076 (0.2559) Acc D Fake: 96.970%
Loss D: 0.505
Loss G: 1.4653 (1.8027) Acc G: 2.586%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.4102 (0.3620) Acc D Real: 81.303%
Loss D Fake: 0.3063 (0.2562) Acc D Fake: 96.938%
Loss D: 0.716
Loss G: 1.4673 (1.8007) Acc G: 2.620%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2320 (0.3612) Acc D Real: 81.338%
Loss D Fake: 0.3053 (0.2565) Acc D Fake: 96.907%
Loss D: 0.537
Loss G: 1.4689 (1.7987) Acc G: 2.655%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.2366 (0.3605) Acc D Real: 81.378%
Loss D Fake: 0.3044 (0.2568) Acc D Fake: 96.876%
Loss D: 0.541
Loss G: 1.4704 (1.7968) Acc G: 2.688%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.2888 (0.3600) Acc D Real: 81.396%
Loss D Fake: 0.3036 (0.2571) Acc D Fake: 96.845%
Loss D: 0.592
Loss G: 1.4715 (1.7948) Acc G: 2.722%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3073 (0.3597) Acc D Real: 81.403%
Loss D Fake: 0.3031 (0.2573) Acc D Fake: 96.814%
Loss D: 0.610
Loss G: 1.4721 (1.7929) Acc G: 2.755%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3781 (0.3598) Acc D Real: 81.379%
Loss D Fake: 0.3033 (0.2576) Acc D Fake: 96.784%
Loss D: 0.681
Loss G: 1.4667 (1.7910) Acc G: 2.788%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3371 (0.3597) Acc D Real: 81.379%
Loss D Fake: 0.3052 (0.2579) Acc D Fake: 96.754%
Loss D: 0.642
Loss G: 1.4602 (1.7891) Acc G: 2.820%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.4482 (0.3602) Acc D Real: 81.327%
Loss D Fake: 0.3070 (0.2582) Acc D Fake: 96.725%
Loss D: 0.755
Loss G: 1.4538 (1.7872) Acc G: 2.852%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1913 (0.3592) Acc D Real: 81.382%
Loss D Fake: 0.3087 (0.2585) Acc D Fake: 96.696%
Loss D: 0.500
Loss G: 1.4485 (1.7852) Acc G: 2.883%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.2595 (0.3587) Acc D Real: 81.409%
Loss D Fake: 0.3101 (0.2588) Acc D Fake: 96.667%
Loss D: 0.570
Loss G: 1.4431 (1.7833) Acc G: 2.914%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.2499 (0.3581) Acc D Real: 81.431%
Loss D Fake: 0.3117 (0.2591) Acc D Fake: 96.639%
Loss D: 0.562
Loss G: 1.4383 (1.7813) Acc G: 2.945%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2919 (0.3577) Acc D Real: 81.443%
Loss D Fake: 0.3129 (0.2594) Acc D Fake: 96.611%
Loss D: 0.605
Loss G: 1.4343 (1.7794) Acc G: 2.976%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.1606 (0.3566) Acc D Real: 81.507%
Loss D Fake: 0.3139 (0.2597) Acc D Fake: 96.583%
Loss D: 0.474
Loss G: 1.4316 (1.7774) Acc G: 3.006%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.2146 (0.3558) Acc D Real: 81.546%
Loss D Fake: 0.3144 (0.2600) Acc D Fake: 96.556%
Loss D: 0.529
Loss G: 1.4299 (1.7755) Acc G: 3.035%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.1526 (0.3547) Acc D Real: 81.610%
Loss D Fake: 0.3147 (0.2603) Acc D Fake: 96.528%
Loss D: 0.467
Loss G: 1.4291 (1.7735) Acc G: 3.065%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.1267 (0.3534) Acc D Real: 81.690%
Loss D Fake: 0.3147 (0.2606) Acc D Fake: 96.501%
Loss D: 0.441
Loss G: 1.4293 (1.7716) Acc G: 3.094%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.2224 (0.3527) Acc D Real: 81.728%
Loss D Fake: 0.3148 (0.2609) Acc D Fake: 96.475%
Loss D: 0.537
Loss G: 1.4272 (1.7697) Acc G: 3.123%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.1587 (0.3516) Acc D Real: 81.784%
Loss D Fake: 0.3157 (0.2612) Acc D Fake: 96.449%
Loss D: 0.474
Loss G: 1.4253 (1.7679) Acc G: 3.151%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.1944 (0.3508) Acc D Real: 81.833%
Loss D Fake: 0.3162 (0.2615) Acc D Fake: 96.423%
Loss D: 0.511
Loss G: 1.4244 (1.7660) Acc G: 3.179%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2708 (0.3503) Acc D Real: 81.852%
Loss D Fake: 0.3164 (0.2618) Acc D Fake: 96.397%
Loss D: 0.587
Loss G: 1.4239 (1.7641) Acc G: 3.207%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.2455 (0.3498) Acc D Real: 81.885%
Loss D Fake: 0.3165 (0.2621) Acc D Fake: 96.372%
Loss D: 0.562
Loss G: 1.4238 (1.7623) Acc G: 3.235%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3598 (0.3498) Acc D Real: 81.870%
Loss D Fake: 0.3164 (0.2624) Acc D Fake: 96.346%
Loss D: 0.676
Loss G: 1.4235 (1.7605) Acc G: 3.262%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.2317 (0.3492) Acc D Real: 81.904%
Loss D Fake: 0.3164 (0.2626) Acc D Fake: 96.321%
Loss D: 0.548
Loss G: 1.4232 (1.7587) Acc G: 3.289%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.1578 (0.3482) Acc D Real: 81.963%
Loss D Fake: 0.3163 (0.2629) Acc D Fake: 96.297%
Loss D: 0.474
Loss G: 1.4238 (1.7569) Acc G: 3.316%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.2471 (0.3476) Acc D Real: 81.987%
Loss D Fake: 0.3159 (0.2632) Acc D Fake: 96.272%
Loss D: 0.563
Loss G: 1.4246 (1.7552) Acc G: 3.342%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.2335 (0.3471) Acc D Real: 82.014%
Loss D Fake: 0.3155 (0.2635) Acc D Fake: 96.248%
Loss D: 0.549
Loss G: 1.4253 (1.7535) Acc G: 3.368%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.2777 (0.3467) Acc D Real: 82.031%
Loss D Fake: 0.3152 (0.2637) Acc D Fake: 96.225%
Loss D: 0.593
Loss G: 1.4257 (1.7517) Acc G: 3.394%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.2178 (0.3460) Acc D Real: 82.066%
Loss D Fake: 0.3151 (0.2640) Acc D Fake: 96.201%
Loss D: 0.533
Loss G: 1.4251 (1.7501) Acc G: 3.420%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2971 (0.3458) Acc D Real: 82.076%
Loss D Fake: 0.3154 (0.2643) Acc D Fake: 96.178%
Loss D: 0.612
Loss G: 1.4245 (1.7484) Acc G: 3.445%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.1354 (0.3447) Acc D Real: 82.137%
Loss D Fake: 0.3155 (0.2645) Acc D Fake: 96.154%
Loss D: 0.451
Loss G: 1.4245 (1.7467) Acc G: 3.470%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.1559 (0.3437) Acc D Real: 82.190%
Loss D Fake: 0.3154 (0.2648) Acc D Fake: 96.131%
Loss D: 0.471
Loss G: 1.4253 (1.7451) Acc G: 3.495%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3784 (0.3439) Acc D Real: 82.175%
Loss D Fake: 0.3151 (0.2651) Acc D Fake: 96.109%
Loss D: 0.693
Loss G: 1.4257 (1.7435) Acc G: 3.519%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.1752 (0.3431) Acc D Real: 82.222%
Loss D Fake: 0.3149 (0.2653) Acc D Fake: 96.086%
Loss D: 0.490
Loss G: 1.4263 (1.7419) Acc G: 3.544%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2313 (0.3425) Acc D Real: 82.255%
Loss D Fake: 0.3147 (0.2656) Acc D Fake: 96.064%
Loss D: 0.546
Loss G: 1.4268 (1.7403) Acc G: 3.568%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2480 (0.3420) Acc D Real: 82.274%
Loss D Fake: 0.3146 (0.2658) Acc D Fake: 96.042%
Loss D: 0.563
Loss G: 1.4264 (1.7387) Acc G: 3.592%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2404 (0.3415) Acc D Real: 82.298%
Loss D Fake: 0.3150 (0.2660) Acc D Fake: 96.020%
Loss D: 0.555
Loss G: 1.4260 (1.7371) Acc G: 3.615%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1874 (0.3407) Acc D Real: 82.339%
Loss D Fake: 0.3151 (0.2663) Acc D Fake: 95.999%
Loss D: 0.502
Loss G: 1.4261 (1.7356) Acc G: 3.639%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.4066 (0.3411) Acc D Real: 82.305%
Loss D Fake: 0.3151 (0.2665) Acc D Fake: 95.978%
Loss D: 0.722
Loss G: 1.4252 (1.7341) Acc G: 3.662%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.2414 (0.3406) Acc D Real: 82.328%
Loss D Fake: 0.3155 (0.2668) Acc D Fake: 95.956%
Loss D: 0.557
Loss G: 1.4244 (1.7326) Acc G: 3.685%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2579 (0.3402) Acc D Real: 82.344%
Loss D Fake: 0.3157 (0.2670) Acc D Fake: 95.935%
Loss D: 0.574
Loss G: 1.4231 (1.7310) Acc G: 3.707%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.2890 (0.3399) Acc D Real: 82.351%
Loss D Fake: 0.3165 (0.2673) Acc D Fake: 95.915%
Loss D: 0.606
Loss G: 1.4189 (1.7295) Acc G: 3.730%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.2368 (0.3394) Acc D Real: 82.374%
Loss D Fake: 0.3183 (0.2675) Acc D Fake: 95.894%
Loss D: 0.555
Loss G: 1.4147 (1.7280) Acc G: 3.752%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.0720 (0.3381) Acc D Real: 82.452%
Loss D Fake: 0.3195 (0.2677) Acc D Fake: 95.874%
Loss D: 0.392
Loss G: 1.4125 (1.7265) Acc G: 3.774%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1866 (0.3374) Acc D Real: 82.489%
Loss D Fake: 0.3201 (0.2680) Acc D Fake: 95.854%
Loss D: 0.507
Loss G: 1.4114 (1.7250) Acc G: 3.796%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.2918 (0.3372) Acc D Real: 82.498%
Loss D Fake: 0.3204 (0.2682) Acc D Fake: 95.834%
Loss D: 0.612
Loss G: 1.4103 (1.7235) Acc G: 3.817%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2291 (0.3367) Acc D Real: 82.523%
Loss D Fake: 0.3208 (0.2685) Acc D Fake: 95.814%
Loss D: 0.550
Loss G: 1.4092 (1.7220) Acc G: 3.839%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.2797 (0.3364) Acc D Real: 82.529%
Loss D Fake: 0.3212 (0.2687) Acc D Fake: 95.795%
Loss D: 0.601
Loss G: 1.4079 (1.7205) Acc G: 3.860%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3145 (0.3363) Acc D Real: 82.532%
Loss D Fake: 0.3218 (0.2690) Acc D Fake: 95.775%
Loss D: 0.636
Loss G: 1.4054 (1.7190) Acc G: 3.881%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.2113 (0.3357) Acc D Real: 82.560%
Loss D Fake: 0.3228 (0.2692) Acc D Fake: 95.756%
Loss D: 0.534
Loss G: 1.4034 (1.7176) Acc G: 3.902%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.2429 (0.3353) Acc D Real: 82.579%
Loss D Fake: 0.3233 (0.2695) Acc D Fake: 95.737%
Loss D: 0.566
Loss G: 1.4019 (1.7161) Acc G: 3.922%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.0815 (0.3341) Acc D Real: 82.649%
Loss D Fake: 0.3236 (0.2697) Acc D Fake: 95.718%
Loss D: 0.405
Loss G: 1.4017 (1.7146) Acc G: 3.943%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2096 (0.3336) Acc D Real: 82.679%
Loss D Fake: 0.3235 (0.2700) Acc D Fake: 95.699%
Loss D: 0.533
Loss G: 1.4018 (1.7132) Acc G: 3.963%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2334 (0.3331) Acc D Real: 82.701%
Loss D Fake: 0.3233 (0.2702) Acc D Fake: 95.681%
Loss D: 0.557
Loss G: 1.4022 (1.7118) Acc G: 3.983%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.1536 (0.3323) Acc D Real: 82.747%
Loss D Fake: 0.3230 (0.2705) Acc D Fake: 95.663%
Loss D: 0.477
Loss G: 1.4030 (1.7104) Acc G: 4.003%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2327 (0.3318) Acc D Real: 82.770%
Loss D Fake: 0.3228 (0.2707) Acc D Fake: 95.644%
Loss D: 0.556
Loss G: 1.4020 (1.7090) Acc G: 4.023%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.1554 (0.3310) Acc D Real: 82.821%
Loss D Fake: 0.3234 (0.2710) Acc D Fake: 95.626%
Loss D: 0.479
Loss G: 1.4014 (1.7076) Acc G: 4.042%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.1566 (0.3302) Acc D Real: 82.870%
Loss D Fake: 0.3236 (0.2712) Acc D Fake: 95.609%
Loss D: 0.480
Loss G: 1.4017 (1.7062) Acc G: 4.062%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2044 (0.3297) Acc D Real: 82.897%
Loss D Fake: 0.3237 (0.2714) Acc D Fake: 95.591%
Loss D: 0.528
Loss G: 1.4000 (1.7048) Acc G: 4.081%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.1100 (0.3287) Acc D Real: 82.956%
Loss D Fake: 0.3245 (0.2717) Acc D Fake: 95.566%
Loss D: 0.434
Loss G: 1.3989 (1.7035) Acc G: 4.107%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.1802 (0.3280) Acc D Real: 82.997%
Loss D Fake: 0.3247 (0.2719) Acc D Fake: 95.541%
Loss D: 0.505
Loss G: 1.3990 (1.7021) Acc G: 4.133%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.2234 (0.3276) Acc D Real: 83.003%
Loss D Fake: 0.3246 (0.2721) Acc D Fake: 95.535%
Loss D: 0.548
Loss G: 1.3996 (1.7008) Acc G: 4.140%
LR: 2.000e-04
Epoch: 4/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.0865 (0.1472) Acc D Real: 93.255%
Loss D Fake: 0.3236 (0.3239) Acc D Fake: 90.000%
Loss D: 0.410
Loss G: 1.4030 (1.4019) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3397 (0.2114) Acc D Real: 88.958%
Loss D Fake: 0.3228 (0.3235) Acc D Fake: 90.000%
Loss D: 0.663
Loss G: 1.4038 (1.4025) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2208 (0.2137) Acc D Real: 88.763%
Loss D Fake: 0.3227 (0.3233) Acc D Fake: 90.000%
Loss D: 0.544
Loss G: 1.4022 (1.4024) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2372 (0.2184) Acc D Real: 88.583%
Loss D Fake: 0.3233 (0.3233) Acc D Fake: 90.000%
Loss D: 0.560
Loss G: 1.4003 (1.4020) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2119 (0.2173) Acc D Real: 88.750%
Loss D Fake: 0.3237 (0.3234) Acc D Fake: 90.000%
Loss D: 0.536
Loss G: 1.3989 (1.4015) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.0954 (0.1999) Acc D Real: 89.918%
Loss D Fake: 0.3239 (0.3235) Acc D Fake: 90.000%
Loss D: 0.419
Loss G: 1.3985 (1.4011) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3112 (0.2138) Acc D Real: 88.939%
Loss D Fake: 0.3239 (0.3235) Acc D Fake: 90.000%
Loss D: 0.635
Loss G: 1.3977 (1.4006) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2748 (0.2206) Acc D Real: 88.403%
Loss D Fake: 0.3241 (0.3236) Acc D Fake: 90.000%
Loss D: 0.599
Loss G: 1.3968 (1.4002) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2965 (0.2282) Acc D Real: 87.885%
Loss D Fake: 0.3244 (0.3237) Acc D Fake: 90.000%
Loss D: 0.621
Loss G: 1.3956 (1.3997) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1698 (0.2229) Acc D Real: 88.163%
Loss D Fake: 0.3248 (0.3238) Acc D Fake: 90.000%
Loss D: 0.495
Loss G: 1.3940 (1.3992) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.1971 (0.2207) Acc D Real: 88.312%
Loss D Fake: 0.3252 (0.3239) Acc D Fake: 90.000%
Loss D: 0.522
Loss G: 1.3929 (1.3987) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3319 (0.2293) Acc D Real: 87.688%
Loss D Fake: 0.3255 (0.3240) Acc D Fake: 90.000%
Loss D: 0.657
Loss G: 1.3919 (1.3982) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2006 (0.2272) Acc D Real: 87.891%
Loss D Fake: 0.3257 (0.3241) Acc D Fake: 90.000%
Loss D: 0.526
Loss G: 1.3914 (1.3977) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2792 (0.2307) Acc D Real: 87.639%
Loss D Fake: 0.3257 (0.3242) Acc D Fake: 90.000%
Loss D: 0.605
Loss G: 1.3906 (1.3972) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2127 (0.2296) Acc D Real: 87.757%
Loss D Fake: 0.3259 (0.3243) Acc D Fake: 90.000%
Loss D: 0.539
Loss G: 1.3900 (1.3968) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.2130 (0.2286) Acc D Real: 87.791%
Loss D Fake: 0.3260 (0.3244) Acc D Fake: 90.000%
Loss D: 0.539
Loss G: 1.3897 (1.3963) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2722 (0.2310) Acc D Real: 87.624%
Loss D Fake: 0.3261 (0.3245) Acc D Fake: 90.000%
Loss D: 0.598
Loss G: 1.3893 (1.3960) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.2307 (0.2310) Acc D Real: 87.560%
Loss D Fake: 0.3261 (0.3246) Acc D Fake: 90.000%
Loss D: 0.557
Loss G: 1.3885 (1.3956) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1947 (0.2292) Acc D Real: 87.677%
Loss D Fake: 0.3262 (0.3247) Acc D Fake: 90.000%
Loss D: 0.521
Loss G: 1.3878 (1.3952) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.1521 (0.2255) Acc D Real: 87.966%
Loss D Fake: 0.3263 (0.3248) Acc D Fake: 90.000%
Loss D: 0.478
Loss G: 1.3876 (1.3948) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1825 (0.2236) Acc D Real: 88.106%
Loss D Fake: 0.3262 (0.3248) Acc D Fake: 90.000%
Loss D: 0.509
Loss G: 1.3877 (1.3945) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1421 (0.2200) Acc D Real: 88.361%
Loss D Fake: 0.3261 (0.3249) Acc D Fake: 90.000%
Loss D: 0.468
Loss G: 1.3883 (1.3942) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.1082 (0.2154) Acc D Real: 88.704%
Loss D Fake: 0.3257 (0.3249) Acc D Fake: 90.000%
Loss D: 0.434
Loss G: 1.3897 (1.3940) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.1535 (0.2129) Acc D Real: 88.908%
Loss D Fake: 0.3251 (0.3249) Acc D Fake: 90.000%
Loss D: 0.479
Loss G: 1.3916 (1.3939) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2174 (0.2131) Acc D Real: 88.900%
Loss D Fake: 0.3244 (0.3249) Acc D Fake: 90.000%
Loss D: 0.542
Loss G: 1.3934 (1.3939) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.1593 (0.2111) Acc D Real: 89.097%
Loss D Fake: 0.3238 (0.3249) Acc D Fake: 90.000%
Loss D: 0.483
Loss G: 1.3954 (1.3940) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.1127 (0.2076) Acc D Real: 89.327%
Loss D Fake: 0.3232 (0.3248) Acc D Fake: 90.000%
Loss D: 0.436
Loss G: 1.3974 (1.3941) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.1221 (0.2046) Acc D Real: 89.547%
Loss D Fake: 0.3225 (0.3247) Acc D Fake: 90.000%
Loss D: 0.445
Loss G: 1.3998 (1.3943) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.1931 (0.2042) Acc D Real: 89.571%
Loss D Fake: 0.3218 (0.3246) Acc D Fake: 90.000%
Loss D: 0.515
Loss G: 1.4019 (1.3945) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.1919 (0.2038) Acc D Real: 89.617%
Loss D Fake: 0.3212 (0.3245) Acc D Fake: 90.000%
Loss D: 0.513
Loss G: 1.4037 (1.3948) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1760 (0.2030) Acc D Real: 89.657%
Loss D Fake: 0.3207 (0.3244) Acc D Fake: 90.000%
Loss D: 0.497
Loss G: 1.4046 (1.3951) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2296 (0.2038) Acc D Real: 89.637%
Loss D Fake: 0.3205 (0.3243) Acc D Fake: 90.000%
Loss D: 0.550
Loss G: 1.4050 (1.3954) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.1270 (0.2015) Acc D Real: 89.786%
Loss D Fake: 0.3203 (0.3242) Acc D Fake: 90.000%
Loss D: 0.447
Loss G: 1.4057 (1.3957) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.1246 (0.1993) Acc D Real: 89.960%
Loss D Fake: 0.3201 (0.3241) Acc D Fake: 90.000%
Loss D: 0.445
Loss G: 1.4067 (1.3961) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.1553 (0.1981) Acc D Real: 90.067%
Loss D Fake: 0.3199 (0.3239) Acc D Fake: 90.000%
Loss D: 0.475
Loss G: 1.4072 (1.3964) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2216 (0.1987) Acc D Real: 90.034%
Loss D Fake: 0.3198 (0.3238) Acc D Fake: 90.000%
Loss D: 0.541
Loss G: 1.4076 (1.3967) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.1813 (0.1983) Acc D Real: 90.073%
Loss D Fake: 0.3198 (0.3237) Acc D Fake: 90.000%
Loss D: 0.501
Loss G: 1.4079 (1.3970) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.1340 (0.1966) Acc D Real: 90.192%
Loss D Fake: 0.3199 (0.3236) Acc D Fake: 90.000%
Loss D: 0.454
Loss G: 1.4074 (1.3972) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.1585 (0.1957) Acc D Real: 90.258%
Loss D Fake: 0.3201 (0.3235) Acc D Fake: 90.000%
Loss D: 0.479
Loss G: 1.4073 (1.3975) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.1387 (0.1943) Acc D Real: 90.360%
Loss D Fake: 0.3200 (0.3235) Acc D Fake: 90.000%
Loss D: 0.459
Loss G: 1.4078 (1.3977) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.1726 (0.1938) Acc D Real: 90.387%
Loss D Fake: 0.3198 (0.3234) Acc D Fake: 90.000%
Loss D: 0.492
Loss G: 1.4086 (1.3980) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.1875 (0.1936) Acc D Real: 90.405%
Loss D Fake: 0.3196 (0.3233) Acc D Fake: 90.000%
Loss D: 0.507
Loss G: 1.4094 (1.3983) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.0988 (0.1915) Acc D Real: 90.575%
Loss D Fake: 0.3194 (0.3232) Acc D Fake: 90.000%
Loss D: 0.418
Loss G: 1.4107 (1.3985) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.1905 (0.1914) Acc D Real: 90.579%
Loss D Fake: 0.3190 (0.3231) Acc D Fake: 90.000%
Loss D: 0.509
Loss G: 1.4122 (1.3988) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.1286 (0.1901) Acc D Real: 90.670%
Loss D Fake: 0.3185 (0.3230) Acc D Fake: 90.000%
Loss D: 0.447
Loss G: 1.4142 (1.3992) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.2548 (0.1914) Acc D Real: 90.582%
Loss D Fake: 0.3180 (0.3229) Acc D Fake: 90.000%
Loss D: 0.573
Loss G: 1.4157 (1.3995) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.1298 (0.1902) Acc D Real: 90.696%
Loss D Fake: 0.3176 (0.3228) Acc D Fake: 90.000%
Loss D: 0.447
Loss G: 1.4175 (1.3999) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2176 (0.1907) Acc D Real: 90.643%
Loss D Fake: 0.3171 (0.3227) Acc D Fake: 90.000%
Loss D: 0.535
Loss G: 1.4190 (1.4003) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.0864 (0.1886) Acc D Real: 90.783%
Loss D Fake: 0.3167 (0.3225) Acc D Fake: 90.000%
Loss D: 0.403
Loss G: 1.4206 (1.4007) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.2449 (0.1897) Acc D Real: 90.699%
Loss D Fake: 0.3164 (0.3224) Acc D Fake: 90.000%
Loss D: 0.561
Loss G: 1.4213 (1.4011) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.1586 (0.1891) Acc D Real: 90.738%
Loss D Fake: 0.3162 (0.3223) Acc D Fake: 90.000%
Loss D: 0.475
Loss G: 1.4220 (1.4015) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2500 (0.1903) Acc D Real: 90.682%
Loss D Fake: 0.3161 (0.3222) Acc D Fake: 90.000%
Loss D: 0.566
Loss G: 1.4225 (1.4019) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.1556 (0.1896) Acc D Real: 90.742%
Loss D Fake: 0.3160 (0.3221) Acc D Fake: 90.000%
Loss D: 0.472
Loss G: 1.4230 (1.4023) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2097 (0.1900) Acc D Real: 90.726%
Loss D Fake: 0.3160 (0.3220) Acc D Fake: 90.000%
Loss D: 0.526
Loss G: 1.4235 (1.4027) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.1930 (0.1901) Acc D Real: 90.712%
Loss D Fake: 0.3169 (0.3219) Acc D Fake: 90.000%
Loss D: 0.510
Loss G: 1.4136 (1.4029) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.1530 (0.1894) Acc D Real: 90.763%
Loss D Fake: 0.3206 (0.3218) Acc D Fake: 90.000%
Loss D: 0.474
Loss G: 1.4012 (1.4028) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.2021 (0.1896) Acc D Real: 90.767%
Loss D Fake: 0.3240 (0.3219) Acc D Fake: 90.000%
Loss D: 0.526
Loss G: 1.3902 (1.4026) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.1696 (0.1893) Acc D Real: 90.774%
Loss D Fake: 0.3273 (0.3220) Acc D Fake: 90.000%
Loss D: 0.497
Loss G: 1.3802 (1.4022) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.2024 (0.1895) Acc D Real: 90.765%
Loss D Fake: 0.3303 (0.3221) Acc D Fake: 89.972%
Loss D: 0.533
Loss G: 1.3714 (1.4017) Acc G: 10.028%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.1137 (0.1883) Acc D Real: 90.850%
Loss D Fake: 0.3334 (0.3223) Acc D Fake: 89.945%
Loss D: 0.447
Loss G: 1.3598 (1.4010) Acc G: 10.055%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.2752 (0.1897) Acc D Real: 90.764%
Loss D Fake: 0.3375 (0.3225) Acc D Fake: 89.919%
Loss D: 0.613
Loss G: 1.3475 (1.4002) Acc G: 10.081%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.1464 (0.1890) Acc D Real: 90.811%
Loss D Fake: 0.3414 (0.3228) Acc D Fake: 89.894%
Loss D: 0.488
Loss G: 1.3371 (1.3992) Acc G: 10.106%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.0850 (0.1874) Acc D Real: 90.916%
Loss D Fake: 0.3446 (0.3232) Acc D Fake: 89.870%
Loss D: 0.430
Loss G: 1.3290 (1.3981) Acc G: 10.130%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.1632 (0.1870) Acc D Real: 90.933%
Loss D Fake: 0.3471 (0.3236) Acc D Fake: 89.846%
Loss D: 0.510
Loss G: 1.3224 (1.3969) Acc G: 10.154%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.2217 (0.1875) Acc D Real: 90.905%
Loss D Fake: 0.3498 (0.3239) Acc D Fake: 89.823%
Loss D: 0.571
Loss G: 1.3120 (1.3956) Acc G: 10.177%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.1133 (0.1864) Acc D Real: 90.979%
Loss D Fake: 0.3538 (0.3244) Acc D Fake: 89.801%
Loss D: 0.467
Loss G: 1.3020 (1.3942) Acc G: 10.199%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.1604 (0.1860) Acc D Real: 91.008%
Loss D Fake: 0.3571 (0.3249) Acc D Fake: 89.779%
Loss D: 0.517
Loss G: 1.2937 (1.3928) Acc G: 10.221%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.0904 (0.1846) Acc D Real: 91.098%
Loss D Fake: 0.3598 (0.3254) Acc D Fake: 89.758%
Loss D: 0.450
Loss G: 1.2873 (1.3912) Acc G: 10.242%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.1033 (0.1835) Acc D Real: 91.174%
Loss D Fake: 0.3618 (0.3259) Acc D Fake: 89.738%
Loss D: 0.465
Loss G: 1.2830 (1.3897) Acc G: 10.262%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.1058 (0.1824) Acc D Real: 91.262%
Loss D Fake: 0.3631 (0.3264) Acc D Fake: 89.718%
Loss D: 0.469
Loss G: 1.2812 (1.3882) Acc G: 10.282%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.0662 (0.1808) Acc D Real: 91.381%
Loss D Fake: 0.3634 (0.3269) Acc D Fake: 89.699%
Loss D: 0.430
Loss G: 1.2821 (1.3867) Acc G: 10.301%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.2112 (0.1812) Acc D Real: 91.351%
Loss D Fake: 0.3634 (0.3274) Acc D Fake: 89.680%
Loss D: 0.575
Loss G: 1.2802 (1.3852) Acc G: 10.320%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.1789 (0.1812) Acc D Real: 91.345%
Loss D Fake: 0.3645 (0.3279) Acc D Fake: 89.662%
Loss D: 0.543
Loss G: 1.2782 (1.3838) Acc G: 10.338%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.1109 (0.1802) Acc D Real: 91.412%
Loss D Fake: 0.3649 (0.3284) Acc D Fake: 89.644%
Loss D: 0.476
Loss G: 1.2787 (1.3824) Acc G: 10.356%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.1345 (0.1796) Acc D Real: 91.454%
Loss D Fake: 0.3644 (0.3289) Acc D Fake: 89.627%
Loss D: 0.499
Loss G: 1.2816 (1.3811) Acc G: 10.373%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.0629 (0.1781) Acc D Real: 91.560%
Loss D Fake: 0.3631 (0.3293) Acc D Fake: 89.610%
Loss D: 0.426
Loss G: 1.2876 (1.3798) Acc G: 10.390%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.1335 (0.1775) Acc D Real: 91.597%
Loss D Fake: 0.3608 (0.3298) Acc D Fake: 89.594%
Loss D: 0.494
Loss G: 1.2951 (1.3787) Acc G: 10.406%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.0671 (0.1761) Acc D Real: 91.698%
Loss D Fake: 0.3581 (0.3301) Acc D Fake: 89.578%
Loss D: 0.425
Loss G: 1.3043 (1.3778) Acc G: 10.422%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.1272 (0.1755) Acc D Real: 91.742%
Loss D Fake: 0.3556 (0.3304) Acc D Fake: 89.562%
Loss D: 0.483
Loss G: 1.3103 (1.3770) Acc G: 10.438%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.0665 (0.1742) Acc D Real: 91.842%
Loss D Fake: 0.3554 (0.3307) Acc D Fake: 89.527%
Loss D: 0.422
Loss G: 1.3151 (1.3762) Acc G: 10.473%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.0744 (0.1730) Acc D Real: 91.924%
Loss D Fake: 0.3553 (0.3310) Acc D Fake: 89.492%
Loss D: 0.430
Loss G: 1.3191 (1.3755) Acc G: 10.508%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.0850 (0.1719) Acc D Real: 91.999%
Loss D Fake: 0.3553 (0.3313) Acc D Fake: 89.458%
Loss D: 0.440
Loss G: 1.3230 (1.3749) Acc G: 10.542%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.1146 (0.1712) Acc D Real: 92.052%
Loss D Fake: 0.3551 (0.3316) Acc D Fake: 89.425%
Loss D: 0.470
Loss G: 1.3272 (1.3743) Acc G: 10.595%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.0690 (0.1700) Acc D Real: 92.142%
Loss D Fake: 0.3546 (0.3319) Acc D Fake: 89.373%
Loss D: 0.424
Loss G: 1.3323 (1.3738) Acc G: 10.647%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.1262 (0.1695) Acc D Real: 92.184%
Loss D Fake: 0.3541 (0.3321) Acc D Fake: 89.322%
Loss D: 0.480
Loss G: 1.3354 (1.3734) Acc G: 10.698%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.0701 (0.1684) Acc D Real: 92.268%
Loss D Fake: 0.3544 (0.3324) Acc D Fake: 89.272%
Loss D: 0.425
Loss G: 1.3388 (1.3730) Acc G: 10.747%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.0692 (0.1672) Acc D Real: 92.347%
Loss D Fake: 0.3541 (0.3326) Acc D Fake: 89.223%
Loss D: 0.423
Loss G: 1.3434 (1.3726) Acc G: 10.795%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.0578 (0.1660) Acc D Real: 92.433%
Loss D Fake: 0.3531 (0.3329) Acc D Fake: 89.176%
Loss D: 0.411
Loss G: 1.3494 (1.3724) Acc G: 10.843%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.1145 (0.1654) Acc D Real: 92.485%
Loss D Fake: 0.3515 (0.3331) Acc D Fake: 89.130%
Loss D: 0.466
Loss G: 1.3562 (1.3722) Acc G: 10.889%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.0789 (0.1645) Acc D Real: 92.548%
Loss D Fake: 0.3496 (0.3333) Acc D Fake: 89.084%
Loss D: 0.428
Loss G: 1.3638 (1.3721) Acc G: 10.934%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.0608 (0.1634) Acc D Real: 92.626%
Loss D Fake: 0.3472 (0.3334) Acc D Fake: 89.040%
Loss D: 0.408
Loss G: 1.3723 (1.3721) Acc G: 10.978%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.1052 (0.1627) Acc D Real: 92.674%
Loss D Fake: 0.3449 (0.3335) Acc D Fake: 88.996%
Loss D: 0.450
Loss G: 1.3773 (1.3722) Acc G: 11.022%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.1033 (0.1621) Acc D Real: 92.722%
Loss D Fake: 0.3435 (0.3336) Acc D Fake: 88.954%
Loss D: 0.447
Loss G: 1.3816 (1.3723) Acc G: 11.064%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.0702 (0.1611) Acc D Real: 92.789%
Loss D Fake: 0.3417 (0.3337) Acc D Fake: 88.912%
Loss D: 0.412
Loss G: 1.3874 (1.3724) Acc G: 11.105%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.1012 (0.1605) Acc D Real: 92.836%
Loss D Fake: 0.3394 (0.3338) Acc D Fake: 88.872%
Loss D: 0.441
Loss G: 1.3939 (1.3726) Acc G: 11.146%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.1190 (0.1601) Acc D Real: 92.867%
Loss D Fake: 0.3371 (0.3338) Acc D Fake: 88.832%
Loss D: 0.456
Loss G: 1.4005 (1.3729) Acc G: 11.186%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.0821 (0.1593) Acc D Real: 92.934%
Loss D Fake: 0.3348 (0.3338) Acc D Fake: 88.793%
Loss D: 0.417
Loss G: 1.4077 (1.3733) Acc G: 11.207%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.0736 (0.1584) Acc D Real: 92.993%
Loss D Fake: 0.3323 (0.3338) Acc D Fake: 88.771%
Loss D: 0.406
Loss G: 1.4155 (1.3737) Acc G: 11.229%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.0824 (0.1577) Acc D Real: 93.048%
Loss D Fake: 0.3297 (0.3338) Acc D Fake: 88.750%
Loss D: 0.412
Loss G: 1.4238 (1.3742) Acc G: 11.250%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.1065 (0.1572) Acc D Real: 93.089%
Loss D Fake: 0.3270 (0.3337) Acc D Fake: 88.729%
Loss D: 0.434
Loss G: 1.4321 (1.3748) Acc G: 11.271%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.0968 (0.1566) Acc D Real: 93.135%
Loss D Fake: 0.3249 (0.3336) Acc D Fake: 88.709%
Loss D: 0.422
Loss G: 1.4381 (1.3754) Acc G: 11.291%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.0591 (0.1556) Acc D Real: 93.195%
Loss D Fake: 0.3238 (0.3335) Acc D Fake: 88.689%
Loss D: 0.383
Loss G: 1.4441 (1.3761) Acc G: 11.311%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.1369 (0.1554) Acc D Real: 93.202%
Loss D Fake: 0.3231 (0.3334) Acc D Fake: 88.670%
Loss D: 0.460
Loss G: 1.4435 (1.3767) Acc G: 11.330%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.0622 (0.1545) Acc D Real: 93.261%
Loss D Fake: 0.3240 (0.3333) Acc D Fake: 88.651%
Loss D: 0.386
Loss G: 1.4424 (1.3773) Acc G: 11.349%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.1210 (0.1542) Acc D Real: 93.295%
Loss D Fake: 0.3243 (0.3333) Acc D Fake: 88.632%
Loss D: 0.445
Loss G: 1.4426 (1.3780) Acc G: 11.368%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.0919 (0.1536) Acc D Real: 93.331%
Loss D Fake: 0.3245 (0.3332) Acc D Fake: 88.614%
Loss D: 0.416
Loss G: 1.4419 (1.3786) Acc G: 11.386%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.0607 (0.1528) Acc D Real: 93.387%
Loss D Fake: 0.3249 (0.3331) Acc D Fake: 88.596%
Loss D: 0.386
Loss G: 1.4426 (1.3791) Acc G: 11.404%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.0897 (0.1522) Acc D Real: 93.422%
Loss D Fake: 0.3436 (0.3332) Acc D Fake: 88.578%
Loss D: 0.433
Loss G: 0.3303 (1.3695) Acc G: 12.003%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.0933 (0.1517) Acc D Real: 93.457%
Loss D Fake: 5.5149 (0.3803) Acc D Fake: 87.909%
Loss D: 5.608
Loss G: 0.2132 (1.3590) Acc G: 12.712%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.0677 (0.1509) Acc D Real: 93.515%
Loss D Fake: 5.9085 (0.4301) Acc D Fake: 87.177%
Loss D: 5.976
Loss G: 0.1788 (1.3484) Acc G: 13.483%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.0523 (0.1500) Acc D Real: 93.573%
Loss D Fake: 6.0087 (0.4799) Acc D Fake: 86.399%
Loss D: 6.061
Loss G: 0.1634 (1.3378) Acc G: 14.256%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.1045 (0.1496) Acc D Real: 93.608%
Loss D Fake: 5.9814 (0.5286) Acc D Fake: 85.634%
Loss D: 6.086
Loss G: 0.1561 (1.3273) Acc G: 15.015%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.0567 (0.1488) Acc D Real: 93.664%
Loss D Fake: 5.8786 (0.5755) Acc D Fake: 84.883%
Loss D: 5.935
Loss G: 0.1531 (1.3170) Acc G: 15.760%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.0684 (0.1481) Acc D Real: 93.719%
Loss D Fake: 5.7185 (0.6202) Acc D Fake: 84.145%
Loss D: 5.787
Loss G: 0.1530 (1.3069) Acc G: 16.493%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.0645 (0.1474) Acc D Real: 93.773%
Loss D Fake: 5.5063 (0.6624) Acc D Fake: 83.420%
Loss D: 5.571
Loss G: 0.1549 (1.2970) Acc G: 17.213%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.0708 (0.1467) Acc D Real: 93.820%
Loss D Fake: 5.2673 (0.7017) Acc D Fake: 82.707%
Loss D: 5.338
Loss G: 0.1583 (1.2873) Acc G: 17.920%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.0727 (0.1461) Acc D Real: 93.873%
Loss D Fake: 5.0379 (0.7385) Acc D Fake: 82.006%
Loss D: 5.111
Loss G: 0.1631 (1.2777) Acc G: 18.616%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.0750 (0.1455) Acc D Real: 93.918%
Loss D Fake: 4.8172 (0.7727) Acc D Fake: 81.317%
Loss D: 4.892
Loss G: 0.1692 (1.2684) Acc G: 19.300%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.1190 (0.1453) Acc D Real: 93.945%
Loss D Fake: 4.5982 (0.8046) Acc D Fake: 80.639%
Loss D: 4.717
Loss G: 0.1764 (1.2593) Acc G: 19.972%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.0678 (0.1447) Acc D Real: 93.995%
Loss D Fake: 4.3928 (0.8343) Acc D Fake: 79.972%
Loss D: 4.461
Loss G: 0.1855 (1.2504) Acc G: 20.634%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.0784 (0.1441) Acc D Real: 94.045%
Loss D Fake: 4.2008 (0.8619) Acc D Fake: 79.317%
Loss D: 4.279
Loss G: 0.1966 (1.2418) Acc G: 21.257%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.0721 (0.1435) Acc D Real: 94.090%
Loss D Fake: 4.0161 (0.8875) Acc D Fake: 78.713%
Loss D: 4.088
Loss G: 0.2103 (1.2334) Acc G: 21.843%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.1100 (0.1433) Acc D Real: 94.118%
Loss D Fake: 3.8280 (0.9112) Acc D Fake: 78.159%
Loss D: 3.938
Loss G: 0.2280 (1.2253) Acc G: 22.392%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.0632 (0.1426) Acc D Real: 94.165%
Loss D Fake: 3.6239 (0.9329) Acc D Fake: 77.617%
Loss D: 3.687
Loss G: 0.2526 (1.2175) Acc G: 22.906%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.0686 (0.1420) Acc D Real: 94.211%
Loss D Fake: 3.3831 (0.9524) Acc D Fake: 77.121%
Loss D: 3.452
Loss G: 0.2911 (1.2102) Acc G: 23.372%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.0816 (0.1416) Acc D Real: 94.250%
Loss D Fake: 3.0469 (0.9689) Acc D Fake: 76.663%
Loss D: 3.129
Loss G: 0.3730 (1.2036) Acc G: 23.792%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.0814 (0.1411) Acc D Real: 94.295%
Loss D Fake: 2.0483 (0.9773) Acc D Fake: 76.298%
Loss D: 2.130
Loss G: 0.1550 (1.1954) Acc G: 24.387%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.0971 (0.1407) Acc D Real: 94.337%
Loss D Fake: 3.3367 (0.9956) Acc D Fake: 75.707%
Loss D: 3.434
Loss G: 0.1233 (1.1871) Acc G: 24.973%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.0910 (0.1404) Acc D Real: 94.381%
Loss D Fake: 3.4060 (1.0141) Acc D Fake: 75.125%
Loss D: 3.497
Loss G: 0.1125 (1.1788) Acc G: 25.550%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.0725 (0.1398) Acc D Real: 94.424%
Loss D Fake: 3.4062 (1.0324) Acc D Fake: 74.551%
Loss D: 3.479
Loss G: 0.1075 (1.1706) Acc G: 26.119%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.0774 (0.1394) Acc D Real: 94.466%
Loss D Fake: 3.3755 (1.0501) Acc D Fake: 73.986%
Loss D: 3.453
Loss G: 0.1052 (1.1626) Acc G: 26.679%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.0707 (0.1389) Acc D Real: 94.508%
Loss D Fake: 3.3261 (1.0673) Acc D Fake: 73.430%
Loss D: 3.397
Loss G: 0.1045 (1.1546) Acc G: 27.230%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.0711 (0.1383) Acc D Real: 94.549%
Loss D Fake: 3.2627 (1.0836) Acc D Fake: 72.882%
Loss D: 3.334
Loss G: 0.1051 (1.1468) Acc G: 27.773%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.0699 (0.1378) Acc D Real: 94.589%
Loss D Fake: 3.1857 (1.0992) Acc D Fake: 72.342%
Loss D: 3.256
Loss G: 0.1068 (1.1391) Acc G: 28.308%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.0790 (0.1374) Acc D Real: 94.628%
Loss D Fake: 3.0962 (1.1139) Acc D Fake: 71.810%
Loss D: 3.175
Loss G: 0.1093 (1.1315) Acc G: 28.835%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.0740 (0.1369) Acc D Real: 94.665%
Loss D Fake: 3.0043 (1.1277) Acc D Fake: 71.298%
Loss D: 3.078
Loss G: 0.1122 (1.1241) Acc G: 29.342%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.0921 (0.1366) Acc D Real: 94.698%
Loss D Fake: 2.9201 (1.1407) Acc D Fake: 70.794%
Loss D: 3.012
Loss G: 0.1154 (1.1168) Acc G: 29.842%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.0826 (0.1362) Acc D Real: 94.732%
Loss D Fake: 2.8446 (1.1529) Acc D Fake: 70.296%
Loss D: 2.927
Loss G: 0.1186 (1.1096) Acc G: 30.335%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.0915 (0.1359) Acc D Real: 94.764%
Loss D Fake: 2.7759 (1.1645) Acc D Fake: 69.806%
Loss D: 2.867
Loss G: 0.1220 (1.1025) Acc G: 30.821%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.0798 (0.1355) Acc D Real: 94.798%
Loss D Fake: 2.7122 (1.1755) Acc D Fake: 69.323%
Loss D: 2.792
Loss G: 0.1254 (1.0956) Acc G: 31.299%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.0901 (0.1352) Acc D Real: 94.829%
Loss D Fake: 2.6521 (1.1859) Acc D Fake: 68.846%
Loss D: 2.742
Loss G: 0.1290 (1.0888) Acc G: 31.772%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.0828 (0.1348) Acc D Real: 94.859%
Loss D Fake: 2.5951 (1.1958) Acc D Fake: 68.377%
Loss D: 2.678
Loss G: 0.1326 (1.0821) Acc G: 32.237%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.0841 (0.1345) Acc D Real: 94.889%
Loss D Fake: 2.5405 (1.2051) Acc D Fake: 67.913%
Loss D: 2.625
Loss G: 0.1364 (1.0755) Acc G: 32.696%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.0839 (0.1341) Acc D Real: 94.920%
Loss D Fake: 2.4880 (1.2140) Acc D Fake: 67.457%
Loss D: 2.572
Loss G: 0.1403 (1.0691) Acc G: 33.149%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.0951 (0.1339) Acc D Real: 94.948%
Loss D Fake: 2.4373 (1.2223) Acc D Fake: 67.006%
Loss D: 2.532
Loss G: 0.1443 (1.0627) Acc G: 33.595%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.0860 (0.1335) Acc D Real: 94.978%
Loss D Fake: 2.3885 (1.2303) Acc D Fake: 66.561%
Loss D: 2.474
Loss G: 0.1484 (1.0565) Acc G: 34.036%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.0854 (0.1332) Acc D Real: 95.005%
Loss D Fake: 2.3412 (1.2378) Acc D Fake: 66.123%
Loss D: 2.427
Loss G: 0.1525 (1.0504) Acc G: 34.470%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.0765 (0.1328) Acc D Real: 95.033%
Loss D Fake: 2.2954 (1.2449) Acc D Fake: 65.690%
Loss D: 2.372
Loss G: 0.1568 (1.0444) Acc G: 34.899%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.1007 (0.1326) Acc D Real: 95.058%
Loss D Fake: 2.2508 (1.2516) Acc D Fake: 65.275%
Loss D: 2.351
Loss G: 0.1612 (1.0385) Acc G: 35.310%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.1009 (0.1324) Acc D Real: 95.081%
Loss D Fake: 2.2077 (1.2579) Acc D Fake: 64.864%
Loss D: 2.309
Loss G: 0.1657 (1.0327) Acc G: 35.717%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.0949 (0.1322) Acc D Real: 95.106%
Loss D Fake: 2.1659 (1.2639) Acc D Fake: 64.460%
Loss D: 2.261
Loss G: 0.1702 (1.0271) Acc G: 36.118%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.0898 (0.1319) Acc D Real: 95.130%
Loss D Fake: 2.1252 (1.2695) Acc D Fake: 64.060%
Loss D: 2.215
Loss G: 0.1749 (1.0215) Acc G: 36.513%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.1114 (0.1317) Acc D Real: 95.153%
Loss D Fake: 2.0857 (1.2748) Acc D Fake: 63.666%
Loss D: 2.197
Loss G: 0.1796 (1.0160) Acc G: 36.904%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.0972 (0.1315) Acc D Real: 95.175%
Loss D Fake: 2.0472 (1.2798) Acc D Fake: 63.277%
Loss D: 2.144
Loss G: 0.1845 (1.0107) Acc G: 37.290%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.1055 (0.1314) Acc D Real: 95.196%
Loss D Fake: 2.0098 (1.2845) Acc D Fake: 62.892%
Loss D: 2.115
Loss G: 0.1893 (1.0054) Acc G: 37.670%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.1034 (0.1312) Acc D Real: 95.217%
Loss D Fake: 1.9734 (1.2889) Acc D Fake: 62.513%
Loss D: 2.077
Loss G: 0.1943 (1.0002) Acc G: 38.046%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.1061 (0.1310) Acc D Real: 95.237%
Loss D Fake: 1.9380 (1.2930) Acc D Fake: 62.138%
Loss D: 2.044
Loss G: 0.1994 (0.9952) Acc G: 38.417%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.1009 (0.1308) Acc D Real: 95.258%
Loss D Fake: 1.9036 (1.2968) Acc D Fake: 61.779%
Loss D: 2.005
Loss G: 0.2045 (0.9902) Acc G: 38.773%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.1059 (0.1307) Acc D Real: 95.277%
Loss D Fake: 1.8700 (1.3004) Acc D Fake: 61.424%
Loss D: 1.976
Loss G: 0.2098 (0.9853) Acc G: 39.124%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.1002 (0.1305) Acc D Real: 95.295%
Loss D Fake: 1.8371 (1.3037) Acc D Fake: 61.074%
Loss D: 1.937
Loss G: 0.2151 (0.9805) Acc G: 39.471%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.1022 (0.1303) Acc D Real: 95.313%
Loss D Fake: 1.8050 (1.3068) Acc D Fake: 60.728%
Loss D: 1.907
Loss G: 0.2205 (0.9758) Acc G: 39.814%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.1254 (0.1303) Acc D Real: 95.329%
Loss D Fake: 1.7737 (1.3097) Acc D Fake: 60.386%
Loss D: 1.899
Loss G: 0.2260 (0.9712) Acc G: 40.153%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.1171 (0.1302) Acc D Real: 95.345%
Loss D Fake: 1.7434 (1.3123) Acc D Fake: 60.048%
Loss D: 1.860
Loss G: 0.2315 (0.9667) Acc G: 40.487%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1128 (0.1301) Acc D Real: 95.360%
Loss D Fake: 1.7138 (1.3148) Acc D Fake: 59.714%
Loss D: 1.827
Loss G: 0.2371 (0.9623) Acc G: 40.818%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.1168 (0.1300) Acc D Real: 95.377%
Loss D Fake: 1.6849 (1.3170) Acc D Fake: 59.385%
Loss D: 1.802
Loss G: 0.2428 (0.9580) Acc G: 41.144%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.1075 (0.1299) Acc D Real: 95.394%
Loss D Fake: 1.6568 (1.3190) Acc D Fake: 59.059%
Loss D: 1.764
Loss G: 0.2485 (0.9537) Acc G: 41.456%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.0960 (0.1297) Acc D Real: 95.412%
Loss D Fake: 1.6291 (1.3209) Acc D Fake: 58.747%
Loss D: 1.725
Loss G: 0.2544 (0.9496) Acc G: 41.765%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.1185 (0.1296) Acc D Real: 95.426%
Loss D Fake: 1.6022 (1.3225) Acc D Fake: 58.439%
Loss D: 1.721
Loss G: 0.2603 (0.9455) Acc G: 42.070%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.1275 (0.1296) Acc D Real: 95.439%
Loss D Fake: 1.5758 (1.3240) Acc D Fake: 58.134%
Loss D: 1.703
Loss G: 0.2662 (0.9415) Acc G: 42.372%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.1247 (0.1296) Acc D Real: 95.452%
Loss D Fake: 1.5502 (1.3253) Acc D Fake: 57.834%
Loss D: 1.675
Loss G: 0.2722 (0.9376) Acc G: 42.670%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.0963 (0.1294) Acc D Real: 95.469%
Loss D Fake: 1.5252 (1.3265) Acc D Fake: 57.536%
Loss D: 1.621
Loss G: 0.2783 (0.9337) Acc G: 42.965%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.1527 (0.1295) Acc D Real: 95.480%
Loss D Fake: 1.5007 (1.3275) Acc D Fake: 57.242%
Loss D: 1.653
Loss G: 0.2844 (0.9300) Acc G: 43.256%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1466 (0.1296) Acc D Real: 95.489%
Loss D Fake: 1.4771 (1.3284) Acc D Fake: 56.951%
Loss D: 1.624
Loss G: 0.2905 (0.9263) Acc G: 43.543%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.1509 (0.1297) Acc D Real: 95.499%
Loss D Fake: 1.4542 (1.3291) Acc D Fake: 56.664%
Loss D: 1.605
Loss G: 0.2966 (0.9227) Acc G: 43.828%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.1568 (0.1299) Acc D Real: 95.506%
Loss D Fake: 1.4321 (1.3297) Acc D Fake: 56.380%
Loss D: 1.589
Loss G: 0.3027 (0.9192) Acc G: 44.109%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.1627 (0.1301) Acc D Real: 95.515%
Loss D Fake: 1.4108 (1.3301) Acc D Fake: 56.108%
Loss D: 1.573
Loss G: 0.3087 (0.9157) Acc G: 44.378%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.1264 (0.1301) Acc D Real: 95.527%
Loss D Fake: 1.3900 (1.3305) Acc D Fake: 55.840%
Loss D: 1.516
Loss G: 0.3149 (0.9124) Acc G: 44.644%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.1337 (0.1301) Acc D Real: 95.537%
Loss D Fake: 1.3695 (1.3307) Acc D Fake: 55.575%
Loss D: 1.503
Loss G: 0.3211 (0.9091) Acc G: 44.906%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.1102 (0.1300) Acc D Real: 95.550%
Loss D Fake: 1.3493 (1.3308) Acc D Fake: 55.312%
Loss D: 1.459
Loss G: 0.3274 (0.9058) Acc G: 45.166%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.1616 (0.1301) Acc D Real: 95.556%
Loss D Fake: 1.3293 (1.3308) Acc D Fake: 55.053%
Loss D: 1.491
Loss G: 0.3337 (0.9027) Acc G: 45.423%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.1816 (0.1304) Acc D Real: 95.561%
Loss D Fake: 1.3100 (1.3307) Acc D Fake: 54.796%
Loss D: 1.492
Loss G: 0.3400 (0.8996) Acc G: 45.677%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.1638 (0.1306) Acc D Real: 95.566%
Loss D Fake: 1.2914 (1.3305) Acc D Fake: 54.542%
Loss D: 1.455
Loss G: 0.3463 (0.8966) Acc G: 45.928%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.1121 (0.1305) Acc D Real: 95.580%
Loss D Fake: 1.2732 (1.3302) Acc D Fake: 54.291%
Loss D: 1.385
Loss G: 0.3527 (0.8936) Acc G: 46.177%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.1766 (0.1308) Acc D Real: 95.586%
Loss D Fake: 1.2551 (1.3297) Acc D Fake: 54.043%
Loss D: 1.432
Loss G: 0.3591 (0.8907) Acc G: 46.423%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1858 (0.1310) Acc D Real: 95.588%
Loss D Fake: 1.2376 (1.3292) Acc D Fake: 53.797%
Loss D: 1.423
Loss G: 0.3655 (0.8879) Acc G: 46.666%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.1352 (0.1311) Acc D Real: 95.599%
Loss D Fake: 1.2208 (1.3287) Acc D Fake: 53.554%
Loss D: 1.356
Loss G: 0.3718 (0.8851) Acc G: 46.907%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.1952 (0.1314) Acc D Real: 95.602%
Loss D Fake: 1.2043 (1.3280) Acc D Fake: 53.313%
Loss D: 1.400
Loss G: 0.3781 (0.8824) Acc G: 47.145%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.1373 (0.1314) Acc D Real: 95.615%
Loss D Fake: 1.1884 (1.3273) Acc D Fake: 53.084%
Loss D: 1.326
Loss G: 0.3844 (0.8798) Acc G: 47.372%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.1460 (0.1315) Acc D Real: 95.624%
Loss D Fake: 1.1726 (1.3265) Acc D Fake: 52.857%
Loss D: 1.319
Loss G: 0.3908 (0.8772) Acc G: 47.596%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.1619 (0.1317) Acc D Real: 95.630%
Loss D Fake: 1.1571 (1.3256) Acc D Fake: 52.633%
Loss D: 1.319
Loss G: 0.3973 (0.8747) Acc G: 47.818%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.1712 (0.1319) Acc D Real: 95.636%
Loss D Fake: 1.1419 (1.3246) Acc D Fake: 52.411%
Loss D: 1.313
Loss G: 0.4037 (0.8723) Acc G: 48.038%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.1361 (0.1319) Acc D Real: 95.645%
Loss D Fake: 1.1271 (1.3236) Acc D Fake: 52.191%
Loss D: 1.263
Loss G: 0.4102 (0.8699) Acc G: 48.255%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2203 (0.1324) Acc D Real: 95.645%
Loss D Fake: 1.1125 (1.3225) Acc D Fake: 51.974%
Loss D: 1.333
Loss G: 0.4166 (0.8675) Acc G: 48.470%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.1634 (0.1325) Acc D Real: 95.651%
Loss D Fake: 1.0986 (1.3214) Acc D Fake: 51.758%
Loss D: 1.262
Loss G: 0.4230 (0.8653) Acc G: 48.683%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.1681 (0.1327) Acc D Real: 95.658%
Loss D Fake: 1.0849 (1.3201) Acc D Fake: 51.545%
Loss D: 1.253
Loss G: 0.4293 (0.8630) Acc G: 48.894%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.1391 (0.1327) Acc D Real: 95.665%
Loss D Fake: 1.0715 (1.3189) Acc D Fake: 51.334%
Loss D: 1.211
Loss G: 0.4358 (0.8609) Acc G: 49.103%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.1325 (0.1327) Acc D Real: 95.674%
Loss D Fake: 1.0580 (1.3176) Acc D Fake: 51.126%
Loss D: 1.190
Loss G: 0.4425 (0.8587) Acc G: 49.309%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.1900 (0.1330) Acc D Real: 95.677%
Loss D Fake: 1.0446 (1.3162) Acc D Fake: 50.919%
Loss D: 1.235
Loss G: 0.4491 (0.8567) Acc G: 49.514%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.1508 (0.1331) Acc D Real: 95.684%
Loss D Fake: 1.0317 (1.3148) Acc D Fake: 50.714%
Loss D: 1.183
Loss G: 0.4557 (0.8547) Acc G: 49.716%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.1823 (0.1334) Acc D Real: 95.687%
Loss D Fake: 1.0190 (1.3133) Acc D Fake: 50.512%
Loss D: 1.201
Loss G: 0.4623 (0.8527) Acc G: 49.917%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1927 (0.1336) Acc D Real: 95.686%
Loss D Fake: 1.0066 (1.3118) Acc D Fake: 50.311%
Loss D: 1.199
Loss G: 0.4688 (0.8508) Acc G: 50.115%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.1739 (0.1338) Acc D Real: 95.690%
Loss D Fake: 0.9947 (1.3102) Acc D Fake: 50.113%
Loss D: 1.169
Loss G: 0.4753 (0.8490) Acc G: 50.311%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.1712 (0.1340) Acc D Real: 95.696%
Loss D Fake: 0.9828 (1.3086) Acc D Fake: 49.916%
Loss D: 1.154
Loss G: 0.4819 (0.8472) Acc G: 50.506%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.1567 (0.1341) Acc D Real: 95.701%
Loss D Fake: 0.9711 (1.3070) Acc D Fake: 49.729%
Loss D: 1.128
Loss G: 0.4886 (0.8454) Acc G: 50.691%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.1549 (0.1342) Acc D Real: 95.709%
Loss D Fake: 0.9596 (1.3053) Acc D Fake: 49.545%
Loss D: 1.115
Loss G: 0.4953 (0.8437) Acc G: 50.873%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.2746 (0.1349) Acc D Real: 95.702%
Loss D Fake: 0.9484 (1.3036) Acc D Fake: 49.362%
Loss D: 1.223
Loss G: 0.5017 (0.8421) Acc G: 51.054%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2063 (0.1353) Acc D Real: 95.705%
Loss D Fake: 0.9380 (1.3018) Acc D Fake: 49.180%
Loss D: 1.144
Loss G: 0.5078 (0.8405) Acc G: 51.233%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1486 (0.1353) Acc D Real: 95.711%
Loss D Fake: 0.9279 (1.3000) Acc D Fake: 49.001%
Loss D: 1.076
Loss G: 0.5141 (0.8389) Acc G: 51.411%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.1607 (0.1354) Acc D Real: 95.717%
Loss D Fake: 0.9176 (1.2982) Acc D Fake: 48.823%
Loss D: 1.078
Loss G: 0.5206 (0.8374) Acc G: 51.587%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2040 (0.1358) Acc D Real: 95.718%
Loss D Fake: 0.9075 (1.2963) Acc D Fake: 48.647%
Loss D: 1.112
Loss G: 0.5269 (0.8359) Acc G: 51.761%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.1754 (0.1360) Acc D Real: 95.724%
Loss D Fake: 0.8977 (1.2945) Acc D Fake: 48.473%
Loss D: 1.073
Loss G: 0.5333 (0.8345) Acc G: 51.933%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.1911 (0.1362) Acc D Real: 95.728%
Loss D Fake: 0.8881 (1.2926) Acc D Fake: 48.300%
Loss D: 1.079
Loss G: 0.5396 (0.8331) Acc G: 52.104%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.1341 (0.1362) Acc D Real: 95.734%
Loss D Fake: 0.8786 (1.2906) Acc D Fake: 48.129%
Loss D: 1.013
Loss G: 0.5461 (0.8318) Acc G: 52.274%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3069 (0.1370) Acc D Real: 95.724%
Loss D Fake: 0.8691 (1.2887) Acc D Fake: 47.959%
Loss D: 1.176
Loss G: 0.5522 (0.8305) Acc G: 52.441%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2283 (0.1374) Acc D Real: 95.724%
Loss D Fake: 0.8606 (1.2867) Acc D Fake: 47.791%
Loss D: 1.089
Loss G: 0.5581 (0.8292) Acc G: 52.608%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2103 (0.1378) Acc D Real: 95.726%
Loss D Fake: 0.8524 (1.2847) Acc D Fake: 47.625%
Loss D: 1.063
Loss G: 0.5639 (0.8280) Acc G: 52.772%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2496 (0.1383) Acc D Real: 95.723%
Loss D Fake: 0.8444 (1.2827) Acc D Fake: 47.460%
Loss D: 1.094
Loss G: 0.5695 (0.8268) Acc G: 52.935%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2186 (0.1386) Acc D Real: 95.724%
Loss D Fake: 0.8368 (1.2806) Acc D Fake: 47.296%
Loss D: 1.055
Loss G: 0.5750 (0.8257) Acc G: 53.097%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.1270 (0.1386) Acc D Real: 95.732%
Loss D Fake: 0.8292 (1.2786) Acc D Fake: 47.134%
Loss D: 0.956
Loss G: 0.5808 (0.8245) Acc G: 53.257%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2243 (0.1390) Acc D Real: 95.735%
Loss D Fake: 0.8212 (1.2765) Acc D Fake: 46.974%
Loss D: 1.046
Loss G: 0.5867 (0.8235) Acc G: 53.416%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3371 (0.1399) Acc D Real: 95.729%
Loss D Fake: 0.8138 (1.2744) Acc D Fake: 46.815%
Loss D: 1.151
Loss G: 0.5920 (0.8224) Acc G: 53.573%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2223 (0.1402) Acc D Real: 95.732%
Loss D Fake: 0.8072 (1.2723) Acc D Fake: 46.657%
Loss D: 1.030
Loss G: 0.5971 (0.8214) Acc G: 53.729%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.1664 (0.1404) Acc D Real: 95.736%
Loss D Fake: 0.8005 (1.2702) Acc D Fake: 46.501%
Loss D: 0.967
Loss G: 0.6026 (0.8204) Acc G: 53.883%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.1906 (0.1406) Acc D Real: 95.739%
Loss D Fake: 0.7935 (1.2681) Acc D Fake: 46.346%
Loss D: 0.984
Loss G: 0.6082 (0.8195) Acc G: 54.029%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.1420 (0.1406) Acc D Real: 95.741%
Loss D Fake: 0.7864 (1.2660) Acc D Fake: 46.309%
Loss D: 0.928
Loss G: 0.6141 (0.8186) Acc G: 54.065%
LR: 2.000e-04
Epoch: 5/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2659 (0.2778) Acc D Real: 94.609%
Loss D Fake: 0.7724 (0.7757) Acc D Fake: 13.333%
Loss D: 1.038
Loss G: 0.6252 (0.6225) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.1772 (0.2443) Acc D Real: 95.156%
Loss D Fake: 0.7660 (0.7725) Acc D Fake: 13.333%
Loss D: 0.943
Loss G: 0.6307 (0.6252) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.3202 (0.2633) Acc D Real: 95.143%
Loss D Fake: 0.7597 (0.7693) Acc D Fake: 13.333%
Loss D: 1.080
Loss G: 0.6358 (0.6279) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2092 (0.2525) Acc D Real: 95.167%
Loss D Fake: 0.7539 (0.7662) Acc D Fake: 13.333%
Loss D: 0.963
Loss G: 0.6409 (0.6305) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2253 (0.2479) Acc D Real: 95.217%
Loss D Fake: 0.7480 (0.7632) Acc D Fake: 13.333%
Loss D: 0.973
Loss G: 0.6460 (0.6331) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2774 (0.2522) Acc D Real: 95.246%
Loss D Fake: 0.7423 (0.7602) Acc D Fake: 13.333%
Loss D: 1.020
Loss G: 0.6508 (0.6356) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.2624 (0.2534) Acc D Real: 95.247%
Loss D Fake: 0.7370 (0.7573) Acc D Fake: 13.333%
Loss D: 0.999
Loss G: 0.6555 (0.6381) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2805 (0.2564) Acc D Real: 95.208%
Loss D Fake: 0.7320 (0.7545) Acc D Fake: 13.333%
Loss D: 1.012
Loss G: 0.6599 (0.6405) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.1516 (0.2460) Acc D Real: 95.354%
Loss D Fake: 0.7270 (0.7517) Acc D Fake: 13.333%
Loss D: 0.879
Loss G: 0.6647 (0.6429) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1370 (0.2361) Acc D Real: 95.521%
Loss D Fake: 0.7216 (0.7490) Acc D Fake: 13.333%
Loss D: 0.859
Loss G: 0.6700 (0.6454) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2626 (0.2383) Acc D Real: 95.490%
Loss D Fake: 0.7158 (0.7462) Acc D Fake: 13.333%
Loss D: 0.978
Loss G: 0.6753 (0.6479) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2317 (0.2378) Acc D Real: 95.489%
Loss D Fake: 0.7103 (0.7435) Acc D Fake: 13.333%
Loss D: 0.942
Loss G: 0.6804 (0.6504) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2171 (0.2363) Acc D Real: 95.499%
Loss D Fake: 0.7049 (0.7407) Acc D Fake: 13.333%
Loss D: 0.922
Loss G: 0.6856 (0.6529) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2947 (0.2402) Acc D Real: 95.424%
Loss D Fake: 0.6997 (0.7380) Acc D Fake: 13.333%
Loss D: 0.994
Loss G: 0.6904 (0.6554) Acc G: 86.667%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2984 (0.2438) Acc D Real: 95.348%
Loss D Fake: 0.6950 (0.7353) Acc D Fake: 13.333%
Loss D: 0.993
Loss G: 0.6949 (0.6579) Acc G: 82.708%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3584 (0.2506) Acc D Real: 93.695%
Loss D Fake: 0.6908 (0.7327) Acc D Fake: 17.255%
Loss D: 1.049
Loss G: 0.6987 (0.6603) Acc G: 78.922%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2172 (0.2487) Acc D Real: 92.925%
Loss D Fake: 0.6871 (0.7301) Acc D Fake: 20.926%
Loss D: 0.904
Loss G: 0.7025 (0.6626) Acc G: 75.370%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.2165 (0.2470) Acc D Real: 92.146%
Loss D Fake: 0.6831 (0.7277) Acc D Fake: 24.298%
Loss D: 0.900
Loss G: 0.7066 (0.6649) Acc G: 72.105%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.2365 (0.2465) Acc D Real: 91.370%
Loss D Fake: 0.6790 (0.7252) Acc D Fake: 27.417%
Loss D: 0.916
Loss G: 0.7108 (0.6672) Acc G: 69.167%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.2041 (0.2445) Acc D Real: 90.836%
Loss D Fake: 0.6748 (0.7228) Acc D Fake: 30.238%
Loss D: 0.879
Loss G: 0.7153 (0.6695) Acc G: 66.429%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.2276 (0.2437) Acc D Real: 90.215%
Loss D Fake: 0.6704 (0.7204) Acc D Fake: 32.879%
Loss D: 0.898
Loss G: 0.7199 (0.6718) Acc G: 63.939%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.2272 (0.2430) Acc D Real: 89.642%
Loss D Fake: 0.6659 (0.7181) Acc D Fake: 35.290%
Loss D: 0.893
Loss G: 0.7247 (0.6741) Acc G: 61.594%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.3181 (0.2461) Acc D Real: 88.609%
Loss D Fake: 0.6616 (0.7157) Acc D Fake: 37.639%
Loss D: 0.980
Loss G: 0.7291 (0.6764) Acc G: 59.375%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2052 (0.2445) Acc D Real: 88.233%
Loss D Fake: 0.6575 (0.7134) Acc D Fake: 39.800%
Loss D: 0.863
Loss G: 0.7335 (0.6787) Acc G: 57.333%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.1951 (0.2426) Acc D Real: 87.975%
Loss D Fake: 0.6532 (0.7111) Acc D Fake: 41.795%
Loss D: 0.848
Loss G: 0.7383 (0.6810) Acc G: 55.449%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.2545 (0.2430) Acc D Real: 87.434%
Loss D Fake: 0.6488 (0.7088) Acc D Fake: 43.642%
Loss D: 0.903
Loss G: 0.7431 (0.6833) Acc G: 53.704%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.2254 (0.2424) Acc D Real: 87.052%
Loss D Fake: 0.6445 (0.7065) Acc D Fake: 45.417%
Loss D: 0.870
Loss G: 0.7478 (0.6856) Acc G: 52.024%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.1820 (0.2403) Acc D Real: 86.945%
Loss D Fake: 0.6401 (0.7042) Acc D Fake: 47.069%
Loss D: 0.822
Loss G: 0.7529 (0.6879) Acc G: 50.460%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.2995 (0.2423) Acc D Real: 86.300%
Loss D Fake: 0.6356 (0.7019) Acc D Fake: 48.611%
Loss D: 0.935
Loss G: 0.7578 (0.6902) Acc G: 49.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3223 (0.2449) Acc D Real: 85.586%
Loss D Fake: 0.6316 (0.6996) Acc D Fake: 50.054%
Loss D: 0.954
Loss G: 0.7620 (0.6925) Acc G: 47.634%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3154 (0.2471) Acc D Real: 84.964%
Loss D Fake: 0.6281 (0.6974) Acc D Fake: 51.406%
Loss D: 0.944
Loss G: 0.7657 (0.6948) Acc G: 46.354%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3471 (0.2501) Acc D Real: 84.242%
Loss D Fake: 0.6251 (0.6952) Acc D Fake: 52.677%
Loss D: 0.972
Loss G: 0.7689 (0.6971) Acc G: 45.152%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.2746 (0.2508) Acc D Real: 83.842%
Loss D Fake: 0.6225 (0.6931) Acc D Fake: 53.873%
Loss D: 0.897
Loss G: 0.7720 (0.6993) Acc G: 44.020%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3526 (0.2537) Acc D Real: 83.188%
Loss D Fake: 0.6200 (0.6910) Acc D Fake: 55.000%
Loss D: 0.973
Loss G: 0.7746 (0.7014) Acc G: 42.905%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3948 (0.2576) Acc D Real: 82.415%
Loss D Fake: 0.6181 (0.6890) Acc D Fake: 56.111%
Loss D: 1.013
Loss G: 0.7766 (0.7035) Acc G: 41.852%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3231 (0.2594) Acc D Real: 81.919%
Loss D Fake: 0.6166 (0.6870) Acc D Fake: 57.162%
Loss D: 0.940
Loss G: 0.7783 (0.7055) Acc G: 40.856%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.1977 (0.2578) Acc D Real: 81.891%
Loss D Fake: 0.6150 (0.6851) Acc D Fake: 58.158%
Loss D: 0.813
Loss G: 0.7805 (0.7075) Acc G: 39.912%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3542 (0.2603) Acc D Real: 81.376%
Loss D Fake: 0.6130 (0.6833) Acc D Fake: 59.103%
Loss D: 0.967
Loss G: 0.7827 (0.7094) Acc G: 39.017%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3041 (0.2614) Acc D Real: 81.008%
Loss D Fake: 0.6113 (0.6815) Acc D Fake: 60.000%
Loss D: 0.915
Loss G: 0.7848 (0.7113) Acc G: 38.167%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3646 (0.2639) Acc D Real: 80.495%
Loss D Fake: 0.6096 (0.6797) Acc D Fake: 60.854%
Loss D: 0.974
Loss G: 0.7866 (0.7132) Acc G: 37.358%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.2412 (0.2633) Acc D Real: 80.392%
Loss D Fake: 0.6081 (0.6780) Acc D Fake: 61.667%
Loss D: 0.849
Loss G: 0.7887 (0.7150) Acc G: 36.587%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2814 (0.2638) Acc D Real: 80.148%
Loss D Fake: 0.6063 (0.6763) Acc D Fake: 62.442%
Loss D: 0.888
Loss G: 0.7909 (0.7167) Acc G: 35.853%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.2628 (0.2637) Acc D Real: 80.007%
Loss D Fake: 0.6043 (0.6747) Acc D Fake: 63.182%
Loss D: 0.867
Loss G: 0.7934 (0.7185) Acc G: 35.152%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.2950 (0.2644) Acc D Real: 79.760%
Loss D Fake: 0.6023 (0.6731) Acc D Fake: 63.889%
Loss D: 0.897
Loss G: 0.7959 (0.7202) Acc G: 34.481%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.2858 (0.2649) Acc D Real: 79.549%
Loss D Fake: 0.6003 (0.6715) Acc D Fake: 64.565%
Loss D: 0.886
Loss G: 0.7984 (0.7219) Acc G: 33.841%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.3367 (0.2664) Acc D Real: 79.207%
Loss D Fake: 0.5984 (0.6699) Acc D Fake: 65.213%
Loss D: 0.935
Loss G: 0.8006 (0.7236) Acc G: 33.227%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.2644 (0.2664) Acc D Real: 79.067%
Loss D Fake: 0.5966 (0.6684) Acc D Fake: 65.833%
Loss D: 0.861
Loss G: 0.8028 (0.7252) Acc G: 32.639%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2143 (0.2653) Acc D Real: 79.054%
Loss D Fake: 0.5947 (0.6669) Acc D Fake: 66.429%
Loss D: 0.809
Loss G: 0.8055 (0.7269) Acc G: 32.075%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.2707 (0.2654) Acc D Real: 78.907%
Loss D Fake: 0.5924 (0.6654) Acc D Fake: 67.000%
Loss D: 0.863
Loss G: 0.8084 (0.7285) Acc G: 31.500%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3387 (0.2669) Acc D Real: 78.597%
Loss D Fake: 0.5903 (0.6640) Acc D Fake: 67.582%
Loss D: 0.929
Loss G: 0.8108 (0.7301) Acc G: 30.915%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.2460 (0.2665) Acc D Real: 78.528%
Loss D Fake: 0.5883 (0.6625) Acc D Fake: 68.173%
Loss D: 0.834
Loss G: 0.8135 (0.7317) Acc G: 30.353%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.4106 (0.2692) Acc D Real: 78.075%
Loss D Fake: 0.5864 (0.6611) Acc D Fake: 68.742%
Loss D: 0.997
Loss G: 0.8155 (0.7333) Acc G: 29.811%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2490 (0.2688) Acc D Real: 78.009%
Loss D Fake: 0.5849 (0.6597) Acc D Fake: 69.290%
Loss D: 0.834
Loss G: 0.8176 (0.7348) Acc G: 29.290%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2969 (0.2693) Acc D Real: 77.846%
Loss D Fake: 0.5832 (0.6583) Acc D Fake: 69.818%
Loss D: 0.880
Loss G: 0.8198 (0.7364) Acc G: 28.788%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.1799 (0.2677) Acc D Real: 77.951%
Loss D Fake: 0.5813 (0.6569) Acc D Fake: 70.327%
Loss D: 0.761
Loss G: 0.8227 (0.7379) Acc G: 28.304%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2452 (0.2673) Acc D Real: 77.905%
Loss D Fake: 0.5789 (0.6555) Acc D Fake: 70.819%
Loss D: 0.824
Loss G: 0.8259 (0.7395) Acc G: 27.836%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.2628 (0.2672) Acc D Real: 77.832%
Loss D Fake: 0.5764 (0.6542) Acc D Fake: 71.293%
Loss D: 0.839
Loss G: 0.8291 (0.7410) Acc G: 27.385%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3527 (0.2687) Acc D Real: 77.576%
Loss D Fake: 0.5741 (0.6528) Acc D Fake: 71.751%
Loss D: 0.927
Loss G: 0.8320 (0.7426) Acc G: 26.949%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3056 (0.2693) Acc D Real: 77.412%
Loss D Fake: 0.5721 (0.6515) Acc D Fake: 72.222%
Loss D: 0.878
Loss G: 0.8345 (0.7441) Acc G: 26.500%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.2649 (0.2692) Acc D Real: 77.342%
Loss D Fake: 0.5701 (0.6501) Acc D Fake: 72.678%
Loss D: 0.835
Loss G: 0.8372 (0.7456) Acc G: 26.066%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.1862 (0.2679) Acc D Real: 77.444%
Loss D Fake: 0.5680 (0.6488) Acc D Fake: 73.118%
Loss D: 0.754
Loss G: 0.8404 (0.7471) Acc G: 25.645%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.2812 (0.2681) Acc D Real: 77.343%
Loss D Fake: 0.5655 (0.6475) Acc D Fake: 73.545%
Loss D: 0.847
Loss G: 0.8437 (0.7487) Acc G: 25.238%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.2327 (0.2676) Acc D Real: 77.340%
Loss D Fake: 0.5631 (0.6462) Acc D Fake: 73.958%
Loss D: 0.796
Loss G: 0.8471 (0.7502) Acc G: 24.844%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3001 (0.2681) Acc D Real: 77.206%
Loss D Fake: 0.5606 (0.6448) Acc D Fake: 74.359%
Loss D: 0.861
Loss G: 0.8503 (0.7518) Acc G: 24.462%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.1845 (0.2668) Acc D Real: 77.295%
Loss D Fake: 0.5582 (0.6435) Acc D Fake: 74.747%
Loss D: 0.743
Loss G: 0.8539 (0.7533) Acc G: 24.091%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.2081 (0.2659) Acc D Real: 77.338%
Loss D Fake: 0.5555 (0.6422) Acc D Fake: 75.124%
Loss D: 0.764
Loss G: 0.8579 (0.7549) Acc G: 23.731%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4305 (0.2683) Acc D Real: 76.988%
Loss D Fake: 0.5528 (0.6409) Acc D Fake: 75.490%
Loss D: 0.983
Loss G: 0.8609 (0.7564) Acc G: 23.382%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.3005 (0.2688) Acc D Real: 76.875%
Loss D Fake: 0.5510 (0.6396) Acc D Fake: 75.845%
Loss D: 0.851
Loss G: 0.8633 (0.7580) Acc G: 23.043%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.1586 (0.2672) Acc D Real: 77.009%
Loss D Fake: 0.5491 (0.6383) Acc D Fake: 76.190%
Loss D: 0.708
Loss G: 0.8664 (0.7595) Acc G: 22.714%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2763 (0.2674) Acc D Real: 76.943%
Loss D Fake: 0.5468 (0.6370) Acc D Fake: 76.526%
Loss D: 0.823
Loss G: 0.8696 (0.7611) Acc G: 22.394%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.2725 (0.2674) Acc D Real: 76.897%
Loss D Fake: 0.5446 (0.6357) Acc D Fake: 76.852%
Loss D: 0.817
Loss G: 0.8727 (0.7626) Acc G: 22.083%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.2362 (0.2670) Acc D Real: 76.906%
Loss D Fake: 0.5424 (0.6345) Acc D Fake: 77.169%
Loss D: 0.779
Loss G: 0.8759 (0.7642) Acc G: 21.781%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3916 (0.2687) Acc D Real: 76.658%
Loss D Fake: 0.5404 (0.6332) Acc D Fake: 77.477%
Loss D: 0.932
Loss G: 0.8784 (0.7657) Acc G: 21.486%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3076 (0.2692) Acc D Real: 76.552%
Loss D Fake: 0.5388 (0.6319) Acc D Fake: 77.778%
Loss D: 0.846
Loss G: 0.8805 (0.7673) Acc G: 21.200%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.2891 (0.2695) Acc D Real: 76.479%
Loss D Fake: 0.5374 (0.6307) Acc D Fake: 78.070%
Loss D: 0.827
Loss G: 0.8826 (0.7688) Acc G: 20.921%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2418 (0.2691) Acc D Real: 76.478%
Loss D Fake: 0.5360 (0.6294) Acc D Fake: 78.355%
Loss D: 0.778
Loss G: 0.8849 (0.7703) Acc G: 20.649%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.1940 (0.2681) Acc D Real: 76.559%
Loss D Fake: 0.5342 (0.6282) Acc D Fake: 78.632%
Loss D: 0.728
Loss G: 0.8877 (0.7718) Acc G: 20.385%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.2606 (0.2680) Acc D Real: 76.532%
Loss D Fake: 0.5322 (0.6270) Acc D Fake: 78.903%
Loss D: 0.793
Loss G: 0.8907 (0.7733) Acc G: 20.127%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.1853 (0.2670) Acc D Real: 76.617%
Loss D Fake: 0.5301 (0.6258) Acc D Fake: 79.167%
Loss D: 0.715
Loss G: 0.8942 (0.7748) Acc G: 19.875%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.1841 (0.2660) Acc D Real: 76.702%
Loss D Fake: 0.5276 (0.6246) Acc D Fake: 79.424%
Loss D: 0.712
Loss G: 0.8981 (0.7763) Acc G: 19.630%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.2182 (0.2654) Acc D Real: 76.741%
Loss D Fake: 0.5249 (0.6234) Acc D Fake: 79.675%
Loss D: 0.743
Loss G: 0.9022 (0.7779) Acc G: 19.390%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.2197 (0.2649) Acc D Real: 76.782%
Loss D Fake: 0.5222 (0.6222) Acc D Fake: 79.920%
Loss D: 0.742
Loss G: 0.9065 (0.7794) Acc G: 19.157%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.4263 (0.2668) Acc D Real: 76.546%
Loss D Fake: 0.5196 (0.6209) Acc D Fake: 80.159%
Loss D: 0.946
Loss G: 0.9096 (0.7810) Acc G: 18.929%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.2965 (0.2671) Acc D Real: 76.486%
Loss D Fake: 0.5178 (0.6197) Acc D Fake: 80.392%
Loss D: 0.814
Loss G: 0.9122 (0.7825) Acc G: 18.706%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.1678 (0.2660) Acc D Real: 76.598%
Loss D Fake: 0.5161 (0.6185) Acc D Fake: 80.620%
Loss D: 0.684
Loss G: 0.9154 (0.7840) Acc G: 18.488%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.2988 (0.2663) Acc D Real: 76.544%
Loss D Fake: 0.5140 (0.6173) Acc D Fake: 80.843%
Loss D: 0.813
Loss G: 0.9185 (0.7856) Acc G: 18.276%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.2042 (0.2656) Acc D Real: 76.612%
Loss D Fake: 0.5119 (0.6161) Acc D Fake: 81.061%
Loss D: 0.716
Loss G: 0.9219 (0.7871) Acc G: 18.068%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3798 (0.2669) Acc D Real: 76.444%
Loss D Fake: 0.5098 (0.6149) Acc D Fake: 81.273%
Loss D: 0.890
Loss G: 0.9248 (0.7887) Acc G: 17.865%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.2807 (0.2671) Acc D Real: 76.409%
Loss D Fake: 0.5082 (0.6137) Acc D Fake: 81.481%
Loss D: 0.789
Loss G: 0.9273 (0.7902) Acc G: 17.667%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3620 (0.2681) Acc D Real: 76.280%
Loss D Fake: 0.5067 (0.6126) Acc D Fake: 81.685%
Loss D: 0.869
Loss G: 0.9294 (0.7918) Acc G: 17.473%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3760 (0.2693) Acc D Real: 76.135%
Loss D Fake: 0.5057 (0.6114) Acc D Fake: 81.884%
Loss D: 0.882
Loss G: 0.9307 (0.7933) Acc G: 17.283%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.4052 (0.2708) Acc D Real: 75.962%
Loss D Fake: 0.5051 (0.6103) Acc D Fake: 82.079%
Loss D: 0.910
Loss G: 0.9312 (0.7948) Acc G: 17.097%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3323 (0.2714) Acc D Real: 75.878%
Loss D Fake: 0.5049 (0.6091) Acc D Fake: 82.270%
Loss D: 0.837
Loss G: 0.9315 (0.7962) Acc G: 16.915%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3865 (0.2726) Acc D Real: 75.727%
Loss D Fake: 0.5048 (0.6080) Acc D Fake: 82.456%
Loss D: 0.891
Loss G: 0.9313 (0.7976) Acc G: 16.737%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.3382 (0.2733) Acc D Real: 75.656%
Loss D Fake: 0.5050 (0.6070) Acc D Fake: 82.639%
Loss D: 0.843
Loss G: 0.9312 (0.7990) Acc G: 16.562%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.3026 (0.2736) Acc D Real: 75.619%
Loss D Fake: 0.5050 (0.6059) Acc D Fake: 82.818%
Loss D: 0.808
Loss G: 0.9313 (0.8004) Acc G: 16.392%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2932 (0.2738) Acc D Real: 75.577%
Loss D Fake: 0.5049 (0.6049) Acc D Fake: 82.993%
Loss D: 0.798
Loss G: 0.9316 (0.8017) Acc G: 16.224%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3315 (0.2744) Acc D Real: 75.497%
Loss D Fake: 0.5047 (0.6039) Acc D Fake: 83.165%
Loss D: 0.836
Loss G: 0.9319 (0.8030) Acc G: 16.061%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2239 (0.2739) Acc D Real: 75.544%
Loss D Fake: 0.5044 (0.6029) Acc D Fake: 83.333%
Loss D: 0.728
Loss G: 0.9328 (0.8043) Acc G: 15.900%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.2191 (0.2733) Acc D Real: 75.588%
Loss D Fake: 0.5036 (0.6019) Acc D Fake: 83.498%
Loss D: 0.723
Loss G: 0.9344 (0.8056) Acc G: 15.743%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3719 (0.2743) Acc D Real: 75.476%
Loss D Fake: 0.5026 (0.6009) Acc D Fake: 83.660%
Loss D: 0.875
Loss G: 0.9356 (0.8069) Acc G: 15.588%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4204 (0.2757) Acc D Real: 75.309%
Loss D Fake: 0.5021 (0.6000) Acc D Fake: 83.819%
Loss D: 0.923
Loss G: 0.9361 (0.8081) Acc G: 15.437%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2801 (0.2758) Acc D Real: 75.291%
Loss D Fake: 0.5019 (0.5990) Acc D Fake: 83.974%
Loss D: 0.782
Loss G: 0.9366 (0.8094) Acc G: 15.288%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.2190 (0.2752) Acc D Real: 75.340%
Loss D Fake: 0.5014 (0.5981) Acc D Fake: 84.127%
Loss D: 0.720
Loss G: 0.9378 (0.8106) Acc G: 15.143%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3265 (0.2757) Acc D Real: 75.273%
Loss D Fake: 0.5006 (0.5972) Acc D Fake: 84.277%
Loss D: 0.827
Loss G: 0.9389 (0.8118) Acc G: 15.000%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2514 (0.2755) Acc D Real: 75.290%
Loss D Fake: 0.4999 (0.5963) Acc D Fake: 84.424%
Loss D: 0.751
Loss G: 0.9403 (0.8130) Acc G: 14.860%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.3299 (0.2760) Acc D Real: 75.230%
Loss D Fake: 0.4990 (0.5954) Acc D Fake: 84.568%
Loss D: 0.829
Loss G: 0.9416 (0.8142) Acc G: 14.722%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3054 (0.2763) Acc D Real: 75.192%
Loss D Fake: 0.4983 (0.5945) Acc D Fake: 84.709%
Loss D: 0.804
Loss G: 0.9427 (0.8154) Acc G: 14.587%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3345 (0.2768) Acc D Real: 75.125%
Loss D Fake: 0.4977 (0.5936) Acc D Fake: 84.848%
Loss D: 0.832
Loss G: 0.9435 (0.8166) Acc G: 14.455%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.2195 (0.2763) Acc D Real: 75.172%
Loss D Fake: 0.4971 (0.5927) Acc D Fake: 84.985%
Loss D: 0.717
Loss G: 0.9448 (0.8177) Acc G: 14.324%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3379 (0.2768) Acc D Real: 75.103%
Loss D Fake: 0.4963 (0.5919) Acc D Fake: 85.119%
Loss D: 0.834
Loss G: 0.9460 (0.8189) Acc G: 14.196%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.2339 (0.2764) Acc D Real: 75.144%
Loss D Fake: 0.4955 (0.5910) Acc D Fake: 85.251%
Loss D: 0.729
Loss G: 0.9476 (0.8200) Acc G: 14.071%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3161 (0.2768) Acc D Real: 75.103%
Loss D Fake: 0.4945 (0.5902) Acc D Fake: 85.380%
Loss D: 0.811
Loss G: 0.9491 (0.8211) Acc G: 13.947%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.2531 (0.2766) Acc D Real: 75.123%
Loss D Fake: 0.4936 (0.5893) Acc D Fake: 85.507%
Loss D: 0.747
Loss G: 0.9508 (0.8223) Acc G: 13.826%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.3319 (0.2771) Acc D Real: 75.069%
Loss D Fake: 0.4926 (0.5885) Acc D Fake: 85.632%
Loss D: 0.824
Loss G: 0.9523 (0.8234) Acc G: 13.707%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.4688 (0.2787) Acc D Real: 74.888%
Loss D Fake: 0.4920 (0.5877) Acc D Fake: 85.755%
Loss D: 0.961
Loss G: 0.9526 (0.8245) Acc G: 13.590%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3197 (0.2790) Acc D Real: 74.846%
Loss D Fake: 0.4920 (0.5868) Acc D Fake: 85.876%
Loss D: 0.812
Loss G: 0.9523 (0.8256) Acc G: 13.475%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.2791 (0.2790) Acc D Real: 74.838%
Loss D Fake: 0.4923 (0.5861) Acc D Fake: 85.994%
Loss D: 0.771
Loss G: 0.9522 (0.8266) Acc G: 13.361%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2820 (0.2791) Acc D Real: 74.830%
Loss D Fake: 0.4923 (0.5853) Acc D Fake: 86.111%
Loss D: 0.774
Loss G: 0.9524 (0.8277) Acc G: 13.250%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2466 (0.2788) Acc D Real: 74.853%
Loss D Fake: 0.4920 (0.5845) Acc D Fake: 86.226%
Loss D: 0.739
Loss G: 0.9531 (0.8287) Acc G: 13.140%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.2730 (0.2788) Acc D Real: 74.855%
Loss D Fake: 0.4915 (0.5837) Acc D Fake: 86.339%
Loss D: 0.765
Loss G: 0.9541 (0.8297) Acc G: 13.033%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2869 (0.2788) Acc D Real: 74.848%
Loss D Fake: 0.4909 (0.5830) Acc D Fake: 86.450%
Loss D: 0.778
Loss G: 0.9554 (0.8308) Acc G: 12.927%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.2375 (0.2785) Acc D Real: 74.883%
Loss D Fake: 0.4901 (0.5822) Acc D Fake: 86.559%
Loss D: 0.728
Loss G: 0.9570 (0.8318) Acc G: 12.823%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3700 (0.2792) Acc D Real: 74.802%
Loss D Fake: 0.4892 (0.5815) Acc D Fake: 86.667%
Loss D: 0.859
Loss G: 0.9582 (0.8328) Acc G: 12.720%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3421 (0.2797) Acc D Real: 74.743%
Loss D Fake: 0.4886 (0.5808) Acc D Fake: 86.772%
Loss D: 0.831
Loss G: 0.9590 (0.8338) Acc G: 12.619%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.2653 (0.2796) Acc D Real: 74.758%
Loss D Fake: 0.4882 (0.5800) Acc D Fake: 86.877%
Loss D: 0.753
Loss G: 0.9599 (0.8348) Acc G: 12.520%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.4989 (0.2813) Acc D Real: 74.577%
Loss D Fake: 0.4878 (0.5793) Acc D Fake: 86.979%
Loss D: 0.987
Loss G: 0.9597 (0.8358) Acc G: 12.422%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3130 (0.2816) Acc D Real: 74.548%
Loss D Fake: 0.4882 (0.5786) Acc D Fake: 87.080%
Loss D: 0.801
Loss G: 0.9593 (0.8367) Acc G: 12.326%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.1872 (0.2808) Acc D Real: 74.621%
Loss D Fake: 0.4882 (0.5779) Acc D Fake: 87.179%
Loss D: 0.675
Loss G: 0.9597 (0.8377) Acc G: 12.231%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3103 (0.2811) Acc D Real: 74.595%
Loss D Fake: 0.4878 (0.5772) Acc D Fake: 87.277%
Loss D: 0.798
Loss G: 0.9603 (0.8386) Acc G: 12.137%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.1676 (0.2802) Acc D Real: 74.686%
Loss D Fake: 0.4873 (0.5765) Acc D Fake: 87.374%
Loss D: 0.655
Loss G: 0.9618 (0.8395) Acc G: 12.045%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3387 (0.2806) Acc D Real: 74.635%
Loss D Fake: 0.4864 (0.5759) Acc D Fake: 87.469%
Loss D: 0.825
Loss G: 0.9632 (0.8405) Acc G: 11.955%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2885 (0.2807) Acc D Real: 74.628%
Loss D Fake: 0.4856 (0.5752) Acc D Fake: 87.562%
Loss D: 0.774
Loss G: 0.9645 (0.8414) Acc G: 11.866%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3128 (0.2809) Acc D Real: 74.600%
Loss D Fake: 0.4849 (0.5745) Acc D Fake: 87.654%
Loss D: 0.798
Loss G: 0.9656 (0.8423) Acc G: 11.778%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.2192 (0.2805) Acc D Real: 74.647%
Loss D Fake: 0.4842 (0.5739) Acc D Fake: 87.745%
Loss D: 0.703
Loss G: 0.9671 (0.8432) Acc G: 11.691%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3188 (0.2808) Acc D Real: 74.624%
Loss D Fake: 0.4833 (0.5732) Acc D Fake: 87.835%
Loss D: 0.802
Loss G: 0.9685 (0.8441) Acc G: 11.606%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.2552 (0.2806) Acc D Real: 74.642%
Loss D Fake: 0.4825 (0.5725) Acc D Fake: 87.923%
Loss D: 0.738
Loss G: 0.9701 (0.8451) Acc G: 11.522%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3119 (0.2808) Acc D Real: 74.617%
Loss D Fake: 0.4816 (0.5719) Acc D Fake: 88.010%
Loss D: 0.793
Loss G: 0.9715 (0.8460) Acc G: 11.439%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3218 (0.2811) Acc D Real: 74.586%
Loss D Fake: 0.4808 (0.5712) Acc D Fake: 88.095%
Loss D: 0.803
Loss G: 0.9728 (0.8469) Acc G: 11.357%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.2846 (0.2811) Acc D Real: 74.590%
Loss D Fake: 0.4801 (0.5706) Acc D Fake: 88.180%
Loss D: 0.765
Loss G: 0.9740 (0.8478) Acc G: 11.277%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.1651 (0.2803) Acc D Real: 74.681%
Loss D Fake: 0.4793 (0.5699) Acc D Fake: 88.263%
Loss D: 0.644
Loss G: 0.9760 (0.8487) Acc G: 11.197%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.3757 (0.2810) Acc D Real: 74.610%
Loss D Fake: 0.4781 (0.5693) Acc D Fake: 88.345%
Loss D: 0.854
Loss G: 0.9775 (0.8496) Acc G: 11.119%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3151 (0.2812) Acc D Real: 74.592%
Loss D Fake: 0.4774 (0.5687) Acc D Fake: 88.426%
Loss D: 0.793
Loss G: 0.9787 (0.8505) Acc G: 11.042%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3187 (0.2815) Acc D Real: 74.567%
Loss D Fake: 0.4768 (0.5680) Acc D Fake: 88.506%
Loss D: 0.796
Loss G: 0.9797 (0.8514) Acc G: 10.966%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.1517 (0.2806) Acc D Real: 74.664%
Loss D Fake: 0.4761 (0.5674) Acc D Fake: 88.584%
Loss D: 0.628
Loss G: 0.9815 (0.8523) Acc G: 10.890%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.2857 (0.2806) Acc D Real: 74.663%
Loss D Fake: 0.4749 (0.5668) Acc D Fake: 88.662%
Loss D: 0.761
Loss G: 0.9835 (0.8531) Acc G: 10.816%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3133 (0.2808) Acc D Real: 74.641%
Loss D Fake: 0.4739 (0.5661) Acc D Fake: 88.739%
Loss D: 0.787
Loss G: 0.9852 (0.8540) Acc G: 10.743%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.4274 (0.2818) Acc D Real: 74.538%
Loss D Fake: 0.4732 (0.5655) Acc D Fake: 88.814%
Loss D: 0.901
Loss G: 0.9858 (0.8549) Acc G: 10.671%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4671 (0.2831) Acc D Real: 74.413%
Loss D Fake: 0.4732 (0.5649) Acc D Fake: 88.889%
Loss D: 0.940
Loss G: 0.9853 (0.8558) Acc G: 10.600%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.4365 (0.2841) Acc D Real: 74.310%
Loss D Fake: 0.4737 (0.5643) Acc D Fake: 88.962%
Loss D: 0.910
Loss G: 0.9839 (0.8566) Acc G: 10.530%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2076 (0.2836) Acc D Real: 74.364%
Loss D Fake: 0.4745 (0.5637) Acc D Fake: 89.035%
Loss D: 0.682
Loss G: 0.9832 (0.8575) Acc G: 10.461%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2021 (0.2830) Acc D Real: 74.421%
Loss D Fake: 0.4745 (0.5631) Acc D Fake: 89.107%
Loss D: 0.677
Loss G: 0.9836 (0.8583) Acc G: 10.392%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.2590 (0.2829) Acc D Real: 74.438%
Loss D Fake: 0.4742 (0.5625) Acc D Fake: 89.177%
Loss D: 0.733
Loss G: 0.9844 (0.8591) Acc G: 10.325%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.2945 (0.2830) Acc D Real: 74.431%
Loss D Fake: 0.4736 (0.5620) Acc D Fake: 89.247%
Loss D: 0.768
Loss G: 0.9854 (0.8599) Acc G: 10.258%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.2257 (0.2826) Acc D Real: 74.472%
Loss D Fake: 0.4730 (0.5614) Acc D Fake: 89.316%
Loss D: 0.699
Loss G: 0.9867 (0.8607) Acc G: 10.192%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.2393 (0.2823) Acc D Real: 74.503%
Loss D Fake: 0.4721 (0.5608) Acc D Fake: 89.384%
Loss D: 0.711
Loss G: 0.9885 (0.8616) Acc G: 10.127%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3502 (0.2827) Acc D Real: 74.459%
Loss D Fake: 0.4712 (0.5603) Acc D Fake: 89.451%
Loss D: 0.821
Loss G: 0.9899 (0.8624) Acc G: 10.063%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.2250 (0.2824) Acc D Real: 74.498%
Loss D Fake: 0.4704 (0.5597) Acc D Fake: 89.518%
Loss D: 0.695
Loss G: 0.9915 (0.8632) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.1757 (0.2817) Acc D Real: 74.570%
Loss D Fake: 0.4693 (0.5591) Acc D Fake: 89.583%
Loss D: 0.645
Loss G: 0.9939 (0.8640) Acc G: 9.938%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2912 (0.2818) Acc D Real: 74.565%
Loss D Fake: 0.4680 (0.5586) Acc D Fake: 89.648%
Loss D: 0.759
Loss G: 0.9962 (0.8648) Acc G: 9.876%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3540 (0.2822) Acc D Real: 74.523%
Loss D Fake: 0.4669 (0.5580) Acc D Fake: 89.712%
Loss D: 0.821
Loss G: 0.9978 (0.8656) Acc G: 9.815%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.3846 (0.2828) Acc D Real: 74.462%
Loss D Fake: 0.4662 (0.5574) Acc D Fake: 89.775%
Loss D: 0.851
Loss G: 0.9986 (0.8665) Acc G: 9.755%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2987 (0.2829) Acc D Real: 74.454%
Loss D Fake: 0.4659 (0.5569) Acc D Fake: 89.837%
Loss D: 0.765
Loss G: 0.9992 (0.8673) Acc G: 9.695%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2351 (0.2827) Acc D Real: 74.488%
Loss D Fake: 0.4654 (0.5563) Acc D Fake: 89.899%
Loss D: 0.701
Loss G: 1.0002 (0.8681) Acc G: 9.636%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.4128 (0.2834) Acc D Real: 74.410%
Loss D Fake: 0.4650 (0.5558) Acc D Fake: 89.960%
Loss D: 0.878
Loss G: 1.0006 (0.8689) Acc G: 9.578%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2682 (0.2833) Acc D Real: 74.428%
Loss D Fake: 0.4649 (0.5552) Acc D Fake: 90.020%
Loss D: 0.733
Loss G: 1.0009 (0.8697) Acc G: 9.521%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.2335 (0.2830) Acc D Real: 74.461%
Loss D Fake: 0.4646 (0.5547) Acc D Fake: 90.079%
Loss D: 0.698
Loss G: 1.0017 (0.8704) Acc G: 9.464%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3188 (0.2833) Acc D Real: 74.444%
Loss D Fake: 0.4641 (0.5542) Acc D Fake: 90.138%
Loss D: 0.783
Loss G: 1.0025 (0.8712) Acc G: 9.408%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.4126 (0.2840) Acc D Real: 74.368%
Loss D Fake: 0.4643 (0.5536) Acc D Fake: 90.196%
Loss D: 0.877
Loss G: 1.0006 (0.8720) Acc G: 9.353%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3509 (0.2844) Acc D Real: 74.331%
Loss D Fake: 0.4662 (0.5531) Acc D Fake: 90.253%
Loss D: 0.817
Loss G: 0.9978 (0.8727) Acc G: 9.298%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3525 (0.2848) Acc D Real: 74.297%
Loss D Fake: 0.4681 (0.5526) Acc D Fake: 90.310%
Loss D: 0.821
Loss G: 0.9948 (0.8734) Acc G: 9.244%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.2720 (0.2847) Acc D Real: 74.308%
Loss D Fake: 0.4700 (0.5521) Acc D Fake: 90.366%
Loss D: 0.742
Loss G: 0.9923 (0.8741) Acc G: 9.191%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3045 (0.2848) Acc D Real: 74.301%
Loss D Fake: 0.4716 (0.5517) Acc D Fake: 90.421%
Loss D: 0.776
Loss G: 0.9902 (0.8748) Acc G: 9.138%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.2831 (0.2848) Acc D Real: 74.305%
Loss D Fake: 0.4729 (0.5512) Acc D Fake: 90.476%
Loss D: 0.756
Loss G: 0.9884 (0.8754) Acc G: 9.086%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3533 (0.2852) Acc D Real: 74.269%
Loss D Fake: 0.4742 (0.5508) Acc D Fake: 90.530%
Loss D: 0.828
Loss G: 0.9863 (0.8761) Acc G: 9.034%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2093 (0.2848) Acc D Real: 74.317%
Loss D Fake: 0.4756 (0.5504) Acc D Fake: 90.584%
Loss D: 0.685
Loss G: 0.9848 (0.8767) Acc G: 8.983%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2342 (0.2845) Acc D Real: 74.350%
Loss D Fake: 0.4765 (0.5500) Acc D Fake: 90.637%
Loss D: 0.711
Loss G: 0.9841 (0.8773) Acc G: 8.933%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.1482 (0.2838) Acc D Real: 74.433%
Loss D Fake: 0.4769 (0.5495) Acc D Fake: 90.689%
Loss D: 0.625
Loss G: 0.9844 (0.8779) Acc G: 8.883%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.3256 (0.2840) Acc D Real: 74.412%
Loss D Fake: 0.4768 (0.5491) Acc D Fake: 90.741%
Loss D: 0.802
Loss G: 0.9847 (0.8785) Acc G: 8.833%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.1486 (0.2832) Acc D Real: 74.490%
Loss D Fake: 0.4767 (0.5487) Acc D Fake: 90.792%
Loss D: 0.625
Loss G: 0.9857 (0.8791) Acc G: 8.785%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.2981 (0.2833) Acc D Real: 74.485%
Loss D Fake: 0.4763 (0.5483) Acc D Fake: 90.842%
Loss D: 0.774
Loss G: 0.9867 (0.8797) Acc G: 8.736%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2666 (0.2832) Acc D Real: 74.496%
Loss D Fake: 0.4759 (0.5479) Acc D Fake: 90.893%
Loss D: 0.743
Loss G: 0.9876 (0.8803) Acc G: 8.689%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3573 (0.2836) Acc D Real: 74.455%
Loss D Fake: 0.4756 (0.5476) Acc D Fake: 90.942%
Loss D: 0.833
Loss G: 0.9880 (0.8808) Acc G: 8.641%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2993 (0.2837) Acc D Real: 74.450%
Loss D Fake: 0.4756 (0.5472) Acc D Fake: 90.991%
Loss D: 0.775
Loss G: 0.9883 (0.8814) Acc G: 8.595%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1791 (0.2832) Acc D Real: 74.509%
Loss D Fake: 0.4755 (0.5468) Acc D Fake: 91.039%
Loss D: 0.655
Loss G: 0.9891 (0.8820) Acc G: 8.548%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.2283 (0.2829) Acc D Real: 74.542%
Loss D Fake: 0.4750 (0.5464) Acc D Fake: 91.087%
Loss D: 0.703
Loss G: 0.9903 (0.8826) Acc G: 8.503%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.1863 (0.2823) Acc D Real: 74.599%
Loss D Fake: 0.4743 (0.5460) Acc D Fake: 91.135%
Loss D: 0.661
Loss G: 0.9921 (0.8832) Acc G: 8.457%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.2720 (0.2823) Acc D Real: 74.608%
Loss D Fake: 0.4733 (0.5456) Acc D Fake: 91.182%
Loss D: 0.745
Loss G: 0.9938 (0.8837) Acc G: 8.413%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.2688 (0.2822) Acc D Real: 74.618%
Loss D Fake: 0.4725 (0.5452) Acc D Fake: 91.228%
Loss D: 0.741
Loss G: 0.9954 (0.8843) Acc G: 8.368%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.2773 (0.2822) Acc D Real: 74.627%
Loss D Fake: 0.4718 (0.5449) Acc D Fake: 91.274%
Loss D: 0.749
Loss G: 0.9968 (0.8849) Acc G: 8.325%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.3234 (0.2824) Acc D Real: 74.606%
Loss D Fake: 0.4712 (0.5445) Acc D Fake: 91.319%
Loss D: 0.795
Loss G: 0.9977 (0.8855) Acc G: 8.281%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3653 (0.2828) Acc D Real: 74.569%
Loss D Fake: 0.4710 (0.5441) Acc D Fake: 91.364%
Loss D: 0.836
Loss G: 0.9979 (0.8861) Acc G: 8.238%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.3103 (0.2830) Acc D Real: 74.560%
Loss D Fake: 0.4711 (0.5437) Acc D Fake: 91.409%
Loss D: 0.781
Loss G: 0.9978 (0.8867) Acc G: 8.196%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.2730 (0.2829) Acc D Real: 74.569%
Loss D Fake: 0.4711 (0.5433) Acc D Fake: 91.453%
Loss D: 0.744
Loss G: 0.9978 (0.8872) Acc G: 8.154%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.3546 (0.2833) Acc D Real: 74.536%
Loss D Fake: 0.4712 (0.5430) Acc D Fake: 91.497%
Loss D: 0.826
Loss G: 0.9975 (0.8878) Acc G: 8.112%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3817 (0.2838) Acc D Real: 74.487%
Loss D Fake: 0.4715 (0.5426) Acc D Fake: 91.540%
Loss D: 0.853
Loss G: 0.9966 (0.8884) Acc G: 8.071%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2378 (0.2836) Acc D Real: 74.514%
Loss D Fake: 0.4721 (0.5423) Acc D Fake: 91.582%
Loss D: 0.710
Loss G: 0.9960 (0.8889) Acc G: 8.030%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2907 (0.2836) Acc D Real: 74.514%
Loss D Fake: 0.4723 (0.5419) Acc D Fake: 91.625%
Loss D: 0.763
Loss G: 0.9956 (0.8894) Acc G: 7.990%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2791 (0.2836) Acc D Real: 74.520%
Loss D Fake: 0.4726 (0.5416) Acc D Fake: 91.667%
Loss D: 0.752
Loss G: 0.9953 (0.8900) Acc G: 7.950%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2393 (0.2834) Acc D Real: 74.548%
Loss D Fake: 0.4727 (0.5412) Acc D Fake: 91.708%
Loss D: 0.712
Loss G: 0.9953 (0.8905) Acc G: 7.910%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1502 (0.2827) Acc D Real: 74.618%
Loss D Fake: 0.4725 (0.5409) Acc D Fake: 91.749%
Loss D: 0.623
Loss G: 0.9963 (0.8910) Acc G: 7.871%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2137 (0.2824) Acc D Real: 74.657%
Loss D Fake: 0.4718 (0.5405) Acc D Fake: 91.790%
Loss D: 0.686
Loss G: 0.9977 (0.8915) Acc G: 7.833%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.1769 (0.2818) Acc D Real: 74.711%
Loss D Fake: 0.4710 (0.5402) Acc D Fake: 91.830%
Loss D: 0.648
Loss G: 0.9997 (0.8921) Acc G: 7.794%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2478 (0.2817) Acc D Real: 74.733%
Loss D Fake: 0.4699 (0.5399) Acc D Fake: 91.870%
Loss D: 0.718
Loss G: 1.0018 (0.8926) Acc G: 7.756%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3370 (0.2819) Acc D Real: 74.710%
Loss D Fake: 0.4689 (0.5395) Acc D Fake: 91.909%
Loss D: 0.806
Loss G: 1.0034 (0.8931) Acc G: 7.718%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.1800 (0.2814) Acc D Real: 74.764%
Loss D Fake: 0.4681 (0.5392) Acc D Fake: 91.948%
Loss D: 0.648
Loss G: 1.0051 (0.8937) Acc G: 7.681%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.1245 (0.2807) Acc D Real: 74.845%
Loss D Fake: 0.4671 (0.5388) Acc D Fake: 91.987%
Loss D: 0.592
Loss G: 1.0077 (0.8942) Acc G: 7.644%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1778 (0.2802) Acc D Real: 74.899%
Loss D Fake: 0.4656 (0.5385) Acc D Fake: 92.026%
Loss D: 0.643
Loss G: 1.0108 (0.8948) Acc G: 7.608%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.1934 (0.2798) Acc D Real: 74.946%
Loss D Fake: 0.4640 (0.5381) Acc D Fake: 92.063%
Loss D: 0.657
Loss G: 1.0139 (0.8954) Acc G: 7.571%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2152 (0.2795) Acc D Real: 74.984%
Loss D Fake: 0.4624 (0.5378) Acc D Fake: 92.101%
Loss D: 0.678
Loss G: 1.0171 (0.8959) Acc G: 7.536%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3160 (0.2797) Acc D Real: 74.973%
Loss D Fake: 0.4610 (0.5374) Acc D Fake: 92.138%
Loss D: 0.777
Loss G: 1.0197 (0.8965) Acc G: 7.500%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2131 (0.2793) Acc D Real: 75.010%
Loss D Fake: 0.4598 (0.5370) Acc D Fake: 92.175%
Loss D: 0.673
Loss G: 1.0221 (0.8971) Acc G: 7.465%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.2398 (0.2792) Acc D Real: 75.036%
Loss D Fake: 0.4586 (0.5367) Acc D Fake: 92.212%
Loss D: 0.698
Loss G: 1.0243 (0.8977) Acc G: 7.430%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.2188 (0.2789) Acc D Real: 75.071%
Loss D Fake: 0.4576 (0.5363) Acc D Fake: 92.248%
Loss D: 0.676
Loss G: 1.0265 (0.8983) Acc G: 7.395%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2240 (0.2786) Acc D Real: 75.102%
Loss D Fake: 0.4565 (0.5359) Acc D Fake: 92.284%
Loss D: 0.681
Loss G: 1.0288 (0.8989) Acc G: 7.361%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3424 (0.2789) Acc D Real: 75.081%
Loss D Fake: 0.4556 (0.5356) Acc D Fake: 92.320%
Loss D: 0.798
Loss G: 1.0303 (0.8995) Acc G: 7.327%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3095 (0.2791) Acc D Real: 75.075%
Loss D Fake: 0.4550 (0.5352) Acc D Fake: 92.355%
Loss D: 0.765
Loss G: 1.0313 (0.9001) Acc G: 7.294%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2483 (0.2789) Acc D Real: 75.096%
Loss D Fake: 0.4546 (0.5348) Acc D Fake: 92.390%
Loss D: 0.703
Loss G: 1.0323 (0.9007) Acc G: 7.260%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2183 (0.2786) Acc D Real: 75.131%
Loss D Fake: 0.4541 (0.5344) Acc D Fake: 92.424%
Loss D: 0.672
Loss G: 1.0335 (0.9013) Acc G: 7.227%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2552 (0.2785) Acc D Real: 75.149%
Loss D Fake: 0.4535 (0.5341) Acc D Fake: 92.459%
Loss D: 0.709
Loss G: 1.0347 (0.9019) Acc G: 7.195%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2184 (0.2783) Acc D Real: 75.183%
Loss D Fake: 0.4528 (0.5337) Acc D Fake: 92.492%
Loss D: 0.671
Loss G: 1.0362 (0.9025) Acc G: 7.162%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.1014 (0.2775) Acc D Real: 75.267%
Loss D Fake: 0.4520 (0.5333) Acc D Fake: 92.526%
Loss D: 0.553
Loss G: 1.0386 (0.9031) Acc G: 7.130%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2575 (0.2774) Acc D Real: 75.283%
Loss D Fake: 0.4507 (0.5330) Acc D Fake: 92.560%
Loss D: 0.708
Loss G: 1.0411 (0.9038) Acc G: 7.098%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3649 (0.2778) Acc D Real: 75.254%
Loss D Fake: 0.4497 (0.5326) Acc D Fake: 92.593%
Loss D: 0.815
Loss G: 1.0426 (0.9044) Acc G: 7.067%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.2962 (0.2779) Acc D Real: 75.254%
Loss D Fake: 0.4492 (0.5322) Acc D Fake: 92.601%
Loss D: 0.745
Loss G: 1.0436 (0.9050) Acc G: 7.059%
LR: 2.000e-04
Epoch: 6/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2853 (0.2803) Acc D Real: 77.161%
Loss D Fake: 0.4486 (0.4488) Acc D Fake: 100.000%
Loss D: 0.734
Loss G: 1.0448 (1.0445) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.2222 (0.2609) Acc D Real: 78.924%
Loss D Fake: 0.4484 (0.4486) Acc D Fake: 100.000%
Loss D: 0.671
Loss G: 1.0455 (1.0448) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2248 (0.2519) Acc D Real: 79.596%
Loss D Fake: 0.4481 (0.4485) Acc D Fake: 100.000%
Loss D: 0.673
Loss G: 1.0463 (1.0452) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2539 (0.2523) Acc D Real: 79.531%
Loss D Fake: 0.4478 (0.4484) Acc D Fake: 100.000%
Loss D: 0.702
Loss G: 1.0471 (1.0456) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.3115 (0.2622) Acc D Real: 78.602%
Loss D Fake: 0.4476 (0.4482) Acc D Fake: 100.000%
Loss D: 0.759
Loss G: 1.0475 (1.0459) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2560 (0.2613) Acc D Real: 78.795%
Loss D Fake: 0.4475 (0.4481) Acc D Fake: 100.000%
Loss D: 0.703
Loss G: 1.0477 (1.0462) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3234 (0.2690) Acc D Real: 78.034%
Loss D Fake: 0.4477 (0.4481) Acc D Fake: 100.000%
Loss D: 0.771
Loss G: 1.0473 (1.0463) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2503 (0.2670) Acc D Real: 78.264%
Loss D Fake: 0.4481 (0.4481) Acc D Fake: 100.000%
Loss D: 0.698
Loss G: 1.0470 (1.0464) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.1782 (0.2581) Acc D Real: 79.078%
Loss D Fake: 0.4482 (0.4481) Acc D Fake: 100.000%
Loss D: 0.626
Loss G: 1.0474 (1.0465) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1653 (0.2497) Acc D Real: 79.867%
Loss D Fake: 0.4480 (0.4481) Acc D Fake: 100.000%
Loss D: 0.613
Loss G: 1.0484 (1.0467) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.1543 (0.2417) Acc D Real: 80.690%
Loss D Fake: 0.4475 (0.4480) Acc D Fake: 100.000%
Loss D: 0.602
Loss G: 1.0500 (1.0469) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.1189 (0.2323) Acc D Real: 81.571%
Loss D Fake: 0.4468 (0.4479) Acc D Fake: 100.000%
Loss D: 0.566
Loss G: 1.0523 (1.0474) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2261 (0.2318) Acc D Real: 81.637%
Loss D Fake: 0.4461 (0.4478) Acc D Fake: 100.000%
Loss D: 0.672
Loss G: 1.0532 (1.0478) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3862 (0.2421) Acc D Real: 80.681%
Loss D Fake: 0.4466 (0.4477) Acc D Fake: 100.000%
Loss D: 0.833
Loss G: 1.0525 (1.0481) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3889 (0.2513) Acc D Real: 79.837%
Loss D Fake: 0.4477 (0.4477) Acc D Fake: 100.000%
Loss D: 0.837
Loss G: 1.0507 (1.0482) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.2260 (0.2498) Acc D Real: 79.985%
Loss D Fake: 0.4490 (0.4478) Acc D Fake: 100.000%
Loss D: 0.675
Loss G: 1.0492 (1.0483) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.1907 (0.2465) Acc D Real: 80.312%
Loss D Fake: 0.4498 (0.4479) Acc D Fake: 100.000%
Loss D: 0.641
Loss G: 1.0485 (1.0483) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.4147 (0.2554) Acc D Real: 79.534%
Loss D Fake: 0.4624 (0.4487) Acc D Fake: 100.000%
Loss D: 0.877
Loss G: 0.2622 (1.0069) Acc G: 4.123%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1597 (0.2506) Acc D Real: 79.969%
Loss D Fake: 3.9349 (0.6230) Acc D Fake: 95.833%
Loss D: 4.095
Loss G: 0.1812 (0.9657) Acc G: 8.167%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3189 (0.2538) Acc D Real: 79.653%
Loss D Fake: 4.1284 (0.7899) Acc D Fake: 91.905%
Loss D: 4.447
Loss G: 0.1629 (0.9274) Acc G: 11.984%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1869 (0.2508) Acc D Real: 79.910%
Loss D Fake: 4.1828 (0.9441) Acc D Fake: 88.258%
Loss D: 4.370
Loss G: 0.1546 (0.8923) Acc G: 15.455%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1888 (0.2481) Acc D Real: 80.170%
Loss D Fake: 4.1846 (1.0850) Acc D Fake: 84.928%
Loss D: 4.373
Loss G: 0.1505 (0.8600) Acc G: 18.623%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.2320 (0.2474) Acc D Real: 80.189%
Loss D Fake: 4.1574 (1.2130) Acc D Fake: 81.875%
Loss D: 4.389
Loss G: 0.1487 (0.8304) Acc G: 21.528%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2012 (0.2456) Acc D Real: 80.344%
Loss D Fake: 4.1124 (1.3290) Acc D Fake: 79.067%
Loss D: 4.314
Loss G: 0.1485 (0.8031) Acc G: 24.200%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.1393 (0.2415) Acc D Real: 80.697%
Loss D Fake: 4.0552 (1.4339) Acc D Fake: 76.474%
Loss D: 4.194
Loss G: 0.1493 (0.7780) Acc G: 26.667%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3167 (0.2443) Acc D Real: 80.411%
Loss D Fake: 3.9901 (1.5285) Acc D Fake: 74.074%
Loss D: 4.307
Loss G: 0.1508 (0.7548) Acc G: 28.951%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.1689 (0.2416) Acc D Real: 80.645%
Loss D Fake: 3.9187 (1.6139) Acc D Fake: 71.845%
Loss D: 4.088
Loss G: 0.1531 (0.7333) Acc G: 31.071%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.2629 (0.2423) Acc D Real: 80.544%
Loss D Fake: 3.8435 (1.6908) Acc D Fake: 69.770%
Loss D: 4.106
Loss G: 0.1558 (0.7134) Acc G: 33.046%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.1910 (0.2406) Acc D Real: 80.688%
Loss D Fake: 3.7658 (1.7600) Acc D Fake: 67.835%
Loss D: 3.957
Loss G: 0.1591 (0.6949) Acc G: 34.833%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.1895 (0.2390) Acc D Real: 80.822%
Loss D Fake: 3.6861 (1.8221) Acc D Fake: 66.077%
Loss D: 3.876
Loss G: 0.1628 (0.6777) Acc G: 36.505%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1837 (0.2372) Acc D Real: 80.975%
Loss D Fake: 3.6051 (1.8778) Acc D Fake: 64.429%
Loss D: 3.789
Loss G: 0.1671 (0.6618) Acc G: 38.073%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.1257 (0.2339) Acc D Real: 81.291%
Loss D Fake: 3.5234 (1.9277) Acc D Fake: 62.880%
Loss D: 3.649
Loss G: 0.1719 (0.6469) Acc G: 39.495%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.1114 (0.2303) Acc D Real: 81.621%
Loss D Fake: 3.4411 (1.9722) Acc D Fake: 61.472%
Loss D: 3.553
Loss G: 0.1772 (0.6331) Acc G: 40.833%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.1312 (0.2274) Acc D Real: 81.879%
Loss D Fake: 3.3589 (2.0118) Acc D Fake: 60.144%
Loss D: 3.490
Loss G: 0.1830 (0.6202) Acc G: 42.057%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.1804 (0.2261) Acc D Real: 81.985%
Loss D Fake: 3.2765 (2.0469) Acc D Fake: 58.937%
Loss D: 3.457
Loss G: 0.1894 (0.6083) Acc G: 43.203%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3141 (0.2285) Acc D Real: 81.722%
Loss D Fake: 3.1933 (2.0779) Acc D Fake: 57.794%
Loss D: 3.507
Loss G: 0.1967 (0.5971) Acc G: 44.243%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2162 (0.2282) Acc D Real: 81.705%
Loss D Fake: 3.1104 (2.1051) Acc D Fake: 56.756%
Loss D: 3.327
Loss G: 0.2040 (0.5868) Acc G: 45.228%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.1578 (0.2264) Acc D Real: 81.867%
Loss D Fake: 3.0298 (2.1288) Acc D Fake: 55.771%
Loss D: 3.188
Loss G: 0.2121 (0.5772) Acc G: 46.119%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2736 (0.2275) Acc D Real: 81.715%
Loss D Fake: 2.9471 (2.1493) Acc D Fake: 54.876%
Loss D: 3.221
Loss G: 0.2214 (0.5683) Acc G: 46.966%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.2228 (0.2274) Acc D Real: 81.709%
Loss D Fake: 2.8616 (2.1666) Acc D Fake: 54.066%
Loss D: 3.084
Loss G: 0.2325 (0.5601) Acc G: 47.731%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.1321 (0.2252) Acc D Real: 81.930%
Loss D Fake: 2.7722 (2.1811) Acc D Fake: 53.335%
Loss D: 2.904
Loss G: 0.2455 (0.5526) Acc G: 48.420%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.1744 (0.2240) Acc D Real: 82.045%
Loss D Fake: 2.6777 (2.1926) Acc D Fake: 52.676%
Loss D: 2.852
Loss G: 0.2613 (0.5458) Acc G: 49.038%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.1969 (0.2234) Acc D Real: 82.095%
Loss D Fake: 2.5739 (2.2013) Acc D Fake: 52.085%
Loss D: 2.771
Loss G: 0.2816 (0.5398) Acc G: 49.553%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.2556 (0.2241) Acc D Real: 81.997%
Loss D Fake: 2.4555 (2.2069) Acc D Fake: 51.594%
Loss D: 2.711
Loss G: 0.3088 (0.5347) Acc G: 49.970%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.2495 (0.2246) Acc D Real: 81.927%
Loss D Fake: 2.3143 (2.2093) Acc D Fake: 51.197%
Loss D: 2.564
Loss G: 0.3459 (0.5306) Acc G: 50.286%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.2447 (0.2251) Acc D Real: 81.863%
Loss D Fake: 2.1308 (2.2076) Acc D Fake: 50.923%
Loss D: 2.375
Loss G: 0.4151 (0.5281) Acc G: 50.422%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.2413 (0.2254) Acc D Real: 81.812%
Loss D Fake: 1.7634 (2.1983) Acc D Fake: 50.904%
Loss D: 2.005
Loss G: 0.9925 (0.5378) Acc G: 49.372%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2628 (0.2262) Acc D Real: 81.737%
Loss D Fake: 0.4409 (2.1625) Acc D Fake: 51.906%
Loss D: 0.704
Loss G: 1.0795 (0.5489) Acc G: 48.364%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.2605 (0.2269) Acc D Real: 81.652%
Loss D Fake: 0.4223 (2.1277) Acc D Fake: 52.868%
Loss D: 0.683
Loss G: 1.1020 (0.5599) Acc G: 47.397%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.1511 (0.2254) Acc D Real: 81.819%
Loss D Fake: 0.4144 (2.0941) Acc D Fake: 53.792%
Loss D: 0.565
Loss G: 1.1141 (0.5708) Acc G: 46.468%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.2223 (0.2253) Acc D Real: 81.837%
Loss D Fake: 0.4097 (2.0617) Acc D Fake: 54.680%
Loss D: 0.632
Loss G: 1.1218 (0.5814) Acc G: 45.574%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2684 (0.2261) Acc D Real: 81.733%
Loss D Fake: 0.4067 (2.0304) Acc D Fake: 55.536%
Loss D: 0.675
Loss G: 1.1272 (0.5917) Acc G: 44.714%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2931 (0.2274) Acc D Real: 81.584%
Loss D Fake: 0.4045 (2.0003) Acc D Fake: 56.359%
Loss D: 0.698
Loss G: 1.1318 (0.6017) Acc G: 43.886%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2541 (0.2278) Acc D Real: 81.538%
Loss D Fake: 0.4025 (1.9713) Acc D Fake: 57.152%
Loss D: 0.657
Loss G: 1.1363 (0.6114) Acc G: 43.088%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3228 (0.2295) Acc D Real: 81.324%
Loss D Fake: 0.4006 (1.9432) Acc D Fake: 57.918%
Loss D: 0.723
Loss G: 1.1406 (0.6209) Acc G: 42.319%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2206 (0.2294) Acc D Real: 81.354%
Loss D Fake: 0.3988 (1.9161) Acc D Fake: 58.656%
Loss D: 0.619
Loss G: 1.1449 (0.6301) Acc G: 41.576%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3303 (0.2311) Acc D Real: 81.158%
Loss D Fake: 0.3971 (1.8900) Acc D Fake: 59.369%
Loss D: 0.727
Loss G: 1.1483 (0.6390) Acc G: 40.859%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.2101 (0.2308) Acc D Real: 81.188%
Loss D Fake: 0.3958 (1.8646) Acc D Fake: 60.057%
Loss D: 0.606
Loss G: 1.1519 (0.6477) Acc G: 40.167%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.4624 (0.2346) Acc D Real: 80.730%
Loss D Fake: 0.3943 (1.8401) Acc D Fake: 60.723%
Loss D: 0.857
Loss G: 1.1555 (0.6561) Acc G: 39.497%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.2803 (0.2354) Acc D Real: 80.633%
Loss D Fake: 0.3929 (1.8164) Acc D Fake: 61.367%
Loss D: 0.673
Loss G: 1.1592 (0.6644) Acc G: 38.850%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.2704 (0.2359) Acc D Real: 80.575%
Loss D Fake: 0.3915 (1.7934) Acc D Fake: 61.990%
Loss D: 0.662
Loss G: 1.1624 (0.6724) Acc G: 38.223%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.2995 (0.2370) Acc D Real: 80.430%
Loss D Fake: 0.3903 (1.7711) Acc D Fake: 62.593%
Loss D: 0.690
Loss G: 1.1657 (0.6803) Acc G: 37.617%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.2914 (0.2378) Acc D Real: 80.349%
Loss D Fake: 0.3890 (1.7495) Acc D Fake: 63.178%
Loss D: 0.680
Loss G: 1.1690 (0.6879) Acc G: 37.029%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.2333 (0.2377) Acc D Real: 80.333%
Loss D Fake: 0.3877 (1.7286) Acc D Fake: 63.744%
Loss D: 0.621
Loss G: 1.1725 (0.6953) Acc G: 36.459%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3211 (0.2390) Acc D Real: 80.186%
Loss D Fake: 0.3864 (1.7083) Acc D Fake: 64.294%
Loss D: 0.707
Loss G: 1.1756 (0.7026) Acc G: 35.907%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.1996 (0.2384) Acc D Real: 80.232%
Loss D Fake: 0.3853 (1.6885) Acc D Fake: 64.827%
Loss D: 0.585
Loss G: 1.1787 (0.7097) Acc G: 35.371%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.1955 (0.2378) Acc D Real: 80.290%
Loss D Fake: 0.3840 (1.6693) Acc D Fake: 65.344%
Loss D: 0.580
Loss G: 1.1821 (0.7167) Acc G: 34.851%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.2632 (0.2381) Acc D Real: 80.215%
Loss D Fake: 0.3827 (1.6507) Acc D Fake: 65.846%
Loss D: 0.646
Loss G: 1.1859 (0.7235) Acc G: 34.346%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.2928 (0.2389) Acc D Real: 80.160%
Loss D Fake: 0.3813 (1.6325) Acc D Fake: 66.334%
Loss D: 0.674
Loss G: 1.1894 (0.7301) Acc G: 33.855%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2876 (0.2396) Acc D Real: 80.050%
Loss D Fake: 0.3800 (1.6149) Acc D Fake: 66.808%
Loss D: 0.668
Loss G: 1.1931 (0.7367) Acc G: 33.378%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3220 (0.2408) Acc D Real: 79.917%
Loss D Fake: 0.3786 (1.5977) Acc D Fake: 67.269%
Loss D: 0.701
Loss G: 1.1968 (0.7430) Acc G: 32.914%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3262 (0.2419) Acc D Real: 79.759%
Loss D Fake: 0.3773 (1.5810) Acc D Fake: 67.718%
Loss D: 0.703
Loss G: 1.2005 (0.7493) Acc G: 32.464%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.2376 (0.2419) Acc D Real: 79.761%
Loss D Fake: 0.3759 (1.5647) Acc D Fake: 68.154%
Loss D: 0.613
Loss G: 1.2043 (0.7555) Acc G: 32.025%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3842 (0.2438) Acc D Real: 79.533%
Loss D Fake: 0.3813 (1.5489) Acc D Fake: 68.578%
Loss D: 0.765
Loss G: 0.8222 (0.7563) Acc G: 31.665%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.2312 (0.2436) Acc D Real: 79.539%
Loss D Fake: 0.5950 (1.5364) Acc D Fake: 68.948%
Loss D: 0.826
Loss G: 0.7987 (0.7569) Acc G: 31.314%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2537 (0.2437) Acc D Real: 79.491%
Loss D Fake: 0.6017 (1.5243) Acc D Fake: 69.286%
Loss D: 0.855
Loss G: 0.7935 (0.7574) Acc G: 30.972%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3611 (0.2452) Acc D Real: 79.261%
Loss D Fake: 0.6045 (1.5125) Acc D Fake: 69.616%
Loss D: 0.966
Loss G: 0.7909 (0.7578) Acc G: 30.660%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.1777 (0.2444) Acc D Real: 79.345%
Loss D Fake: 0.6060 (1.5010) Acc D Fake: 69.937%
Loss D: 0.784
Loss G: 0.7901 (0.7582) Acc G: 30.336%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3502 (0.2457) Acc D Real: 79.148%
Loss D Fake: 0.6061 (1.4898) Acc D Fake: 70.230%
Loss D: 0.956
Loss G: 0.7901 (0.7586) Acc G: 30.019%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.2095 (0.2453) Acc D Real: 79.185%
Loss D Fake: 0.6058 (1.4789) Acc D Fake: 70.536%
Loss D: 0.815
Loss G: 0.7911 (0.7590) Acc G: 29.710%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.2706 (0.2456) Acc D Real: 79.122%
Loss D Fake: 0.6046 (1.4682) Acc D Fake: 70.834%
Loss D: 0.875
Loss G: 0.7929 (0.7594) Acc G: 29.388%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.1767 (0.2447) Acc D Real: 79.212%
Loss D Fake: 0.6028 (1.4578) Acc D Fake: 71.125%
Loss D: 0.780
Loss G: 0.7956 (0.7599) Acc G: 29.095%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3994 (0.2466) Acc D Real: 78.946%
Loss D Fake: 0.6005 (1.4476) Acc D Fake: 71.409%
Loss D: 1.000
Loss G: 0.7980 (0.7603) Acc G: 28.808%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3190 (0.2474) Acc D Real: 78.811%
Loss D Fake: 0.5987 (1.4376) Acc D Fake: 71.687%
Loss D: 0.918
Loss G: 0.8002 (0.7608) Acc G: 28.528%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.3768 (0.2489) Acc D Real: 78.587%
Loss D Fake: 0.5971 (1.4278) Acc D Fake: 71.958%
Loss D: 0.974
Loss G: 0.8020 (0.7613) Acc G: 28.254%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3417 (0.2500) Acc D Real: 78.439%
Loss D Fake: 0.5957 (1.4183) Acc D Fake: 72.223%
Loss D: 0.937
Loss G: 0.8036 (0.7618) Acc G: 27.968%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3508 (0.2511) Acc D Real: 78.275%
Loss D Fake: 0.5945 (1.4089) Acc D Fake: 72.501%
Loss D: 0.945
Loss G: 0.8051 (0.7622) Acc G: 27.688%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3947 (0.2528) Acc D Real: 78.040%
Loss D Fake: 0.5934 (1.3997) Acc D Fake: 72.772%
Loss D: 0.988
Loss G: 0.8062 (0.7627) Acc G: 27.414%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.2769 (0.2530) Acc D Real: 77.987%
Loss D Fake: 0.5926 (1.3908) Acc D Fake: 73.038%
Loss D: 0.869
Loss G: 0.8075 (0.7632) Acc G: 27.146%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.2978 (0.2535) Acc D Real: 77.906%
Loss D Fake: 0.5913 (1.3820) Acc D Fake: 73.297%
Loss D: 0.889
Loss G: 0.8092 (0.7637) Acc G: 26.885%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.2693 (0.2537) Acc D Real: 77.864%
Loss D Fake: 0.5899 (1.3734) Acc D Fake: 73.551%
Loss D: 0.859
Loss G: 0.8113 (0.7643) Acc G: 26.629%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2926 (0.2541) Acc D Real: 77.788%
Loss D Fake: 0.5882 (1.3649) Acc D Fake: 73.800%
Loss D: 0.881
Loss G: 0.8135 (0.7648) Acc G: 26.378%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.4062 (0.2557) Acc D Real: 77.567%
Loss D Fake: 0.5866 (1.3567) Acc D Fake: 74.043%
Loss D: 0.993
Loss G: 0.8152 (0.7653) Acc G: 26.133%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3081 (0.2563) Acc D Real: 77.480%
Loss D Fake: 0.5853 (1.3485) Acc D Fake: 74.281%
Loss D: 0.893
Loss G: 0.8170 (0.7659) Acc G: 25.893%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2740 (0.2565) Acc D Real: 77.442%
Loss D Fake: 0.5838 (1.3406) Acc D Fake: 74.514%
Loss D: 0.858
Loss G: 0.8190 (0.7664) Acc G: 25.658%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.2728 (0.2566) Acc D Real: 77.403%
Loss D Fake: 0.5821 (1.3328) Acc D Fake: 74.743%
Loss D: 0.855
Loss G: 0.8214 (0.7670) Acc G: 25.428%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2919 (0.2570) Acc D Real: 77.337%
Loss D Fake: 0.5803 (1.3251) Acc D Fake: 74.967%
Loss D: 0.872
Loss G: 0.8238 (0.7676) Acc G: 25.202%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3276 (0.2577) Acc D Real: 77.234%
Loss D Fake: 0.5784 (1.3175) Acc D Fake: 75.186%
Loss D: 0.906
Loss G: 0.8261 (0.7682) Acc G: 24.982%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2790 (0.2579) Acc D Real: 77.191%
Loss D Fake: 0.5766 (1.3101) Acc D Fake: 75.401%
Loss D: 0.856
Loss G: 0.8286 (0.7688) Acc G: 24.748%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3038 (0.2584) Acc D Real: 77.119%
Loss D Fake: 0.5747 (1.3028) Acc D Fake: 75.644%
Loss D: 0.878
Loss G: 0.8312 (0.7694) Acc G: 24.503%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.2278 (0.2581) Acc D Real: 77.143%
Loss D Fake: 0.5725 (1.2957) Acc D Fake: 75.883%
Loss D: 0.800
Loss G: 0.8343 (0.7700) Acc G: 24.263%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3511 (0.2590) Acc D Real: 77.012%
Loss D Fake: 0.5702 (1.2886) Acc D Fake: 76.117%
Loss D: 0.921
Loss G: 0.8372 (0.7707) Acc G: 24.028%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3335 (0.2597) Acc D Real: 76.904%
Loss D Fake: 0.5683 (1.2817) Acc D Fake: 76.347%
Loss D: 0.902
Loss G: 0.8397 (0.7713) Acc G: 23.797%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.4093 (0.2611) Acc D Real: 76.710%
Loss D Fake: 0.5665 (1.2749) Acc D Fake: 76.572%
Loss D: 0.976
Loss G: 0.8416 (0.7720) Acc G: 23.570%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3064 (0.2615) Acc D Real: 76.644%
Loss D Fake: 0.5653 (1.2682) Acc D Fake: 76.793%
Loss D: 0.872
Loss G: 0.8434 (0.7727) Acc G: 23.348%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2994 (0.2619) Acc D Real: 76.584%
Loss D Fake: 0.5639 (1.2616) Acc D Fake: 77.010%
Loss D: 0.863
Loss G: 0.8453 (0.7734) Acc G: 23.129%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2682 (0.2620) Acc D Real: 76.565%
Loss D Fake: 0.5624 (1.2552) Acc D Fake: 77.223%
Loss D: 0.831
Loss G: 0.8475 (0.7740) Acc G: 22.915%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3061 (0.2624) Acc D Real: 76.502%
Loss D Fake: 0.5607 (1.2488) Acc D Fake: 77.432%
Loss D: 0.867
Loss G: 0.8498 (0.7747) Acc G: 22.705%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.4024 (0.2636) Acc D Real: 76.330%
Loss D Fake: 0.5592 (1.2425) Acc D Fake: 77.637%
Loss D: 0.962
Loss G: 0.8515 (0.7754) Acc G: 22.499%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3001 (0.2640) Acc D Real: 76.274%
Loss D Fake: 0.5581 (1.2363) Acc D Fake: 77.838%
Loss D: 0.858
Loss G: 0.8531 (0.7761) Acc G: 22.296%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3729 (0.2649) Acc D Real: 76.146%
Loss D Fake: 0.5570 (1.2303) Acc D Fake: 78.036%
Loss D: 0.930
Loss G: 0.8545 (0.7768) Acc G: 22.097%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3314 (0.2655) Acc D Real: 76.061%
Loss D Fake: 0.5561 (1.2243) Acc D Fake: 78.231%
Loss D: 0.887
Loss G: 0.8558 (0.7775) Acc G: 21.901%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.2571 (0.2655) Acc D Real: 76.060%
Loss D Fake: 0.5551 (1.2184) Acc D Fake: 78.422%
Loss D: 0.812
Loss G: 0.8574 (0.7782) Acc G: 21.709%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3013 (0.2658) Acc D Real: 76.015%
Loss D Fake: 0.5538 (1.2127) Acc D Fake: 78.609%
Loss D: 0.855
Loss G: 0.8592 (0.7789) Acc G: 21.520%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.3709 (0.2667) Acc D Real: 75.893%
Loss D Fake: 0.5526 (1.2070) Acc D Fake: 78.794%
Loss D: 0.924
Loss G: 0.8607 (0.7796) Acc G: 21.335%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3620 (0.2675) Acc D Real: 75.782%
Loss D Fake: 0.5517 (1.2014) Acc D Fake: 78.975%
Loss D: 0.914
Loss G: 0.8618 (0.7804) Acc G: 21.153%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3312 (0.2680) Acc D Real: 75.705%
Loss D Fake: 0.5510 (1.1959) Acc D Fake: 79.153%
Loss D: 0.882
Loss G: 0.8629 (0.7811) Acc G: 20.973%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3137 (0.2684) Acc D Real: 75.653%
Loss D Fake: 0.5502 (1.1904) Acc D Fake: 79.328%
Loss D: 0.864
Loss G: 0.8640 (0.7817) Acc G: 20.797%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3249 (0.2689) Acc D Real: 75.586%
Loss D Fake: 0.5493 (1.1851) Acc D Fake: 79.500%
Loss D: 0.874
Loss G: 0.8652 (0.7824) Acc G: 20.624%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2150 (0.2684) Acc D Real: 75.636%
Loss D Fake: 0.5483 (1.1798) Acc D Fake: 79.670%
Loss D: 0.763
Loss G: 0.8671 (0.7831) Acc G: 20.453%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.2897 (0.2686) Acc D Real: 75.607%
Loss D Fake: 0.5469 (1.1746) Acc D Fake: 79.836%
Loss D: 0.837
Loss G: 0.8693 (0.7838) Acc G: 20.286%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3040 (0.2689) Acc D Real: 75.567%
Loss D Fake: 0.5453 (1.1695) Acc D Fake: 80.000%
Loss D: 0.849
Loss G: 0.8715 (0.7846) Acc G: 20.121%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3888 (0.2699) Acc D Real: 75.443%
Loss D Fake: 0.5439 (1.1645) Acc D Fake: 80.162%
Loss D: 0.933
Loss G: 0.8732 (0.7853) Acc G: 19.958%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3659 (0.2706) Acc D Real: 75.344%
Loss D Fake: 0.5428 (1.1595) Acc D Fake: 80.320%
Loss D: 0.909
Loss G: 0.8746 (0.7860) Acc G: 19.799%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.2289 (0.2703) Acc D Real: 75.377%
Loss D Fake: 0.5418 (1.1546) Acc D Fake: 80.477%
Loss D: 0.771
Loss G: 0.8764 (0.7867) Acc G: 19.642%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.2229 (0.2699) Acc D Real: 75.424%
Loss D Fake: 0.5404 (1.1498) Acc D Fake: 80.630%
Loss D: 0.763
Loss G: 0.8789 (0.7874) Acc G: 19.487%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3509 (0.2706) Acc D Real: 75.337%
Loss D Fake: 0.5386 (1.1450) Acc D Fake: 80.782%
Loss D: 0.889
Loss G: 0.8812 (0.7882) Acc G: 19.335%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.2870 (0.2707) Acc D Real: 75.320%
Loss D Fake: 0.5371 (1.1403) Acc D Fake: 80.931%
Loss D: 0.824
Loss G: 0.8835 (0.7889) Acc G: 19.185%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.2554 (0.2706) Acc D Real: 75.332%
Loss D Fake: 0.5355 (1.1356) Acc D Fake: 81.077%
Loss D: 0.791
Loss G: 0.8859 (0.7897) Acc G: 19.037%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.2673 (0.2705) Acc D Real: 75.335%
Loss D Fake: 0.5338 (1.1310) Acc D Fake: 81.222%
Loss D: 0.801
Loss G: 0.8886 (0.7904) Acc G: 18.892%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3023 (0.2708) Acc D Real: 75.301%
Loss D Fake: 0.5320 (1.1265) Acc D Fake: 81.364%
Loss D: 0.834
Loss G: 0.8911 (0.7912) Acc G: 18.749%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.2963 (0.2710) Acc D Real: 75.276%
Loss D Fake: 0.5304 (1.1220) Acc D Fake: 81.504%
Loss D: 0.827
Loss G: 0.8937 (0.7919) Acc G: 18.608%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3447 (0.2715) Acc D Real: 75.212%
Loss D Fake: 0.5287 (1.1176) Acc D Fake: 81.642%
Loss D: 0.873
Loss G: 0.8959 (0.7927) Acc G: 18.469%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.2836 (0.2716) Acc D Real: 75.201%
Loss D Fake: 0.5273 (1.1132) Acc D Fake: 81.778%
Loss D: 0.811
Loss G: 0.8981 (0.7935) Acc G: 18.332%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3188 (0.2720) Acc D Real: 75.160%
Loss D Fake: 0.5259 (1.1089) Acc D Fake: 81.912%
Loss D: 0.845
Loss G: 0.9001 (0.7943) Acc G: 18.197%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3284 (0.2724) Acc D Real: 75.118%
Loss D Fake: 0.5246 (1.1046) Acc D Fake: 82.044%
Loss D: 0.853
Loss G: 0.9021 (0.7951) Acc G: 18.065%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.2813 (0.2724) Acc D Real: 75.120%
Loss D Fake: 0.5233 (1.1004) Acc D Fake: 82.174%
Loss D: 0.805
Loss G: 0.9041 (0.7959) Acc G: 17.934%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.2998 (0.2726) Acc D Real: 75.097%
Loss D Fake: 0.5220 (1.0963) Acc D Fake: 82.303%
Loss D: 0.822
Loss G: 0.9062 (0.7967) Acc G: 17.805%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2970 (0.2728) Acc D Real: 75.076%
Loss D Fake: 0.5206 (1.0921) Acc D Fake: 82.429%
Loss D: 0.818
Loss G: 0.9083 (0.7974) Acc G: 17.677%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3190 (0.2731) Acc D Real: 75.040%
Loss D Fake: 0.5193 (1.0881) Acc D Fake: 82.554%
Loss D: 0.838
Loss G: 0.9103 (0.7982) Acc G: 17.552%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3713 (0.2738) Acc D Real: 74.961%
Loss D Fake: 0.5181 (1.0841) Acc D Fake: 82.676%
Loss D: 0.889
Loss G: 0.9118 (0.7990) Acc G: 17.428%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2975 (0.2740) Acc D Real: 74.945%
Loss D Fake: 0.5172 (1.0801) Acc D Fake: 82.798%
Loss D: 0.815
Loss G: 0.9134 (0.7998) Acc G: 17.307%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.2404 (0.2738) Acc D Real: 74.977%
Loss D Fake: 0.5161 (1.0762) Acc D Fake: 82.917%
Loss D: 0.756
Loss G: 0.9154 (0.8007) Acc G: 17.186%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2710 (0.2737) Acc D Real: 74.989%
Loss D Fake: 0.5147 (1.0723) Acc D Fake: 83.035%
Loss D: 0.786
Loss G: 0.9176 (0.8015) Acc G: 17.068%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.2434 (0.2735) Acc D Real: 75.014%
Loss D Fake: 0.5132 (1.0685) Acc D Fake: 83.151%
Loss D: 0.757
Loss G: 0.9202 (0.8023) Acc G: 16.951%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.1889 (0.2730) Acc D Real: 75.081%
Loss D Fake: 0.5114 (1.0647) Acc D Fake: 83.266%
Loss D: 0.700
Loss G: 0.9234 (0.8031) Acc G: 16.836%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.2934 (0.2731) Acc D Real: 75.067%
Loss D Fake: 0.5094 (1.0609) Acc D Fake: 83.379%
Loss D: 0.803
Loss G: 0.9266 (0.8039) Acc G: 16.722%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.4748 (0.2745) Acc D Real: 74.922%
Loss D Fake: 0.5077 (1.0572) Acc D Fake: 83.490%
Loss D: 0.982
Loss G: 0.9285 (0.8048) Acc G: 16.610%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.2661 (0.2744) Acc D Real: 74.933%
Loss D Fake: 0.5067 (1.0536) Acc D Fake: 83.600%
Loss D: 0.773
Loss G: 0.9302 (0.8056) Acc G: 16.499%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3319 (0.2748) Acc D Real: 74.895%
Loss D Fake: 0.5057 (1.0499) Acc D Fake: 83.709%
Loss D: 0.838
Loss G: 0.9318 (0.8064) Acc G: 16.390%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3086 (0.2750) Acc D Real: 74.876%
Loss D Fake: 0.5047 (1.0463) Acc D Fake: 83.816%
Loss D: 0.813
Loss G: 0.9333 (0.8073) Acc G: 16.282%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2762 (0.2750) Acc D Real: 74.880%
Loss D Fake: 0.5038 (1.0428) Acc D Fake: 83.922%
Loss D: 0.780
Loss G: 0.9349 (0.8081) Acc G: 16.175%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3062 (0.2752) Acc D Real: 74.862%
Loss D Fake: 0.5028 (1.0393) Acc D Fake: 84.026%
Loss D: 0.809
Loss G: 0.9366 (0.8089) Acc G: 16.070%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.2964 (0.2753) Acc D Real: 74.852%
Loss D Fake: 0.5017 (1.0358) Acc D Fake: 84.129%
Loss D: 0.798
Loss G: 0.9383 (0.8098) Acc G: 15.967%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4065 (0.2762) Acc D Real: 74.765%
Loss D Fake: 0.5009 (1.0324) Acc D Fake: 84.231%
Loss D: 0.907
Loss G: 0.9393 (0.8106) Acc G: 15.864%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.4073 (0.2770) Acc D Real: 74.675%
Loss D Fake: 0.5005 (1.0290) Acc D Fake: 84.332%
Loss D: 0.908
Loss G: 0.9396 (0.8114) Acc G: 15.763%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.2973 (0.2771) Acc D Real: 74.668%
Loss D Fake: 0.5004 (1.0257) Acc D Fake: 84.431%
Loss D: 0.798
Loss G: 0.9399 (0.8122) Acc G: 15.664%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.5731 (0.2790) Acc D Real: 74.462%
Loss D Fake: 0.5005 (1.0224) Acc D Fake: 84.529%
Loss D: 1.074
Loss G: 0.9389 (0.8130) Acc G: 15.565%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.2578 (0.2789) Acc D Real: 74.481%
Loss D Fake: 0.5012 (1.0191) Acc D Fake: 84.625%
Loss D: 0.759
Loss G: 0.9381 (0.8138) Acc G: 15.468%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.4324 (0.2798) Acc D Real: 74.377%
Loss D Fake: 0.5016 (1.0159) Acc D Fake: 84.721%
Loss D: 0.934
Loss G: 0.9371 (0.8146) Acc G: 15.372%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.2753 (0.2798) Acc D Real: 74.390%
Loss D Fake: 0.5023 (1.0127) Acc D Fake: 84.815%
Loss D: 0.778
Loss G: 0.9364 (0.8153) Acc G: 15.277%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.2991 (0.2799) Acc D Real: 74.383%
Loss D Fake: 0.5026 (1.0096) Acc D Fake: 84.908%
Loss D: 0.802
Loss G: 0.9360 (0.8161) Acc G: 15.183%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.4379 (0.2809) Acc D Real: 74.282%
Loss D Fake: 0.5030 (1.0065) Acc D Fake: 85.000%
Loss D: 0.941
Loss G: 0.9352 (0.8168) Acc G: 15.091%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2672 (0.2808) Acc D Real: 74.295%
Loss D Fake: 0.5035 (1.0035) Acc D Fake: 85.091%
Loss D: 0.771
Loss G: 0.9347 (0.8175) Acc G: 14.999%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3617 (0.2813) Acc D Real: 74.245%
Loss D Fake: 0.5037 (1.0004) Acc D Fake: 85.181%
Loss D: 0.865
Loss G: 0.9344 (0.8182) Acc G: 14.909%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.3881 (0.2819) Acc D Real: 74.178%
Loss D Fake: 0.5039 (0.9975) Acc D Fake: 85.270%
Loss D: 0.892
Loss G: 0.9338 (0.8189) Acc G: 14.819%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4534 (0.2829) Acc D Real: 74.064%
Loss D Fake: 0.5045 (0.9945) Acc D Fake: 85.357%
Loss D: 0.958
Loss G: 0.9326 (0.8196) Acc G: 14.731%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3281 (0.2832) Acc D Real: 74.037%
Loss D Fake: 0.5053 (0.9916) Acc D Fake: 85.444%
Loss D: 0.833
Loss G: 0.9316 (0.8203) Acc G: 14.644%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3979 (0.2839) Acc D Real: 73.963%
Loss D Fake: 0.5059 (0.9888) Acc D Fake: 85.530%
Loss D: 0.904
Loss G: 0.9304 (0.8209) Acc G: 14.558%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3832 (0.2845) Acc D Real: 73.899%
Loss D Fake: 0.5067 (0.9860) Acc D Fake: 85.614%
Loss D: 0.890
Loss G: 0.9291 (0.8215) Acc G: 14.473%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.2730 (0.2844) Acc D Real: 73.910%
Loss D Fake: 0.5074 (0.9832) Acc D Fake: 85.698%
Loss D: 0.780
Loss G: 0.9285 (0.8222) Acc G: 14.389%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.4017 (0.2851) Acc D Real: 73.834%
Loss D Fake: 0.5077 (0.9804) Acc D Fake: 85.781%
Loss D: 0.909
Loss G: 0.9277 (0.8228) Acc G: 14.305%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3538 (0.2855) Acc D Real: 73.793%
Loss D Fake: 0.5082 (0.9777) Acc D Fake: 85.862%
Loss D: 0.862
Loss G: 0.9269 (0.8234) Acc G: 14.223%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.1956 (0.2850) Acc D Real: 73.854%
Loss D Fake: 0.5085 (0.9750) Acc D Fake: 85.943%
Loss D: 0.704
Loss G: 0.9271 (0.8240) Acc G: 14.142%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3042 (0.2851) Acc D Real: 73.846%
Loss D Fake: 0.5082 (0.9724) Acc D Fake: 86.023%
Loss D: 0.812
Loss G: 0.9277 (0.8245) Acc G: 14.062%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2435 (0.2848) Acc D Real: 73.874%
Loss D Fake: 0.5077 (0.9698) Acc D Fake: 86.102%
Loss D: 0.751
Loss G: 0.9288 (0.8251) Acc G: 13.982%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4332 (0.2857) Acc D Real: 73.783%
Loss D Fake: 0.5071 (0.9672) Acc D Fake: 86.180%
Loss D: 0.940
Loss G: 0.9293 (0.8257) Acc G: 13.904%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3203 (0.2859) Acc D Real: 73.764%
Loss D Fake: 0.5070 (0.9646) Acc D Fake: 86.257%
Loss D: 0.827
Loss G: 0.9296 (0.8263) Acc G: 13.826%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.3555 (0.2863) Acc D Real: 73.725%
Loss D Fake: 0.5068 (0.9620) Acc D Fake: 86.334%
Loss D: 0.862
Loss G: 0.9298 (0.8269) Acc G: 13.749%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.2875 (0.2863) Acc D Real: 73.724%
Loss D Fake: 0.5067 (0.9595) Acc D Fake: 86.409%
Loss D: 0.794
Loss G: 0.9301 (0.8274) Acc G: 13.673%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3181 (0.2864) Acc D Real: 73.704%
Loss D Fake: 0.5065 (0.9570) Acc D Fake: 86.484%
Loss D: 0.825
Loss G: 0.9305 (0.8280) Acc G: 13.598%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2550 (0.2863) Acc D Real: 73.726%
Loss D Fake: 0.5061 (0.9546) Acc D Fake: 86.558%
Loss D: 0.761
Loss G: 0.9315 (0.8286) Acc G: 13.524%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3821 (0.2868) Acc D Real: 73.669%
Loss D Fake: 0.5055 (0.9521) Acc D Fake: 86.631%
Loss D: 0.888
Loss G: 0.9321 (0.8291) Acc G: 13.450%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2411 (0.2865) Acc D Real: 73.699%
Loss D Fake: 0.5051 (0.9497) Acc D Fake: 86.703%
Loss D: 0.746
Loss G: 0.9332 (0.8297) Acc G: 13.378%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3764 (0.2870) Acc D Real: 73.644%
Loss D Fake: 0.5044 (0.9473) Acc D Fake: 86.774%
Loss D: 0.881
Loss G: 0.9340 (0.8303) Acc G: 13.306%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.2181 (0.2867) Acc D Real: 73.688%
Loss D Fake: 0.5039 (0.9450) Acc D Fake: 86.845%
Loss D: 0.722
Loss G: 0.9353 (0.8308) Acc G: 13.234%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3321 (0.2869) Acc D Real: 73.663%
Loss D Fake: 0.5031 (0.9426) Acc D Fake: 86.915%
Loss D: 0.835
Loss G: 0.9365 (0.8314) Acc G: 13.164%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.2495 (0.2867) Acc D Real: 73.689%
Loss D Fake: 0.5023 (0.9403) Acc D Fake: 86.984%
Loss D: 0.752
Loss G: 0.9380 (0.8320) Acc G: 13.094%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3490 (0.2870) Acc D Real: 73.656%
Loss D Fake: 0.5014 (0.9380) Acc D Fake: 87.053%
Loss D: 0.850
Loss G: 0.9393 (0.8325) Acc G: 13.025%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.3269 (0.2872) Acc D Real: 73.635%
Loss D Fake: 0.5008 (0.9357) Acc D Fake: 87.121%
Loss D: 0.828
Loss G: 0.9403 (0.8331) Acc G: 12.957%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.4659 (0.2882) Acc D Real: 73.535%
Loss D Fake: 0.5004 (0.9334) Acc D Fake: 87.188%
Loss D: 0.966
Loss G: 0.9402 (0.8336) Acc G: 12.890%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.2710 (0.2881) Acc D Real: 73.547%
Loss D Fake: 0.5005 (0.9312) Acc D Fake: 87.254%
Loss D: 0.772
Loss G: 0.9404 (0.8342) Acc G: 12.823%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.3497 (0.2884) Acc D Real: 73.516%
Loss D Fake: 0.5004 (0.9289) Acc D Fake: 87.320%
Loss D: 0.850
Loss G: 0.9404 (0.8347) Acc G: 12.757%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3001 (0.2885) Acc D Real: 73.513%
Loss D Fake: 0.5004 (0.9267) Acc D Fake: 87.385%
Loss D: 0.800
Loss G: 0.9405 (0.8353) Acc G: 12.692%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.2917 (0.2885) Acc D Real: 73.513%
Loss D Fake: 0.5003 (0.9246) Acc D Fake: 87.449%
Loss D: 0.792
Loss G: 0.9409 (0.8358) Acc G: 12.627%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.2420 (0.2882) Acc D Real: 73.543%
Loss D Fake: 0.5000 (0.9224) Acc D Fake: 87.513%
Loss D: 0.742
Loss G: 0.9417 (0.8364) Acc G: 12.563%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.3169 (0.2884) Acc D Real: 73.529%
Loss D Fake: 0.4994 (0.9203) Acc D Fake: 87.576%
Loss D: 0.816
Loss G: 0.9426 (0.8369) Acc G: 12.499%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3222 (0.2885) Acc D Real: 73.513%
Loss D Fake: 0.4989 (0.9182) Acc D Fake: 87.638%
Loss D: 0.821
Loss G: 0.9433 (0.8374) Acc G: 12.436%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2975 (0.2886) Acc D Real: 73.512%
Loss D Fake: 0.4985 (0.9161) Acc D Fake: 87.700%
Loss D: 0.796
Loss G: 0.9441 (0.8380) Acc G: 12.374%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3162 (0.2887) Acc D Real: 73.499%
Loss D Fake: 0.4981 (0.9140) Acc D Fake: 87.761%
Loss D: 0.814
Loss G: 0.9448 (0.8385) Acc G: 12.313%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.2080 (0.2883) Acc D Real: 73.551%
Loss D Fake: 0.4975 (0.9119) Acc D Fake: 87.822%
Loss D: 0.706
Loss G: 0.9461 (0.8390) Acc G: 12.252%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2794 (0.2883) Acc D Real: 73.560%
Loss D Fake: 0.4966 (0.9099) Acc D Fake: 87.882%
Loss D: 0.776
Loss G: 0.9478 (0.8396) Acc G: 12.191%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.4970 (0.2893) Acc D Real: 73.452%
Loss D Fake: 0.4958 (0.9078) Acc D Fake: 87.941%
Loss D: 0.993
Loss G: 0.9483 (0.8401) Acc G: 12.132%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2661 (0.2892) Acc D Real: 73.466%
Loss D Fake: 0.4957 (0.9058) Acc D Fake: 88.000%
Loss D: 0.762
Loss G: 0.9488 (0.8406) Acc G: 12.072%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3163 (0.2893) Acc D Real: 73.455%
Loss D Fake: 0.4954 (0.9038) Acc D Fake: 88.059%
Loss D: 0.812
Loss G: 0.9493 (0.8412) Acc G: 12.014%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4430 (0.2901) Acc D Real: 73.377%
Loss D Fake: 0.4953 (0.9019) Acc D Fake: 88.116%
Loss D: 0.938
Loss G: 0.9490 (0.8417) Acc G: 11.956%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2695 (0.2900) Acc D Real: 73.392%
Loss D Fake: 0.4955 (0.8999) Acc D Fake: 88.173%
Loss D: 0.765
Loss G: 0.9488 (0.8422) Acc G: 11.898%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.2996 (0.2900) Acc D Real: 73.389%
Loss D Fake: 0.4955 (0.8980) Acc D Fake: 88.230%
Loss D: 0.795
Loss G: 0.9490 (0.8427) Acc G: 11.841%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3284 (0.2902) Acc D Real: 73.372%
Loss D Fake: 0.4954 (0.8961) Acc D Fake: 88.286%
Loss D: 0.824
Loss G: 0.9492 (0.8432) Acc G: 11.785%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2349 (0.2899) Acc D Real: 73.402%
Loss D Fake: 0.4952 (0.8942) Acc D Fake: 88.341%
Loss D: 0.730
Loss G: 0.9498 (0.8437) Acc G: 11.729%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.2779 (0.2899) Acc D Real: 73.415%
Loss D Fake: 0.4947 (0.8923) Acc D Fake: 88.396%
Loss D: 0.773
Loss G: 0.9508 (0.8442) Acc G: 11.674%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2974 (0.2899) Acc D Real: 73.415%
Loss D Fake: 0.4941 (0.8904) Acc D Fake: 88.451%
Loss D: 0.792
Loss G: 0.9519 (0.8447) Acc G: 11.619%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3362 (0.2901) Acc D Real: 73.397%
Loss D Fake: 0.4936 (0.8886) Acc D Fake: 88.505%
Loss D: 0.830
Loss G: 0.9527 (0.8452) Acc G: 11.565%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3011 (0.2902) Acc D Real: 73.395%
Loss D Fake: 0.4931 (0.8867) Acc D Fake: 88.558%
Loss D: 0.794
Loss G: 0.9536 (0.8457) Acc G: 11.511%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3184 (0.2903) Acc D Real: 73.384%
Loss D Fake: 0.4927 (0.8849) Acc D Fake: 88.611%
Loss D: 0.811
Loss G: 0.9543 (0.8462) Acc G: 11.458%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2543 (0.2901) Acc D Real: 73.407%
Loss D Fake: 0.4922 (0.8831) Acc D Fake: 88.664%
Loss D: 0.746
Loss G: 0.9553 (0.8467) Acc G: 11.405%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2446 (0.2899) Acc D Real: 73.433%
Loss D Fake: 0.4915 (0.8813) Acc D Fake: 88.716%
Loss D: 0.736
Loss G: 0.9568 (0.8472) Acc G: 11.352%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2664 (0.2898) Acc D Real: 73.450%
Loss D Fake: 0.4906 (0.8795) Acc D Fake: 88.767%
Loss D: 0.757
Loss G: 0.9584 (0.8477) Acc G: 11.301%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2718 (0.2897) Acc D Real: 73.463%
Loss D Fake: 0.4896 (0.8777) Acc D Fake: 88.818%
Loss D: 0.761
Loss G: 0.9602 (0.8483) Acc G: 11.249%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2398 (0.2895) Acc D Real: 73.493%
Loss D Fake: 0.4885 (0.8760) Acc D Fake: 88.869%
Loss D: 0.728
Loss G: 0.9622 (0.8488) Acc G: 11.198%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2400 (0.2893) Acc D Real: 73.522%
Loss D Fake: 0.4873 (0.8742) Acc D Fake: 88.919%
Loss D: 0.727
Loss G: 0.9645 (0.8493) Acc G: 11.148%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3015 (0.2894) Acc D Real: 73.521%
Loss D Fake: 0.4860 (0.8725) Acc D Fake: 88.969%
Loss D: 0.788
Loss G: 0.9667 (0.8498) Acc G: 11.098%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3273 (0.2895) Acc D Real: 73.507%
Loss D Fake: 0.4849 (0.8707) Acc D Fake: 89.018%
Loss D: 0.812
Loss G: 0.9684 (0.8504) Acc G: 11.048%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3823 (0.2899) Acc D Real: 73.466%
Loss D Fake: 0.4842 (0.8690) Acc D Fake: 89.067%
Loss D: 0.866
Loss G: 0.9694 (0.8509) Acc G: 10.999%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.5218 (0.2910) Acc D Real: 73.440%
Loss D Fake: 0.4840 (0.8673) Acc D Fake: 89.079%
Loss D: 1.006
Loss G: 0.9688 (0.8514) Acc G: 10.987%
LR: 2.000e-04
Epoch: 7/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3155 (0.3111) Acc D Real: 72.057%
Loss D Fake: 0.4850 (0.4848) Acc D Fake: 100.000%
Loss D: 0.800
Loss G: 0.9673 (0.9676) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3252 (0.3158) Acc D Real: 71.667%
Loss D Fake: 0.4854 (0.4850) Acc D Fake: 100.000%
Loss D: 0.811
Loss G: 0.9667 (0.9673) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.3015 (0.3122) Acc D Real: 72.044%
Loss D Fake: 0.4857 (0.4851) Acc D Fake: 100.000%
Loss D: 0.787
Loss G: 0.9662 (0.9671) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3880 (0.3274) Acc D Real: 70.417%
Loss D Fake: 0.4860 (0.4853) Acc D Fake: 100.000%
Loss D: 0.874
Loss G: 0.9655 (0.9667) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2142 (0.3085) Acc D Real: 72.465%
Loss D Fake: 0.4864 (0.4855) Acc D Fake: 100.000%
Loss D: 0.701
Loss G: 0.9653 (0.9665) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2174 (0.2955) Acc D Real: 73.847%
Loss D Fake: 0.4862 (0.4856) Acc D Fake: 100.000%
Loss D: 0.704
Loss G: 0.9660 (0.9664) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.4102 (0.3098) Acc D Real: 72.240%
Loss D Fake: 0.4859 (0.4856) Acc D Fake: 100.000%
Loss D: 0.896
Loss G: 0.9662 (0.9664) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.3847 (0.3182) Acc D Real: 71.354%
Loss D Fake: 0.4860 (0.4857) Acc D Fake: 100.000%
Loss D: 0.871
Loss G: 0.9658 (0.9663) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3568 (0.3220) Acc D Real: 70.979%
Loss D Fake: 0.4864 (0.4857) Acc D Fake: 100.000%
Loss D: 0.843
Loss G: 0.9650 (0.9662) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.3240 (0.3222) Acc D Real: 70.971%
Loss D Fake: 0.4869 (0.4858) Acc D Fake: 100.000%
Loss D: 0.811
Loss G: 0.9642 (0.9660) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.4400 (0.3320) Acc D Real: 69.900%
Loss D Fake: 0.4875 (0.4860) Acc D Fake: 100.000%
Loss D: 0.928
Loss G: 0.9628 (0.9658) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.4422 (0.3405) Acc D Real: 68.970%
Loss D Fake: 0.4885 (0.4862) Acc D Fake: 100.000%
Loss D: 0.931
Loss G: 0.9607 (0.9654) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.3803 (0.3433) Acc D Real: 68.650%
Loss D Fake: 0.4899 (0.4864) Acc D Fake: 100.000%
Loss D: 0.870
Loss G: 0.9583 (0.9649) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3479 (0.3436) Acc D Real: 68.587%
Loss D Fake: 0.4912 (0.4868) Acc D Fake: 100.000%
Loss D: 0.839
Loss G: 0.9562 (0.9643) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2101 (0.3353) Acc D Real: 69.482%
Loss D Fake: 0.4923 (0.4871) Acc D Fake: 100.000%
Loss D: 0.702
Loss G: 0.9550 (0.9637) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.4729 (0.3434) Acc D Real: 68.597%
Loss D Fake: 0.4930 (0.4875) Acc D Fake: 100.000%
Loss D: 0.966
Loss G: 0.9533 (0.9631) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2981 (0.3409) Acc D Real: 68.825%
Loss D Fake: 0.4941 (0.4878) Acc D Fake: 100.000%
Loss D: 0.792
Loss G: 0.9518 (0.9625) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3543 (0.3416) Acc D Real: 68.739%
Loss D Fake: 0.4949 (0.4882) Acc D Fake: 100.000%
Loss D: 0.849
Loss G: 0.9504 (0.9618) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.3430 (0.3417) Acc D Real: 68.721%
Loss D Fake: 0.4957 (0.4886) Acc D Fake: 100.000%
Loss D: 0.839
Loss G: 0.9491 (0.9612) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3481 (0.3420) Acc D Real: 68.653%
Loss D Fake: 0.4965 (0.4890) Acc D Fake: 100.000%
Loss D: 0.845
Loss G: 0.9477 (0.9605) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.2703 (0.3387) Acc D Real: 68.991%
Loss D Fake: 0.4972 (0.4893) Acc D Fake: 100.000%
Loss D: 0.768
Loss G: 0.9469 (0.9599) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3032 (0.3372) Acc D Real: 69.160%
Loss D Fake: 0.4976 (0.4897) Acc D Fake: 100.000%
Loss D: 0.801
Loss G: 0.9464 (0.9593) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.3774 (0.3388) Acc D Real: 68.956%
Loss D Fake: 0.4980 (0.4900) Acc D Fake: 100.000%
Loss D: 0.875
Loss G: 0.9456 (0.9588) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2396 (0.3349) Acc D Real: 69.369%
Loss D Fake: 0.4985 (0.4904) Acc D Fake: 100.000%
Loss D: 0.738
Loss G: 0.9453 (0.9582) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2519 (0.3317) Acc D Real: 69.718%
Loss D Fake: 0.4985 (0.4907) Acc D Fake: 100.000%
Loss D: 0.750
Loss G: 0.9456 (0.9577) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.2422 (0.3284) Acc D Real: 70.068%
Loss D Fake: 0.4982 (0.4910) Acc D Fake: 100.000%
Loss D: 0.740
Loss G: 0.9464 (0.9573) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.2949 (0.3272) Acc D Real: 70.197%
Loss D Fake: 0.4977 (0.4912) Acc D Fake: 100.000%
Loss D: 0.793
Loss G: 0.9473 (0.9570) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3593 (0.3283) Acc D Real: 70.075%
Loss D Fake: 0.4973 (0.4914) Acc D Fake: 100.000%
Loss D: 0.857
Loss G: 0.9478 (0.9566) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.4618 (0.3327) Acc D Real: 69.575%
Loss D Fake: 0.4973 (0.4916) Acc D Fake: 100.000%
Loss D: 0.959
Loss G: 0.9472 (0.9563) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.4308 (0.3359) Acc D Real: 69.236%
Loss D Fake: 0.4980 (0.4918) Acc D Fake: 100.000%
Loss D: 0.929
Loss G: 0.9458 (0.9560) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.2740 (0.3340) Acc D Real: 69.451%
Loss D Fake: 0.4989 (0.4920) Acc D Fake: 100.000%
Loss D: 0.773
Loss G: 0.9446 (0.9556) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.4133 (0.3364) Acc D Real: 69.176%
Loss D Fake: 0.4996 (0.4923) Acc D Fake: 100.000%
Loss D: 0.913
Loss G: 0.9431 (0.9553) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.3907 (0.3380) Acc D Real: 68.992%
Loss D Fake: 0.5006 (0.4925) Acc D Fake: 100.000%
Loss D: 0.891
Loss G: 0.9413 (0.9548) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3646 (0.3387) Acc D Real: 68.897%
Loss D Fake: 0.5018 (0.4928) Acc D Fake: 100.000%
Loss D: 0.866
Loss G: 0.9395 (0.9544) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3051 (0.3378) Acc D Real: 68.993%
Loss D Fake: 0.5028 (0.4931) Acc D Fake: 100.000%
Loss D: 0.808
Loss G: 0.9380 (0.9540) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.4601 (0.3411) Acc D Real: 68.619%
Loss D Fake: 0.5038 (0.4933) Acc D Fake: 100.000%
Loss D: 0.964
Loss G: 0.9360 (0.9535) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.3417 (0.3411) Acc D Real: 68.606%
Loss D Fake: 0.5052 (0.4937) Acc D Fake: 100.000%
Loss D: 0.847
Loss G: 0.9338 (0.9530) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.4153 (0.3430) Acc D Real: 68.369%
Loss D Fake: 0.5066 (0.4940) Acc D Fake: 100.000%
Loss D: 0.922
Loss G: 0.9315 (0.9524) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3753 (0.3438) Acc D Real: 68.268%
Loss D Fake: 0.5081 (0.4943) Acc D Fake: 100.000%
Loss D: 0.883
Loss G: 0.9291 (0.9518) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.1916 (0.3401) Acc D Real: 68.667%
Loss D Fake: 0.5094 (0.4947) Acc D Fake: 100.000%
Loss D: 0.701
Loss G: 0.9277 (0.9512) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.2681 (0.3384) Acc D Real: 68.843%
Loss D Fake: 0.5100 (0.4951) Acc D Fake: 100.000%
Loss D: 0.778
Loss G: 0.9270 (0.9507) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2624 (0.3366) Acc D Real: 69.021%
Loss D Fake: 0.5103 (0.4954) Acc D Fake: 100.000%
Loss D: 0.773
Loss G: 0.9269 (0.9501) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.2680 (0.3351) Acc D Real: 69.177%
Loss D Fake: 0.5104 (0.4958) Acc D Fake: 100.000%
Loss D: 0.778
Loss G: 0.9271 (0.9496) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3680 (0.3358) Acc D Real: 69.080%
Loss D Fake: 0.5104 (0.4961) Acc D Fake: 100.000%
Loss D: 0.878
Loss G: 0.9269 (0.9491) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.2967 (0.3349) Acc D Real: 69.154%
Loss D Fake: 0.5106 (0.4964) Acc D Fake: 100.000%
Loss D: 0.807
Loss G: 0.9268 (0.9486) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.2408 (0.3329) Acc D Real: 69.359%
Loss D Fake: 0.5106 (0.4967) Acc D Fake: 100.000%
Loss D: 0.751
Loss G: 0.9271 (0.9481) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.4078 (0.3345) Acc D Real: 69.178%
Loss D Fake: 0.5105 (0.4970) Acc D Fake: 100.000%
Loss D: 0.918
Loss G: 0.9269 (0.9477) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2617 (0.3330) Acc D Real: 69.330%
Loss D Fake: 0.5108 (0.4973) Acc D Fake: 100.000%
Loss D: 0.772
Loss G: 0.9268 (0.9473) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.2404 (0.3312) Acc D Real: 69.531%
Loss D Fake: 0.5107 (0.4975) Acc D Fake: 100.000%
Loss D: 0.751
Loss G: 0.9272 (0.9469) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3492 (0.3315) Acc D Real: 69.484%
Loss D Fake: 0.5106 (0.4978) Acc D Fake: 100.000%
Loss D: 0.860
Loss G: 0.9273 (0.9465) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3033 (0.3310) Acc D Real: 69.540%
Loss D Fake: 0.5107 (0.4981) Acc D Fake: 100.000%
Loss D: 0.814
Loss G: 0.9273 (0.9461) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2167 (0.3288) Acc D Real: 69.774%
Loss D Fake: 0.5106 (0.4983) Acc D Fake: 100.000%
Loss D: 0.727
Loss G: 0.9278 (0.9458) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2959 (0.3282) Acc D Real: 69.837%
Loss D Fake: 0.5103 (0.4985) Acc D Fake: 100.000%
Loss D: 0.806
Loss G: 0.9284 (0.9454) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4063 (0.3296) Acc D Real: 69.668%
Loss D Fake: 0.5102 (0.4987) Acc D Fake: 100.000%
Loss D: 0.917
Loss G: 0.9283 (0.9451) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.2293 (0.3278) Acc D Real: 69.867%
Loss D Fake: 0.5104 (0.4989) Acc D Fake: 100.000%
Loss D: 0.740
Loss G: 0.9284 (0.9448) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2672 (0.3268) Acc D Real: 69.989%
Loss D Fake: 0.5104 (0.4991) Acc D Fake: 100.000%
Loss D: 0.778
Loss G: 0.9287 (0.9446) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.4223 (0.3284) Acc D Real: 69.802%
Loss D Fake: 0.5104 (0.4993) Acc D Fake: 100.000%
Loss D: 0.933
Loss G: 0.9284 (0.9443) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3845 (0.3294) Acc D Real: 69.699%
Loss D Fake: 0.5110 (0.4995) Acc D Fake: 100.000%
Loss D: 0.895
Loss G: 0.9274 (0.9440) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.2300 (0.3277) Acc D Real: 69.874%
Loss D Fake: 0.5117 (0.4997) Acc D Fake: 100.000%
Loss D: 0.742
Loss G: 0.9267 (0.9437) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.2978 (0.3272) Acc D Real: 69.924%
Loss D Fake: 0.5121 (0.4999) Acc D Fake: 100.000%
Loss D: 0.810
Loss G: 0.9262 (0.9434) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.2468 (0.3259) Acc D Real: 70.064%
Loss D Fake: 0.5125 (0.5001) Acc D Fake: 100.000%
Loss D: 0.759
Loss G: 0.9260 (0.9431) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.2078 (0.3241) Acc D Real: 70.265%
Loss D Fake: 0.5127 (0.5003) Acc D Fake: 100.000%
Loss D: 0.720
Loss G: 0.9262 (0.9429) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3216 (0.3240) Acc D Real: 70.267%
Loss D Fake: 0.5127 (0.5005) Acc D Fake: 100.000%
Loss D: 0.834
Loss G: 0.9263 (0.9426) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.2381 (0.3227) Acc D Real: 70.407%
Loss D Fake: 0.5129 (0.5007) Acc D Fake: 100.000%
Loss D: 0.751
Loss G: 0.9264 (0.9424) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.2372 (0.3214) Acc D Real: 70.554%
Loss D Fake: 0.5129 (0.5009) Acc D Fake: 100.000%
Loss D: 0.750
Loss G: 0.9268 (0.9421) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.3220 (0.3214) Acc D Real: 70.554%
Loss D Fake: 0.5130 (0.5011) Acc D Fake: 100.000%
Loss D: 0.835
Loss G: 0.9268 (0.9419) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.2413 (0.3202) Acc D Real: 70.677%
Loss D Fake: 0.5134 (0.5013) Acc D Fake: 100.000%
Loss D: 0.755
Loss G: 0.9267 (0.9417) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4554 (0.3222) Acc D Real: 70.463%
Loss D Fake: 0.5141 (0.5015) Acc D Fake: 100.000%
Loss D: 0.969
Loss G: 0.9254 (0.9414) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3014 (0.3219) Acc D Real: 70.501%
Loss D Fake: 0.5155 (0.5017) Acc D Fake: 100.000%
Loss D: 0.817
Loss G: 0.9236 (0.9412) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2700 (0.3212) Acc D Real: 70.581%
Loss D Fake: 0.5170 (0.5019) Acc D Fake: 100.000%
Loss D: 0.787
Loss G: 0.9218 (0.9409) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.2900 (0.3207) Acc D Real: 70.629%
Loss D Fake: 0.5187 (0.5021) Acc D Fake: 100.000%
Loss D: 0.809
Loss G: 0.9196 (0.9406) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3195 (0.3207) Acc D Real: 70.625%
Loss D Fake: 0.5208 (0.5024) Acc D Fake: 100.000%
Loss D: 0.840
Loss G: 0.9169 (0.9403) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.2734 (0.3201) Acc D Real: 70.702%
Loss D Fake: 0.5234 (0.5026) Acc D Fake: 99.970%
Loss D: 0.797
Loss G: 0.9136 (0.9399) Acc G: 0.032%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.2516 (0.3192) Acc D Real: 70.803%
Loss D Fake: 0.5268 (0.5030) Acc D Fake: 99.881%
Loss D: 0.778
Loss G: 0.9096 (0.9395) Acc G: 0.121%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.2636 (0.3184) Acc D Real: 70.882%
Loss D Fake: 0.5313 (0.5033) Acc D Fake: 99.751%
Loss D: 0.795
Loss G: 0.9041 (0.9391) Acc G: 0.251%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2585 (0.3177) Acc D Real: 70.959%
Loss D Fake: 0.5383 (0.5038) Acc D Fake: 99.544%
Loss D: 0.797
Loss G: 0.8959 (0.9385) Acc G: 0.444%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.2664 (0.3170) Acc D Real: 71.028%
Loss D Fake: 0.5522 (0.5044) Acc D Fake: 99.255%
Loss D: 0.819
Loss G: 0.8806 (0.9377) Acc G: 0.716%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.2465 (0.3161) Acc D Real: 71.122%
Loss D Fake: 2.6876 (0.5320) Acc D Fake: 98.041%
Loss D: 2.934
Loss G: 0.8978 (0.9372) Acc G: 0.897%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.2949 (0.3158) Acc D Real: 71.151%
Loss D Fake: 0.5322 (0.5320) Acc D Fake: 97.899%
Loss D: 0.827
Loss G: 0.9151 (0.9370) Acc G: 0.990%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3023 (0.3157) Acc D Real: 71.170%
Loss D Fake: 0.5225 (0.5319) Acc D Fake: 97.802%
Loss D: 0.825
Loss G: 0.9209 (0.9368) Acc G: 1.060%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.2985 (0.3155) Acc D Real: 71.193%
Loss D Fake: 0.5190 (0.5318) Acc D Fake: 97.727%
Loss D: 0.817
Loss G: 0.9231 (0.9366) Acc G: 1.128%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.4105 (0.3166) Acc D Real: 71.068%
Loss D Fake: 0.5179 (0.5316) Acc D Fake: 97.654%
Loss D: 0.928
Loss G: 0.9227 (0.9364) Acc G: 1.195%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.2715 (0.3161) Acc D Real: 71.127%
Loss D Fake: 0.5184 (0.5314) Acc D Fake: 97.582%
Loss D: 0.790
Loss G: 0.9214 (0.9363) Acc G: 1.260%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.2764 (0.3156) Acc D Real: 71.176%
Loss D Fake: 0.5198 (0.5313) Acc D Fake: 97.513%
Loss D: 0.796
Loss G: 0.9191 (0.9361) Acc G: 1.343%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4118 (0.3167) Acc D Real: 71.055%
Loss D Fake: 0.5222 (0.5312) Acc D Fake: 97.426%
Loss D: 0.934
Loss G: 0.9155 (0.9358) Acc G: 1.444%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.2731 (0.3162) Acc D Real: 71.119%
Loss D Fake: 0.5259 (0.5311) Acc D Fake: 97.321%
Loss D: 0.799
Loss G: 0.9108 (0.9355) Acc G: 1.544%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3016 (0.3161) Acc D Real: 71.140%
Loss D Fake: 0.5308 (0.5311) Acc D Fake: 97.200%
Loss D: 0.832
Loss G: 0.9046 (0.9352) Acc G: 1.678%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3732 (0.3167) Acc D Real: 71.066%
Loss D Fake: 0.5384 (0.5312) Acc D Fake: 97.058%
Loss D: 0.912
Loss G: 0.8955 (0.9347) Acc G: 1.828%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4002 (0.3176) Acc D Real: 70.978%
Loss D Fake: 0.5523 (0.5315) Acc D Fake: 96.868%
Loss D: 0.953
Loss G: 0.8796 (0.9341) Acc G: 2.030%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.2238 (0.3166) Acc D Real: 71.085%
Loss D Fake: 0.6047 (0.5323) Acc D Fake: 96.536%
Loss D: 0.828
Loss G: 0.8376 (0.9331) Acc G: 2.337%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3138 (0.3166) Acc D Real: 71.081%
Loss D Fake: 3.2759 (0.5621) Acc D Fake: 95.523%
Loss D: 3.590
Loss G: 0.1497 (0.9245) Acc G: 3.362%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.4261 (0.3177) Acc D Real: 70.954%
Loss D Fake: 3.4030 (0.5926) Acc D Fake: 94.532%
Loss D: 3.829
Loss G: 0.1264 (0.9160) Acc G: 4.365%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3486 (0.3181) Acc D Real: 70.910%
Loss D Fake: 3.4504 (0.6230) Acc D Fake: 93.562%
Loss D: 3.799
Loss G: 0.1157 (0.9074) Acc G: 5.347%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3367 (0.3183) Acc D Real: 70.889%
Loss D Fake: 3.4609 (0.6529) Acc D Fake: 92.612%
Loss D: 3.798
Loss G: 0.1097 (0.8991) Acc G: 6.309%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.3302 (0.3184) Acc D Real: 70.873%
Loss D Fake: 3.4493 (0.6820) Acc D Fake: 91.682%
Loss D: 3.780
Loss G: 0.1063 (0.8908) Acc G: 7.250%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.3809 (0.3190) Acc D Real: 70.796%
Loss D Fake: 3.4238 (0.7103) Acc D Fake: 90.771%
Loss D: 3.805
Loss G: 0.1042 (0.8827) Acc G: 8.172%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3438 (0.3193) Acc D Real: 70.760%
Loss D Fake: 3.3889 (0.7376) Acc D Fake: 89.879%
Loss D: 3.733
Loss G: 0.1031 (0.8747) Acc G: 9.075%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3501 (0.3196) Acc D Real: 70.712%
Loss D Fake: 3.3475 (0.7640) Acc D Fake: 89.005%
Loss D: 3.698
Loss G: 0.1027 (0.8669) Acc G: 9.959%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2682 (0.3191) Acc D Real: 70.758%
Loss D Fake: 3.3013 (0.7894) Acc D Fake: 88.148%
Loss D: 3.569
Loss G: 0.1029 (0.8593) Acc G: 10.827%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.4742 (0.3206) Acc D Real: 70.552%
Loss D Fake: 3.2516 (0.8137) Acc D Fake: 87.308%
Loss D: 3.726
Loss G: 0.1035 (0.8518) Acc G: 11.676%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.2777 (0.3202) Acc D Real: 70.584%
Loss D Fake: 3.1993 (0.8371) Acc D Fake: 86.485%
Loss D: 3.477
Loss G: 0.1044 (0.8445) Acc G: 12.510%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.2952 (0.3200) Acc D Real: 70.594%
Loss D Fake: 3.1452 (0.8595) Acc D Fake: 85.678%
Loss D: 3.440
Loss G: 0.1056 (0.8373) Acc G: 13.327%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2396 (0.3192) Acc D Real: 70.672%
Loss D Fake: 3.0901 (0.8810) Acc D Fake: 84.886%
Loss D: 3.330
Loss G: 0.1071 (0.8303) Acc G: 14.128%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.4316 (0.3203) Acc D Real: 70.511%
Loss D Fake: 3.0345 (0.9015) Acc D Fake: 84.109%
Loss D: 3.466
Loss G: 0.1089 (0.8234) Acc G: 14.914%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3058 (0.3201) Acc D Real: 70.505%
Loss D Fake: 2.9785 (0.9211) Acc D Fake: 83.347%
Loss D: 3.284
Loss G: 0.1108 (0.8167) Acc G: 15.685%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4112 (0.3210) Acc D Real: 70.368%
Loss D Fake: 2.9226 (0.9398) Acc D Fake: 82.599%
Loss D: 3.334
Loss G: 0.1130 (0.8101) Acc G: 16.442%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.3690 (0.3214) Acc D Real: 70.286%
Loss D Fake: 2.8671 (0.9576) Acc D Fake: 81.865%
Loss D: 3.236
Loss G: 0.1153 (0.8037) Acc G: 17.185%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.4125 (0.3222) Acc D Real: 70.155%
Loss D Fake: 2.8120 (0.9747) Acc D Fake: 81.145%
Loss D: 3.224
Loss G: 0.1178 (0.7974) Acc G: 17.914%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3440 (0.3224) Acc D Real: 70.109%
Loss D Fake: 2.7575 (0.9909) Acc D Fake: 80.438%
Loss D: 3.101
Loss G: 0.1205 (0.7912) Acc G: 18.630%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3820 (0.3230) Acc D Real: 70.012%
Loss D Fake: 2.7037 (1.0063) Acc D Fake: 79.743%
Loss D: 3.086
Loss G: 0.1234 (0.7852) Acc G: 19.333%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3892 (0.3236) Acc D Real: 69.915%
Loss D Fake: 2.6508 (1.0210) Acc D Fake: 79.061%
Loss D: 3.040
Loss G: 0.1264 (0.7793) Acc G: 20.024%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3773 (0.3240) Acc D Real: 69.837%
Loss D Fake: 2.5987 (1.0349) Acc D Fake: 78.390%
Loss D: 2.976
Loss G: 0.1296 (0.7736) Acc G: 20.702%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3651 (0.3244) Acc D Real: 69.770%
Loss D Fake: 2.5476 (1.0482) Acc D Fake: 77.732%
Loss D: 2.913
Loss G: 0.1330 (0.7680) Acc G: 21.368%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.1745 (0.3231) Acc D Real: 69.942%
Loss D Fake: 2.4976 (1.0608) Acc D Fake: 77.085%
Loss D: 2.672
Loss G: 0.1364 (0.7625) Acc G: 22.023%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.2790 (0.3227) Acc D Real: 69.981%
Loss D Fake: 2.4489 (1.0728) Acc D Fake: 76.449%
Loss D: 2.728
Loss G: 0.1400 (0.7571) Acc G: 22.667%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3524 (0.3230) Acc D Real: 69.934%
Loss D Fake: 2.4013 (1.0841) Acc D Fake: 75.824%
Loss D: 2.754
Loss G: 0.1438 (0.7519) Acc G: 23.299%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3265 (0.3230) Acc D Real: 69.926%
Loss D Fake: 2.3546 (1.0949) Acc D Fake: 75.210%
Loss D: 2.681
Loss G: 0.1476 (0.7467) Acc G: 23.921%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3508 (0.3232) Acc D Real: 69.889%
Loss D Fake: 2.3089 (1.1051) Acc D Fake: 74.606%
Loss D: 2.660
Loss G: 0.1517 (0.7417) Acc G: 24.532%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3575 (0.3235) Acc D Real: 69.854%
Loss D Fake: 2.2642 (1.1148) Acc D Fake: 74.012%
Loss D: 2.622
Loss G: 0.1558 (0.7369) Acc G: 25.133%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.4566 (0.3246) Acc D Real: 69.724%
Loss D Fake: 2.2203 (1.1239) Acc D Fake: 73.428%
Loss D: 2.677
Loss G: 0.1602 (0.7321) Acc G: 25.724%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.2421 (0.3240) Acc D Real: 69.921%
Loss D Fake: 2.1773 (1.1325) Acc D Fake: 72.853%
Loss D: 2.419
Loss G: 0.1646 (0.7274) Acc G: 26.306%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2838 (0.3236) Acc D Real: 70.113%
Loss D Fake: 2.1357 (1.1407) Acc D Fake: 72.288%
Loss D: 2.419
Loss G: 0.1691 (0.7229) Acc G: 26.864%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3772 (0.3241) Acc D Real: 70.284%
Loss D Fake: 2.0952 (1.1484) Acc D Fake: 71.746%
Loss D: 2.472
Loss G: 0.1738 (0.7185) Acc G: 27.414%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.4036 (0.3247) Acc D Real: 70.437%
Loss D Fake: 2.0554 (1.1556) Acc D Fake: 71.212%
Loss D: 2.459
Loss G: 0.1786 (0.7142) Acc G: 27.955%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3810 (0.3251) Acc D Real: 70.591%
Loss D Fake: 2.0164 (1.1625) Acc D Fake: 70.686%
Loss D: 2.397
Loss G: 0.1836 (0.7099) Acc G: 28.487%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3368 (0.3252) Acc D Real: 70.755%
Loss D Fake: 1.9784 (1.1689) Acc D Fake: 70.169%
Loss D: 2.315
Loss G: 0.1886 (0.7058) Acc G: 29.010%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3535 (0.3255) Acc D Real: 70.918%
Loss D Fake: 1.9414 (1.1749) Acc D Fake: 69.660%
Loss D: 2.295
Loss G: 0.1938 (0.7018) Acc G: 29.526%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3265 (0.3255) Acc D Real: 71.095%
Loss D Fake: 1.9053 (1.1806) Acc D Fake: 69.159%
Loss D: 2.232
Loss G: 0.1991 (0.6979) Acc G: 30.034%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3194 (0.3254) Acc D Real: 71.267%
Loss D Fake: 1.8704 (1.1859) Acc D Fake: 68.665%
Loss D: 2.190
Loss G: 0.2044 (0.6941) Acc G: 30.533%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3570 (0.3257) Acc D Real: 71.419%
Loss D Fake: 1.8364 (1.1909) Acc D Fake: 68.179%
Loss D: 2.193
Loss G: 0.2098 (0.6905) Acc G: 31.025%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3353 (0.3257) Acc D Real: 71.570%
Loss D Fake: 1.8034 (1.1955) Acc D Fake: 67.700%
Loss D: 2.139
Loss G: 0.2153 (0.6869) Acc G: 31.510%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3449 (0.3259) Acc D Real: 71.727%
Loss D Fake: 1.7714 (1.1998) Acc D Fake: 67.229%
Loss D: 2.116
Loss G: 0.2209 (0.6833) Acc G: 31.987%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3538 (0.3261) Acc D Real: 71.875%
Loss D Fake: 1.7403 (1.2039) Acc D Fake: 66.765%
Loss D: 2.094
Loss G: 0.2265 (0.6799) Acc G: 32.458%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3601 (0.3263) Acc D Real: 72.018%
Loss D Fake: 1.7100 (1.2076) Acc D Fake: 66.307%
Loss D: 2.070
Loss G: 0.2322 (0.6766) Acc G: 32.921%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3250 (0.3263) Acc D Real: 72.168%
Loss D Fake: 1.6806 (1.2111) Acc D Fake: 65.856%
Loss D: 2.006
Loss G: 0.2380 (0.6734) Acc G: 33.377%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3276 (0.3263) Acc D Real: 72.313%
Loss D Fake: 1.6523 (1.2143) Acc D Fake: 65.412%
Loss D: 1.980
Loss G: 0.2437 (0.6703) Acc G: 33.827%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.3740 (0.3267) Acc D Real: 72.449%
Loss D Fake: 1.6249 (1.2173) Acc D Fake: 64.974%
Loss D: 1.999
Loss G: 0.2495 (0.6672) Acc G: 34.270%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3702 (0.3270) Acc D Real: 72.581%
Loss D Fake: 1.5981 (1.2200) Acc D Fake: 64.543%
Loss D: 1.968
Loss G: 0.2554 (0.6643) Acc G: 34.707%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3623 (0.3272) Acc D Real: 72.721%
Loss D Fake: 1.5720 (1.2225) Acc D Fake: 64.118%
Loss D: 1.934
Loss G: 0.2614 (0.6614) Acc G: 35.138%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3847 (0.3276) Acc D Real: 72.846%
Loss D Fake: 1.5466 (1.2248) Acc D Fake: 63.698%
Loss D: 1.931
Loss G: 0.2674 (0.6586) Acc G: 35.563%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3508 (0.3278) Acc D Real: 72.973%
Loss D Fake: 1.5219 (1.2269) Acc D Fake: 63.285%
Loss D: 1.873
Loss G: 0.2734 (0.6559) Acc G: 35.981%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.4661 (0.3288) Acc D Real: 73.095%
Loss D Fake: 1.4979 (1.2288) Acc D Fake: 62.877%
Loss D: 1.964
Loss G: 0.2796 (0.6532) Acc G: 36.394%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3423 (0.3289) Acc D Real: 73.222%
Loss D Fake: 1.4739 (1.2305) Acc D Fake: 62.475%
Loss D: 1.816
Loss G: 0.2857 (0.6507) Acc G: 36.801%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.4192 (0.3295) Acc D Real: 73.330%
Loss D Fake: 1.4509 (1.2321) Acc D Fake: 62.079%
Loss D: 1.870
Loss G: 0.2920 (0.6482) Acc G: 37.202%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3721 (0.3298) Acc D Real: 73.436%
Loss D Fake: 1.4284 (1.2334) Acc D Fake: 61.688%
Loss D: 1.801
Loss G: 0.2982 (0.6458) Acc G: 37.598%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3704 (0.3301) Acc D Real: 73.558%
Loss D Fake: 1.4070 (1.2346) Acc D Fake: 61.302%
Loss D: 1.777
Loss G: 0.3044 (0.6435) Acc G: 37.989%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3711 (0.3303) Acc D Real: 73.675%
Loss D Fake: 1.3864 (1.2356) Acc D Fake: 60.922%
Loss D: 1.757
Loss G: 0.3104 (0.6412) Acc G: 38.374%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3396 (0.3304) Acc D Real: 73.803%
Loss D Fake: 1.3669 (1.2365) Acc D Fake: 60.547%
Loss D: 1.706
Loss G: 0.3163 (0.6391) Acc G: 38.754%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4120 (0.3309) Acc D Real: 73.916%
Loss D Fake: 1.3483 (1.2372) Acc D Fake: 60.176%
Loss D: 1.760
Loss G: 0.3221 (0.6369) Acc G: 39.129%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3565 (0.3311) Acc D Real: 74.045%
Loss D Fake: 1.3301 (1.2378) Acc D Fake: 59.811%
Loss D: 1.687
Loss G: 0.3279 (0.6349) Acc G: 39.499%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3870 (0.3315) Acc D Real: 74.158%
Loss D Fake: 1.3129 (1.2383) Acc D Fake: 59.450%
Loss D: 1.700
Loss G: 0.3336 (0.6329) Acc G: 39.864%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3800 (0.3318) Acc D Real: 74.274%
Loss D Fake: 1.2963 (1.2387) Acc D Fake: 59.094%
Loss D: 1.676
Loss G: 0.3392 (0.6310) Acc G: 40.224%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3733 (0.3321) Acc D Real: 74.393%
Loss D Fake: 1.2804 (1.2390) Acc D Fake: 58.743%
Loss D: 1.654
Loss G: 0.3446 (0.6291) Acc G: 40.580%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.4147 (0.3326) Acc D Real: 74.484%
Loss D Fake: 1.2652 (1.2392) Acc D Fake: 58.397%
Loss D: 1.680
Loss G: 0.3501 (0.6273) Acc G: 40.931%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4168 (0.3331) Acc D Real: 74.588%
Loss D Fake: 1.2500 (1.2392) Acc D Fake: 58.054%
Loss D: 1.667
Loss G: 0.3557 (0.6256) Acc G: 41.278%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3923 (0.3335) Acc D Real: 74.702%
Loss D Fake: 1.2350 (1.2392) Acc D Fake: 57.716%
Loss D: 1.627
Loss G: 0.3612 (0.6239) Acc G: 41.620%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.4293 (0.3341) Acc D Real: 74.800%
Loss D Fake: 1.2204 (1.2391) Acc D Fake: 57.383%
Loss D: 1.650
Loss G: 0.3669 (0.6223) Acc G: 41.947%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.4122 (0.3346) Acc D Real: 74.901%
Loss D Fake: 1.2058 (1.2389) Acc D Fake: 57.064%
Loss D: 1.618
Loss G: 0.3726 (0.6207) Acc G: 42.270%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.4050 (0.3351) Acc D Real: 75.007%
Loss D Fake: 1.1917 (1.2386) Acc D Fake: 56.749%
Loss D: 1.597
Loss G: 0.3781 (0.6192) Acc G: 42.590%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.4257 (0.3356) Acc D Real: 75.093%
Loss D Fake: 1.1781 (1.2382) Acc D Fake: 56.438%
Loss D: 1.604
Loss G: 0.3837 (0.6177) Acc G: 42.905%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.4177 (0.3361) Acc D Real: 75.200%
Loss D Fake: 1.1648 (1.2377) Acc D Fake: 56.130%
Loss D: 1.582
Loss G: 0.3893 (0.6163) Acc G: 43.216%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4149 (0.3366) Acc D Real: 75.300%
Loss D Fake: 1.1517 (1.2372) Acc D Fake: 55.827%
Loss D: 1.567
Loss G: 0.3948 (0.6150) Acc G: 43.523%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.4113 (0.3371) Acc D Real: 75.401%
Loss D Fake: 1.1393 (1.2366) Acc D Fake: 55.527%
Loss D: 1.551
Loss G: 0.4000 (0.6137) Acc G: 43.827%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.4124 (0.3375) Acc D Real: 75.508%
Loss D Fake: 1.1278 (1.2360) Acc D Fake: 55.231%
Loss D: 1.540
Loss G: 0.4050 (0.6124) Acc G: 44.127%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.4223 (0.3380) Acc D Real: 75.602%
Loss D Fake: 1.1168 (1.2352) Acc D Fake: 54.939%
Loss D: 1.539
Loss G: 0.4099 (0.6112) Acc G: 44.424%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.4371 (0.3386) Acc D Real: 75.679%
Loss D Fake: 1.1062 (1.2345) Acc D Fake: 54.649%
Loss D: 1.543
Loss G: 0.4148 (0.6100) Acc G: 44.717%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4334 (0.3392) Acc D Real: 75.763%
Loss D Fake: 1.0960 (1.2336) Acc D Fake: 54.364%
Loss D: 1.529
Loss G: 0.4195 (0.6089) Acc G: 45.006%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.4311 (0.3397) Acc D Real: 75.854%
Loss D Fake: 1.0864 (1.2328) Acc D Fake: 54.082%
Loss D: 1.517
Loss G: 0.4239 (0.6078) Acc G: 45.292%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.4333 (0.3403) Acc D Real: 75.941%
Loss D Fake: 1.0771 (1.2319) Acc D Fake: 53.803%
Loss D: 1.510
Loss G: 0.4285 (0.6067) Acc G: 45.574%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.4318 (0.3408) Acc D Real: 76.029%
Loss D Fake: 1.0678 (1.2309) Acc D Fake: 53.527%
Loss D: 1.500
Loss G: 0.4330 (0.6057) Acc G: 45.854%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.4285 (0.3413) Acc D Real: 76.129%
Loss D Fake: 1.0589 (1.2299) Acc D Fake: 53.255%
Loss D: 1.487
Loss G: 0.4373 (0.6047) Acc G: 46.130%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.4458 (0.3419) Acc D Real: 76.206%
Loss D Fake: 1.0506 (1.2289) Acc D Fake: 52.985%
Loss D: 1.496
Loss G: 0.4414 (0.6038) Acc G: 46.403%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.4333 (0.3425) Acc D Real: 76.300%
Loss D Fake: 1.0428 (1.2278) Acc D Fake: 52.719%
Loss D: 1.476
Loss G: 0.4453 (0.6029) Acc G: 46.672%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.4505 (0.3431) Acc D Real: 76.372%
Loss D Fake: 1.0352 (1.2267) Acc D Fake: 52.456%
Loss D: 1.486
Loss G: 0.4493 (0.6020) Acc G: 46.939%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.4476 (0.3437) Acc D Real: 76.458%
Loss D Fake: 1.0280 (1.2256) Acc D Fake: 52.196%
Loss D: 1.476
Loss G: 0.4528 (0.6011) Acc G: 47.203%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4431 (0.3442) Acc D Real: 76.538%
Loss D Fake: 1.0214 (1.2244) Acc D Fake: 51.939%
Loss D: 1.464
Loss G: 0.4563 (0.6003) Acc G: 47.463%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4599 (0.3449) Acc D Real: 76.621%
Loss D Fake: 1.0151 (1.2232) Acc D Fake: 51.684%
Loss D: 1.475
Loss G: 0.4595 (0.5995) Acc G: 47.721%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.4472 (0.3455) Acc D Real: 76.700%
Loss D Fake: 1.0095 (1.2220) Acc D Fake: 51.433%
Loss D: 1.457
Loss G: 0.4625 (0.5988) Acc G: 47.976%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.4603 (0.3461) Acc D Real: 76.763%
Loss D Fake: 1.0038 (1.2208) Acc D Fake: 51.184%
Loss D: 1.464
Loss G: 0.4657 (0.5980) Acc G: 48.228%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.4295 (0.3466) Acc D Real: 76.849%
Loss D Fake: 0.9980 (1.2196) Acc D Fake: 50.938%
Loss D: 1.428
Loss G: 0.4691 (0.5973) Acc G: 48.477%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.4544 (0.3471) Acc D Real: 76.928%
Loss D Fake: 0.9919 (1.2183) Acc D Fake: 50.695%
Loss D: 1.446
Loss G: 0.4724 (0.5966) Acc G: 48.723%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.4615 (0.3478) Acc D Real: 77.000%
Loss D Fake: 0.9862 (1.2171) Acc D Fake: 50.454%
Loss D: 1.448
Loss G: 0.4755 (0.5960) Acc G: 48.967%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.4522 (0.3483) Acc D Real: 77.079%
Loss D Fake: 0.9809 (1.2158) Acc D Fake: 50.216%
Loss D: 1.433
Loss G: 0.4785 (0.5953) Acc G: 49.208%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.4590 (0.3489) Acc D Real: 77.163%
Loss D Fake: 0.9758 (1.2145) Acc D Fake: 49.981%
Loss D: 1.435
Loss G: 0.4813 (0.5947) Acc G: 49.447%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.4651 (0.3496) Acc D Real: 77.227%
Loss D Fake: 0.9712 (1.2132) Acc D Fake: 49.748%
Loss D: 1.436
Loss G: 0.4839 (0.5941) Acc G: 49.683%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.4743 (0.3502) Acc D Real: 77.296%
Loss D Fake: 0.9669 (1.2119) Acc D Fake: 49.518%
Loss D: 1.441
Loss G: 0.4863 (0.5935) Acc G: 49.916%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.4731 (0.3509) Acc D Real: 77.374%
Loss D Fake: 0.9631 (1.2106) Acc D Fake: 49.290%
Loss D: 1.436
Loss G: 0.4883 (0.5930) Acc G: 50.147%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.4637 (0.3515) Acc D Real: 77.451%
Loss D Fake: 0.9600 (1.2092) Acc D Fake: 49.064%
Loss D: 1.424
Loss G: 0.4901 (0.5924) Acc G: 50.376%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.4631 (0.3521) Acc D Real: 77.513%
Loss D Fake: 0.9569 (1.2079) Acc D Fake: 48.841%
Loss D: 1.420
Loss G: 0.4921 (0.5919) Acc G: 50.602%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.4652 (0.3527) Acc D Real: 77.577%
Loss D Fake: 0.9535 (1.2066) Acc D Fake: 48.620%
Loss D: 1.419
Loss G: 0.4941 (0.5914) Acc G: 50.825%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.4579 (0.3532) Acc D Real: 77.662%
Loss D Fake: 0.9502 (1.2052) Acc D Fake: 48.402%
Loss D: 1.408
Loss G: 0.4960 (0.5909) Acc G: 51.047%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.4461 (0.3537) Acc D Real: 77.716%
Loss D Fake: 0.9469 (1.2039) Acc D Fake: 48.185%
Loss D: 1.393
Loss G: 0.4984 (0.5904) Acc G: 51.266%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.4681 (0.3543) Acc D Real: 77.784%
Loss D Fake: 0.9429 (1.2025) Acc D Fake: 47.971%
Loss D: 1.411
Loss G: 0.5008 (0.5900) Acc G: 51.483%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.4595 (0.3548) Acc D Real: 77.846%
Loss D Fake: 0.9390 (1.2012) Acc D Fake: 47.760%
Loss D: 1.399
Loss G: 0.5032 (0.5895) Acc G: 51.697%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.4681 (0.3554) Acc D Real: 77.912%
Loss D Fake: 0.9351 (1.1998) Acc D Fake: 47.550%
Loss D: 1.403
Loss G: 0.5056 (0.5891) Acc G: 51.910%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4782 (0.3560) Acc D Real: 77.988%
Loss D Fake: 0.9315 (1.1985) Acc D Fake: 47.342%
Loss D: 1.410
Loss G: 0.5076 (0.5887) Acc G: 52.120%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.4614 (0.3565) Acc D Real: 78.050%
Loss D Fake: 0.9285 (1.1971) Acc D Fake: 47.137%
Loss D: 1.390
Loss G: 0.5096 (0.5883) Acc G: 52.328%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.4911 (0.3572) Acc D Real: 78.113%
Loss D Fake: 0.9255 (1.1957) Acc D Fake: 46.934%
Loss D: 1.417
Loss G: 0.5112 (0.5879) Acc G: 52.534%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.4759 (0.3578) Acc D Real: 78.172%
Loss D Fake: 0.9231 (1.1944) Acc D Fake: 46.732%
Loss D: 1.399
Loss G: 0.5127 (0.5875) Acc G: 52.738%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.4817 (0.3584) Acc D Real: 78.243%
Loss D Fake: 0.9209 (1.1930) Acc D Fake: 46.533%
Loss D: 1.403
Loss G: 0.5139 (0.5871) Acc G: 52.940%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.4457 (0.3589) Acc D Real: 78.313%
Loss D Fake: 0.9191 (1.1917) Acc D Fake: 46.336%
Loss D: 1.365
Loss G: 0.5153 (0.5868) Acc G: 53.140%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.4974 (0.3595) Acc D Real: 78.371%
Loss D Fake: 0.9170 (1.1903) Acc D Fake: 46.140%
Loss D: 1.414
Loss G: 0.5164 (0.5864) Acc G: 53.338%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.4771 (0.3601) Acc D Real: 78.436%
Loss D Fake: 0.9153 (1.1890) Acc D Fake: 45.947%
Loss D: 1.392
Loss G: 0.5174 (0.5861) Acc G: 53.534%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.4665 (0.3606) Acc D Real: 78.493%
Loss D Fake: 0.9137 (1.1876) Acc D Fake: 45.755%
Loss D: 1.380
Loss G: 0.5186 (0.5858) Acc G: 53.728%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.4669 (0.3612) Acc D Real: 78.548%
Loss D Fake: 0.9118 (1.1863) Acc D Fake: 45.565%
Loss D: 1.379
Loss G: 0.5199 (0.5854) Acc G: 53.921%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4579 (0.3616) Acc D Real: 78.614%
Loss D Fake: 0.9097 (1.1849) Acc D Fake: 45.377%
Loss D: 1.368
Loss G: 0.5213 (0.5851) Acc G: 54.111%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.4820 (0.3622) Acc D Real: 78.678%
Loss D Fake: 0.9076 (1.1836) Acc D Fake: 45.191%
Loss D: 1.390
Loss G: 0.5226 (0.5848) Acc G: 54.300%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.5015 (0.3629) Acc D Real: 78.727%
Loss D Fake: 0.9060 (1.1823) Acc D Fake: 45.007%
Loss D: 1.407
Loss G: 0.5235 (0.5845) Acc G: 54.486%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.4877 (0.3635) Acc D Real: 78.793%
Loss D Fake: 0.9048 (1.1810) Acc D Fake: 44.824%
Loss D: 1.392
Loss G: 0.5241 (0.5843) Acc G: 54.671%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.4714 (0.3640) Acc D Real: 78.848%
Loss D Fake: 0.9039 (1.1797) Acc D Fake: 44.644%
Loss D: 1.375
Loss G: 0.5248 (0.5840) Acc G: 54.855%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.4717 (0.3645) Acc D Real: 78.917%
Loss D Fake: 0.9029 (1.1783) Acc D Fake: 44.464%
Loss D: 1.375
Loss G: 0.5254 (0.5837) Acc G: 55.036%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.4640 (0.3650) Acc D Real: 78.972%
Loss D Fake: 0.9019 (1.1770) Acc D Fake: 44.287%
Loss D: 1.366
Loss G: 0.5261 (0.5834) Acc G: 55.216%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.4754 (0.3655) Acc D Real: 79.035%
Loss D Fake: 0.9007 (1.1758) Acc D Fake: 44.111%
Loss D: 1.376
Loss G: 0.5268 (0.5832) Acc G: 55.394%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.4900 (0.3660) Acc D Real: 79.097%
Loss D Fake: 0.8998 (1.1745) Acc D Fake: 43.937%
Loss D: 1.390
Loss G: 0.5273 (0.5829) Acc G: 55.570%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.4524 (0.3664) Acc D Real: 79.135%
Loss D Fake: 0.8990 (1.1732) Acc D Fake: 43.764%
Loss D: 1.351
Loss G: 0.5282 (0.5826) Acc G: 55.745%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.4649 (0.3669) Acc D Real: 79.188%
Loss D Fake: 0.8974 (1.1719) Acc D Fake: 43.594%
Loss D: 1.362
Loss G: 0.5292 (0.5824) Acc G: 55.919%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.4850 (0.3674) Acc D Real: 79.250%
Loss D Fake: 0.8959 (1.1707) Acc D Fake: 43.424%
Loss D: 1.381
Loss G: 0.5301 (0.5822) Acc G: 56.090%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.4685 (0.3679) Acc D Real: 79.293%
Loss D Fake: 0.8947 (1.1694) Acc D Fake: 43.256%
Loss D: 1.363
Loss G: 0.5310 (0.5819) Acc G: 56.260%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.4354 (0.3682) Acc D Real: 79.346%
Loss D Fake: 0.8930 (1.1681) Acc D Fake: 43.090%
Loss D: 1.328
Loss G: 0.5324 (0.5817) Acc G: 56.429%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.4622 (0.3686) Acc D Real: 79.397%
Loss D Fake: 0.8908 (1.1669) Acc D Fake: 42.925%
Loss D: 1.353
Loss G: 0.5338 (0.5815) Acc G: 56.596%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.4685 (0.3691) Acc D Real: 79.447%
Loss D Fake: 0.8887 (1.1656) Acc D Fake: 42.762%
Loss D: 1.357
Loss G: 0.5353 (0.5813) Acc G: 56.761%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.4327 (0.3694) Acc D Real: 79.494%
Loss D Fake: 0.8864 (1.1644) Acc D Fake: 42.600%
Loss D: 1.319
Loss G: 0.5371 (0.5811) Acc G: 56.925%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.4376 (0.3697) Acc D Real: 79.550%
Loss D Fake: 0.8835 (1.1631) Acc D Fake: 42.440%
Loss D: 1.321
Loss G: 0.5391 (0.5809) Acc G: 57.088%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.4754 (0.3701) Acc D Real: 79.603%
Loss D Fake: 0.8806 (1.1619) Acc D Fake: 42.281%
Loss D: 1.356
Loss G: 0.5409 (0.5807) Acc G: 57.249%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.4457 (0.3705) Acc D Real: 79.617%
Loss D Fake: 0.8781 (1.1606) Acc D Fake: 42.241%
Loss D: 1.324
Loss G: 0.5427 (0.5805) Acc G: 57.289%
LR: 2.000e-04
Epoch: 8/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.4689 (0.4838) Acc D Real: 92.214%
Loss D Fake: 0.8741 (0.8749) Acc D Fake: 6.667%
Loss D: 1.343
Loss G: 0.5452 (0.5446) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.5015 (0.4897) Acc D Real: 91.372%
Loss D Fake: 0.8727 (0.8742) Acc D Fake: 6.667%
Loss D: 1.374
Loss G: 0.5460 (0.5451) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.4914 (0.4901) Acc D Real: 91.328%
Loss D Fake: 0.8717 (0.8735) Acc D Fake: 6.667%
Loss D: 1.363
Loss G: 0.5465 (0.5454) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.4636 (0.4848) Acc D Real: 91.302%
Loss D Fake: 0.8709 (0.8730) Acc D Fake: 6.667%
Loss D: 1.334
Loss G: 0.5471 (0.5458) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.4523 (0.4794) Acc D Real: 91.554%
Loss D Fake: 0.8700 (0.8725) Acc D Fake: 6.667%
Loss D: 1.322
Loss G: 0.5478 (0.5461) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.4568 (0.4761) Acc D Real: 91.592%
Loss D Fake: 0.8689 (0.8720) Acc D Fake: 6.667%
Loss D: 1.326
Loss G: 0.5486 (0.5465) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.5149 (0.4810) Acc D Real: 91.120%
Loss D Fake: 0.8679 (0.8715) Acc D Fake: 6.667%
Loss D: 1.383
Loss G: 0.5492 (0.5468) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.5010 (0.4832) Acc D Real: 91.007%
Loss D Fake: 0.8672 (0.8710) Acc D Fake: 6.667%
Loss D: 1.368
Loss G: 0.5495 (0.5471) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.5084 (0.4857) Acc D Real: 90.964%
Loss D Fake: 0.8670 (0.8706) Acc D Fake: 6.667%
Loss D: 1.375
Loss G: 0.5496 (0.5474) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.4835 (0.4855) Acc D Real: 91.075%
Loss D Fake: 0.8670 (0.8703) Acc D Fake: 6.667%
Loss D: 1.351
Loss G: 0.5495 (0.5476) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.4854 (0.4855) Acc D Real: 91.328%
Loss D Fake: 0.8673 (0.8700) Acc D Fake: 6.667%
Loss D: 1.353
Loss G: 0.5492 (0.5477) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.4938 (0.4862) Acc D Real: 91.342%
Loss D Fake: 0.8678 (0.8699) Acc D Fake: 6.667%
Loss D: 1.362
Loss G: 0.5488 (0.5478) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.4268 (0.4819) Acc D Real: 91.332%
Loss D Fake: 0.8682 (0.8697) Acc D Fake: 6.667%
Loss D: 1.295
Loss G: 0.5488 (0.5479) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.4789 (0.4817) Acc D Real: 91.285%
Loss D Fake: 0.8678 (0.8696) Acc D Fake: 6.667%
Loss D: 1.347
Loss G: 0.5490 (0.5479) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.4259 (0.4782) Acc D Real: 91.260%
Loss D Fake: 0.8673 (0.8695) Acc D Fake: 6.667%
Loss D: 1.293
Loss G: 0.5497 (0.5480) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.4576 (0.4770) Acc D Real: 91.351%
Loss D Fake: 0.8662 (0.8693) Acc D Fake: 6.667%
Loss D: 1.324
Loss G: 0.5505 (0.5482) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.4930 (0.4779) Acc D Real: 91.270%
Loss D Fake: 0.8652 (0.8691) Acc D Fake: 6.667%
Loss D: 1.358
Loss G: 0.5510 (0.5483) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.4572 (0.4768) Acc D Real: 91.118%
Loss D Fake: 0.8644 (0.8688) Acc D Fake: 6.667%
Loss D: 1.322
Loss G: 0.5518 (0.5485) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.4983 (0.4779) Acc D Real: 91.156%
Loss D Fake: 0.8633 (0.8685) Acc D Fake: 6.667%
Loss D: 1.362
Loss G: 0.5524 (0.5487) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.5017 (0.4790) Acc D Real: 91.094%
Loss D Fake: 0.8628 (0.8683) Acc D Fake: 6.667%
Loss D: 1.364
Loss G: 0.5526 (0.5489) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.4453 (0.4775) Acc D Real: 91.153%
Loss D Fake: 0.8624 (0.8680) Acc D Fake: 6.667%
Loss D: 1.308
Loss G: 0.5530 (0.5491) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.4696 (0.4771) Acc D Real: 91.107%
Loss D Fake: 0.8617 (0.8677) Acc D Fake: 6.667%
Loss D: 1.331
Loss G: 0.5535 (0.5493) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4389 (0.4756) Acc D Real: 91.115%
Loss D Fake: 0.8609 (0.8674) Acc D Fake: 6.667%
Loss D: 1.300
Loss G: 0.5543 (0.5495) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.4351 (0.4739) Acc D Real: 91.133%
Loss D Fake: 0.8595 (0.8671) Acc D Fake: 6.667%
Loss D: 1.295
Loss G: 0.5554 (0.5497) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.4498 (0.4730) Acc D Real: 91.120%
Loss D Fake: 0.8579 (0.8668) Acc D Fake: 6.667%
Loss D: 1.308
Loss G: 0.5567 (0.5500) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.5027 (0.4741) Acc D Real: 91.202%
Loss D Fake: 0.8563 (0.8664) Acc D Fake: 6.667%
Loss D: 1.359
Loss G: 0.5575 (0.5503) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.4487 (0.4732) Acc D Real: 91.265%
Loss D Fake: 0.8554 (0.8660) Acc D Fake: 6.667%
Loss D: 1.304
Loss G: 0.5582 (0.5506) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.4700 (0.4731) Acc D Real: 91.180%
Loss D Fake: 0.8544 (0.8656) Acc D Fake: 6.667%
Loss D: 1.324
Loss G: 0.5589 (0.5508) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.5268 (0.4749) Acc D Real: 91.160%
Loss D Fake: 0.8536 (0.8652) Acc D Fake: 6.667%
Loss D: 1.380
Loss G: 0.5592 (0.5511) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.4468 (0.4740) Acc D Real: 91.178%
Loss D Fake: 0.8534 (0.8648) Acc D Fake: 6.667%
Loss D: 1.300
Loss G: 0.5595 (0.5514) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.5068 (0.4750) Acc D Real: 91.154%
Loss D Fake: 0.8530 (0.8644) Acc D Fake: 6.667%
Loss D: 1.360
Loss G: 0.5596 (0.5517) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.4886 (0.4754) Acc D Real: 91.097%
Loss D Fake: 0.8529 (0.8641) Acc D Fake: 6.667%
Loss D: 1.342
Loss G: 0.5596 (0.5519) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.4556 (0.4748) Acc D Real: 91.131%
Loss D Fake: 0.8528 (0.8638) Acc D Fake: 6.667%
Loss D: 1.308
Loss G: 0.5598 (0.5521) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.4832 (0.4751) Acc D Real: 91.042%
Loss D Fake: 0.8525 (0.8634) Acc D Fake: 6.667%
Loss D: 1.336
Loss G: 0.5601 (0.5524) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.4924 (0.4755) Acc D Real: 91.056%
Loss D Fake: 0.8522 (0.8631) Acc D Fake: 6.667%
Loss D: 1.345
Loss G: 0.5602 (0.5526) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.4570 (0.4750) Acc D Real: 90.999%
Loss D Fake: 0.8520 (0.8628) Acc D Fake: 6.667%
Loss D: 1.309
Loss G: 0.5604 (0.5528) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.4420 (0.4742) Acc D Real: 91.039%
Loss D Fake: 0.8515 (0.8625) Acc D Fake: 6.667%
Loss D: 1.293
Loss G: 0.5609 (0.5530) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.4774 (0.4743) Acc D Real: 91.126%
Loss D Fake: 0.8509 (0.8622) Acc D Fake: 6.667%
Loss D: 1.328
Loss G: 0.5612 (0.5532) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.4336 (0.4732) Acc D Real: 91.112%
Loss D Fake: 0.8504 (0.8619) Acc D Fake: 6.667%
Loss D: 1.284
Loss G: 0.5618 (0.5534) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.5340 (0.4747) Acc D Real: 91.033%
Loss D Fake: 0.8497 (0.8616) Acc D Fake: 6.667%
Loss D: 1.384
Loss G: 0.5620 (0.5536) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.4907 (0.4751) Acc D Real: 91.049%
Loss D Fake: 0.8497 (0.8613) Acc D Fake: 6.667%
Loss D: 1.340
Loss G: 0.5619 (0.5538) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.4788 (0.4752) Acc D Real: 91.047%
Loss D Fake: 0.8499 (0.8611) Acc D Fake: 6.667%
Loss D: 1.329
Loss G: 0.5618 (0.5540) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.4801 (0.4753) Acc D Real: 90.997%
Loss D Fake: 0.8500 (0.8608) Acc D Fake: 6.667%
Loss D: 1.330
Loss G: 0.5617 (0.5542) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.4786 (0.4754) Acc D Real: 90.985%
Loss D Fake: 0.8500 (0.8606) Acc D Fake: 6.667%
Loss D: 1.329
Loss G: 0.5617 (0.5544) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.4139 (0.4740) Acc D Real: 90.966%
Loss D Fake: 0.8498 (0.8604) Acc D Fake: 6.667%
Loss D: 1.264
Loss G: 0.5622 (0.5545) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4798 (0.4742) Acc D Real: 90.957%
Loss D Fake: 0.8489 (0.8601) Acc D Fake: 6.667%
Loss D: 1.329
Loss G: 0.5628 (0.5547) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.4789 (0.4743) Acc D Real: 90.984%
Loss D Fake: 0.8482 (0.8599) Acc D Fake: 6.667%
Loss D: 1.327
Loss G: 0.5632 (0.5549) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.4630 (0.4740) Acc D Real: 90.960%
Loss D Fake: 0.8477 (0.8596) Acc D Fake: 6.667%
Loss D: 1.311
Loss G: 0.5637 (0.5551) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.4864 (0.4743) Acc D Real: 91.004%
Loss D Fake: 0.8471 (0.8594) Acc D Fake: 6.667%
Loss D: 1.334
Loss G: 0.5639 (0.5552) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.4885 (0.4746) Acc D Real: 91.056%
Loss D Fake: 0.8471 (0.8591) Acc D Fake: 6.667%
Loss D: 1.336
Loss G: 0.5638 (0.5554) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4422 (0.4739) Acc D Real: 91.061%
Loss D Fake: 0.8472 (0.8589) Acc D Fake: 6.667%
Loss D: 1.289
Loss G: 0.5638 (0.5556) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.4376 (0.4732) Acc D Real: 91.069%
Loss D Fake: 0.8470 (0.8587) Acc D Fake: 6.667%
Loss D: 1.285
Loss G: 0.5641 (0.5557) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.4652 (0.4731) Acc D Real: 91.073%
Loss D Fake: 0.8464 (0.8584) Acc D Fake: 6.667%
Loss D: 1.312
Loss G: 0.5645 (0.5559) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4432 (0.4726) Acc D Real: 91.061%
Loss D Fake: 0.8458 (0.8582) Acc D Fake: 6.667%
Loss D: 1.289
Loss G: 0.5651 (0.5561) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.4356 (0.4719) Acc D Real: 91.081%
Loss D Fake: 0.8449 (0.8580) Acc D Fake: 6.667%
Loss D: 1.280
Loss G: 0.5659 (0.5562) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.4604 (0.4717) Acc D Real: 91.100%
Loss D Fake: 0.8438 (0.8577) Acc D Fake: 6.667%
Loss D: 1.304
Loss G: 0.5666 (0.5564) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.4300 (0.4710) Acc D Real: 91.116%
Loss D Fake: 0.8428 (0.8575) Acc D Fake: 6.667%
Loss D: 1.273
Loss G: 0.5675 (0.5566) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4385 (0.4704) Acc D Real: 91.110%
Loss D Fake: 0.8415 (0.8572) Acc D Fake: 6.667%
Loss D: 1.280
Loss G: 0.5686 (0.5568) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.4693 (0.4704) Acc D Real: 91.101%
Loss D Fake: 0.8400 (0.8569) Acc D Fake: 6.667%
Loss D: 1.309
Loss G: 0.5696 (0.5570) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.4499 (0.4701) Acc D Real: 91.124%
Loss D Fake: 0.8388 (0.8566) Acc D Fake: 6.667%
Loss D: 1.289
Loss G: 0.5705 (0.5572) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.4587 (0.4699) Acc D Real: 91.135%
Loss D Fake: 0.8376 (0.8563) Acc D Fake: 6.667%
Loss D: 1.296
Loss G: 0.5714 (0.5575) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.4781 (0.4700) Acc D Real: 91.149%
Loss D Fake: 0.8366 (0.8560) Acc D Fake: 6.667%
Loss D: 1.315
Loss G: 0.5720 (0.5577) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.4692 (0.4700) Acc D Real: 91.129%
Loss D Fake: 0.8358 (0.8557) Acc D Fake: 6.667%
Loss D: 1.305
Loss G: 0.5725 (0.5579) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.5379 (0.4710) Acc D Real: 91.101%
Loss D Fake: 0.8353 (0.8554) Acc D Fake: 6.665%
Loss D: 1.373
Loss G: 0.5726 (0.5582) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.4444 (0.4706) Acc D Real: 91.103%
Loss D Fake: 0.8354 (0.8551) Acc D Fake: 6.640%
Loss D: 1.280
Loss G: 0.5727 (0.5584) Acc G: 93.359%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.4219 (0.4699) Acc D Real: 91.122%
Loss D Fake: 0.8350 (0.8548) Acc D Fake: 6.615%
Loss D: 1.257
Loss G: 0.5731 (0.5586) Acc G: 93.383%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4466 (0.4696) Acc D Real: 91.121%
Loss D Fake: 0.8343 (0.8545) Acc D Fake: 6.592%
Loss D: 1.281
Loss G: 0.5737 (0.5588) Acc G: 93.407%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4520 (0.4693) Acc D Real: 91.137%
Loss D Fake: 0.8334 (0.8542) Acc D Fake: 6.569%
Loss D: 1.285
Loss G: 0.5744 (0.5590) Acc G: 93.430%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.4949 (0.4697) Acc D Real: 91.144%
Loss D Fake: 0.8326 (0.8539) Acc D Fake: 6.546%
Loss D: 1.328
Loss G: 0.5748 (0.5593) Acc G: 93.452%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.5279 (0.4705) Acc D Real: 91.144%
Loss D Fake: 0.8325 (0.8536) Acc D Fake: 6.524%
Loss D: 1.360
Loss G: 0.5746 (0.5595) Acc G: 93.474%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.4930 (0.4708) Acc D Real: 91.148%
Loss D Fake: 0.8331 (0.8533) Acc D Fake: 6.503%
Loss D: 1.326
Loss G: 0.5741 (0.5597) Acc G: 93.495%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.4685 (0.4708) Acc D Real: 91.149%
Loss D Fake: 0.8337 (0.8530) Acc D Fake: 6.483%
Loss D: 1.302
Loss G: 0.5736 (0.5599) Acc G: 93.516%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.4316 (0.4703) Acc D Real: 91.147%
Loss D Fake: 0.8341 (0.8527) Acc D Fake: 6.463%
Loss D: 1.266
Loss G: 0.5736 (0.5601) Acc G: 93.536%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.4322 (0.4697) Acc D Real: 91.165%
Loss D Fake: 0.8339 (0.8525) Acc D Fake: 6.443%
Loss D: 1.266
Loss G: 0.5738 (0.5603) Acc G: 93.556%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4945 (0.4701) Acc D Real: 91.165%
Loss D Fake: 0.8336 (0.8522) Acc D Fake: 6.424%
Loss D: 1.328
Loss G: 0.5739 (0.5604) Acc G: 93.575%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.4331 (0.4696) Acc D Real: 91.198%
Loss D Fake: 0.8335 (0.8520) Acc D Fake: 6.406%
Loss D: 1.267
Loss G: 0.5741 (0.5606) Acc G: 93.593%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.4962 (0.4699) Acc D Real: 91.174%
Loss D Fake: 0.8334 (0.8518) Acc D Fake: 6.388%
Loss D: 1.330
Loss G: 0.5741 (0.5608) Acc G: 93.611%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.4188 (0.4693) Acc D Real: 91.205%
Loss D Fake: 0.8333 (0.8515) Acc D Fake: 6.370%
Loss D: 1.252
Loss G: 0.5743 (0.5610) Acc G: 93.629%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.4838 (0.4695) Acc D Real: 91.223%
Loss D Fake: 0.8330 (0.8513) Acc D Fake: 6.353%
Loss D: 1.317
Loss G: 0.5744 (0.5611) Acc G: 93.646%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.4861 (0.4697) Acc D Real: 91.245%
Loss D Fake: 0.8331 (0.8511) Acc D Fake: 6.336%
Loss D: 1.319
Loss G: 0.5742 (0.5613) Acc G: 93.663%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.5009 (0.4701) Acc D Real: 91.254%
Loss D Fake: 0.8335 (0.8509) Acc D Fake: 6.320%
Loss D: 1.334
Loss G: 0.5737 (0.5614) Acc G: 93.679%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3925 (0.4691) Acc D Real: 91.251%
Loss D Fake: 0.8339 (0.8507) Acc D Fake: 6.304%
Loss D: 1.226
Loss G: 0.5738 (0.5616) Acc G: 93.695%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.4814 (0.4693) Acc D Real: 91.229%
Loss D Fake: 0.8335 (0.8504) Acc D Fake: 6.288%
Loss D: 1.315
Loss G: 0.5741 (0.5617) Acc G: 93.710%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.4731 (0.4693) Acc D Real: 91.239%
Loss D Fake: 0.8332 (0.8502) Acc D Fake: 6.273%
Loss D: 1.306
Loss G: 0.5742 (0.5619) Acc G: 93.725%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4432 (0.4690) Acc D Real: 91.272%
Loss D Fake: 0.8331 (0.8500) Acc D Fake: 6.258%
Loss D: 1.276
Loss G: 0.5743 (0.5620) Acc G: 93.740%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.5271 (0.4697) Acc D Real: 91.306%
Loss D Fake: 0.8332 (0.8499) Acc D Fake: 6.244%
Loss D: 1.360
Loss G: 0.5738 (0.5622) Acc G: 93.755%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.5013 (0.4700) Acc D Real: 91.336%
Loss D Fake: 0.8342 (0.8497) Acc D Fake: 6.230%
Loss D: 1.336
Loss G: 0.5729 (0.5623) Acc G: 93.769%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.4335 (0.4696) Acc D Real: 91.316%
Loss D Fake: 0.8354 (0.8495) Acc D Fake: 6.216%
Loss D: 1.269
Loss G: 0.5723 (0.5624) Acc G: 93.783%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4646 (0.4696) Acc D Real: 91.319%
Loss D Fake: 0.8358 (0.8494) Acc D Fake: 6.203%
Loss D: 1.300
Loss G: 0.5721 (0.5625) Acc G: 93.796%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.4583 (0.4694) Acc D Real: 91.323%
Loss D Fake: 0.8361 (0.8492) Acc D Fake: 6.189%
Loss D: 1.294
Loss G: 0.5719 (0.5626) Acc G: 93.810%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.4553 (0.4693) Acc D Real: 91.343%
Loss D Fake: 0.8363 (0.8491) Acc D Fake: 6.176%
Loss D: 1.292
Loss G: 0.5718 (0.5627) Acc G: 93.822%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.4351 (0.4689) Acc D Real: 91.363%
Loss D Fake: 0.8364 (0.8489) Acc D Fake: 6.164%
Loss D: 1.271
Loss G: 0.5718 (0.5628) Acc G: 93.835%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3810 (0.4680) Acc D Real: 91.382%
Loss D Fake: 0.8360 (0.8488) Acc D Fake: 6.151%
Loss D: 1.217
Loss G: 0.5724 (0.5629) Acc G: 93.848%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.4843 (0.4682) Acc D Real: 91.388%
Loss D Fake: 0.8351 (0.8487) Acc D Fake: 6.139%
Loss D: 1.319
Loss G: 0.5729 (0.5630) Acc G: 93.860%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4252 (0.4677) Acc D Real: 91.395%
Loss D Fake: 0.8345 (0.8485) Acc D Fake: 6.127%
Loss D: 1.260
Loss G: 0.5735 (0.5631) Acc G: 93.872%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4003 (0.4670) Acc D Real: 91.387%
Loss D Fake: 0.8334 (0.8484) Acc D Fake: 6.116%
Loss D: 1.234
Loss G: 0.5747 (0.5632) Acc G: 93.883%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.4534 (0.4669) Acc D Real: 91.400%
Loss D Fake: 0.8318 (0.8482) Acc D Fake: 6.104%
Loss D: 1.285
Loss G: 0.5758 (0.5634) Acc G: 93.895%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.4287 (0.4665) Acc D Real: 91.391%
Loss D Fake: 0.8302 (0.8480) Acc D Fake: 6.093%
Loss D: 1.259
Loss G: 0.5771 (0.5635) Acc G: 93.906%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.4362 (0.4662) Acc D Real: 91.400%
Loss D Fake: 0.8285 (0.8478) Acc D Fake: 6.082%
Loss D: 1.265
Loss G: 0.5784 (0.5637) Acc G: 93.917%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.5131 (0.4666) Acc D Real: 91.408%
Loss D Fake: 0.8270 (0.8476) Acc D Fake: 6.072%
Loss D: 1.340
Loss G: 0.5792 (0.5638) Acc G: 93.927%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.5160 (0.4671) Acc D Real: 91.410%
Loss D Fake: 0.8265 (0.8474) Acc D Fake: 6.061%
Loss D: 1.343
Loss G: 0.5793 (0.5640) Acc G: 93.938%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3981 (0.4665) Acc D Real: 91.403%
Loss D Fake: 0.8263 (0.8472) Acc D Fake: 6.051%
Loss D: 1.224
Loss G: 0.5798 (0.5641) Acc G: 93.948%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3400 (0.4652) Acc D Real: 91.399%
Loss D Fake: 0.8251 (0.8470) Acc D Fake: 6.041%
Loss D: 1.165
Loss G: 0.5813 (0.5643) Acc G: 93.958%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.4496 (0.4651) Acc D Real: 91.405%
Loss D Fake: 0.8229 (0.8468) Acc D Fake: 6.031%
Loss D: 1.272
Loss G: 0.5829 (0.5645) Acc G: 93.968%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.5074 (0.4655) Acc D Real: 91.403%
Loss D Fake: 0.8211 (0.8465) Acc D Fake: 6.021%
Loss D: 1.329
Loss G: 0.5839 (0.5646) Acc G: 93.978%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.5031 (0.4658) Acc D Real: 91.386%
Loss D Fake: 0.8201 (0.8463) Acc D Fake: 6.011%
Loss D: 1.323
Loss G: 0.5845 (0.5648) Acc G: 93.988%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.5352 (0.4665) Acc D Real: 91.370%
Loss D Fake: 0.8197 (0.8460) Acc D Fake: 6.002%
Loss D: 1.355
Loss G: 0.5845 (0.5650) Acc G: 93.997%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.4783 (0.4666) Acc D Real: 91.379%
Loss D Fake: 0.8199 (0.8458) Acc D Fake: 5.993%
Loss D: 1.298
Loss G: 0.5842 (0.5652) Acc G: 94.006%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.5130 (0.4670) Acc D Real: 91.370%
Loss D Fake: 0.8204 (0.8455) Acc D Fake: 5.984%
Loss D: 1.333
Loss G: 0.5837 (0.5654) Acc G: 94.015%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.4648 (0.4670) Acc D Real: 91.380%
Loss D Fake: 0.8212 (0.8453) Acc D Fake: 5.975%
Loss D: 1.286
Loss G: 0.5831 (0.5655) Acc G: 94.024%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.4927 (0.4672) Acc D Real: 91.384%
Loss D Fake: 0.8220 (0.8451) Acc D Fake: 5.966%
Loss D: 1.315
Loss G: 0.5823 (0.5657) Acc G: 94.033%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4089 (0.4667) Acc D Real: 91.375%
Loss D Fake: 0.8228 (0.8449) Acc D Fake: 5.958%
Loss D: 1.232
Loss G: 0.5821 (0.5658) Acc G: 94.041%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.4183 (0.4663) Acc D Real: 91.391%
Loss D Fake: 0.8228 (0.8447) Acc D Fake: 5.949%
Loss D: 1.241
Loss G: 0.5823 (0.5660) Acc G: 94.050%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4026 (0.4657) Acc D Real: 91.413%
Loss D Fake: 0.8223 (0.8445) Acc D Fake: 5.941%
Loss D: 1.225
Loss G: 0.5828 (0.5661) Acc G: 94.058%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4545 (0.4656) Acc D Real: 91.442%
Loss D Fake: 0.8216 (0.8443) Acc D Fake: 5.933%
Loss D: 1.276
Loss G: 0.5832 (0.5662) Acc G: 94.066%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.4221 (0.4653) Acc D Real: 91.455%
Loss D Fake: 0.8211 (0.8441) Acc D Fake: 5.925%
Loss D: 1.243
Loss G: 0.5837 (0.5664) Acc G: 94.074%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.4527 (0.4652) Acc D Real: 91.456%
Loss D Fake: 0.8204 (0.8439) Acc D Fake: 5.917%
Loss D: 1.273
Loss G: 0.5843 (0.5665) Acc G: 94.082%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.4082 (0.4647) Acc D Real: 91.470%
Loss D Fake: 0.8196 (0.8437) Acc D Fake: 5.909%
Loss D: 1.228
Loss G: 0.5850 (0.5667) Acc G: 94.090%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.4476 (0.4645) Acc D Real: 91.476%
Loss D Fake: 0.8186 (0.8435) Acc D Fake: 5.902%
Loss D: 1.266
Loss G: 0.5858 (0.5669) Acc G: 94.097%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.4676 (0.4646) Acc D Real: 91.482%
Loss D Fake: 0.8177 (0.8433) Acc D Fake: 5.894%
Loss D: 1.285
Loss G: 0.5864 (0.5670) Acc G: 94.105%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4836 (0.4647) Acc D Real: 91.492%
Loss D Fake: 0.8171 (0.8431) Acc D Fake: 5.887%
Loss D: 1.301
Loss G: 0.5866 (0.5672) Acc G: 94.112%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.4481 (0.4646) Acc D Real: 91.490%
Loss D Fake: 0.8170 (0.8429) Acc D Fake: 5.880%
Loss D: 1.265
Loss G: 0.5868 (0.5673) Acc G: 94.119%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.4460 (0.4644) Acc D Real: 91.509%
Loss D Fake: 0.8167 (0.8427) Acc D Fake: 5.873%
Loss D: 1.263
Loss G: 0.5870 (0.5675) Acc G: 94.126%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.5064 (0.4648) Acc D Real: 91.514%
Loss D Fake: 0.8166 (0.8425) Acc D Fake: 5.866%
Loss D: 1.323
Loss G: 0.5868 (0.5677) Acc G: 94.133%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.4136 (0.4644) Acc D Real: 91.523%
Loss D Fake: 0.8169 (0.8423) Acc D Fake: 5.859%
Loss D: 1.230
Loss G: 0.5868 (0.5678) Acc G: 94.140%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.4066 (0.4639) Acc D Real: 91.524%
Loss D Fake: 0.8166 (0.8421) Acc D Fake: 5.852%
Loss D: 1.223
Loss G: 0.5873 (0.5680) Acc G: 94.147%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.4042 (0.4634) Acc D Real: 91.527%
Loss D Fake: 0.8157 (0.8418) Acc D Fake: 5.846%
Loss D: 1.220
Loss G: 0.5882 (0.5681) Acc G: 94.154%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3839 (0.4628) Acc D Real: 91.537%
Loss D Fake: 0.8142 (0.8416) Acc D Fake: 5.839%
Loss D: 1.198
Loss G: 0.5896 (0.5683) Acc G: 94.160%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.4722 (0.4629) Acc D Real: 91.535%
Loss D Fake: 0.8125 (0.8414) Acc D Fake: 5.833%
Loss D: 1.285
Loss G: 0.5907 (0.5685) Acc G: 94.167%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.4181 (0.4626) Acc D Real: 91.538%
Loss D Fake: 0.8110 (0.8412) Acc D Fake: 5.826%
Loss D: 1.229
Loss G: 0.5920 (0.5686) Acc G: 94.173%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.4092 (0.4622) Acc D Real: 91.545%
Loss D Fake: 0.8094 (0.8409) Acc D Fake: 5.820%
Loss D: 1.219
Loss G: 0.5934 (0.5688) Acc G: 94.179%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.5053 (0.4625) Acc D Real: 91.551%
Loss D Fake: 0.8078 (0.8407) Acc D Fake: 5.814%
Loss D: 1.313
Loss G: 0.5943 (0.5690) Acc G: 94.185%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.4738 (0.4626) Acc D Real: 91.569%
Loss D Fake: 0.8070 (0.8404) Acc D Fake: 5.808%
Loss D: 1.281
Loss G: 0.5947 (0.5692) Acc G: 94.192%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3897 (0.4620) Acc D Real: 91.585%
Loss D Fake: 0.8066 (0.8402) Acc D Fake: 5.802%
Loss D: 1.196
Loss G: 0.5953 (0.5694) Acc G: 94.198%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.4302 (0.4618) Acc D Real: 91.603%
Loss D Fake: 0.8057 (0.8399) Acc D Fake: 5.796%
Loss D: 1.236
Loss G: 0.5960 (0.5696) Acc G: 94.203%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.4438 (0.4617) Acc D Real: 91.613%
Loss D Fake: 0.8048 (0.8397) Acc D Fake: 5.790%
Loss D: 1.249
Loss G: 0.5966 (0.5698) Acc G: 94.209%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.4922 (0.4619) Acc D Real: 91.631%
Loss D Fake: 0.8042 (0.8394) Acc D Fake: 5.784%
Loss D: 1.296
Loss G: 0.5968 (0.5700) Acc G: 94.215%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.4936 (0.4621) Acc D Real: 91.648%
Loss D Fake: 0.8044 (0.8392) Acc D Fake: 5.779%
Loss D: 1.298
Loss G: 0.5965 (0.5702) Acc G: 94.221%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.4639 (0.4621) Acc D Real: 91.643%
Loss D Fake: 0.8049 (0.8389) Acc D Fake: 5.773%
Loss D: 1.269
Loss G: 0.5961 (0.5704) Acc G: 94.226%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.5130 (0.4625) Acc D Real: 91.635%
Loss D Fake: 0.8055 (0.8387) Acc D Fake: 5.768%
Loss D: 1.318
Loss G: 0.5954 (0.5705) Acc G: 94.232%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.4771 (0.4626) Acc D Real: 91.643%
Loss D Fake: 0.8064 (0.8385) Acc D Fake: 5.762%
Loss D: 1.284
Loss G: 0.5946 (0.5707) Acc G: 94.237%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.4117 (0.4622) Acc D Real: 91.660%
Loss D Fake: 0.8073 (0.8382) Acc D Fake: 5.757%
Loss D: 1.219
Loss G: 0.5941 (0.5709) Acc G: 94.242%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.5024 (0.4625) Acc D Real: 91.667%
Loss D Fake: 0.8079 (0.8380) Acc D Fake: 5.752%
Loss D: 1.310
Loss G: 0.5934 (0.5710) Acc G: 94.248%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.4069 (0.4621) Acc D Real: 91.671%
Loss D Fake: 0.8087 (0.8378) Acc D Fake: 5.746%
Loss D: 1.216
Loss G: 0.5931 (0.5712) Acc G: 94.253%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.4616 (0.4621) Acc D Real: 91.686%
Loss D Fake: 0.8090 (0.8376) Acc D Fake: 5.741%
Loss D: 1.271
Loss G: 0.5928 (0.5713) Acc G: 94.258%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.4336 (0.4619) Acc D Real: 91.703%
Loss D Fake: 0.8094 (0.8374) Acc D Fake: 5.736%
Loss D: 1.243
Loss G: 0.5925 (0.5715) Acc G: 94.263%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.4840 (0.4621) Acc D Real: 91.716%
Loss D Fake: 0.8098 (0.8373) Acc D Fake: 5.731%
Loss D: 1.294
Loss G: 0.5920 (0.5716) Acc G: 94.268%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.5013 (0.4623) Acc D Real: 91.723%
Loss D Fake: 0.8106 (0.8371) Acc D Fake: 5.726%
Loss D: 1.312
Loss G: 0.5912 (0.5717) Acc G: 94.273%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.5379 (0.4628) Acc D Real: 91.731%
Loss D Fake: 0.8120 (0.8369) Acc D Fake: 5.722%
Loss D: 1.350
Loss G: 0.5898 (0.5719) Acc G: 94.278%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.4648 (0.4629) Acc D Real: 91.738%
Loss D Fake: 0.8139 (0.8368) Acc D Fake: 5.717%
Loss D: 1.279
Loss G: 0.5883 (0.5720) Acc G: 94.283%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.5101 (0.4632) Acc D Real: 91.742%
Loss D Fake: 0.8158 (0.8366) Acc D Fake: 5.712%
Loss D: 1.326
Loss G: 0.5867 (0.5721) Acc G: 94.287%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.5015 (0.4634) Acc D Real: 91.743%
Loss D Fake: 0.8180 (0.8365) Acc D Fake: 5.707%
Loss D: 1.319
Loss G: 0.5849 (0.5722) Acc G: 94.292%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.4548 (0.4634) Acc D Real: 91.749%
Loss D Fake: 0.8202 (0.8364) Acc D Fake: 5.703%
Loss D: 1.275
Loss G: 0.5833 (0.5722) Acc G: 94.297%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.4920 (0.4635) Acc D Real: 91.765%
Loss D Fake: 0.8222 (0.8363) Acc D Fake: 5.698%
Loss D: 1.314
Loss G: 0.5817 (0.5723) Acc G: 94.301%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4293 (0.4633) Acc D Real: 91.770%
Loss D Fake: 0.8242 (0.8362) Acc D Fake: 5.694%
Loss D: 1.253
Loss G: 0.5804 (0.5723) Acc G: 94.306%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.4641 (0.4633) Acc D Real: 91.773%
Loss D Fake: 0.8257 (0.8362) Acc D Fake: 5.689%
Loss D: 1.290
Loss G: 0.5792 (0.5724) Acc G: 94.310%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.4128 (0.4630) Acc D Real: 91.781%
Loss D Fake: 0.8270 (0.8361) Acc D Fake: 5.685%
Loss D: 1.240
Loss G: 0.5785 (0.5724) Acc G: 94.314%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.4243 (0.4628) Acc D Real: 91.791%
Loss D Fake: 0.8277 (0.8360) Acc D Fake: 5.681%
Loss D: 1.252
Loss G: 0.5781 (0.5725) Acc G: 94.319%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.4216 (0.4625) Acc D Real: 91.800%
Loss D Fake: 0.8281 (0.8360) Acc D Fake: 5.676%
Loss D: 1.250
Loss G: 0.5780 (0.5725) Acc G: 94.323%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3862 (0.4620) Acc D Real: 91.806%
Loss D Fake: 0.8279 (0.8359) Acc D Fake: 5.672%
Loss D: 1.214
Loss G: 0.5784 (0.5725) Acc G: 94.327%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3750 (0.4615) Acc D Real: 91.805%
Loss D Fake: 0.8270 (0.8359) Acc D Fake: 5.668%
Loss D: 1.202
Loss G: 0.5794 (0.5726) Acc G: 94.331%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4489 (0.4614) Acc D Real: 91.807%
Loss D Fake: 0.8255 (0.8358) Acc D Fake: 5.664%
Loss D: 1.274
Loss G: 0.5805 (0.5726) Acc G: 94.335%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.4421 (0.4613) Acc D Real: 91.799%
Loss D Fake: 0.8241 (0.8358) Acc D Fake: 5.660%
Loss D: 1.266
Loss G: 0.5816 (0.5727) Acc G: 94.339%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.4212 (0.4611) Acc D Real: 91.798%
Loss D Fake: 0.8226 (0.8357) Acc D Fake: 5.656%
Loss D: 1.244
Loss G: 0.5829 (0.5727) Acc G: 94.343%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3877 (0.4606) Acc D Real: 91.802%
Loss D Fake: 0.8208 (0.8356) Acc D Fake: 5.652%
Loss D: 1.209
Loss G: 0.5844 (0.5728) Acc G: 94.347%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.4160 (0.4604) Acc D Real: 91.809%
Loss D Fake: 0.8187 (0.8355) Acc D Fake: 5.648%
Loss D: 1.235
Loss G: 0.5861 (0.5729) Acc G: 94.351%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4520 (0.4603) Acc D Real: 91.800%
Loss D Fake: 0.8167 (0.8354) Acc D Fake: 5.644%
Loss D: 1.269
Loss G: 0.5876 (0.5730) Acc G: 94.355%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.4217 (0.4601) Acc D Real: 91.814%
Loss D Fake: 0.8148 (0.8352) Acc D Fake: 5.640%
Loss D: 1.236
Loss G: 0.5891 (0.5731) Acc G: 94.359%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.4234 (0.4599) Acc D Real: 91.812%
Loss D Fake: 0.8129 (0.8351) Acc D Fake: 5.637%
Loss D: 1.236
Loss G: 0.5905 (0.5732) Acc G: 94.363%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.4188 (0.4596) Acc D Real: 91.821%
Loss D Fake: 0.8111 (0.8350) Acc D Fake: 5.633%
Loss D: 1.230
Loss G: 0.5920 (0.5733) Acc G: 94.366%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3345 (0.4589) Acc D Real: 91.828%
Loss D Fake: 0.8090 (0.8348) Acc D Fake: 5.629%
Loss D: 1.144
Loss G: 0.5940 (0.5734) Acc G: 94.370%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3726 (0.4584) Acc D Real: 91.847%
Loss D Fake: 0.8063 (0.8347) Acc D Fake: 5.626%
Loss D: 1.179
Loss G: 0.5962 (0.5735) Acc G: 94.374%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.4761 (0.4585) Acc D Real: 91.853%
Loss D Fake: 0.8037 (0.8345) Acc D Fake: 5.622%
Loss D: 1.280
Loss G: 0.5979 (0.5737) Acc G: 94.377%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.4294 (0.4583) Acc D Real: 91.869%
Loss D Fake: 0.8019 (0.8343) Acc D Fake: 5.618%
Loss D: 1.231
Loss G: 0.5993 (0.5738) Acc G: 94.381%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.5449 (0.4588) Acc D Real: 91.876%
Loss D Fake: 0.8007 (0.8341) Acc D Fake: 5.615%
Loss D: 1.346
Loss G: 0.5996 (0.5740) Acc G: 94.384%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4764 (0.4589) Acc D Real: 91.887%
Loss D Fake: 0.8008 (0.8339) Acc D Fake: 5.611%
Loss D: 1.277
Loss G: 0.5994 (0.5741) Acc G: 94.388%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4357 (0.4588) Acc D Real: 91.898%
Loss D Fake: 0.8011 (0.8337) Acc D Fake: 5.608%
Loss D: 1.237
Loss G: 0.5991 (0.5743) Acc G: 94.391%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.4472 (0.4587) Acc D Real: 91.898%
Loss D Fake: 0.8014 (0.8336) Acc D Fake: 5.605%
Loss D: 1.249
Loss G: 0.5989 (0.5744) Acc G: 94.395%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.4554 (0.4587) Acc D Real: 91.905%
Loss D Fake: 0.8016 (0.8334) Acc D Fake: 5.601%
Loss D: 1.257
Loss G: 0.5987 (0.5745) Acc G: 94.398%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.4727 (0.4588) Acc D Real: 91.913%
Loss D Fake: 0.8020 (0.8332) Acc D Fake: 5.598%
Loss D: 1.275
Loss G: 0.5983 (0.5747) Acc G: 94.401%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3818 (0.4584) Acc D Real: 91.921%
Loss D Fake: 0.8024 (0.8330) Acc D Fake: 5.595%
Loss D: 1.184
Loss G: 0.5983 (0.5748) Acc G: 94.405%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.4738 (0.4584) Acc D Real: 91.929%
Loss D Fake: 0.8023 (0.8329) Acc D Fake: 5.591%
Loss D: 1.276
Loss G: 0.5982 (0.5749) Acc G: 94.408%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.4208 (0.4582) Acc D Real: 91.930%
Loss D Fake: 0.8024 (0.8327) Acc D Fake: 5.588%
Loss D: 1.223
Loss G: 0.5983 (0.5750) Acc G: 94.411%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.4903 (0.4584) Acc D Real: 91.935%
Loss D Fake: 0.8023 (0.8325) Acc D Fake: 5.585%
Loss D: 1.293
Loss G: 0.5981 (0.5752) Acc G: 94.414%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.4651 (0.4584) Acc D Real: 91.940%
Loss D Fake: 0.8027 (0.8324) Acc D Fake: 5.582%
Loss D: 1.268
Loss G: 0.5977 (0.5753) Acc G: 94.418%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.4637 (0.4585) Acc D Real: 91.948%
Loss D Fake: 0.8033 (0.8322) Acc D Fake: 5.579%
Loss D: 1.267
Loss G: 0.5972 (0.5754) Acc G: 94.421%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.4294 (0.4583) Acc D Real: 91.958%
Loss D Fake: 0.8038 (0.8321) Acc D Fake: 5.576%
Loss D: 1.233
Loss G: 0.5969 (0.5755) Acc G: 94.424%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.4473 (0.4583) Acc D Real: 91.960%
Loss D Fake: 0.8042 (0.8319) Acc D Fake: 5.573%
Loss D: 1.252
Loss G: 0.5966 (0.5756) Acc G: 94.427%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.5145 (0.4586) Acc D Real: 91.962%
Loss D Fake: 0.8048 (0.8318) Acc D Fake: 5.570%
Loss D: 1.319
Loss G: 0.5958 (0.5757) Acc G: 94.430%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.4202 (0.4584) Acc D Real: 91.972%
Loss D Fake: 0.8058 (0.8316) Acc D Fake: 5.567%
Loss D: 1.226
Loss G: 0.5952 (0.5758) Acc G: 94.433%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.4381 (0.4583) Acc D Real: 91.980%
Loss D Fake: 0.8063 (0.8315) Acc D Fake: 5.564%
Loss D: 1.244
Loss G: 0.5948 (0.5759) Acc G: 94.436%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.4442 (0.4582) Acc D Real: 91.992%
Loss D Fake: 0.8068 (0.8314) Acc D Fake: 5.561%
Loss D: 1.251
Loss G: 0.5945 (0.5760) Acc G: 94.439%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.4889 (0.4583) Acc D Real: 92.001%
Loss D Fake: 0.8074 (0.8313) Acc D Fake: 5.558%
Loss D: 1.296
Loss G: 0.5938 (0.5761) Acc G: 94.442%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3938 (0.4580) Acc D Real: 92.013%
Loss D Fake: 0.8082 (0.8311) Acc D Fake: 5.555%
Loss D: 1.202
Loss G: 0.5934 (0.5762) Acc G: 94.444%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.3913 (0.4577) Acc D Real: 92.020%
Loss D Fake: 0.8083 (0.8310) Acc D Fake: 5.552%
Loss D: 1.200
Loss G: 0.5936 (0.5763) Acc G: 94.447%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4239 (0.4575) Acc D Real: 92.023%
Loss D Fake: 0.8079 (0.8309) Acc D Fake: 5.549%
Loss D: 1.232
Loss G: 0.5940 (0.5764) Acc G: 94.450%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.5276 (0.4578) Acc D Real: 92.030%
Loss D Fake: 0.8075 (0.8308) Acc D Fake: 5.547%
Loss D: 1.335
Loss G: 0.5939 (0.5765) Acc G: 94.453%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3961 (0.4575) Acc D Real: 92.032%
Loss D Fake: 0.8079 (0.8307) Acc D Fake: 5.544%
Loss D: 1.204
Loss G: 0.5939 (0.5766) Acc G: 94.456%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.4444 (0.4575) Acc D Real: 92.042%
Loss D Fake: 0.8077 (0.8306) Acc D Fake: 5.541%
Loss D: 1.252
Loss G: 0.5939 (0.5767) Acc G: 94.458%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.4130 (0.4573) Acc D Real: 92.056%
Loss D Fake: 0.8076 (0.8304) Acc D Fake: 5.538%
Loss D: 1.221
Loss G: 0.5941 (0.5767) Acc G: 94.461%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.4464 (0.4572) Acc D Real: 92.063%
Loss D Fake: 0.8074 (0.8303) Acc D Fake: 5.536%
Loss D: 1.254
Loss G: 0.5942 (0.5768) Acc G: 94.464%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.4324 (0.4571) Acc D Real: 92.070%
Loss D Fake: 0.8072 (0.8302) Acc D Fake: 5.533%
Loss D: 1.240
Loss G: 0.5944 (0.5769) Acc G: 94.466%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3927 (0.4568) Acc D Real: 92.075%
Loss D Fake: 0.8069 (0.8301) Acc D Fake: 5.531%
Loss D: 1.200
Loss G: 0.5949 (0.5770) Acc G: 94.469%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.4199 (0.4566) Acc D Real: 92.085%
Loss D Fake: 0.8061 (0.8300) Acc D Fake: 5.528%
Loss D: 1.226
Loss G: 0.5955 (0.5771) Acc G: 94.472%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.4186 (0.4564) Acc D Real: 92.097%
Loss D Fake: 0.8053 (0.8299) Acc D Fake: 5.525%
Loss D: 1.224
Loss G: 0.5962 (0.5772) Acc G: 94.474%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4722 (0.4565) Acc D Real: 92.104%
Loss D Fake: 0.8046 (0.8297) Acc D Fake: 5.523%
Loss D: 1.277
Loss G: 0.5965 (0.5773) Acc G: 94.477%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.4634 (0.4565) Acc D Real: 92.113%
Loss D Fake: 0.8044 (0.8296) Acc D Fake: 5.520%
Loss D: 1.268
Loss G: 0.5965 (0.5774) Acc G: 94.479%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.4314 (0.4564) Acc D Real: 92.120%
Loss D Fake: 0.8045 (0.8295) Acc D Fake: 5.518%
Loss D: 1.236
Loss G: 0.5964 (0.5775) Acc G: 94.482%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3901 (0.4561) Acc D Real: 92.126%
Loss D Fake: 0.8044 (0.8294) Acc D Fake: 5.515%
Loss D: 1.194
Loss G: 0.5968 (0.5776) Acc G: 94.484%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.4321 (0.4560) Acc D Real: 92.131%
Loss D Fake: 0.8038 (0.8293) Acc D Fake: 5.513%
Loss D: 1.236
Loss G: 0.5973 (0.5776) Acc G: 94.487%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.4259 (0.4558) Acc D Real: 92.134%
Loss D Fake: 0.8031 (0.8291) Acc D Fake: 5.511%
Loss D: 1.229
Loss G: 0.5979 (0.5777) Acc G: 94.489%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.4774 (0.4559) Acc D Real: 92.131%
Loss D Fake: 0.8024 (0.8290) Acc D Fake: 5.508%
Loss D: 1.280
Loss G: 0.5983 (0.5778) Acc G: 94.491%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.4048 (0.4557) Acc D Real: 92.131%
Loss D Fake: 0.8019 (0.8289) Acc D Fake: 5.506%
Loss D: 1.207
Loss G: 0.5990 (0.5779) Acc G: 94.494%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.4129 (0.4555) Acc D Real: 92.135%
Loss D Fake: 0.8010 (0.8288) Acc D Fake: 5.503%
Loss D: 1.214
Loss G: 0.5997 (0.5780) Acc G: 94.496%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.4903 (0.4556) Acc D Real: 92.138%
Loss D Fake: 0.8001 (0.8286) Acc D Fake: 5.501%
Loss D: 1.290
Loss G: 0.6001 (0.5781) Acc G: 94.498%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.4490 (0.4556) Acc D Real: 92.151%
Loss D Fake: 0.7999 (0.8285) Acc D Fake: 5.499%
Loss D: 1.249
Loss G: 0.6002 (0.5782) Acc G: 94.501%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.4827 (0.4557) Acc D Real: 92.165%
Loss D Fake: 0.8001 (0.8284) Acc D Fake: 5.496%
Loss D: 1.283
Loss G: 0.5998 (0.5783) Acc G: 94.503%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.4389 (0.4557) Acc D Real: 92.169%
Loss D Fake: 0.8007 (0.8282) Acc D Fake: 5.494%
Loss D: 1.240
Loss G: 0.5993 (0.5784) Acc G: 94.505%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.4553 (0.4557) Acc D Real: 92.180%
Loss D Fake: 0.8013 (0.8281) Acc D Fake: 5.492%
Loss D: 1.257
Loss G: 0.5988 (0.5785) Acc G: 94.508%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.4166 (0.4555) Acc D Real: 92.186%
Loss D Fake: 0.8019 (0.8280) Acc D Fake: 5.490%
Loss D: 1.218
Loss G: 0.5984 (0.5786) Acc G: 94.510%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.5150 (0.4557) Acc D Real: 92.190%
Loss D Fake: 0.8025 (0.8279) Acc D Fake: 5.488%
Loss D: 1.317
Loss G: 0.5977 (0.5787) Acc G: 94.512%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.4825 (0.4559) Acc D Real: 92.193%
Loss D Fake: 0.8037 (0.8278) Acc D Fake: 5.485%
Loss D: 1.286
Loss G: 0.5966 (0.5788) Acc G: 94.514%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.4510 (0.4558) Acc D Real: 92.195%
Loss D Fake: 0.8050 (0.8277) Acc D Fake: 5.483%
Loss D: 1.256
Loss G: 0.5956 (0.5789) Acc G: 94.516%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.4178 (0.4557) Acc D Real: 92.200%
Loss D Fake: 0.8060 (0.8276) Acc D Fake: 5.481%
Loss D: 1.224
Loss G: 0.5950 (0.5789) Acc G: 94.519%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.4876 (0.4558) Acc D Real: 92.204%
Loss D Fake: 0.8069 (0.8275) Acc D Fake: 5.480%
Loss D: 1.294
Loss G: 0.5941 (0.5790) Acc G: 94.519%
LR: 2.000e-04
Epoch: 9/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3994 (0.4316) Acc D Real: 93.750%
Loss D Fake: 0.8093 (0.8087) Acc D Fake: 5.000%
Loss D: 1.209
Loss G: 0.5923 (0.5927) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.4408 (0.4347) Acc D Real: 93.542%
Loss D Fake: 0.8099 (0.8091) Acc D Fake: 5.000%
Loss D: 1.251
Loss G: 0.5919 (0.5924) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.3675 (0.4179) Acc D Real: 93.880%
Loss D Fake: 0.8103 (0.8094) Acc D Fake: 5.000%
Loss D: 1.178
Loss G: 0.5919 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.4259 (0.4195) Acc D Real: 93.917%
Loss D Fake: 0.8100 (0.8095) Acc D Fake: 5.000%
Loss D: 1.236
Loss G: 0.5922 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.4633 (0.4268) Acc D Real: 93.984%
Loss D Fake: 0.8097 (0.8096) Acc D Fake: 5.000%
Loss D: 1.273
Loss G: 0.5922 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.4288 (0.4271) Acc D Real: 94.048%
Loss D Fake: 0.8098 (0.8096) Acc D Fake: 5.000%
Loss D: 1.239
Loss G: 0.5922 (0.5922) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.4198 (0.4262) Acc D Real: 94.017%
Loss D Fake: 0.8098 (0.8096) Acc D Fake: 5.000%
Loss D: 1.230
Loss G: 0.5922 (0.5922) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.4369 (0.4274) Acc D Real: 94.010%
Loss D Fake: 0.8097 (0.8096) Acc D Fake: 5.000%
Loss D: 1.247
Loss G: 0.5923 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3945 (0.4241) Acc D Real: 94.052%
Loss D Fake: 0.8095 (0.8096) Acc D Fake: 5.000%
Loss D: 1.204
Loss G: 0.5926 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.4744 (0.4287) Acc D Real: 94.119%
Loss D Fake: 0.8092 (0.8096) Acc D Fake: 5.000%
Loss D: 1.284
Loss G: 0.5926 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.4257 (0.4284) Acc D Real: 94.136%
Loss D Fake: 0.8094 (0.8096) Acc D Fake: 5.000%
Loss D: 1.235
Loss G: 0.5925 (0.5923) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3734 (0.4242) Acc D Real: 94.107%
Loss D Fake: 0.8092 (0.8095) Acc D Fake: 5.000%
Loss D: 1.183
Loss G: 0.5930 (0.5924) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.5118 (0.4304) Acc D Real: 94.040%
Loss D Fake: 0.8087 (0.8095) Acc D Fake: 5.000%
Loss D: 1.321
Loss G: 0.5930 (0.5924) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3831 (0.4273) Acc D Real: 94.101%
Loss D Fake: 0.8088 (0.8094) Acc D Fake: 5.000%
Loss D: 1.192
Loss G: 0.5931 (0.5925) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3745 (0.4240) Acc D Real: 94.020%
Loss D Fake: 0.8083 (0.8094) Acc D Fake: 5.000%
Loss D: 1.183
Loss G: 0.5939 (0.5926) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.4040 (0.4228) Acc D Real: 93.955%
Loss D Fake: 0.8071 (0.8092) Acc D Fake: 5.000%
Loss D: 1.211
Loss G: 0.5949 (0.5927) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.3809 (0.4205) Acc D Real: 93.909%
Loss D Fake: 0.8055 (0.8090) Acc D Fake: 5.000%
Loss D: 1.186
Loss G: 0.5964 (0.5929) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.4499 (0.4220) Acc D Real: 93.936%
Loss D Fake: 0.8037 (0.8087) Acc D Fake: 5.000%
Loss D: 1.254
Loss G: 0.5976 (0.5931) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.5011 (0.4260) Acc D Real: 93.935%
Loss D Fake: 0.8027 (0.8084) Acc D Fake: 5.000%
Loss D: 1.304
Loss G: 0.5980 (0.5934) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.4318 (0.4263) Acc D Real: 93.946%
Loss D Fake: 0.8024 (0.8082) Acc D Fake: 5.000%
Loss D: 1.234
Loss G: 0.5983 (0.5936) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.4045 (0.4253) Acc D Real: 93.956%
Loss D Fake: 0.8020 (0.8079) Acc D Fake: 5.000%
Loss D: 1.207
Loss G: 0.5986 (0.5939) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.4122 (0.4247) Acc D Real: 93.954%
Loss D Fake: 0.8015 (0.8076) Acc D Fake: 5.000%
Loss D: 1.214
Loss G: 0.5991 (0.5941) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4884 (0.4274) Acc D Real: 93.961%
Loss D Fake: 0.8010 (0.8073) Acc D Fake: 5.000%
Loss D: 1.289
Loss G: 0.5992 (0.5943) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.4112 (0.4267) Acc D Real: 93.992%
Loss D Fake: 0.8011 (0.8071) Acc D Fake: 5.000%
Loss D: 1.212
Loss G: 0.5992 (0.5945) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.3757 (0.4248) Acc D Real: 94.012%
Loss D Fake: 0.8008 (0.8068) Acc D Fake: 5.000%
Loss D: 1.177
Loss G: 0.5997 (0.5947) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.4569 (0.4259) Acc D Real: 93.999%
Loss D Fake: 0.8002 (0.8066) Acc D Fake: 5.000%
Loss D: 1.257
Loss G: 0.6001 (0.5949) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.4380 (0.4264) Acc D Real: 93.992%
Loss D Fake: 0.7998 (0.8063) Acc D Fake: 5.000%
Loss D: 1.238
Loss G: 0.6003 (0.5951) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.4343 (0.4266) Acc D Real: 93.962%
Loss D Fake: 0.7996 (0.8061) Acc D Fake: 5.000%
Loss D: 1.234
Loss G: 0.6005 (0.5953) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.4728 (0.4282) Acc D Real: 93.958%
Loss D Fake: 0.7995 (0.8059) Acc D Fake: 5.000%
Loss D: 1.272
Loss G: 0.6004 (0.5954) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.4933 (0.4303) Acc D Real: 93.982%
Loss D Fake: 0.7999 (0.8057) Acc D Fake: 5.000%
Loss D: 1.293
Loss G: 0.5997 (0.5956) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.4003 (0.4293) Acc D Real: 93.981%
Loss D Fake: 0.8008 (0.8055) Acc D Fake: 5.000%
Loss D: 1.201
Loss G: 0.5992 (0.5957) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.4104 (0.4288) Acc D Real: 93.976%
Loss D Fake: 0.8012 (0.8054) Acc D Fake: 5.000%
Loss D: 1.212
Loss G: 0.5991 (0.5958) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.4285 (0.4288) Acc D Real: 93.968%
Loss D Fake: 0.8012 (0.8053) Acc D Fake: 5.000%
Loss D: 1.230
Loss G: 0.5991 (0.5959) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.4636 (0.4298) Acc D Real: 93.932%
Loss D Fake: 0.8012 (0.8052) Acc D Fake: 5.000%
Loss D: 1.265
Loss G: 0.5991 (0.5960) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.4084 (0.4292) Acc D Real: 93.955%
Loss D Fake: 0.8012 (0.8051) Acc D Fake: 5.000%
Loss D: 1.210
Loss G: 0.5991 (0.5961) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.4213 (0.4290) Acc D Real: 93.957%
Loss D Fake: 0.8011 (0.8050) Acc D Fake: 5.000%
Loss D: 1.222
Loss G: 0.5993 (0.5962) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.4479 (0.4294) Acc D Real: 93.998%
Loss D Fake: 0.8009 (0.8048) Acc D Fake: 5.000%
Loss D: 1.249
Loss G: 0.5993 (0.5962) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.4578 (0.4302) Acc D Real: 93.998%
Loss D Fake: 0.8011 (0.8048) Acc D Fake: 5.000%
Loss D: 1.259
Loss G: 0.5990 (0.5963) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.4093 (0.4297) Acc D Real: 94.020%
Loss D Fake: 0.8015 (0.8047) Acc D Fake: 5.000%
Loss D: 1.211
Loss G: 0.5988 (0.5964) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.4341 (0.4298) Acc D Real: 94.071%
Loss D Fake: 0.8017 (0.8046) Acc D Fake: 5.000%
Loss D: 1.236
Loss G: 0.5985 (0.5964) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3920 (0.4289) Acc D Real: 94.091%
Loss D Fake: 0.8020 (0.8045) Acc D Fake: 5.000%
Loss D: 1.194
Loss G: 0.5985 (0.5965) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.4652 (0.4297) Acc D Real: 94.111%
Loss D Fake: 0.8020 (0.8045) Acc D Fake: 5.000%
Loss D: 1.267
Loss G: 0.5983 (0.5965) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3943 (0.4289) Acc D Real: 94.132%
Loss D Fake: 0.8023 (0.8044) Acc D Fake: 5.000%
Loss D: 1.197
Loss G: 0.5982 (0.5966) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3547 (0.4273) Acc D Real: 94.133%
Loss D Fake: 0.8020 (0.8044) Acc D Fake: 5.000%
Loss D: 1.157
Loss G: 0.5988 (0.5966) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3738 (0.4261) Acc D Real: 94.144%
Loss D Fake: 0.8011 (0.8043) Acc D Fake: 5.000%
Loss D: 1.175
Loss G: 0.5997 (0.5967) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4365 (0.4263) Acc D Real: 94.163%
Loss D Fake: 0.7999 (0.8042) Acc D Fake: 5.000%
Loss D: 1.236
Loss G: 0.6005 (0.5968) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3952 (0.4257) Acc D Real: 94.156%
Loss D Fake: 0.7990 (0.8041) Acc D Fake: 5.000%
Loss D: 1.194
Loss G: 0.6013 (0.5969) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.4017 (0.4252) Acc D Real: 94.172%
Loss D Fake: 0.7978 (0.8040) Acc D Fake: 5.000%
Loss D: 1.200
Loss G: 0.6023 (0.5970) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3974 (0.4246) Acc D Real: 94.178%
Loss D Fake: 0.7966 (0.8038) Acc D Fake: 5.000%
Loss D: 1.194
Loss G: 0.6034 (0.5971) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3503 (0.4232) Acc D Real: 94.198%
Loss D Fake: 0.7950 (0.8036) Acc D Fake: 5.000%
Loss D: 1.145
Loss G: 0.6050 (0.5972) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4097 (0.4229) Acc D Real: 94.214%
Loss D Fake: 0.7930 (0.8034) Acc D Fake: 5.000%
Loss D: 1.203
Loss G: 0.6065 (0.5974) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3981 (0.4224) Acc D Real: 94.214%
Loss D Fake: 0.7912 (0.8032) Acc D Fake: 5.000%
Loss D: 1.189
Loss G: 0.6080 (0.5976) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.4114 (0.4222) Acc D Real: 94.228%
Loss D Fake: 0.7894 (0.8030) Acc D Fake: 5.000%
Loss D: 1.201
Loss G: 0.6094 (0.5978) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.3988 (0.4218) Acc D Real: 94.230%
Loss D Fake: 0.7879 (0.8027) Acc D Fake: 5.000%
Loss D: 1.187
Loss G: 0.6107 (0.5981) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3995 (0.4214) Acc D Real: 94.229%
Loss D Fake: 0.7863 (0.8024) Acc D Fake: 5.000%
Loss D: 1.186
Loss G: 0.6120 (0.5983) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.4873 (0.4226) Acc D Real: 94.237%
Loss D Fake: 0.7850 (0.8021) Acc D Fake: 5.000%
Loss D: 1.272
Loss G: 0.6127 (0.5986) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3922 (0.4220) Acc D Real: 94.255%
Loss D Fake: 0.7844 (0.8018) Acc D Fake: 5.000%
Loss D: 1.177
Loss G: 0.6132 (0.5988) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4020 (0.4217) Acc D Real: 94.258%
Loss D Fake: 0.7837 (0.8015) Acc D Fake: 5.000%
Loss D: 1.186
Loss G: 0.6138 (0.5991) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3886 (0.4212) Acc D Real: 94.253%
Loss D Fake: 0.7828 (0.8012) Acc D Fake: 5.000%
Loss D: 1.171
Loss G: 0.6147 (0.5993) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3962 (0.4207) Acc D Real: 94.267%
Loss D Fake: 0.7817 (0.8008) Acc D Fake: 5.000%
Loss D: 1.178
Loss G: 0.6157 (0.5996) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.4244 (0.4208) Acc D Real: 94.290%
Loss D Fake: 0.7806 (0.8005) Acc D Fake: 5.000%
Loss D: 1.205
Loss G: 0.6164 (0.5999) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.4122 (0.4207) Acc D Real: 94.300%
Loss D Fake: 0.7799 (0.8002) Acc D Fake: 5.000%
Loss D: 1.192
Loss G: 0.6170 (0.6002) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.4295 (0.4208) Acc D Real: 94.312%
Loss D Fake: 0.7793 (0.7999) Acc D Fake: 5.000%
Loss D: 1.209
Loss G: 0.6174 (0.6004) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.4425 (0.4211) Acc D Real: 94.320%
Loss D Fake: 0.7790 (0.7995) Acc D Fake: 5.000%
Loss D: 1.222
Loss G: 0.6175 (0.6007) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3705 (0.4204) Acc D Real: 94.339%
Loss D Fake: 0.7789 (0.7992) Acc D Fake: 5.000%
Loss D: 1.149
Loss G: 0.6178 (0.6009) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.5040 (0.4216) Acc D Real: 94.332%
Loss D Fake: 0.7786 (0.7989) Acc D Fake: 5.000%
Loss D: 1.283
Loss G: 0.6176 (0.6012) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4129 (0.4215) Acc D Real: 94.339%
Loss D Fake: 0.7792 (0.7986) Acc D Fake: 5.000%
Loss D: 1.192
Loss G: 0.6172 (0.6014) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4019 (0.4212) Acc D Real: 94.354%
Loss D Fake: 0.7795 (0.7984) Acc D Fake: 5.000%
Loss D: 1.181
Loss G: 0.6170 (0.6017) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3824 (0.4207) Acc D Real: 94.371%
Loss D Fake: 0.7796 (0.7981) Acc D Fake: 5.000%
Loss D: 1.162
Loss G: 0.6171 (0.6019) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.3336 (0.4194) Acc D Real: 94.389%
Loss D Fake: 0.7791 (0.7978) Acc D Fake: 5.000%
Loss D: 1.113
Loss G: 0.6179 (0.6021) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3830 (0.4189) Acc D Real: 94.394%
Loss D Fake: 0.7778 (0.7975) Acc D Fake: 5.000%
Loss D: 1.161
Loss G: 0.6192 (0.6023) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3591 (0.4181) Acc D Real: 94.399%
Loss D Fake: 0.7761 (0.7972) Acc D Fake: 5.000%
Loss D: 1.135
Loss G: 0.6208 (0.6026) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.4122 (0.4180) Acc D Real: 94.410%
Loss D Fake: 0.7741 (0.7969) Acc D Fake: 5.000%
Loss D: 1.186
Loss G: 0.6223 (0.6029) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.4299 (0.4182) Acc D Real: 94.414%
Loss D Fake: 0.7726 (0.7966) Acc D Fake: 5.000%
Loss D: 1.203
Loss G: 0.6234 (0.6031) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3791 (0.4177) Acc D Real: 94.419%
Loss D Fake: 0.7714 (0.7963) Acc D Fake: 5.000%
Loss D: 1.151
Loss G: 0.6246 (0.6034) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.4615 (0.4182) Acc D Real: 94.427%
Loss D Fake: 0.7702 (0.7959) Acc D Fake: 5.000%
Loss D: 1.232
Loss G: 0.6252 (0.6037) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.5304 (0.4197) Acc D Real: 94.428%
Loss D Fake: 0.7701 (0.7956) Acc D Fake: 5.000%
Loss D: 1.301
Loss G: 0.6247 (0.6040) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3903 (0.4193) Acc D Real: 94.438%
Loss D Fake: 0.7710 (0.7953) Acc D Fake: 5.000%
Loss D: 1.161
Loss G: 0.6241 (0.6042) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.4923 (0.4202) Acc D Real: 94.447%
Loss D Fake: 0.7717 (0.7950) Acc D Fake: 5.000%
Loss D: 1.264
Loss G: 0.6231 (0.6045) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.4573 (0.4207) Acc D Real: 94.466%
Loss D Fake: 0.7733 (0.7947) Acc D Fake: 5.000%
Loss D: 1.231
Loss G: 0.6216 (0.6047) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.4095 (0.4205) Acc D Real: 94.469%
Loss D Fake: 0.7750 (0.7945) Acc D Fake: 5.000%
Loss D: 1.185
Loss G: 0.6202 (0.6049) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3250 (0.4194) Acc D Real: 94.481%
Loss D Fake: 0.7761 (0.7943) Acc D Fake: 5.000%
Loss D: 1.101
Loss G: 0.6200 (0.6050) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.4358 (0.4196) Acc D Real: 94.473%
Loss D Fake: 0.7762 (0.7941) Acc D Fake: 5.000%
Loss D: 1.212
Loss G: 0.6197 (0.6052) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.4540 (0.4200) Acc D Real: 94.471%
Loss D Fake: 0.7767 (0.7939) Acc D Fake: 5.000%
Loss D: 1.231
Loss G: 0.6191 (0.6054) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4001 (0.4198) Acc D Real: 94.476%
Loss D Fake: 0.7773 (0.7937) Acc D Fake: 5.000%
Loss D: 1.177
Loss G: 0.6187 (0.6055) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3850 (0.4194) Acc D Real: 94.480%
Loss D Fake: 0.7776 (0.7935) Acc D Fake: 5.000%
Loss D: 1.163
Loss G: 0.6187 (0.6057) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.4048 (0.4192) Acc D Real: 94.483%
Loss D Fake: 0.7774 (0.7933) Acc D Fake: 5.000%
Loss D: 1.182
Loss G: 0.6190 (0.6058) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3879 (0.4188) Acc D Real: 94.494%
Loss D Fake: 0.7769 (0.7931) Acc D Fake: 5.000%
Loss D: 1.165
Loss G: 0.6195 (0.6060) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4730 (0.4194) Acc D Real: 94.501%
Loss D Fake: 0.7765 (0.7929) Acc D Fake: 5.000%
Loss D: 1.249
Loss G: 0.6194 (0.6061) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3667 (0.4189) Acc D Real: 94.505%
Loss D Fake: 0.7767 (0.7927) Acc D Fake: 5.000%
Loss D: 1.143
Loss G: 0.6196 (0.6063) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.4118 (0.4188) Acc D Real: 94.509%
Loss D Fake: 0.7763 (0.7926) Acc D Fake: 5.000%
Loss D: 1.188
Loss G: 0.6199 (0.6064) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3517 (0.4181) Acc D Real: 94.521%
Loss D Fake: 0.7758 (0.7924) Acc D Fake: 5.000%
Loss D: 1.127
Loss G: 0.6207 (0.6066) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3806 (0.4177) Acc D Real: 94.523%
Loss D Fake: 0.7746 (0.7922) Acc D Fake: 5.000%
Loss D: 1.155
Loss G: 0.6219 (0.6068) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3955 (0.4174) Acc D Real: 94.527%
Loss D Fake: 0.7731 (0.7920) Acc D Fake: 5.000%
Loss D: 1.169
Loss G: 0.6231 (0.6069) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4831 (0.4181) Acc D Real: 94.522%
Loss D Fake: 0.7720 (0.7918) Acc D Fake: 5.000%
Loss D: 1.255
Loss G: 0.6235 (0.6071) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4314 (0.4182) Acc D Real: 94.518%
Loss D Fake: 0.7719 (0.7916) Acc D Fake: 5.000%
Loss D: 1.203
Loss G: 0.6235 (0.6073) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3773 (0.4178) Acc D Real: 94.525%
Loss D Fake: 0.7719 (0.7914) Acc D Fake: 5.000%
Loss D: 1.149
Loss G: 0.6238 (0.6074) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.4008 (0.4177) Acc D Real: 94.531%
Loss D Fake: 0.7713 (0.7912) Acc D Fake: 5.000%
Loss D: 1.172
Loss G: 0.6243 (0.6076) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.4148 (0.4176) Acc D Real: 94.542%
Loss D Fake: 0.7708 (0.7910) Acc D Fake: 5.000%
Loss D: 1.186
Loss G: 0.6247 (0.6078) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3761 (0.4172) Acc D Real: 94.553%
Loss D Fake: 0.7703 (0.7908) Acc D Fake: 5.000%
Loss D: 1.146
Loss G: 0.6253 (0.6080) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.4127 (0.4172) Acc D Real: 94.559%
Loss D Fake: 0.7696 (0.7906) Acc D Fake: 5.000%
Loss D: 1.182
Loss G: 0.6258 (0.6081) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4563 (0.4176) Acc D Real: 94.568%
Loss D Fake: 0.7694 (0.7904) Acc D Fake: 5.000%
Loss D: 1.226
Loss G: 0.6255 (0.6083) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.4330 (0.4177) Acc D Real: 94.578%
Loss D Fake: 0.7701 (0.7902) Acc D Fake: 5.000%
Loss D: 1.203
Loss G: 0.6248 (0.6085) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3466 (0.4170) Acc D Real: 94.584%
Loss D Fake: 0.7706 (0.7900) Acc D Fake: 5.000%
Loss D: 1.117
Loss G: 0.6249 (0.6086) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.4308 (0.4172) Acc D Real: 94.586%
Loss D Fake: 0.7702 (0.7898) Acc D Fake: 5.000%
Loss D: 1.201
Loss G: 0.6251 (0.6088) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4216 (0.4172) Acc D Real: 94.590%
Loss D Fake: 0.7701 (0.7896) Acc D Fake: 5.000%
Loss D: 1.192
Loss G: 0.6251 (0.6089) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.4784 (0.4178) Acc D Real: 94.595%
Loss D Fake: 0.7705 (0.7894) Acc D Fake: 5.000%
Loss D: 1.249
Loss G: 0.6242 (0.6091) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3564 (0.4172) Acc D Real: 94.598%
Loss D Fake: 0.7715 (0.7893) Acc D Fake: 5.000%
Loss D: 1.128
Loss G: 0.6239 (0.6092) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.4209 (0.4172) Acc D Real: 94.611%
Loss D Fake: 0.7717 (0.7891) Acc D Fake: 5.000%
Loss D: 1.193
Loss G: 0.6235 (0.6093) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.4401 (0.4174) Acc D Real: 94.612%
Loss D Fake: 0.7724 (0.7890) Acc D Fake: 5.000%
Loss D: 1.212
Loss G: 0.6227 (0.6094) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3916 (0.4172) Acc D Real: 94.621%
Loss D Fake: 0.7733 (0.7888) Acc D Fake: 5.000%
Loss D: 1.165
Loss G: 0.6222 (0.6096) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4252 (0.4173) Acc D Real: 94.626%
Loss D Fake: 0.7738 (0.7887) Acc D Fake: 5.000%
Loss D: 1.199
Loss G: 0.6217 (0.6097) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3966 (0.4171) Acc D Real: 94.632%
Loss D Fake: 0.7743 (0.7886) Acc D Fake: 5.000%
Loss D: 1.171
Loss G: 0.6215 (0.6098) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4075 (0.4170) Acc D Real: 94.637%
Loss D Fake: 0.7745 (0.7884) Acc D Fake: 5.000%
Loss D: 1.182
Loss G: 0.6214 (0.6099) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4121 (0.4170) Acc D Real: 94.640%
Loss D Fake: 0.7745 (0.7883) Acc D Fake: 5.000%
Loss D: 1.187
Loss G: 0.6213 (0.6100) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.4181 (0.4170) Acc D Real: 94.641%
Loss D Fake: 0.7746 (0.7882) Acc D Fake: 5.000%
Loss D: 1.193
Loss G: 0.6213 (0.6101) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.4036 (0.4169) Acc D Real: 94.655%
Loss D Fake: 0.7747 (0.7881) Acc D Fake: 5.000%
Loss D: 1.178
Loss G: 0.6212 (0.6102) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3640 (0.4164) Acc D Real: 94.663%
Loss D Fake: 0.7745 (0.7880) Acc D Fake: 5.000%
Loss D: 1.139
Loss G: 0.6220 (0.6103) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.4502 (0.4167) Acc D Real: 94.670%
Loss D Fake: 0.7735 (0.7878) Acc D Fake: 5.000%
Loss D: 1.224
Loss G: 0.6224 (0.6104) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.3446 (0.4161) Acc D Real: 94.678%
Loss D Fake: 0.7729 (0.7877) Acc D Fake: 5.000%
Loss D: 1.117
Loss G: 0.6238 (0.6105) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4009 (0.4160) Acc D Real: 94.683%
Loss D Fake: 0.7708 (0.7876) Acc D Fake: 5.000%
Loss D: 1.172
Loss G: 0.6255 (0.6106) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.4524 (0.4163) Acc D Real: 94.690%
Loss D Fake: 0.7694 (0.7874) Acc D Fake: 5.000%
Loss D: 1.222
Loss G: 0.6262 (0.6107) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.4816 (0.4168) Acc D Real: 94.693%
Loss D Fake: 0.7694 (0.7873) Acc D Fake: 5.000%
Loss D: 1.251
Loss G: 0.6255 (0.6108) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.4203 (0.4168) Acc D Real: 94.697%
Loss D Fake: 0.7706 (0.7872) Acc D Fake: 5.000%
Loss D: 1.191
Loss G: 0.6247 (0.6110) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3634 (0.4164) Acc D Real: 94.708%
Loss D Fake: 0.7711 (0.7870) Acc D Fake: 5.000%
Loss D: 1.134
Loss G: 0.6251 (0.6111) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3395 (0.4158) Acc D Real: 94.711%
Loss D Fake: 0.7694 (0.7869) Acc D Fake: 5.000%
Loss D: 1.109
Loss G: 0.6280 (0.6112) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.4156 (0.4158) Acc D Real: 94.714%
Loss D Fake: 0.7656 (0.7867) Acc D Fake: 5.000%
Loss D: 1.181
Loss G: 0.6310 (0.6114) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.4179 (0.4158) Acc D Real: 94.722%
Loss D Fake: 0.7626 (0.7865) Acc D Fake: 5.000%
Loss D: 1.181
Loss G: 0.6333 (0.6115) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.4016 (0.4157) Acc D Real: 94.729%
Loss D Fake: 0.7602 (0.7863) Acc D Fake: 5.000%
Loss D: 1.162
Loss G: 0.6355 (0.6117) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.4379 (0.4159) Acc D Real: 94.736%
Loss D Fake: 0.7584 (0.7861) Acc D Fake: 5.000%
Loss D: 1.196
Loss G: 0.6361 (0.6119) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.4153 (0.4159) Acc D Real: 94.744%
Loss D Fake: 0.7584 (0.7859) Acc D Fake: 5.000%
Loss D: 1.174
Loss G: 0.6359 (0.6121) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3519 (0.4154) Acc D Real: 94.752%
Loss D Fake: 0.7584 (0.7857) Acc D Fake: 5.000%
Loss D: 1.110
Loss G: 0.6370 (0.6123) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3956 (0.4152) Acc D Real: 94.755%
Loss D Fake: 0.7574 (0.7855) Acc D Fake: 5.000%
Loss D: 1.153
Loss G: 0.6369 (0.6124) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.5044 (0.4159) Acc D Real: 94.731%
Loss D Fake: 0.7601 (0.7853) Acc D Fake: 5.000%
Loss D: 1.264
Loss G: 0.6316 (0.6126) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3784 (0.4156) Acc D Real: 94.734%
Loss D Fake: 0.7681 (0.7852) Acc D Fake: 5.000%
Loss D: 1.146
Loss G: 0.6276 (0.6127) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.4061 (0.4156) Acc D Real: 94.714%
Loss D Fake: 0.7690 (0.7851) Acc D Fake: 5.000%
Loss D: 1.175
Loss G: 0.6316 (0.6128) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.3828 (0.4153) Acc D Real: 94.721%
Loss D Fake: 0.7597 (0.7849) Acc D Fake: 5.000%
Loss D: 1.142
Loss G: 0.6403 (0.6130) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3500 (0.4149) Acc D Real: 94.723%
Loss D Fake: 0.7495 (0.7846) Acc D Fake: 5.000%
Loss D: 1.099
Loss G: 0.6479 (0.6133) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.4100 (0.4148) Acc D Real: 94.703%
Loss D Fake: 0.7426 (0.7843) Acc D Fake: 5.000%
Loss D: 1.153
Loss G: 0.6524 (0.6136) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3960 (0.4147) Acc D Real: 94.685%
Loss D Fake: 0.7393 (0.7840) Acc D Fake: 5.000%
Loss D: 1.135
Loss G: 0.6540 (0.6139) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3726 (0.4144) Acc D Real: 94.651%
Loss D Fake: 0.7387 (0.7837) Acc D Fake: 5.000%
Loss D: 1.111
Loss G: 0.6540 (0.6141) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.4268 (0.4145) Acc D Real: 94.573%
Loss D Fake: 0.7393 (0.7834) Acc D Fake: 5.000%
Loss D: 1.166
Loss G: 0.6534 (0.6144) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3474 (0.4140) Acc D Real: 94.559%
Loss D Fake: 0.7410 (0.7831) Acc D Fake: 5.000%
Loss D: 1.088
Loss G: 0.6504 (0.6147) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3865 (0.4138) Acc D Real: 94.522%
Loss D Fake: 0.7464 (0.7828) Acc D Fake: 5.000%
Loss D: 1.133
Loss G: 0.6457 (0.6149) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3986 (0.4137) Acc D Real: 94.490%
Loss D Fake: 0.7516 (0.7826) Acc D Fake: 5.000%
Loss D: 1.150
Loss G: 0.6465 (0.6151) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3634 (0.4134) Acc D Real: 94.467%
Loss D Fake: 0.7449 (0.7824) Acc D Fake: 5.000%
Loss D: 1.108
Loss G: 0.6536 (0.6154) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.4000 (0.4133) Acc D Real: 94.427%
Loss D Fake: 0.7379 (0.7821) Acc D Fake: 5.000%
Loss D: 1.138
Loss G: 0.6560 (0.6156) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3597 (0.4129) Acc D Real: 94.407%
Loss D Fake: 0.7375 (0.7818) Acc D Fake: 5.000%
Loss D: 1.097
Loss G: 0.6561 (0.6159) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4376 (0.4131) Acc D Real: 94.335%
Loss D Fake: 0.7390 (0.7815) Acc D Fake: 5.000%
Loss D: 1.177
Loss G: 0.6534 (0.6161) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3768 (0.4128) Acc D Real: 94.285%
Loss D Fake: 0.7439 (0.7812) Acc D Fake: 5.000%
Loss D: 1.121
Loss G: 0.6506 (0.6164) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3563 (0.4125) Acc D Real: 94.271%
Loss D Fake: 0.7444 (0.7810) Acc D Fake: 5.000%
Loss D: 1.101
Loss G: 0.6544 (0.6166) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3583 (0.4121) Acc D Real: 94.199%
Loss D Fake: 0.7360 (0.7807) Acc D Fake: 5.000%
Loss D: 1.094
Loss G: 0.6612 (0.6169) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3159 (0.4115) Acc D Real: 94.172%
Loss D Fake: 0.7292 (0.7804) Acc D Fake: 5.000%
Loss D: 1.045
Loss G: 0.6657 (0.6172) Acc G: 94.773%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3307 (0.4110) Acc D Real: 94.110%
Loss D Fake: 0.7245 (0.7800) Acc D Fake: 5.269%
Loss D: 1.055
Loss G: 0.6708 (0.6176) Acc G: 94.430%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.2983 (0.4103) Acc D Real: 94.064%
Loss D Fake: 0.7195 (0.7796) Acc D Fake: 5.620%
Loss D: 1.018
Loss G: 0.6746 (0.6179) Acc G: 94.060%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3820 (0.4101) Acc D Real: 94.022%
Loss D Fake: 0.7195 (0.7792) Acc D Fake: 5.966%
Loss D: 1.102
Loss G: 0.6669 (0.6183) Acc G: 93.790%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3269 (0.4095) Acc D Real: 93.984%
Loss D Fake: 0.7443 (0.7790) Acc D Fake: 5.960%
Loss D: 1.071
Loss G: 0.6662 (0.6186) Acc G: 93.534%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3923 (0.4094) Acc D Real: 93.931%
Loss D Fake: 0.7194 (0.7786) Acc D Fake: 6.279%
Loss D: 1.112
Loss G: 0.6791 (0.6189) Acc G: 93.155%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3454 (0.4090) Acc D Real: 93.872%
Loss D Fake: 0.7089 (0.7782) Acc D Fake: 6.667%
Loss D: 1.054
Loss G: 0.6870 (0.6194) Acc G: 92.750%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3946 (0.4089) Acc D Real: 93.712%
Loss D Fake: 0.7021 (0.7777) Acc D Fake: 7.081%
Loss D: 1.097
Loss G: 0.6925 (0.6198) Acc G: 92.329%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.4399 (0.4091) Acc D Real: 93.592%
Loss D Fake: 0.7007 (0.7772) Acc D Fake: 7.490%
Loss D: 1.141
Loss G: 0.6862 (0.6202) Acc G: 91.944%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4196 (0.4092) Acc D Real: 93.514%
Loss D Fake: 0.7149 (0.7769) Acc D Fake: 7.822%
Loss D: 1.134
Loss G: 0.6674 (0.6205) Acc G: 91.697%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3139 (0.4086) Acc D Real: 93.503%
Loss D Fake: 0.7661 (0.7768) Acc D Fake: 7.805%
Loss D: 1.080
Loss G: 0.6928 (0.6210) Acc G: 91.291%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.3914 (0.4085) Acc D Real: 93.406%
Loss D Fake: 0.6876 (0.7763) Acc D Fake: 8.242%
Loss D: 1.079
Loss G: 0.7102 (0.6215) Acc G: 90.848%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3580 (0.4082) Acc D Real: 93.290%
Loss D Fake: 0.6782 (0.7757) Acc D Fake: 8.695%
Loss D: 1.036
Loss G: 0.7167 (0.6221) Acc G: 90.402%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.4198 (0.4083) Acc D Real: 93.081%
Loss D Fake: 0.6737 (0.7751) Acc D Fake: 9.152%
Loss D: 1.093
Loss G: 0.7200 (0.6227) Acc G: 89.950%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.3874 (0.4082) Acc D Real: 92.927%
Loss D Fake: 0.6712 (0.7744) Acc D Fake: 9.603%
Loss D: 1.059
Loss G: 0.7219 (0.6233) Acc G: 89.504%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.4510 (0.4084) Acc D Real: 92.714%
Loss D Fake: 0.6698 (0.7738) Acc D Fake: 10.049%
Loss D: 1.121
Loss G: 0.7226 (0.6238) Acc G: 89.063%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3507 (0.4081) Acc D Real: 92.552%
Loss D Fake: 0.6694 (0.7732) Acc D Fake: 10.490%
Loss D: 1.020
Loss G: 0.7232 (0.6244) Acc G: 88.627%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3484 (0.4077) Acc D Real: 92.399%
Loss D Fake: 0.6686 (0.7726) Acc D Fake: 10.926%
Loss D: 1.017
Loss G: 0.7241 (0.6250) Acc G: 88.197%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.4615 (0.4080) Acc D Real: 92.166%
Loss D Fake: 0.6679 (0.7720) Acc D Fake: 11.357%
Loss D: 1.129
Loss G: 0.7242 (0.6256) Acc G: 87.762%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.4811 (0.4085) Acc D Real: 91.913%
Loss D Fake: 0.6684 (0.7714) Acc D Fake: 11.782%
Loss D: 1.149
Loss G: 0.7230 (0.6261) Acc G: 87.331%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.5358 (0.4092) Acc D Real: 91.618%
Loss D Fake: 0.6701 (0.7708) Acc D Fake: 12.203%
Loss D: 1.206
Loss G: 0.7202 (0.6267) Acc G: 86.916%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3883 (0.4091) Acc D Real: 91.447%
Loss D Fake: 0.6729 (0.7702) Acc D Fake: 12.619%
Loss D: 1.061
Loss G: 0.7175 (0.6272) Acc G: 86.505%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.4055 (0.4091) Acc D Real: 91.262%
Loss D Fake: 0.6752 (0.7697) Acc D Fake: 13.030%
Loss D: 1.081
Loss G: 0.7152 (0.6277) Acc G: 86.098%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4922 (0.4095) Acc D Real: 91.018%
Loss D Fake: 0.6776 (0.7692) Acc D Fake: 13.437%
Loss D: 1.170
Loss G: 0.7121 (0.6282) Acc G: 85.697%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4717 (0.4099) Acc D Real: 90.801%
Loss D Fake: 0.6809 (0.7687) Acc D Fake: 13.839%
Loss D: 1.153
Loss G: 0.7084 (0.6286) Acc G: 85.300%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.4496 (0.4101) Acc D Real: 90.606%
Loss D Fake: 0.6846 (0.7682) Acc D Fake: 14.236%
Loss D: 1.134
Loss G: 0.7044 (0.6291) Acc G: 84.907%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.3847 (0.4100) Acc D Real: 90.491%
Loss D Fake: 0.6883 (0.7678) Acc D Fake: 14.630%
Loss D: 1.073
Loss G: 0.7010 (0.6295) Acc G: 84.528%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3585 (0.4097) Acc D Real: 90.382%
Loss D Fake: 0.6910 (0.7673) Acc D Fake: 15.009%
Loss D: 1.050
Loss G: 0.6987 (0.6298) Acc G: 84.153%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.4192 (0.4097) Acc D Real: 90.227%
Loss D Fake: 0.6930 (0.7669) Acc D Fake: 15.385%
Loss D: 1.112
Loss G: 0.6966 (0.6302) Acc G: 83.782%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.3394 (0.4093) Acc D Real: 90.134%
Loss D Fake: 0.6947 (0.7665) Acc D Fake: 15.756%
Loss D: 1.034
Loss G: 0.6955 (0.6306) Acc G: 83.415%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3514 (0.4090) Acc D Real: 90.092%
Loss D Fake: 0.6953 (0.7662) Acc D Fake: 16.123%
Loss D: 1.047
Loss G: 0.6953 (0.6309) Acc G: 83.053%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.4685 (0.4093) Acc D Real: 89.943%
Loss D Fake: 0.6956 (0.7658) Acc D Fake: 16.486%
Loss D: 1.164
Loss G: 0.6945 (0.6313) Acc G: 82.694%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.4288 (0.4094) Acc D Real: 89.838%
Loss D Fake: 0.6967 (0.7654) Acc D Fake: 16.846%
Loss D: 1.126
Loss G: 0.6933 (0.6316) Acc G: 82.339%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.4191 (0.4095) Acc D Real: 89.742%
Loss D Fake: 0.6980 (0.7650) Acc D Fake: 17.201%
Loss D: 1.117
Loss G: 0.6919 (0.6319) Acc G: 81.988%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.4929 (0.4099) Acc D Real: 89.624%
Loss D Fake: 0.6998 (0.7647) Acc D Fake: 17.544%
Loss D: 1.193
Loss G: 0.6894 (0.6322) Acc G: 81.649%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.5085 (0.4105) Acc D Real: 89.523%
Loss D Fake: 0.7030 (0.7644) Acc D Fake: 17.884%
Loss D: 1.212
Loss G: 0.6854 (0.6325) Acc G: 81.323%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3487 (0.4101) Acc D Real: 89.488%
Loss D Fake: 0.7070 (0.7641) Acc D Fake: 18.211%
Loss D: 1.056
Loss G: 0.6822 (0.6328) Acc G: 81.009%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.4639 (0.4104) Acc D Real: 89.371%
Loss D Fake: 0.7101 (0.7638) Acc D Fake: 18.517%
Loss D: 1.174
Loss G: 0.6788 (0.6330) Acc G: 80.707%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.4907 (0.4108) Acc D Real: 89.301%
Loss D Fake: 0.7140 (0.7635) Acc D Fake: 18.785%
Loss D: 1.205
Loss G: 0.6744 (0.6332) Acc G: 80.781%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3389 (0.4105) Acc D Real: 89.319%
Loss D Fake: 0.7184 (0.7633) Acc D Fake: 18.713%
Loss D: 1.057
Loss G: 0.6710 (0.6334) Acc G: 80.855%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.4120 (0.4105) Acc D Real: 89.301%
Loss D Fake: 0.7215 (0.7631) Acc D Fake: 18.643%
Loss D: 1.134
Loss G: 0.6681 (0.6336) Acc G: 80.928%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.4291 (0.4106) Acc D Real: 89.288%
Loss D Fake: 0.7245 (0.7629) Acc D Fake: 18.573%
Loss D: 1.154
Loss G: 0.6651 (0.6338) Acc G: 81.000%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.4002 (0.4105) Acc D Real: 89.304%
Loss D Fake: 0.7282 (0.7627) Acc D Fake: 18.503%
Loss D: 1.128
Loss G: 0.6608 (0.6339) Acc G: 81.071%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3041 (0.4100) Acc D Real: 89.314%
Loss D Fake: 0.7326 (0.7625) Acc D Fake: 18.435%
Loss D: 1.037
Loss G: 0.6579 (0.6340) Acc G: 81.142%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.4031 (0.4099) Acc D Real: 89.305%
Loss D Fake: 0.7349 (0.7624) Acc D Fake: 18.367%
Loss D: 1.138
Loss G: 0.6559 (0.6341) Acc G: 81.212%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3790 (0.4098) Acc D Real: 89.325%
Loss D Fake: 0.7370 (0.7623) Acc D Fake: 18.300%
Loss D: 1.116
Loss G: 0.6542 (0.6342) Acc G: 81.281%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.3665 (0.4096) Acc D Real: 89.339%
Loss D Fake: 0.7385 (0.7622) Acc D Fake: 18.233%
Loss D: 1.105
Loss G: 0.6534 (0.6343) Acc G: 81.350%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3220 (0.4091) Acc D Real: 89.356%
Loss D Fake: 0.7386 (0.7620) Acc D Fake: 18.167%
Loss D: 1.061
Loss G: 0.6543 (0.6344) Acc G: 81.418%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.4782 (0.4095) Acc D Real: 89.361%
Loss D Fake: 0.7379 (0.7619) Acc D Fake: 18.102%
Loss D: 1.216
Loss G: 0.6536 (0.6345) Acc G: 81.485%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.3713 (0.4093) Acc D Real: 89.378%
Loss D Fake: 0.7391 (0.7618) Acc D Fake: 18.038%
Loss D: 1.110
Loss G: 0.6529 (0.6346) Acc G: 81.552%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3910 (0.4092) Acc D Real: 89.368%
Loss D Fake: 0.7396 (0.7617) Acc D Fake: 17.974%
Loss D: 1.131
Loss G: 0.6528 (0.6347) Acc G: 81.618%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.3381 (0.4088) Acc D Real: 89.352%
Loss D Fake: 0.7391 (0.7616) Acc D Fake: 17.911%
Loss D: 1.077
Loss G: 0.6543 (0.6348) Acc G: 81.683%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3796 (0.4087) Acc D Real: 89.367%
Loss D Fake: 0.7371 (0.7615) Acc D Fake: 17.848%
Loss D: 1.117
Loss G: 0.6560 (0.6349) Acc G: 81.748%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.3388 (0.4084) Acc D Real: 89.390%
Loss D Fake: 0.7350 (0.7613) Acc D Fake: 17.786%
Loss D: 1.074
Loss G: 0.6585 (0.6350) Acc G: 81.812%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.3886 (0.4083) Acc D Real: 89.386%
Loss D Fake: 0.7326 (0.7612) Acc D Fake: 17.724%
Loss D: 1.121
Loss G: 0.6599 (0.6351) Acc G: 81.875%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.4132 (0.4083) Acc D Real: 89.392%
Loss D Fake: 0.7320 (0.7611) Acc D Fake: 17.663%
Loss D: 1.145
Loss G: 0.6598 (0.6352) Acc G: 81.938%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3993 (0.4083) Acc D Real: 89.380%
Loss D Fake: 0.7327 (0.7609) Acc D Fake: 17.603%
Loss D: 1.132
Loss G: 0.6588 (0.6354) Acc G: 82.000%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.4599 (0.4085) Acc D Real: 89.354%
Loss D Fake: 0.7348 (0.7608) Acc D Fake: 17.543%
Loss D: 1.195
Loss G: 0.6557 (0.6355) Acc G: 82.062%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3538 (0.4082) Acc D Real: 89.365%
Loss D Fake: 0.7388 (0.7607) Acc D Fake: 17.484%
Loss D: 1.093
Loss G: 0.6523 (0.6355) Acc G: 82.123%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3556 (0.4080) Acc D Real: 89.356%
Loss D Fake: 0.7420 (0.7606) Acc D Fake: 17.426%
Loss D: 1.098
Loss G: 0.6502 (0.6356) Acc G: 82.183%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3206 (0.4076) Acc D Real: 89.369%
Loss D Fake: 0.7433 (0.7605) Acc D Fake: 17.368%
Loss D: 1.064
Loss G: 0.6500 (0.6357) Acc G: 82.243%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3811 (0.4075) Acc D Real: 89.347%
Loss D Fake: 0.7428 (0.7605) Acc D Fake: 17.310%
Loss D: 1.124
Loss G: 0.6509 (0.6357) Acc G: 82.302%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2777 (0.4069) Acc D Real: 89.354%
Loss D Fake: 0.7405 (0.7604) Acc D Fake: 17.253%
Loss D: 1.018
Loss G: 0.6554 (0.6358) Acc G: 82.361%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2794 (0.4063) Acc D Real: 89.384%
Loss D Fake: 0.7348 (0.7602) Acc D Fake: 17.197%
Loss D: 1.014
Loss G: 0.6604 (0.6359) Acc G: 82.419%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3965 (0.4062) Acc D Real: 89.337%
Loss D Fake: 0.7304 (0.7601) Acc D Fake: 17.141%
Loss D: 1.127
Loss G: 0.6634 (0.6361) Acc G: 82.477%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.3858 (0.4061) Acc D Real: 89.297%
Loss D Fake: 0.7289 (0.7600) Acc D Fake: 17.085%
Loss D: 1.115
Loss G: 0.6634 (0.6362) Acc G: 82.534%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3246 (0.4058) Acc D Real: 89.272%
Loss D Fake: 0.7295 (0.7598) Acc D Fake: 17.030%
Loss D: 1.054
Loss G: 0.6639 (0.6363) Acc G: 82.545%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.3493 (0.4055) Acc D Real: 89.251%
Loss D Fake: 0.7296 (0.7597) Acc D Fake: 16.976%
Loss D: 1.079
Loss G: 0.6624 (0.6364) Acc G: 82.564%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2494 (0.4048) Acc D Real: 89.249%
Loss D Fake: 0.7312 (0.7596) Acc D Fake: 16.922%
Loss D: 0.981
Loss G: 0.6637 (0.6366) Acc G: 82.508%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3885 (0.4047) Acc D Real: 89.196%
Loss D Fake: 0.7305 (0.7594) Acc D Fake: 16.898%
Loss D: 1.119
Loss G: 0.6628 (0.6367) Acc G: 82.474%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3265 (0.4044) Acc D Real: 89.171%
Loss D Fake: 0.7330 (0.7593) Acc D Fake: 16.845%
Loss D: 1.060
Loss G: 0.6648 (0.6368) Acc G: 82.411%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3466 (0.4041) Acc D Real: 89.136%
Loss D Fake: 0.7297 (0.7592) Acc D Fake: 16.815%
Loss D: 1.076
Loss G: 0.6708 (0.6370) Acc G: 82.281%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.2554 (0.4035) Acc D Real: 89.136%
Loss D Fake: 0.7212 (0.7590) Acc D Fake: 16.844%
Loss D: 0.977
Loss G: 0.6814 (0.6372) Acc G: 82.233%
LR: 2.000e-04
Epoch: 10/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3129 (0.3092) Acc D Real: 78.542%
Loss D Fake: 0.6846 (0.6949) Acc D Fake: 70.000%
Loss D: 0.998
Loss G: 0.7184 (0.7090) Acc G: 24.167%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.2968 (0.3050) Acc D Real: 80.000%
Loss D Fake: 0.6713 (0.6870) Acc D Fake: 72.778%
Loss D: 0.968
Loss G: 0.7248 (0.7143) Acc G: 22.778%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2927 (0.3020) Acc D Real: 81.484%
Loss D Fake: 0.6727 (0.6834) Acc D Fake: 74.583%
Loss D: 0.965
Loss G: 0.7146 (0.7144) Acc G: 22.500%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2836 (0.2983) Acc D Real: 81.958%
Loss D Fake: 0.6955 (0.6858) Acc D Fake: 74.667%
Loss D: 0.979
Loss G: 0.7035 (0.7122) Acc G: 22.667%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2981 (0.2983) Acc D Real: 82.196%
Loss D Fake: 0.7063 (0.6892) Acc D Fake: 74.167%
Loss D: 1.004
Loss G: 0.7196 (0.7134) Acc G: 22.500%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2594 (0.2927) Acc D Real: 82.939%
Loss D Fake: 0.6618 (0.6853) Acc D Fake: 75.000%
Loss D: 0.921
Loss G: 0.7460 (0.7181) Acc G: 21.905%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3250 (0.2967) Acc D Real: 82.109%
Loss D Fake: 0.6450 (0.6803) Acc D Fake: 76.042%
Loss D: 0.970
Loss G: 0.7583 (0.7231) Acc G: 21.250%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2539 (0.2920) Acc D Real: 82.245%
Loss D Fake: 0.6373 (0.6755) Acc D Fake: 76.852%
Loss D: 0.891
Loss G: 0.7658 (0.7279) Acc G: 20.741%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3206 (0.2948) Acc D Real: 81.578%
Loss D Fake: 0.6318 (0.6711) Acc D Fake: 77.500%
Loss D: 0.952
Loss G: 0.7718 (0.7322) Acc G: 20.333%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.3306 (0.2981) Acc D Real: 81.373%
Loss D Fake: 0.6320 (0.6676) Acc D Fake: 78.030%
Loss D: 0.963
Loss G: 0.7589 (0.7347) Acc G: 20.303%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2917 (0.2976) Acc D Real: 81.476%
Loss D Fake: 0.6582 (0.6668) Acc D Fake: 77.778%
Loss D: 0.950
Loss G: 0.7313 (0.7344) Acc G: 20.833%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3186 (0.2992) Acc D Real: 80.978%
Loss D Fake: 0.6940 (0.6689) Acc D Fake: 76.538%
Loss D: 1.013
Loss G: 0.7845 (0.7383) Acc G: 20.513%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2309 (0.2943) Acc D Real: 81.522%
Loss D Fake: 0.6027 (0.6642) Acc D Fake: 77.262%
Loss D: 0.834
Loss G: 0.8176 (0.7439) Acc G: 19.881%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2863 (0.2938) Acc D Real: 81.389%
Loss D Fake: 0.5849 (0.6589) Acc D Fake: 78.000%
Loss D: 0.871
Loss G: 0.8341 (0.7499) Acc G: 19.333%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.4901 (0.3060) Acc D Real: 79.691%
Loss D Fake: 0.5751 (0.6536) Acc D Fake: 78.646%
Loss D: 1.065
Loss G: 0.8440 (0.7558) Acc G: 18.854%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3453 (0.3083) Acc D Real: 79.317%
Loss D Fake: 0.5693 (0.6487) Acc D Fake: 79.314%
Loss D: 0.915
Loss G: 0.8499 (0.7613) Acc G: 18.333%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.4938 (0.3187) Acc D Real: 77.905%
Loss D Fake: 0.5662 (0.6441) Acc D Fake: 79.907%
Loss D: 1.060
Loss G: 0.8529 (0.7664) Acc G: 17.870%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3911 (0.3225) Acc D Real: 77.289%
Loss D Fake: 0.5646 (0.6399) Acc D Fake: 80.439%
Loss D: 0.956
Loss G: 0.8544 (0.7711) Acc G: 17.456%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.4337 (0.3280) Acc D Real: 76.513%
Loss D Fake: 0.5641 (0.6361) Acc D Fake: 80.917%
Loss D: 0.998
Loss G: 0.8541 (0.7752) Acc G: 17.083%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3402 (0.3286) Acc D Real: 76.245%
Loss D Fake: 0.5648 (0.6327) Acc D Fake: 81.349%
Loss D: 0.905
Loss G: 0.8533 (0.7789) Acc G: 16.746%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.4110 (0.3324) Acc D Real: 75.720%
Loss D Fake: 0.5656 (0.6297) Acc D Fake: 81.742%
Loss D: 0.977
Loss G: 0.8519 (0.7823) Acc G: 16.439%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.4813 (0.3388) Acc D Real: 74.846%
Loss D Fake: 0.5671 (0.6270) Acc D Fake: 82.029%
Loss D: 1.048
Loss G: 0.8490 (0.7852) Acc G: 16.232%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4798 (0.3447) Acc D Real: 74.080%
Loss D Fake: 0.5699 (0.6246) Acc D Fake: 82.292%
Loss D: 1.050
Loss G: 0.8445 (0.7876) Acc G: 16.042%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.3145 (0.3435) Acc D Real: 74.135%
Loss D Fake: 0.5735 (0.6225) Acc D Fake: 82.533%
Loss D: 0.888
Loss G: 0.8402 (0.7897) Acc G: 15.867%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.4812 (0.3488) Acc D Real: 73.393%
Loss D Fake: 0.5769 (0.6208) Acc D Fake: 82.756%
Loss D: 1.058
Loss G: 0.8352 (0.7915) Acc G: 15.705%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.4776 (0.3536) Acc D Real: 72.699%
Loss D Fake: 0.5811 (0.6193) Acc D Fake: 82.963%
Loss D: 1.059
Loss G: 0.8293 (0.7929) Acc G: 15.556%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3670 (0.3540) Acc D Real: 72.545%
Loss D Fake: 0.5858 (0.6181) Acc D Fake: 83.155%
Loss D: 0.953
Loss G: 0.8234 (0.7940) Acc G: 15.476%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3909 (0.3553) Acc D Real: 72.277%
Loss D Fake: 0.5904 (0.6172) Acc D Fake: 83.276%
Loss D: 0.981
Loss G: 0.8177 (0.7948) Acc G: 15.402%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.4343 (0.3579) Acc D Real: 71.872%
Loss D Fake: 0.5950 (0.6164) Acc D Fake: 83.389%
Loss D: 1.029
Loss G: 0.8116 (0.7953) Acc G: 15.333%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3663 (0.3582) Acc D Real: 71.736%
Loss D Fake: 0.6000 (0.6159) Acc D Fake: 83.495%
Loss D: 0.966
Loss G: 0.8058 (0.7957) Acc G: 15.269%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3959 (0.3594) Acc D Real: 71.509%
Loss D Fake: 0.6056 (0.6156) Acc D Fake: 83.542%
Loss D: 1.002
Loss G: 0.7968 (0.7957) Acc G: 15.260%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2739 (0.3568) Acc D Real: 71.739%
Loss D Fake: 0.6142 (0.6155) Acc D Fake: 83.535%
Loss D: 0.888
Loss G: 0.7869 (0.7954) Acc G: 15.303%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.3186 (0.3557) Acc D Real: 71.800%
Loss D Fake: 0.6229 (0.6157) Acc D Fake: 83.480%
Loss D: 0.942
Loss G: 0.7773 (0.7949) Acc G: 15.392%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3160 (0.3545) Acc D Real: 71.908%
Loss D Fake: 0.6319 (0.6162) Acc D Fake: 83.381%
Loss D: 0.948
Loss G: 0.7677 (0.7941) Acc G: 15.524%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.2760 (0.3524) Acc D Real: 72.099%
Loss D Fake: 0.6412 (0.6169) Acc D Fake: 83.241%
Loss D: 0.917
Loss G: 0.7595 (0.7932) Acc G: 15.741%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3773 (0.3530) Acc D Real: 72.023%
Loss D Fake: 0.6508 (0.6178) Acc D Fake: 83.018%
Loss D: 1.028
Loss G: 0.7504 (0.7920) Acc G: 16.036%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.3294 (0.3524) Acc D Real: 71.987%
Loss D Fake: 0.6624 (0.6190) Acc D Fake: 82.675%
Loss D: 0.992
Loss G: 0.7471 (0.7908) Acc G: 16.360%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.2561 (0.3499) Acc D Real: 72.229%
Loss D Fake: 0.6628 (0.6201) Acc D Fake: 82.350%
Loss D: 0.919
Loss G: 0.7550 (0.7899) Acc G: 16.624%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2654 (0.3478) Acc D Real: 72.396%
Loss D Fake: 0.6493 (0.6208) Acc D Fake: 82.125%
Loss D: 0.915
Loss G: 0.7693 (0.7894) Acc G: 16.792%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3669 (0.3483) Acc D Real: 72.320%
Loss D Fake: 0.6359 (0.6212) Acc D Fake: 81.992%
Loss D: 1.003
Loss G: 0.7793 (0.7892) Acc G: 16.911%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3377 (0.3480) Acc D Real: 72.273%
Loss D Fake: 0.6283 (0.6214) Acc D Fake: 81.865%
Loss D: 0.966
Loss G: 0.7871 (0.7891) Acc G: 16.984%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2896 (0.3467) Acc D Real: 72.432%
Loss D Fake: 0.6271 (0.6215) Acc D Fake: 81.744%
Loss D: 0.917
Loss G: 0.7786 (0.7889) Acc G: 17.132%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3660 (0.3471) Acc D Real: 72.347%
Loss D Fake: 0.6578 (0.6223) Acc D Fake: 81.402%
Loss D: 1.024
Loss G: 0.7728 (0.7885) Acc G: 17.348%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3253 (0.3466) Acc D Real: 72.356%
Loss D Fake: 0.6454 (0.6229) Acc D Fake: 81.148%
Loss D: 0.971
Loss G: 0.7876 (0.7885) Acc G: 17.481%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.4372 (0.3486) Acc D Real: 72.086%
Loss D Fake: 0.6234 (0.6229) Acc D Fake: 81.051%
Loss D: 1.061
Loss G: 0.8001 (0.7887) Acc G: 17.536%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4803 (0.3514) Acc D Real: 71.704%
Loss D Fake: 0.6133 (0.6227) Acc D Fake: 80.993%
Loss D: 1.094
Loss G: 0.8073 (0.7891) Acc G: 17.589%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3898 (0.3522) Acc D Real: 71.539%
Loss D Fake: 0.6081 (0.6224) Acc D Fake: 80.972%
Loss D: 0.998
Loss G: 0.8109 (0.7896) Acc G: 17.639%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2750 (0.3506) Acc D Real: 71.679%
Loss D Fake: 0.6058 (0.6220) Acc D Fake: 80.952%
Loss D: 0.881
Loss G: 0.8134 (0.7901) Acc G: 17.653%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.5116 (0.3539) Acc D Real: 71.275%
Loss D Fake: 0.6046 (0.6217) Acc D Fake: 80.933%
Loss D: 1.116
Loss G: 0.8135 (0.7905) Acc G: 17.700%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3891 (0.3545) Acc D Real: 71.200%
Loss D Fake: 0.6065 (0.6214) Acc D Fake: 80.915%
Loss D: 0.996
Loss G: 0.8096 (0.7909) Acc G: 17.745%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4431 (0.3562) Acc D Real: 70.993%
Loss D Fake: 0.6133 (0.6212) Acc D Fake: 80.865%
Loss D: 1.056
Loss G: 0.8005 (0.7911) Acc G: 17.821%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.4426 (0.3579) Acc D Real: 70.773%
Loss D Fake: 0.6268 (0.6213) Acc D Fake: 80.755%
Loss D: 1.069
Loss G: 0.7890 (0.7911) Acc G: 17.956%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.4074 (0.3588) Acc D Real: 70.659%
Loss D Fake: 0.6469 (0.6218) Acc D Fake: 80.556%
Loss D: 1.054
Loss G: 0.7799 (0.7908) Acc G: 18.117%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4586 (0.3606) Acc D Real: 70.421%
Loss D Fake: 0.6514 (0.6223) Acc D Fake: 80.333%
Loss D: 1.110
Loss G: 0.7865 (0.7908) Acc G: 18.242%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.2834 (0.3592) Acc D Real: 70.519%
Loss D Fake: 0.6292 (0.6225) Acc D Fake: 80.238%
Loss D: 0.913
Loss G: 0.7977 (0.7909) Acc G: 18.333%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3312 (0.3587) Acc D Real: 70.529%
Loss D Fake: 0.6178 (0.6224) Acc D Fake: 80.175%
Loss D: 0.949
Loss G: 0.8053 (0.7911) Acc G: 18.392%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.4940 (0.3611) Acc D Real: 70.228%
Loss D Fake: 0.6116 (0.6222) Acc D Fake: 80.144%
Loss D: 1.106
Loss G: 0.8089 (0.7915) Acc G: 18.420%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4054 (0.3618) Acc D Real: 70.094%
Loss D Fake: 0.6089 (0.6220) Acc D Fake: 80.113%
Loss D: 1.014
Loss G: 0.8108 (0.7918) Acc G: 18.446%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.2952 (0.3607) Acc D Real: 70.186%
Loss D Fake: 0.6072 (0.6217) Acc D Fake: 80.089%
Loss D: 0.902
Loss G: 0.8123 (0.7921) Acc G: 18.472%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3791 (0.3610) Acc D Real: 70.102%
Loss D Fake: 0.6059 (0.6215) Acc D Fake: 80.088%
Loss D: 0.985
Loss G: 0.8134 (0.7925) Acc G: 18.497%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3264 (0.3605) Acc D Real: 70.114%
Loss D Fake: 0.6050 (0.6212) Acc D Fake: 80.087%
Loss D: 0.931
Loss G: 0.8144 (0.7928) Acc G: 18.522%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.5314 (0.3632) Acc D Real: 69.785%
Loss D Fake: 0.6047 (0.6209) Acc D Fake: 80.085%
Loss D: 1.136
Loss G: 0.8138 (0.7932) Acc G: 18.545%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.5264 (0.3657) Acc D Real: 69.471%
Loss D Fake: 0.6061 (0.6207) Acc D Fake: 80.071%
Loss D: 1.132
Loss G: 0.8112 (0.7934) Acc G: 18.568%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3470 (0.3654) Acc D Real: 69.449%
Loss D Fake: 0.6084 (0.6205) Acc D Fake: 80.044%
Loss D: 0.955
Loss G: 0.8087 (0.7937) Acc G: 18.590%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.6377 (0.3696) Acc D Real: 68.953%
Loss D Fake: 0.6111 (0.6204) Acc D Fake: 80.018%
Loss D: 1.249
Loss G: 0.8043 (0.7938) Acc G: 18.636%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.5020 (0.3715) Acc D Real: 68.686%
Loss D Fake: 0.6156 (0.6203) Acc D Fake: 79.993%
Loss D: 1.118
Loss G: 0.7988 (0.7939) Acc G: 18.682%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4916 (0.3733) Acc D Real: 68.454%
Loss D Fake: 0.6208 (0.6203) Acc D Fake: 79.969%
Loss D: 1.112
Loss G: 0.7924 (0.7939) Acc G: 18.725%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.5641 (0.3761) Acc D Real: 68.096%
Loss D Fake: 0.6272 (0.6204) Acc D Fake: 79.921%
Loss D: 1.191
Loss G: 0.7849 (0.7938) Acc G: 18.792%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.4893 (0.3777) Acc D Real: 67.868%
Loss D Fake: 0.6345 (0.6206) Acc D Fake: 79.874%
Loss D: 1.124
Loss G: 0.7770 (0.7935) Acc G: 18.857%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.5256 (0.3798) Acc D Real: 67.587%
Loss D Fake: 0.6431 (0.6209) Acc D Fake: 79.806%
Loss D: 1.169
Loss G: 0.7670 (0.7931) Acc G: 18.944%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.5976 (0.3828) Acc D Real: 67.185%
Loss D Fake: 0.6553 (0.6214) Acc D Fake: 79.716%
Loss D: 1.253
Loss G: 0.7552 (0.7926) Acc G: 19.051%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.5210 (0.3847) Acc D Real: 66.905%
Loss D Fake: 0.6704 (0.6221) Acc D Fake: 79.605%
Loss D: 1.191
Loss G: 0.7423 (0.7919) Acc G: 19.178%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.4233 (0.3852) Acc D Real: 66.808%
Loss D Fake: 0.6912 (0.6230) Acc D Fake: 79.453%
Loss D: 1.114
Loss G: 0.7280 (0.7911) Acc G: 19.324%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3784 (0.3851) Acc D Real: 66.763%
Loss D Fake: 0.7333 (0.6245) Acc D Fake: 79.216%
Loss D: 1.112
Loss G: 0.7204 (0.7901) Acc G: 19.489%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4686 (0.3862) Acc D Real: 66.578%
Loss D Fake: 0.7477 (0.6261) Acc D Fake: 78.958%
Loss D: 1.216
Loss G: 0.7242 (0.7893) Acc G: 19.627%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.5092 (0.3878) Acc D Real: 66.331%
Loss D Fake: 0.7239 (0.6274) Acc D Fake: 78.757%
Loss D: 1.233
Loss G: 0.7260 (0.7884) Acc G: 19.744%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.5274 (0.3896) Acc D Real: 66.052%
Loss D Fake: 0.7481 (0.6289) Acc D Fake: 78.517%
Loss D: 1.276
Loss G: 0.7219 (0.7876) Acc G: 19.875%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.4645 (0.3905) Acc D Real: 65.876%
Loss D Fake: 1.8774 (0.6447) Acc D Fake: 77.607%
Loss D: 2.342
Loss G: 0.7425 (0.7870) Acc G: 19.940%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.4817 (0.3917) Acc D Real: 65.680%
Loss D Fake: 0.6593 (0.6449) Acc D Fake: 77.575%
Loss D: 1.141
Loss G: 0.7596 (0.7867) Acc G: 19.941%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.5010 (0.3930) Acc D Real: 65.441%
Loss D Fake: 0.6424 (0.6449) Acc D Fake: 77.605%
Loss D: 1.143
Loss G: 0.7652 (0.7864) Acc G: 19.921%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.5057 (0.3944) Acc D Real: 65.208%
Loss D Fake: 0.6368 (0.6448) Acc D Fake: 77.634%
Loss D: 1.143
Loss G: 0.7661 (0.7862) Acc G: 19.902%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3887 (0.3943) Acc D Real: 65.147%
Loss D Fake: 0.6354 (0.6447) Acc D Fake: 77.683%
Loss D: 1.024
Loss G: 0.7652 (0.7859) Acc G: 19.863%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.6557 (0.3975) Acc D Real: 64.691%
Loss D Fake: 0.6361 (0.6446) Acc D Fake: 77.750%
Loss D: 1.292
Loss G: 0.7618 (0.7856) Acc G: 19.825%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.6006 (0.3998) Acc D Real: 64.320%
Loss D Fake: 0.6392 (0.6445) Acc D Fake: 77.816%
Loss D: 1.240
Loss G: 0.7565 (0.7853) Acc G: 19.787%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.5521 (0.4016) Acc D Real: 64.027%
Loss D Fake: 0.6438 (0.6445) Acc D Fake: 77.880%
Loss D: 1.196
Loss G: 0.7503 (0.7849) Acc G: 19.751%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.5110 (0.4029) Acc D Real: 63.810%
Loss D Fake: 0.6491 (0.6445) Acc D Fake: 77.923%
Loss D: 1.160
Loss G: 0.7436 (0.7844) Acc G: 19.734%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.5272 (0.4043) Acc D Real: 63.554%
Loss D Fake: 0.6550 (0.6447) Acc D Fake: 77.966%
Loss D: 1.182
Loss G: 0.7367 (0.7838) Acc G: 19.718%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.4755 (0.4051) Acc D Real: 63.361%
Loss D Fake: 0.6613 (0.6448) Acc D Fake: 77.989%
Loss D: 1.137
Loss G: 0.7297 (0.7832) Acc G: 19.709%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4747 (0.4059) Acc D Real: 63.179%
Loss D Fake: 0.6677 (0.6451) Acc D Fake: 78.011%
Loss D: 1.142
Loss G: 0.7227 (0.7826) Acc G: 19.712%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.5075 (0.4070) Acc D Real: 62.941%
Loss D Fake: 0.6745 (0.6454) Acc D Fake: 78.015%
Loss D: 1.182
Loss G: 0.7154 (0.7818) Acc G: 19.733%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.4825 (0.4078) Acc D Real: 62.744%
Loss D Fake: 0.6820 (0.6458) Acc D Fake: 78.000%
Loss D: 1.165
Loss G: 0.7078 (0.7810) Acc G: 19.772%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.5194 (0.4090) Acc D Real: 62.492%
Loss D Fake: 0.6907 (0.6463) Acc D Fake: 77.968%
Loss D: 1.210
Loss G: 0.6992 (0.7801) Acc G: 19.829%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.4786 (0.4097) Acc D Real: 62.303%
Loss D Fake: 0.7032 (0.6469) Acc D Fake: 77.901%
Loss D: 1.182
Loss G: 0.6886 (0.7792) Acc G: 19.919%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.5100 (0.4108) Acc D Real: 62.075%
Loss D Fake: 2.6403 (0.6679) Acc D Fake: 77.168%
Loss D: 3.150
Loss G: 0.6875 (0.7782) Acc G: 19.990%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4287 (0.4110) Acc D Real: 61.962%
Loss D Fake: 0.7045 (0.6683) Acc D Fake: 77.133%
Loss D: 1.133
Loss G: 0.6869 (0.7773) Acc G: 20.025%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.5017 (0.4119) Acc D Real: 61.792%
Loss D Fake: 0.7058 (0.6687) Acc D Fake: 77.096%
Loss D: 1.207
Loss G: 0.6829 (0.7763) Acc G: 20.059%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.4583 (0.4124) Acc D Real: 61.799%
Loss D Fake: 0.7098 (0.6691) Acc D Fake: 77.057%
Loss D: 1.168
Loss G: 0.6781 (0.7753) Acc G: 20.143%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.4743 (0.4130) Acc D Real: 62.025%
Loss D Fake: 0.7145 (0.6695) Acc D Fake: 76.919%
Loss D: 1.189
Loss G: 0.6731 (0.7742) Acc G: 20.866%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.5943 (0.4148) Acc D Real: 62.354%
Loss D Fake: 0.7198 (0.6700) Acc D Fake: 76.233%
Loss D: 1.314
Loss G: 0.6674 (0.7732) Acc G: 21.574%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.4852 (0.4155) Acc D Real: 62.684%
Loss D Fake: 0.7259 (0.6706) Acc D Fake: 75.561%
Loss D: 1.211
Loss G: 0.6616 (0.7721) Acc G: 22.268%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.4233 (0.4156) Acc D Real: 63.015%
Loss D Fake: 0.7317 (0.6712) Acc D Fake: 74.901%
Loss D: 1.155
Loss G: 0.6564 (0.7709) Acc G: 22.948%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4579 (0.4160) Acc D Real: 63.332%
Loss D Fake: 0.7371 (0.6718) Acc D Fake: 74.255%
Loss D: 1.195
Loss G: 0.6515 (0.7698) Acc G: 23.615%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.4742 (0.4166) Acc D Real: 63.643%
Loss D Fake: 0.7422 (0.6725) Acc D Fake: 73.621%
Loss D: 1.216
Loss G: 0.6468 (0.7686) Acc G: 24.270%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.4998 (0.4174) Acc D Real: 63.946%
Loss D Fake: 0.7474 (0.6732) Acc D Fake: 73.000%
Loss D: 1.247
Loss G: 0.6421 (0.7674) Acc G: 24.912%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.5117 (0.4182) Acc D Real: 64.245%
Loss D Fake: 0.7527 (0.6740) Acc D Fake: 72.389%
Loss D: 1.264
Loss G: 0.6372 (0.7662) Acc G: 25.541%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4593 (0.4186) Acc D Real: 64.541%
Loss D Fake: 0.7581 (0.6748) Acc D Fake: 71.791%
Loss D: 1.217
Loss G: 0.6326 (0.7649) Acc G: 26.159%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.5061 (0.4194) Acc D Real: 64.829%
Loss D Fake: 0.7634 (0.6756) Acc D Fake: 71.203%
Loss D: 1.269
Loss G: 0.6279 (0.7636) Acc G: 26.766%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3677 (0.4190) Acc D Real: 65.119%
Loss D Fake: 0.7685 (0.6764) Acc D Fake: 70.626%
Loss D: 1.136
Loss G: 0.6239 (0.7624) Acc G: 27.361%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.5275 (0.4200) Acc D Real: 65.403%
Loss D Fake: 0.7730 (0.6773) Acc D Fake: 70.060%
Loss D: 1.301
Loss G: 0.6198 (0.7611) Acc G: 27.946%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.4142 (0.4199) Acc D Real: 65.678%
Loss D Fake: 0.7779 (0.6782) Acc D Fake: 69.504%
Loss D: 1.192
Loss G: 0.6160 (0.7598) Acc G: 28.520%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.5081 (0.4207) Acc D Real: 65.950%
Loss D Fake: 0.7823 (0.6791) Acc D Fake: 68.958%
Loss D: 1.290
Loss G: 0.6122 (0.7584) Acc G: 29.084%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4387 (0.4208) Acc D Real: 66.216%
Loss D Fake: 0.7870 (0.6801) Acc D Fake: 68.421%
Loss D: 1.226
Loss G: 0.6084 (0.7571) Acc G: 29.638%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.4210 (0.4209) Acc D Real: 66.481%
Loss D Fake: 0.7913 (0.6811) Acc D Fake: 67.881%
Loss D: 1.212
Loss G: 0.6051 (0.7558) Acc G: 30.196%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4512 (0.4211) Acc D Real: 66.738%
Loss D Fake: 0.7953 (0.6821) Acc D Fake: 67.349%
Loss D: 1.246
Loss G: 0.6019 (0.7544) Acc G: 30.745%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4932 (0.4217) Acc D Real: 66.993%
Loss D Fake: 0.7993 (0.6831) Acc D Fake: 66.826%
Loss D: 1.293
Loss G: 0.5984 (0.7531) Acc G: 31.285%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.4107 (0.4216) Acc D Real: 67.244%
Loss D Fake: 0.8036 (0.6841) Acc D Fake: 66.311%
Loss D: 1.214
Loss G: 0.5953 (0.7518) Acc G: 31.815%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.4878 (0.4222) Acc D Real: 67.490%
Loss D Fake: 0.8075 (0.6852) Acc D Fake: 65.806%
Loss D: 1.295
Loss G: 0.5920 (0.7504) Acc G: 32.337%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.4694 (0.4226) Acc D Real: 67.728%
Loss D Fake: 0.8117 (0.6862) Acc D Fake: 65.309%
Loss D: 1.281
Loss G: 0.5888 (0.7490) Acc G: 32.849%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.4823 (0.4231) Acc D Real: 67.964%
Loss D Fake: 0.8160 (0.6873) Acc D Fake: 64.820%
Loss D: 1.298
Loss G: 0.5854 (0.7477) Acc G: 33.353%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.4492 (0.4233) Acc D Real: 68.197%
Loss D Fake: 0.8204 (0.6884) Acc D Fake: 64.340%
Loss D: 1.270
Loss G: 0.5821 (0.7463) Acc G: 33.849%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4616 (0.4236) Acc D Real: 68.421%
Loss D Fake: 0.8246 (0.6895) Acc D Fake: 63.867%
Loss D: 1.286
Loss G: 0.5788 (0.7449) Acc G: 34.337%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.4106 (0.4235) Acc D Real: 68.650%
Loss D Fake: 0.8288 (0.6906) Acc D Fake: 63.402%
Loss D: 1.239
Loss G: 0.5759 (0.7436) Acc G: 34.816%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3234 (0.4227) Acc D Real: 68.881%
Loss D Fake: 0.8323 (0.6918) Acc D Fake: 62.944%
Loss D: 1.156
Loss G: 0.5737 (0.7422) Acc G: 35.288%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3772 (0.4223) Acc D Real: 69.102%
Loss D Fake: 0.8348 (0.6929) Acc D Fake: 62.494%
Loss D: 1.212
Loss G: 0.5721 (0.7408) Acc G: 35.752%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.4497 (0.4226) Acc D Real: 69.320%
Loss D Fake: 0.8369 (0.6941) Acc D Fake: 62.051%
Loss D: 1.287
Loss G: 0.5703 (0.7395) Acc G: 36.209%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.4221 (0.4226) Acc D Real: 69.534%
Loss D Fake: 0.8393 (0.6952) Acc D Fake: 61.615%
Loss D: 1.261
Loss G: 0.5686 (0.7381) Acc G: 36.659%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.4629 (0.4229) Acc D Real: 69.741%
Loss D Fake: 0.8419 (0.6964) Acc D Fake: 61.186%
Loss D: 1.305
Loss G: 0.5666 (0.7368) Acc G: 37.102%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.4482 (0.4231) Acc D Real: 69.942%
Loss D Fake: 0.8447 (0.6975) Acc D Fake: 60.763%
Loss D: 1.293
Loss G: 0.5645 (0.7355) Acc G: 37.538%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.4451 (0.4232) Acc D Real: 70.146%
Loss D Fake: 0.8477 (0.6987) Acc D Fake: 60.347%
Loss D: 1.293
Loss G: 0.5623 (0.7341) Acc G: 37.967%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3718 (0.4228) Acc D Real: 70.349%
Loss D Fake: 0.8505 (0.6998) Acc D Fake: 59.937%
Loss D: 1.222
Loss G: 0.5605 (0.7328) Acc G: 38.390%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.4416 (0.4230) Acc D Real: 70.548%
Loss D Fake: 0.8529 (0.7010) Acc D Fake: 59.534%
Loss D: 1.294
Loss G: 0.5587 (0.7315) Acc G: 38.806%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3804 (0.4227) Acc D Real: 70.745%
Loss D Fake: 0.8554 (0.7022) Acc D Fake: 59.136%
Loss D: 1.236
Loss G: 0.5571 (0.7302) Acc G: 39.216%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.4807 (0.4231) Acc D Real: 70.935%
Loss D Fake: 0.8576 (0.7033) Acc D Fake: 58.745%
Loss D: 1.338
Loss G: 0.5553 (0.7289) Acc G: 39.620%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.4082 (0.4230) Acc D Real: 71.124%
Loss D Fake: 0.8602 (0.7045) Acc D Fake: 58.359%
Loss D: 1.268
Loss G: 0.5535 (0.7276) Acc G: 40.018%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3928 (0.4228) Acc D Real: 71.310%
Loss D Fake: 0.8627 (0.7056) Acc D Fake: 57.979%
Loss D: 1.255
Loss G: 0.5520 (0.7263) Acc G: 40.410%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.4012 (0.4226) Acc D Real: 71.496%
Loss D Fake: 0.8648 (0.7068) Acc D Fake: 57.604%
Loss D: 1.266
Loss G: 0.5505 (0.7250) Acc G: 40.796%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.4462 (0.4228) Acc D Real: 71.674%
Loss D Fake: 0.8669 (0.7080) Acc D Fake: 57.235%
Loss D: 1.313
Loss G: 0.5490 (0.7237) Acc G: 41.177%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.4711 (0.4231) Acc D Real: 71.851%
Loss D Fake: 0.8694 (0.7091) Acc D Fake: 56.871%
Loss D: 1.340
Loss G: 0.5471 (0.7224) Acc G: 41.552%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3892 (0.4229) Acc D Real: 72.027%
Loss D Fake: 0.8721 (0.7103) Acc D Fake: 56.513%
Loss D: 1.261
Loss G: 0.5454 (0.7212) Acc G: 41.922%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3983 (0.4227) Acc D Real: 72.202%
Loss D Fake: 0.8743 (0.7114) Acc D Fake: 56.159%
Loss D: 1.273
Loss G: 0.5440 (0.7199) Acc G: 42.286%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3954 (0.4225) Acc D Real: 72.376%
Loss D Fake: 0.8764 (0.7126) Acc D Fake: 55.811%
Loss D: 1.272
Loss G: 0.5426 (0.7187) Acc G: 42.646%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.3956 (0.4223) Acc D Real: 72.544%
Loss D Fake: 0.8783 (0.7138) Acc D Fake: 55.467%
Loss D: 1.274
Loss G: 0.5414 (0.7174) Acc G: 43.000%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.4006 (0.4222) Acc D Real: 72.708%
Loss D Fake: 0.8800 (0.7149) Acc D Fake: 55.128%
Loss D: 1.281
Loss G: 0.5403 (0.7162) Acc G: 43.350%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3556 (0.4217) Acc D Real: 72.871%
Loss D Fake: 0.8814 (0.7161) Acc D Fake: 54.794%
Loss D: 1.237
Loss G: 0.5396 (0.7150) Acc G: 43.695%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3659 (0.4213) Acc D Real: 73.034%
Loss D Fake: 0.8823 (0.7172) Acc D Fake: 54.464%
Loss D: 1.248
Loss G: 0.5391 (0.7138) Acc G: 44.035%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3917 (0.4211) Acc D Real: 73.193%
Loss D Fake: 0.8830 (0.7183) Acc D Fake: 54.139%
Loss D: 1.275
Loss G: 0.5386 (0.7126) Acc G: 44.370%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3549 (0.4207) Acc D Real: 73.359%
Loss D Fake: 0.8836 (0.7195) Acc D Fake: 53.818%
Loss D: 1.238
Loss G: 0.5383 (0.7114) Acc G: 44.701%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3904 (0.4205) Acc D Real: 73.513%
Loss D Fake: 0.8840 (0.7206) Acc D Fake: 53.502%
Loss D: 1.274
Loss G: 0.5380 (0.7102) Acc G: 45.027%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3881 (0.4203) Acc D Real: 73.666%
Loss D Fake: 0.8845 (0.7216) Acc D Fake: 53.190%
Loss D: 1.273
Loss G: 0.5377 (0.7091) Acc G: 45.349%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3956 (0.4201) Acc D Real: 73.816%
Loss D Fake: 0.8849 (0.7227) Acc D Fake: 52.881%
Loss D: 1.280
Loss G: 0.5375 (0.7080) Acc G: 45.667%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3898 (0.4199) Acc D Real: 73.965%
Loss D Fake: 0.8854 (0.7238) Acc D Fake: 52.577%
Loss D: 1.275
Loss G: 0.5371 (0.7068) Acc G: 45.981%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.4170 (0.4199) Acc D Real: 74.109%
Loss D Fake: 0.8860 (0.7249) Acc D Fake: 52.277%
Loss D: 1.303
Loss G: 0.5366 (0.7057) Acc G: 46.290%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.4280 (0.4199) Acc D Real: 74.257%
Loss D Fake: 0.8869 (0.7259) Acc D Fake: 51.981%
Loss D: 1.315
Loss G: 0.5358 (0.7046) Acc G: 46.596%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3848 (0.4197) Acc D Real: 74.403%
Loss D Fake: 0.8882 (0.7270) Acc D Fake: 51.689%
Loss D: 1.273
Loss G: 0.5351 (0.7035) Acc G: 46.897%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4164 (0.4197) Acc D Real: 74.542%
Loss D Fake: 0.8893 (0.7280) Acc D Fake: 51.400%
Loss D: 1.306
Loss G: 0.5342 (0.7024) Acc G: 47.195%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3895 (0.4195) Acc D Real: 74.680%
Loss D Fake: 0.8906 (0.7290) Acc D Fake: 51.115%
Loss D: 1.280
Loss G: 0.5335 (0.7014) Acc G: 47.489%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3713 (0.4192) Acc D Real: 74.817%
Loss D Fake: 0.8917 (0.7301) Acc D Fake: 50.834%
Loss D: 1.263
Loss G: 0.5328 (0.7003) Acc G: 47.779%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3744 (0.4189) Acc D Real: 74.953%
Loss D Fake: 0.8926 (0.7311) Acc D Fake: 50.556%
Loss D: 1.267
Loss G: 0.5323 (0.6992) Acc G: 48.065%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3856 (0.4187) Acc D Real: 75.089%
Loss D Fake: 0.8932 (0.7321) Acc D Fake: 50.282%
Loss D: 1.279
Loss G: 0.5319 (0.6982) Acc G: 48.348%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3868 (0.4185) Acc D Real: 75.221%
Loss D Fake: 0.8939 (0.7331) Acc D Fake: 50.011%
Loss D: 1.281
Loss G: 0.5315 (0.6972) Acc G: 48.628%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3587 (0.4181) Acc D Real: 75.355%
Loss D Fake: 0.8944 (0.7341) Acc D Fake: 49.743%
Loss D: 1.253
Loss G: 0.5313 (0.6961) Acc G: 48.904%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4137 (0.4181) Acc D Real: 75.483%
Loss D Fake: 0.8947 (0.7351) Acc D Fake: 49.479%
Loss D: 1.308
Loss G: 0.5310 (0.6951) Acc G: 49.176%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3705 (0.4178) Acc D Real: 75.613%
Loss D Fake: 0.8952 (0.7361) Acc D Fake: 49.218%
Loss D: 1.266
Loss G: 0.5308 (0.6941) Acc G: 49.446%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.3721 (0.4175) Acc D Real: 75.741%
Loss D Fake: 0.8955 (0.7370) Acc D Fake: 48.960%
Loss D: 1.268
Loss G: 0.5306 (0.6931) Acc G: 49.711%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3597 (0.4172) Acc D Real: 75.868%
Loss D Fake: 0.8956 (0.7380) Acc D Fake: 48.705%
Loss D: 1.255
Loss G: 0.5306 (0.6922) Acc G: 49.974%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.3842 (0.4170) Acc D Real: 75.992%
Loss D Fake: 0.8955 (0.7389) Acc D Fake: 48.454%
Loss D: 1.280
Loss G: 0.5307 (0.6912) Acc G: 50.234%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4091 (0.4170) Acc D Real: 76.114%
Loss D Fake: 0.8956 (0.7399) Acc D Fake: 48.205%
Loss D: 1.305
Loss G: 0.5305 (0.6902) Acc G: 50.490%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3107 (0.4163) Acc D Real: 76.237%
Loss D Fake: 0.8957 (0.7408) Acc D Fake: 47.959%
Loss D: 1.206
Loss G: 0.5307 (0.6893) Acc G: 50.744%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3318 (0.4158) Acc D Real: 76.358%
Loss D Fake: 0.8951 (0.7417) Acc D Fake: 47.716%
Loss D: 1.227
Loss G: 0.5313 (0.6884) Acc G: 50.994%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3732 (0.4156) Acc D Real: 76.479%
Loss D Fake: 0.8941 (0.7426) Acc D Fake: 47.476%
Loss D: 1.267
Loss G: 0.5319 (0.6874) Acc G: 51.242%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3848 (0.4154) Acc D Real: 76.597%
Loss D Fake: 0.8933 (0.7435) Acc D Fake: 47.239%
Loss D: 1.278
Loss G: 0.5323 (0.6865) Acc G: 51.487%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3996 (0.4153) Acc D Real: 76.713%
Loss D Fake: 0.8927 (0.7443) Acc D Fake: 47.004%
Loss D: 1.292
Loss G: 0.5326 (0.6857) Acc G: 51.729%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.4125 (0.4153) Acc D Real: 76.824%
Loss D Fake: 0.8925 (0.7452) Acc D Fake: 46.773%
Loss D: 1.305
Loss G: 0.5326 (0.6848) Acc G: 51.968%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3838 (0.4151) Acc D Real: 76.939%
Loss D Fake: 0.8926 (0.7460) Acc D Fake: 46.543%
Loss D: 1.276
Loss G: 0.5326 (0.6839) Acc G: 52.204%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3351 (0.4147) Acc D Real: 77.053%
Loss D Fake: 0.8925 (0.7468) Acc D Fake: 46.317%
Loss D: 1.228
Loss G: 0.5328 (0.6830) Acc G: 52.438%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.3755 (0.4144) Acc D Real: 77.163%
Loss D Fake: 0.8921 (0.7477) Acc D Fake: 46.093%
Loss D: 1.268
Loss G: 0.5331 (0.6822) Acc G: 52.669%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.3466 (0.4141) Acc D Real: 77.272%
Loss D Fake: 0.8916 (0.7485) Acc D Fake: 45.871%
Loss D: 1.238
Loss G: 0.5335 (0.6814) Acc G: 52.889%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3908 (0.4139) Acc D Real: 77.378%
Loss D Fake: 0.8909 (0.7493) Acc D Fake: 45.662%
Loss D: 1.282
Loss G: 0.5339 (0.6805) Acc G: 53.106%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.4278 (0.4140) Acc D Real: 77.483%
Loss D Fake: 0.8905 (0.7501) Acc D Fake: 45.454%
Loss D: 1.318
Loss G: 0.5340 (0.6797) Acc G: 53.320%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3818 (0.4138) Acc D Real: 77.588%
Loss D Fake: 0.8905 (0.7508) Acc D Fake: 45.249%
Loss D: 1.272
Loss G: 0.5339 (0.6789) Acc G: 53.532%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3974 (0.4137) Acc D Real: 77.693%
Loss D Fake: 0.8907 (0.7516) Acc D Fake: 45.046%
Loss D: 1.288
Loss G: 0.5338 (0.6781) Acc G: 53.741%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.3787 (0.4135) Acc D Real: 77.799%
Loss D Fake: 0.8910 (0.7524) Acc D Fake: 44.846%
Loss D: 1.270
Loss G: 0.5336 (0.6773) Acc G: 53.949%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3509 (0.4132) Acc D Real: 77.902%
Loss D Fake: 0.8912 (0.7531) Acc D Fake: 44.647%
Loss D: 1.242
Loss G: 0.5336 (0.6765) Acc G: 54.154%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.3450 (0.4128) Acc D Real: 78.004%
Loss D Fake: 0.8909 (0.7539) Acc D Fake: 44.451%
Loss D: 1.236
Loss G: 0.5339 (0.6758) Acc G: 54.356%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3430 (0.4125) Acc D Real: 78.106%
Loss D Fake: 0.8903 (0.7546) Acc D Fake: 44.257%
Loss D: 1.233
Loss G: 0.5344 (0.6750) Acc G: 54.557%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.4614 (0.4127) Acc D Real: 78.203%
Loss D Fake: 0.8898 (0.7553) Acc D Fake: 44.065%
Loss D: 1.351
Loss G: 0.5344 (0.6743) Acc G: 54.755%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3722 (0.4125) Acc D Real: 78.302%
Loss D Fake: 0.8900 (0.7560) Acc D Fake: 43.875%
Loss D: 1.262
Loss G: 0.5343 (0.6735) Acc G: 54.952%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.3878 (0.4124) Acc D Real: 78.398%
Loss D Fake: 0.8901 (0.7567) Acc D Fake: 43.687%
Loss D: 1.278
Loss G: 0.5342 (0.6728) Acc G: 55.146%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3191 (0.4119) Acc D Real: 78.495%
Loss D Fake: 0.8901 (0.7574) Acc D Fake: 43.501%
Loss D: 1.209
Loss G: 0.5344 (0.6721) Acc G: 55.338%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.3729 (0.4117) Acc D Real: 78.589%
Loss D Fake: 0.8896 (0.7581) Acc D Fake: 43.316%
Loss D: 1.262
Loss G: 0.5348 (0.6713) Acc G: 55.528%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.3781 (0.4115) Acc D Real: 78.679%
Loss D Fake: 0.8890 (0.7588) Acc D Fake: 43.134%
Loss D: 1.267
Loss G: 0.5352 (0.6706) Acc G: 55.717%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3778 (0.4113) Acc D Real: 78.770%
Loss D Fake: 0.8884 (0.7595) Acc D Fake: 42.954%
Loss D: 1.266
Loss G: 0.5356 (0.6699) Acc G: 55.903%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.3981 (0.4113) Acc D Real: 78.859%
Loss D Fake: 0.8878 (0.7602) Acc D Fake: 42.775%
Loss D: 1.286
Loss G: 0.5358 (0.6692) Acc G: 56.087%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3312 (0.4109) Acc D Real: 78.952%
Loss D Fake: 0.8874 (0.7608) Acc D Fake: 42.599%
Loss D: 1.219
Loss G: 0.5362 (0.6686) Acc G: 56.270%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.3758 (0.4107) Acc D Real: 79.042%
Loss D Fake: 0.8867 (0.7614) Acc D Fake: 42.424%
Loss D: 1.262
Loss G: 0.5367 (0.6679) Acc G: 56.450%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4051 (0.4106) Acc D Real: 79.129%
Loss D Fake: 0.8861 (0.7621) Acc D Fake: 42.251%
Loss D: 1.291
Loss G: 0.5370 (0.6672) Acc G: 56.629%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.3839 (0.4105) Acc D Real: 79.215%
Loss D Fake: 0.8857 (0.7627) Acc D Fake: 42.080%
Loss D: 1.270
Loss G: 0.5372 (0.6666) Acc G: 56.806%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.4395 (0.4107) Acc D Real: 79.298%
Loss D Fake: 0.8856 (0.7633) Acc D Fake: 41.910%
Loss D: 1.325
Loss G: 0.5370 (0.6659) Acc G: 56.981%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.4014 (0.4106) Acc D Real: 79.383%
Loss D Fake: 0.8861 (0.7639) Acc D Fake: 41.742%
Loss D: 1.287
Loss G: 0.5367 (0.6653) Acc G: 57.155%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3847 (0.4105) Acc D Real: 79.469%
Loss D Fake: 0.8865 (0.7645) Acc D Fake: 41.576%
Loss D: 1.271
Loss G: 0.5364 (0.6646) Acc G: 57.326%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.3677 (0.4103) Acc D Real: 79.554%
Loss D Fake: 0.8869 (0.7652) Acc D Fake: 41.411%
Loss D: 1.255
Loss G: 0.5362 (0.6640) Acc G: 57.496%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.3441 (0.4099) Acc D Real: 79.637%
Loss D Fake: 0.8870 (0.7658) Acc D Fake: 41.248%
Loss D: 1.231
Loss G: 0.5363 (0.6634) Acc G: 57.665%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3825 (0.4098) Acc D Real: 79.721%
Loss D Fake: 0.8869 (0.7663) Acc D Fake: 41.087%
Loss D: 1.269
Loss G: 0.5364 (0.6627) Acc G: 57.831%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.4136 (0.4098) Acc D Real: 79.800%
Loss D Fake: 0.8868 (0.7669) Acc D Fake: 40.927%
Loss D: 1.300
Loss G: 0.5363 (0.6621) Acc G: 57.996%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3333 (0.4095) Acc D Real: 79.882%
Loss D Fake: 0.8869 (0.7675) Acc D Fake: 40.769%
Loss D: 1.220
Loss G: 0.5365 (0.6615) Acc G: 58.160%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4049 (0.4094) Acc D Real: 79.962%
Loss D Fake: 0.8865 (0.7681) Acc D Fake: 40.612%
Loss D: 1.291
Loss G: 0.5367 (0.6609) Acc G: 58.322%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.3461 (0.4091) Acc D Real: 80.043%
Loss D Fake: 0.8861 (0.7687) Acc D Fake: 40.457%
Loss D: 1.232
Loss G: 0.5370 (0.6603) Acc G: 58.482%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.3160 (0.4087) Acc D Real: 80.126%
Loss D Fake: 0.8853 (0.7692) Acc D Fake: 40.304%
Loss D: 1.201
Loss G: 0.5377 (0.6597) Acc G: 58.641%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.4161 (0.4087) Acc D Real: 80.202%
Loss D Fake: 0.8842 (0.7698) Acc D Fake: 40.151%
Loss D: 1.300
Loss G: 0.5383 (0.6591) Acc G: 58.798%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3724 (0.4085) Acc D Real: 80.278%
Loss D Fake: 0.8836 (0.7703) Acc D Fake: 40.000%
Loss D: 1.256
Loss G: 0.5387 (0.6586) Acc G: 58.954%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3431 (0.4082) Acc D Real: 80.354%
Loss D Fake: 0.8828 (0.7708) Acc D Fake: 39.851%
Loss D: 1.226
Loss G: 0.5394 (0.6580) Acc G: 59.108%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3590 (0.4080) Acc D Real: 80.430%
Loss D Fake: 0.8816 (0.7714) Acc D Fake: 39.703%
Loss D: 1.241
Loss G: 0.5402 (0.6575) Acc G: 59.261%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3853 (0.4079) Acc D Real: 80.506%
Loss D Fake: 0.8804 (0.7719) Acc D Fake: 39.557%
Loss D: 1.266
Loss G: 0.5409 (0.6569) Acc G: 59.412%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.4838 (0.4083) Acc D Real: 80.573%
Loss D Fake: 0.8799 (0.7724) Acc D Fake: 39.411%
Loss D: 1.364
Loss G: 0.5408 (0.6564) Acc G: 59.562%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3720 (0.4081) Acc D Real: 80.648%
Loss D Fake: 0.8802 (0.7729) Acc D Fake: 39.267%
Loss D: 1.252
Loss G: 0.5407 (0.6558) Acc G: 59.711%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3754 (0.4079) Acc D Real: 80.719%
Loss D Fake: 0.8804 (0.7734) Acc D Fake: 39.125%
Loss D: 1.256
Loss G: 0.5406 (0.6553) Acc G: 59.858%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3965 (0.4079) Acc D Real: 80.793%
Loss D Fake: 0.8805 (0.7739) Acc D Fake: 38.984%
Loss D: 1.277
Loss G: 0.5404 (0.6548) Acc G: 60.004%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.3212 (0.4075) Acc D Real: 80.866%
Loss D Fake: 0.8805 (0.7743) Acc D Fake: 38.844%
Loss D: 1.202
Loss G: 0.5407 (0.6543) Acc G: 60.149%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3699 (0.4073) Acc D Real: 80.938%
Loss D Fake: 0.8799 (0.7748) Acc D Fake: 38.705%
Loss D: 1.250
Loss G: 0.5411 (0.6537) Acc G: 60.292%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.3634 (0.4071) Acc D Real: 81.009%
Loss D Fake: 0.8793 (0.7753) Acc D Fake: 38.568%
Loss D: 1.243
Loss G: 0.5416 (0.6532) Acc G: 60.434%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3971 (0.4071) Acc D Real: 81.076%
Loss D Fake: 0.8786 (0.7758) Acc D Fake: 38.431%
Loss D: 1.276
Loss G: 0.5420 (0.6527) Acc G: 60.575%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3916 (0.4070) Acc D Real: 81.142%
Loss D Fake: 0.8780 (0.7762) Acc D Fake: 38.296%
Loss D: 1.270
Loss G: 0.5423 (0.6522) Acc G: 60.714%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.4301 (0.4071) Acc D Real: 81.208%
Loss D Fake: 0.8778 (0.7767) Acc D Fake: 38.163%
Loss D: 1.308
Loss G: 0.5423 (0.6518) Acc G: 60.852%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3874 (0.4070) Acc D Real: 81.275%
Loss D Fake: 0.8779 (0.7771) Acc D Fake: 38.030%
Loss D: 1.265
Loss G: 0.5422 (0.6513) Acc G: 60.989%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.3450 (0.4067) Acc D Real: 81.293%
Loss D Fake: 0.8778 (0.7776) Acc D Fake: 37.997%
Loss D: 1.223
Loss G: 0.5424 (0.6508) Acc G: 61.022%
LR: 2.000e-04
Epoch: 11/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3421 (0.3913) Acc D Real: 96.328%
Loss D Fake: 0.8777 (0.8776) Acc D Fake: 10.000%
Loss D: 1.220
Loss G: 0.5424 (0.5424) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3942 (0.3922) Acc D Real: 96.424%
Loss D Fake: 0.8775 (0.8776) Acc D Fake: 10.000%
Loss D: 1.272
Loss G: 0.5424 (0.5424) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.3620 (0.3847) Acc D Real: 96.406%
Loss D Fake: 0.8775 (0.8776) Acc D Fake: 10.000%
Loss D: 1.239
Loss G: 0.5426 (0.5425) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3644 (0.3806) Acc D Real: 96.469%
Loss D Fake: 0.8771 (0.8775) Acc D Fake: 10.000%
Loss D: 1.241
Loss G: 0.5429 (0.5425) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.3462 (0.3749) Acc D Real: 96.528%
Loss D Fake: 0.8765 (0.8773) Acc D Fake: 10.000%
Loss D: 1.223
Loss G: 0.5434 (0.5427) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.4567 (0.3866) Acc D Real: 96.391%
Loss D Fake: 0.8759 (0.8771) Acc D Fake: 10.000%
Loss D: 1.333
Loss G: 0.5435 (0.5428) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3775 (0.3854) Acc D Real: 96.374%
Loss D Fake: 0.8759 (0.8770) Acc D Fake: 10.000%
Loss D: 1.253
Loss G: 0.5435 (0.5429) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2873 (0.3745) Acc D Real: 96.400%
Loss D Fake: 0.8755 (0.8768) Acc D Fake: 10.000%
Loss D: 1.163
Loss G: 0.5442 (0.5430) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3651 (0.3736) Acc D Real: 96.328%
Loss D Fake: 0.8743 (0.8765) Acc D Fake: 10.000%
Loss D: 1.239
Loss G: 0.5451 (0.5432) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.4682 (0.3822) Acc D Real: 96.312%
Loss D Fake: 0.8733 (0.8763) Acc D Fake: 10.000%
Loss D: 1.341
Loss G: 0.5453 (0.5434) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.3195 (0.3770) Acc D Real: 96.354%
Loss D Fake: 0.8731 (0.8760) Acc D Fake: 10.000%
Loss D: 1.193
Loss G: 0.5457 (0.5436) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3775 (0.3770) Acc D Real: 96.346%
Loss D Fake: 0.8723 (0.8757) Acc D Fake: 10.000%
Loss D: 1.250
Loss G: 0.5462 (0.5438) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.4584 (0.3828) Acc D Real: 96.302%
Loss D Fake: 0.8718 (0.8754) Acc D Fake: 10.000%
Loss D: 1.330
Loss G: 0.5463 (0.5440) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3547 (0.3809) Acc D Real: 96.316%
Loss D Fake: 0.8718 (0.8752) Acc D Fake: 10.000%
Loss D: 1.227
Loss G: 0.5463 (0.5442) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.4042 (0.3824) Acc D Real: 96.289%
Loss D Fake: 0.8717 (0.8750) Acc D Fake: 10.000%
Loss D: 1.276
Loss G: 0.5463 (0.5443) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3600 (0.3811) Acc D Real: 96.308%
Loss D Fake: 0.8717 (0.8748) Acc D Fake: 10.000%
Loss D: 1.232
Loss G: 0.5464 (0.5444) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.3780 (0.3809) Acc D Real: 96.319%
Loss D Fake: 0.8715 (0.8746) Acc D Fake: 10.000%
Loss D: 1.250
Loss G: 0.5466 (0.5445) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.4013 (0.3820) Acc D Real: 96.305%
Loss D Fake: 0.8712 (0.8744) Acc D Fake: 10.000%
Loss D: 1.273
Loss G: 0.5467 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.4319 (0.3845) Acc D Real: 96.302%
Loss D Fake: 0.8713 (0.8743) Acc D Fake: 10.000%
Loss D: 1.303
Loss G: 0.5464 (0.5447) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3819 (0.3844) Acc D Real: 96.285%
Loss D Fake: 0.8718 (0.8742) Acc D Fake: 10.000%
Loss D: 1.254
Loss G: 0.5462 (0.5448) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.4299 (0.3864) Acc D Real: 96.262%
Loss D Fake: 0.8721 (0.8741) Acc D Fake: 10.000%
Loss D: 1.302
Loss G: 0.5458 (0.5448) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3546 (0.3850) Acc D Real: 96.252%
Loss D Fake: 0.8727 (0.8740) Acc D Fake: 10.000%
Loss D: 1.227
Loss G: 0.5456 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4165 (0.3864) Acc D Real: 96.235%
Loss D Fake: 0.8729 (0.8740) Acc D Fake: 10.000%
Loss D: 1.289
Loss G: 0.5453 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.3721 (0.3858) Acc D Real: 96.248%
Loss D Fake: 0.8734 (0.8739) Acc D Fake: 10.000%
Loss D: 1.245
Loss G: 0.5451 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.4030 (0.3864) Acc D Real: 96.242%
Loss D Fake: 0.8736 (0.8739) Acc D Fake: 10.000%
Loss D: 1.277
Loss G: 0.5449 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3756 (0.3860) Acc D Real: 96.242%
Loss D Fake: 0.8739 (0.8739) Acc D Fake: 10.000%
Loss D: 1.250
Loss G: 0.5448 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3575 (0.3850) Acc D Real: 96.231%
Loss D Fake: 0.8739 (0.8739) Acc D Fake: 10.000%
Loss D: 1.231
Loss G: 0.5449 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3538 (0.3839) Acc D Real: 96.227%
Loss D Fake: 0.8734 (0.8739) Acc D Fake: 10.000%
Loss D: 1.227
Loss G: 0.5454 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.3892 (0.3841) Acc D Real: 96.231%
Loss D Fake: 0.8726 (0.8739) Acc D Fake: 10.000%
Loss D: 1.262
Loss G: 0.5459 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.4418 (0.3860) Acc D Real: 96.200%
Loss D Fake: 0.8722 (0.8738) Acc D Fake: 10.000%
Loss D: 1.314
Loss G: 0.5459 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.4088 (0.3867) Acc D Real: 96.206%
Loss D Fake: 0.8724 (0.8738) Acc D Fake: 10.000%
Loss D: 1.281
Loss G: 0.5456 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3895 (0.3868) Acc D Real: 96.215%
Loss D Fake: 0.8728 (0.8737) Acc D Fake: 10.000%
Loss D: 1.262
Loss G: 0.5454 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.4640 (0.3891) Acc D Real: 96.210%
Loss D Fake: 0.8734 (0.8737) Acc D Fake: 10.000%
Loss D: 1.337
Loss G: 0.5447 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.4047 (0.3895) Acc D Real: 96.192%
Loss D Fake: 0.8746 (0.8737) Acc D Fake: 10.000%
Loss D: 1.279
Loss G: 0.5439 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.4100 (0.3901) Acc D Real: 96.188%
Loss D Fake: 0.8758 (0.8738) Acc D Fake: 10.000%
Loss D: 1.286
Loss G: 0.5430 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3964 (0.3902) Acc D Real: 96.189%
Loss D Fake: 0.8769 (0.8739) Acc D Fake: 10.000%
Loss D: 1.273
Loss G: 0.5423 (0.5448) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.3468 (0.3891) Acc D Real: 96.208%
Loss D Fake: 0.8778 (0.8740) Acc D Fake: 10.000%
Loss D: 1.225
Loss G: 0.5419 (0.5448) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3455 (0.3880) Acc D Real: 96.217%
Loss D Fake: 0.8780 (0.8741) Acc D Fake: 10.000%
Loss D: 1.224
Loss G: 0.5420 (0.5447) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3557 (0.3872) Acc D Real: 96.223%
Loss D Fake: 0.8776 (0.8742) Acc D Fake: 10.000%
Loss D: 1.233
Loss G: 0.5424 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3559 (0.3864) Acc D Real: 96.239%
Loss D Fake: 0.8768 (0.8742) Acc D Fake: 10.000%
Loss D: 1.233
Loss G: 0.5430 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3728 (0.3861) Acc D Real: 96.238%
Loss D Fake: 0.8759 (0.8743) Acc D Fake: 10.000%
Loss D: 1.249
Loss G: 0.5437 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.4002 (0.3864) Acc D Real: 96.227%
Loss D Fake: 0.8749 (0.8743) Acc D Fake: 10.000%
Loss D: 1.275
Loss G: 0.5442 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.4042 (0.3868) Acc D Real: 96.222%
Loss D Fake: 0.8743 (0.8743) Acc D Fake: 10.000%
Loss D: 1.278
Loss G: 0.5446 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3589 (0.3862) Acc D Real: 96.223%
Loss D Fake: 0.8737 (0.8743) Acc D Fake: 10.000%
Loss D: 1.233
Loss G: 0.5451 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.4098 (0.3867) Acc D Real: 96.232%
Loss D Fake: 0.8731 (0.8743) Acc D Fake: 10.000%
Loss D: 1.283
Loss G: 0.5454 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.3677 (0.3863) Acc D Real: 96.240%
Loss D Fake: 0.8727 (0.8742) Acc D Fake: 10.000%
Loss D: 1.240
Loss G: 0.5457 (0.5446) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3733 (0.3860) Acc D Real: 96.237%
Loss D Fake: 0.8722 (0.8742) Acc D Fake: 10.000%
Loss D: 1.245
Loss G: 0.5461 (0.5447) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.4026 (0.3864) Acc D Real: 96.224%
Loss D Fake: 0.8717 (0.8741) Acc D Fake: 10.000%
Loss D: 1.274
Loss G: 0.5463 (0.5447) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3328 (0.3853) Acc D Real: 96.233%
Loss D Fake: 0.8712 (0.8741) Acc D Fake: 10.000%
Loss D: 1.204
Loss G: 0.5469 (0.5447) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3424 (0.3845) Acc D Real: 96.242%
Loss D Fake: 0.8702 (0.8740) Acc D Fake: 10.000%
Loss D: 1.213
Loss G: 0.5477 (0.5448) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3706 (0.3842) Acc D Real: 96.252%
Loss D Fake: 0.8688 (0.8739) Acc D Fake: 10.000%
Loss D: 1.239
Loss G: 0.5486 (0.5449) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3213 (0.3830) Acc D Real: 96.258%
Loss D Fake: 0.8674 (0.8738) Acc D Fake: 10.000%
Loss D: 1.189
Loss G: 0.5499 (0.5450) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3879 (0.3831) Acc D Real: 96.255%
Loss D Fake: 0.8655 (0.8736) Acc D Fake: 10.000%
Loss D: 1.253
Loss G: 0.5510 (0.5451) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4450 (0.3842) Acc D Real: 96.245%
Loss D Fake: 0.8642 (0.8734) Acc D Fake: 10.000%
Loss D: 1.309
Loss G: 0.5516 (0.5452) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3483 (0.3836) Acc D Real: 96.248%
Loss D Fake: 0.8635 (0.8733) Acc D Fake: 10.000%
Loss D: 1.212
Loss G: 0.5522 (0.5453) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.4026 (0.3839) Acc D Real: 96.237%
Loss D Fake: 0.8627 (0.8731) Acc D Fake: 10.000%
Loss D: 1.265
Loss G: 0.5526 (0.5454) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3717 (0.3837) Acc D Real: 96.245%
Loss D Fake: 0.8622 (0.8729) Acc D Fake: 10.000%
Loss D: 1.234
Loss G: 0.5530 (0.5456) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3503 (0.3831) Acc D Real: 96.249%
Loss D Fake: 0.8615 (0.8727) Acc D Fake: 10.000%
Loss D: 1.212
Loss G: 0.5536 (0.5457) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3938 (0.3833) Acc D Real: 96.241%
Loss D Fake: 0.8607 (0.8725) Acc D Fake: 10.000%
Loss D: 1.255
Loss G: 0.5540 (0.5458) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3784 (0.3832) Acc D Real: 96.241%
Loss D Fake: 0.8601 (0.8723) Acc D Fake: 10.000%
Loss D: 1.239
Loss G: 0.5545 (0.5460) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3662 (0.3830) Acc D Real: 96.255%
Loss D Fake: 0.8594 (0.8721) Acc D Fake: 10.000%
Loss D: 1.226
Loss G: 0.5550 (0.5461) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3726 (0.3828) Acc D Real: 96.257%
Loss D Fake: 0.8587 (0.8719) Acc D Fake: 10.000%
Loss D: 1.231
Loss G: 0.5555 (0.5463) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3268 (0.3819) Acc D Real: 96.261%
Loss D Fake: 0.8578 (0.8717) Acc D Fake: 10.000%
Loss D: 1.185
Loss G: 0.5564 (0.5464) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3821 (0.3819) Acc D Real: 96.261%
Loss D Fake: 0.8565 (0.8714) Acc D Fake: 10.000%
Loss D: 1.239
Loss G: 0.5573 (0.5466) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.4163 (0.3825) Acc D Real: 96.249%
Loss D Fake: 0.8554 (0.8712) Acc D Fake: 10.000%
Loss D: 1.272
Loss G: 0.5578 (0.5468) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.3378 (0.3818) Acc D Real: 96.254%
Loss D Fake: 0.8547 (0.8709) Acc D Fake: 10.000%
Loss D: 1.192
Loss G: 0.5585 (0.5470) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4045 (0.3821) Acc D Real: 96.250%
Loss D Fake: 0.8537 (0.8707) Acc D Fake: 10.000%
Loss D: 1.258
Loss G: 0.5590 (0.5471) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4180 (0.3826) Acc D Real: 96.251%
Loss D Fake: 0.8532 (0.8704) Acc D Fake: 10.000%
Loss D: 1.271
Loss G: 0.5591 (0.5473) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3385 (0.3820) Acc D Real: 96.258%
Loss D Fake: 0.8531 (0.8702) Acc D Fake: 10.000%
Loss D: 1.192
Loss G: 0.5595 (0.5475) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.3636 (0.3817) Acc D Real: 96.251%
Loss D Fake: 0.8523 (0.8699) Acc D Fake: 10.000%
Loss D: 1.216
Loss G: 0.5601 (0.5477) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3658 (0.3815) Acc D Real: 96.254%
Loss D Fake: 0.8514 (0.8697) Acc D Fake: 10.000%
Loss D: 1.217
Loss G: 0.5608 (0.5478) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3435 (0.3810) Acc D Real: 96.254%
Loss D Fake: 0.8503 (0.8694) Acc D Fake: 10.000%
Loss D: 1.194
Loss G: 0.5618 (0.5480) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3505 (0.3806) Acc D Real: 96.260%
Loss D Fake: 0.8488 (0.8691) Acc D Fake: 10.000%
Loss D: 1.199
Loss G: 0.5629 (0.5482) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3751 (0.3805) Acc D Real: 96.262%
Loss D Fake: 0.8472 (0.8688) Acc D Fake: 10.000%
Loss D: 1.222
Loss G: 0.5640 (0.5484) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3907 (0.3807) Acc D Real: 96.262%
Loss D Fake: 0.8459 (0.8685) Acc D Fake: 10.000%
Loss D: 1.237
Loss G: 0.5648 (0.5487) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3633 (0.3804) Acc D Real: 96.268%
Loss D Fake: 0.8448 (0.8682) Acc D Fake: 10.000%
Loss D: 1.208
Loss G: 0.5656 (0.5489) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3900 (0.3806) Acc D Real: 96.267%
Loss D Fake: 0.8438 (0.8679) Acc D Fake: 10.000%
Loss D: 1.234
Loss G: 0.5662 (0.5491) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3125 (0.3797) Acc D Real: 96.270%
Loss D Fake: 0.8429 (0.8676) Acc D Fake: 10.000%
Loss D: 1.155
Loss G: 0.5672 (0.5493) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3470 (0.3793) Acc D Real: 96.274%
Loss D Fake: 0.8413 (0.8673) Acc D Fake: 10.000%
Loss D: 1.188
Loss G: 0.5685 (0.5496) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3892 (0.3794) Acc D Real: 96.269%
Loss D Fake: 0.8397 (0.8669) Acc D Fake: 10.000%
Loss D: 1.229
Loss G: 0.5695 (0.5498) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.4365 (0.3801) Acc D Real: 96.266%
Loss D Fake: 0.8388 (0.8666) Acc D Fake: 10.000%
Loss D: 1.275
Loss G: 0.5696 (0.5501) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3744 (0.3800) Acc D Real: 96.271%
Loss D Fake: 0.8388 (0.8663) Acc D Fake: 10.000%
Loss D: 1.213
Loss G: 0.5696 (0.5503) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3801 (0.3800) Acc D Real: 96.270%
Loss D Fake: 0.8389 (0.8659) Acc D Fake: 10.000%
Loss D: 1.219
Loss G: 0.5696 (0.5505) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3979 (0.3802) Acc D Real: 96.263%
Loss D Fake: 0.8390 (0.8656) Acc D Fake: 10.000%
Loss D: 1.237
Loss G: 0.5694 (0.5507) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.3595 (0.3800) Acc D Real: 96.258%
Loss D Fake: 0.8391 (0.8653) Acc D Fake: 10.000%
Loss D: 1.199
Loss G: 0.5695 (0.5510) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3346 (0.3795) Acc D Real: 96.269%
Loss D Fake: 0.8387 (0.8650) Acc D Fake: 10.000%
Loss D: 1.173
Loss G: 0.5701 (0.5512) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3711 (0.3794) Acc D Real: 96.266%
Loss D Fake: 0.8378 (0.8647) Acc D Fake: 10.000%
Loss D: 1.209
Loss G: 0.5708 (0.5514) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.2924 (0.3784) Acc D Real: 96.276%
Loss D Fake: 0.8365 (0.8644) Acc D Fake: 10.000%
Loss D: 1.129
Loss G: 0.5723 (0.5516) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3586 (0.3782) Acc D Real: 96.280%
Loss D Fake: 0.8341 (0.8640) Acc D Fake: 10.000%
Loss D: 1.193
Loss G: 0.5741 (0.5519) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3532 (0.3779) Acc D Real: 96.285%
Loss D Fake: 0.8317 (0.8637) Acc D Fake: 10.000%
Loss D: 1.185
Loss G: 0.5760 (0.5522) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3421 (0.3775) Acc D Real: 96.283%
Loss D Fake: 0.8292 (0.8633) Acc D Fake: 10.000%
Loss D: 1.171
Loss G: 0.5779 (0.5524) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3340 (0.3771) Acc D Real: 96.283%
Loss D Fake: 0.8264 (0.8629) Acc D Fake: 10.000%
Loss D: 1.160
Loss G: 0.5802 (0.5527) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3731 (0.3770) Acc D Real: 96.282%
Loss D Fake: 0.8235 (0.8625) Acc D Fake: 10.000%
Loss D: 1.197
Loss G: 0.5821 (0.5530) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3957 (0.3772) Acc D Real: 96.280%
Loss D Fake: 0.8214 (0.8621) Acc D Fake: 10.000%
Loss D: 1.217
Loss G: 0.5834 (0.5534) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4062 (0.3775) Acc D Real: 96.274%
Loss D Fake: 0.8202 (0.8616) Acc D Fake: 10.000%
Loss D: 1.226
Loss G: 0.5840 (0.5537) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.3700 (0.3774) Acc D Real: 96.274%
Loss D Fake: 0.8197 (0.8612) Acc D Fake: 10.000%
Loss D: 1.190
Loss G: 0.5844 (0.5540) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3254 (0.3769) Acc D Real: 96.276%
Loss D Fake: 0.8190 (0.8608) Acc D Fake: 10.000%
Loss D: 1.144
Loss G: 0.5853 (0.5543) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.4091 (0.3772) Acc D Real: 96.272%
Loss D Fake: 0.8179 (0.8603) Acc D Fake: 10.000%
Loss D: 1.227
Loss G: 0.5858 (0.5546) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.3450 (0.3769) Acc D Real: 96.276%
Loss D Fake: 0.8173 (0.8599) Acc D Fake: 10.000%
Loss D: 1.162
Loss G: 0.5866 (0.5550) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3842 (0.3770) Acc D Real: 96.267%
Loss D Fake: 0.8163 (0.8595) Acc D Fake: 10.000%
Loss D: 1.200
Loss G: 0.5872 (0.5553) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3885 (0.3771) Acc D Real: 96.260%
Loss D Fake: 0.8157 (0.8590) Acc D Fake: 10.000%
Loss D: 1.204
Loss G: 0.5876 (0.5556) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3829 (0.3771) Acc D Real: 96.256%
Loss D Fake: 0.8155 (0.8586) Acc D Fake: 10.000%
Loss D: 1.198
Loss G: 0.5877 (0.5559) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3912 (0.3773) Acc D Real: 96.251%
Loss D Fake: 0.8156 (0.8582) Acc D Fake: 10.000%
Loss D: 1.207
Loss G: 0.5874 (0.5562) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.4072 (0.3776) Acc D Real: 96.249%
Loss D Fake: 0.8163 (0.8578) Acc D Fake: 10.000%
Loss D: 1.224
Loss G: 0.5867 (0.5565) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3662 (0.3775) Acc D Real: 96.250%
Loss D Fake: 0.8173 (0.8574) Acc D Fake: 10.000%
Loss D: 1.183
Loss G: 0.5863 (0.5568) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.3558 (0.3773) Acc D Real: 96.252%
Loss D Fake: 0.8172 (0.8570) Acc D Fake: 10.000%
Loss D: 1.173
Loss G: 0.5871 (0.5571) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.3082 (0.3766) Acc D Real: 96.258%
Loss D Fake: 0.8151 (0.8567) Acc D Fake: 10.000%
Loss D: 1.123
Loss G: 0.5898 (0.5574) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3938 (0.3768) Acc D Real: 96.254%
Loss D Fake: 0.8115 (0.8562) Acc D Fake: 10.000%
Loss D: 1.205
Loss G: 0.5920 (0.5577) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.4221 (0.3772) Acc D Real: 96.253%
Loss D Fake: 0.8093 (0.8558) Acc D Fake: 10.000%
Loss D: 1.231
Loss G: 0.5933 (0.5580) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.4006 (0.3774) Acc D Real: 96.249%
Loss D Fake: 0.8080 (0.8554) Acc D Fake: 10.000%
Loss D: 1.209
Loss G: 0.5942 (0.5583) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3461 (0.3771) Acc D Real: 96.252%
Loss D Fake: 0.8067 (0.8549) Acc D Fake: 10.000%
Loss D: 1.153
Loss G: 0.5955 (0.5587) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3330 (0.3767) Acc D Real: 96.253%
Loss D Fake: 0.8046 (0.8545) Acc D Fake: 10.000%
Loss D: 1.138
Loss G: 0.5976 (0.5590) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3785 (0.3767) Acc D Real: 96.251%
Loss D Fake: 0.8020 (0.8540) Acc D Fake: 10.000%
Loss D: 1.180
Loss G: 0.5992 (0.5594) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3322 (0.3764) Acc D Real: 96.255%
Loss D Fake: 0.8001 (0.8536) Acc D Fake: 10.000%
Loss D: 1.132
Loss G: 0.6008 (0.5597) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.3798 (0.3764) Acc D Real: 96.253%
Loss D Fake: 0.7987 (0.8531) Acc D Fake: 10.000%
Loss D: 1.178
Loss G: 0.6013 (0.5601) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3980 (0.3766) Acc D Real: 96.251%
Loss D Fake: 0.7990 (0.8526) Acc D Fake: 10.000%
Loss D: 1.197
Loss G: 0.6005 (0.5604) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.4001 (0.3768) Acc D Real: 96.250%
Loss D Fake: 0.8009 (0.8522) Acc D Fake: 10.000%
Loss D: 1.201
Loss G: 0.5991 (0.5608) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3221 (0.3763) Acc D Real: 96.254%
Loss D Fake: 0.8017 (0.8518) Acc D Fake: 10.000%
Loss D: 1.124
Loss G: 0.6004 (0.5611) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3078 (0.3757) Acc D Real: 96.257%
Loss D Fake: 0.7980 (0.8513) Acc D Fake: 10.000%
Loss D: 1.106
Loss G: 0.6041 (0.5614) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.3663 (0.3757) Acc D Real: 96.254%
Loss D Fake: 0.7931 (0.8508) Acc D Fake: 10.000%
Loss D: 1.159
Loss G: 0.6074 (0.5618) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.3528 (0.3755) Acc D Real: 96.258%
Loss D Fake: 0.7897 (0.8503) Acc D Fake: 10.000%
Loss D: 1.142
Loss G: 0.6094 (0.5622) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3654 (0.3754) Acc D Real: 96.257%
Loss D Fake: 0.7883 (0.8498) Acc D Fake: 10.000%
Loss D: 1.154
Loss G: 0.6096 (0.5626) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3416 (0.3751) Acc D Real: 96.258%
Loss D Fake: 0.7889 (0.8493) Acc D Fake: 10.000%
Loss D: 1.131
Loss G: 0.6093 (0.5630) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.4203 (0.3755) Acc D Real: 96.251%
Loss D Fake: 0.7904 (0.8489) Acc D Fake: 10.000%
Loss D: 1.211
Loss G: 0.6070 (0.5633) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3673 (0.3754) Acc D Real: 96.248%
Loss D Fake: 0.7946 (0.8484) Acc D Fake: 10.000%
Loss D: 1.162
Loss G: 0.6050 (0.5637) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.4006 (0.3756) Acc D Real: 96.241%
Loss D Fake: 0.7962 (0.8480) Acc D Fake: 10.000%
Loss D: 1.197
Loss G: 0.6049 (0.5640) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3889 (0.3757) Acc D Real: 96.236%
Loss D Fake: 0.7940 (0.8476) Acc D Fake: 10.000%
Loss D: 1.183
Loss G: 0.6083 (0.5643) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3123 (0.3752) Acc D Real: 96.238%
Loss D Fake: 0.7883 (0.8472) Acc D Fake: 10.000%
Loss D: 1.101
Loss G: 0.6123 (0.5647) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3044 (0.3747) Acc D Real: 96.238%
Loss D Fake: 0.7834 (0.8467) Acc D Fake: 10.000%
Loss D: 1.088
Loss G: 0.6158 (0.5651) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3219 (0.3743) Acc D Real: 96.242%
Loss D Fake: 0.7794 (0.8461) Acc D Fake: 10.000%
Loss D: 1.101
Loss G: 0.6187 (0.5655) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3685 (0.3742) Acc D Real: 96.241%
Loss D Fake: 0.7770 (0.8456) Acc D Fake: 10.000%
Loss D: 1.146
Loss G: 0.6192 (0.5659) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3708 (0.3742) Acc D Real: 96.242%
Loss D Fake: 0.7780 (0.8451) Acc D Fake: 9.989%
Loss D: 1.149
Loss G: 0.6178 (0.5663) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3614 (0.3741) Acc D Real: 96.243%
Loss D Fake: 0.7813 (0.8446) Acc D Fake: 9.977%
Loss D: 1.143
Loss G: 0.6145 (0.5667) Acc G: 90.012%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3564 (0.3740) Acc D Real: 96.243%
Loss D Fake: 0.7872 (0.8442) Acc D Fake: 9.965%
Loss D: 1.144
Loss G: 0.6111 (0.5670) Acc G: 90.025%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3963 (0.3741) Acc D Real: 96.237%
Loss D Fake: 0.7914 (0.8438) Acc D Fake: 9.953%
Loss D: 1.188
Loss G: 0.6097 (0.5673) Acc G: 90.037%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3399 (0.3739) Acc D Real: 96.237%
Loss D Fake: 0.7906 (0.8434) Acc D Fake: 9.941%
Loss D: 1.131
Loss G: 0.6127 (0.5676) Acc G: 90.049%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.4086 (0.3741) Acc D Real: 96.234%
Loss D Fake: 0.7849 (0.8430) Acc D Fake: 9.929%
Loss D: 1.193
Loss G: 0.6161 (0.5680) Acc G: 90.060%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3368 (0.3739) Acc D Real: 96.236%
Loss D Fake: 0.7812 (0.8426) Acc D Fake: 9.918%
Loss D: 1.118
Loss G: 0.6179 (0.5683) Acc G: 90.072%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3855 (0.3740) Acc D Real: 96.229%
Loss D Fake: 0.7806 (0.8421) Acc D Fake: 9.907%
Loss D: 1.166
Loss G: 0.6166 (0.5687) Acc G: 90.083%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3317 (0.3737) Acc D Real: 96.235%
Loss D Fake: 0.7835 (0.8417) Acc D Fake: 9.895%
Loss D: 1.115
Loss G: 0.6152 (0.5690) Acc G: 90.095%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3287 (0.3733) Acc D Real: 96.238%
Loss D Fake: 0.7845 (0.8413) Acc D Fake: 9.884%
Loss D: 1.113
Loss G: 0.6158 (0.5694) Acc G: 90.106%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.3286 (0.3730) Acc D Real: 96.237%
Loss D Fake: 0.7835 (0.8409) Acc D Fake: 9.874%
Loss D: 1.112
Loss G: 0.6162 (0.5697) Acc G: 90.117%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3923 (0.3732) Acc D Real: 96.237%
Loss D Fake: 0.7844 (0.8405) Acc D Fake: 9.863%
Loss D: 1.177
Loss G: 0.6149 (0.5700) Acc G: 90.127%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3664 (0.3731) Acc D Real: 96.236%
Loss D Fake: 0.7869 (0.8401) Acc D Fake: 9.852%
Loss D: 1.153
Loss G: 0.6148 (0.5703) Acc G: 90.138%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3674 (0.3731) Acc D Real: 96.239%
Loss D Fake: 0.7868 (0.8398) Acc D Fake: 9.842%
Loss D: 1.154
Loss G: 0.6145 (0.5706) Acc G: 90.148%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3494 (0.3729) Acc D Real: 96.238%
Loss D Fake: 0.7872 (0.8394) Acc D Fake: 9.832%
Loss D: 1.137
Loss G: 0.6156 (0.5709) Acc G: 90.159%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3629 (0.3729) Acc D Real: 96.237%
Loss D Fake: 0.7845 (0.8390) Acc D Fake: 9.822%
Loss D: 1.147
Loss G: 0.6178 (0.5712) Acc G: 90.169%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3688 (0.3728) Acc D Real: 96.234%
Loss D Fake: 0.7813 (0.8387) Acc D Fake: 9.812%
Loss D: 1.150
Loss G: 0.6198 (0.5716) Acc G: 90.179%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3887 (0.3729) Acc D Real: 96.231%
Loss D Fake: 0.7787 (0.8383) Acc D Fake: 9.802%
Loss D: 1.167
Loss G: 0.6216 (0.5719) Acc G: 90.189%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3987 (0.3731) Acc D Real: 96.228%
Loss D Fake: 0.7772 (0.8379) Acc D Fake: 9.792%
Loss D: 1.176
Loss G: 0.6214 (0.5722) Acc G: 90.199%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2929 (0.3726) Acc D Real: 96.229%
Loss D Fake: 0.7778 (0.8375) Acc D Fake: 9.782%
Loss D: 1.071
Loss G: 0.6218 (0.5725) Acc G: 90.208%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3451 (0.3724) Acc D Real: 96.222%
Loss D Fake: 0.7789 (0.8371) Acc D Fake: 9.773%
Loss D: 1.124
Loss G: 0.6184 (0.5728) Acc G: 90.218%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3985 (0.3726) Acc D Real: 96.203%
Loss D Fake: 0.7885 (0.8368) Acc D Fake: 9.764%
Loss D: 1.187
Loss G: 0.6125 (0.5731) Acc G: 90.227%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.4415 (0.3730) Acc D Real: 96.194%
Loss D Fake: 0.7993 (0.8365) Acc D Fake: 9.754%
Loss D: 1.241
Loss G: 0.6114 (0.5733) Acc G: 90.237%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3465 (0.3728) Acc D Real: 96.182%
Loss D Fake: 0.7932 (0.8362) Acc D Fake: 9.745%
Loss D: 1.140
Loss G: 0.6143 (0.5736) Acc G: 90.246%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3398 (0.3726) Acc D Real: 96.171%
Loss D Fake: 0.7867 (0.8359) Acc D Fake: 9.736%
Loss D: 1.127
Loss G: 0.6184 (0.5739) Acc G: 90.255%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.2849 (0.3721) Acc D Real: 96.167%
Loss D Fake: 0.7814 (0.8356) Acc D Fake: 9.727%
Loss D: 1.066
Loss G: 0.6204 (0.5742) Acc G: 90.264%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.4476 (0.3725) Acc D Real: 96.139%
Loss D Fake: 0.7839 (0.8353) Acc D Fake: 9.719%
Loss D: 1.232
Loss G: 0.6133 (0.5744) Acc G: 90.273%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3849 (0.3726) Acc D Real: 96.105%
Loss D Fake: 0.8037 (0.8351) Acc D Fake: 9.710%
Loss D: 1.189
Loss G: 0.6117 (0.5747) Acc G: 90.281%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3819 (0.3727) Acc D Real: 96.093%
Loss D Fake: 0.7902 (0.8348) Acc D Fake: 9.701%
Loss D: 1.172
Loss G: 0.6174 (0.5749) Acc G: 90.290%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3972 (0.3728) Acc D Real: 96.060%
Loss D Fake: 0.7850 (0.8345) Acc D Fake: 9.693%
Loss D: 1.182
Loss G: 0.6159 (0.5752) Acc G: 90.298%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4453 (0.3733) Acc D Real: 95.968%
Loss D Fake: 0.7887 (0.8342) Acc D Fake: 9.685%
Loss D: 1.234
Loss G: 0.6162 (0.5754) Acc G: 90.307%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.4165 (0.3735) Acc D Real: 95.909%
Loss D Fake: 0.7875 (0.8339) Acc D Fake: 9.676%
Loss D: 1.204
Loss G: 0.6138 (0.5757) Acc G: 90.315%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.4145 (0.3738) Acc D Real: 95.842%
Loss D Fake: 0.7913 (0.8336) Acc D Fake: 9.668%
Loss D: 1.206
Loss G: 0.6137 (0.5759) Acc G: 90.323%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.4372 (0.3742) Acc D Real: 95.730%
Loss D Fake: 0.7872 (0.8334) Acc D Fake: 9.660%
Loss D: 1.224
Loss G: 0.6168 (0.5761) Acc G: 90.331%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2829 (0.3736) Acc D Real: 95.695%
Loss D Fake: 0.7812 (0.8331) Acc D Fake: 9.652%
Loss D: 1.064
Loss G: 0.6212 (0.5764) Acc G: 90.339%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4502 (0.3741) Acc D Real: 95.617%
Loss D Fake: 0.7784 (0.8327) Acc D Fake: 9.644%
Loss D: 1.229
Loss G: 0.6181 (0.5767) Acc G: 90.347%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.4029 (0.3743) Acc D Real: 95.581%
Loss D Fake: 0.7866 (0.8325) Acc D Fake: 9.637%
Loss D: 1.190
Loss G: 0.6115 (0.5769) Acc G: 90.355%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3890 (0.3743) Acc D Real: 95.528%
Loss D Fake: 0.7960 (0.8322) Acc D Fake: 9.629%
Loss D: 1.185
Loss G: 0.6082 (0.5771) Acc G: 90.363%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.4047 (0.3745) Acc D Real: 95.458%
Loss D Fake: 0.7945 (0.8320) Acc D Fake: 9.621%
Loss D: 1.199
Loss G: 0.6122 (0.5773) Acc G: 90.370%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3694 (0.3745) Acc D Real: 95.412%
Loss D Fake: 0.7863 (0.8318) Acc D Fake: 9.604%
Loss D: 1.156
Loss G: 0.6172 (0.5775) Acc G: 90.388%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3657 (0.3744) Acc D Real: 95.362%
Loss D Fake: 0.7800 (0.8315) Acc D Fake: 9.587%
Loss D: 1.146
Loss G: 0.6213 (0.5777) Acc G: 90.405%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.4353 (0.3748) Acc D Real: 95.299%
Loss D Fake: 0.7760 (0.8311) Acc D Fake: 9.570%
Loss D: 1.211
Loss G: 0.6226 (0.5780) Acc G: 90.421%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.4699 (0.3753) Acc D Real: 95.222%
Loss D Fake: 0.7761 (0.8308) Acc D Fake: 9.554%
Loss D: 1.246
Loss G: 0.6212 (0.5783) Acc G: 90.438%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.4070 (0.3755) Acc D Real: 95.143%
Loss D Fake: 0.7782 (0.8305) Acc D Fake: 9.537%
Loss D: 1.185
Loss G: 0.6200 (0.5785) Acc G: 90.455%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4700 (0.3760) Acc D Real: 95.104%
Loss D Fake: 0.7795 (0.8302) Acc D Fake: 9.521%
Loss D: 1.250
Loss G: 0.6188 (0.5787) Acc G: 90.471%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4215 (0.3763) Acc D Real: 95.060%
Loss D Fake: 0.7808 (0.8300) Acc D Fake: 9.505%
Loss D: 1.202
Loss G: 0.6176 (0.5789) Acc G: 90.487%
LR: 2.000e-04
Loss D Real: 0.4172 (0.3827) Acc D Real: 94.918%
Loss D Fake: 0.8167 (0.8273) Acc D Fake: 8.984%
Loss D: 1.234
Loss G: 0.5917 (0.5811) Acc G: 91.009%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.4211 (0.3829) Acc D Real: 94.919%
Loss D Fake: 0.8125 (0.8272) Acc D Fake: 8.974%
Loss D: 1.234
Loss G: 0.5937 (0.5812) Acc G: 91.020%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.4387 (0.3832) Acc D Real: 94.925%
Loss D Fake: 0.8100 (0.8271) Acc D Fake: 8.963%
Loss D: 1.249
Loss G: 0.5951 (0.5812) Acc G: 91.030%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.4955 (0.3837) Acc D Real: 94.926%
Loss D Fake: 0.8098 (0.8271) Acc D Fake: 8.953%
Loss D: 1.305
Loss G: 0.5929 (0.5813) Acc G: 91.041%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3912 (0.3837) Acc D Real: 94.930%
Loss D Fake: 0.8138 (0.8270) Acc D Fake: 8.943%
Loss D: 1.205
Loss G: 0.5904 (0.5813) Acc G: 91.051%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3175 (0.3834) Acc D Real: 94.938%
Loss D Fake: 0.8160 (0.8269) Acc D Fake: 8.932%
Loss D: 1.133
Loss G: 0.5900 (0.5814) Acc G: 91.061%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3684 (0.3834) Acc D Real: 94.947%
Loss D Fake: 0.8168 (0.8269) Acc D Fake: 8.922%
Loss D: 1.185
Loss G: 0.5879 (0.5814) Acc G: 91.071%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3998 (0.3834) Acc D Real: 94.952%
Loss D Fake: 0.8210 (0.8269) Acc D Fake: 8.912%
Loss D: 1.221
Loss G: 0.5856 (0.5814) Acc G: 91.081%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.4554 (0.3837) Acc D Real: 94.954%
Loss D Fake: 0.8228 (0.8269) Acc D Fake: 8.910%
Loss D: 1.278
Loss G: 0.5860 (0.5814) Acc G: 91.084%
LR: 2.000e-04
Epoch: 12/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.4277 (0.3905) Acc D Real: 96.380%
Loss D Fake: 0.8157 (0.8179) Acc D Fake: 6.667%
Loss D: 1.243
Loss G: 0.5915 (0.5901) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3646 (0.3819) Acc D Real: 96.406%
Loss D Fake: 0.8118 (0.8158) Acc D Fake: 6.667%
Loss D: 1.176
Loss G: 0.5944 (0.5916) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.4550 (0.4002) Acc D Real: 96.328%
Loss D Fake: 0.8084 (0.8140) Acc D Fake: 6.667%
Loss D: 1.263
Loss G: 0.5960 (0.5927) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.4281 (0.4057) Acc D Real: 96.354%
Loss D Fake: 0.8070 (0.8126) Acc D Fake: 6.667%
Loss D: 1.235
Loss G: 0.5964 (0.5934) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.3488 (0.3962) Acc D Real: 96.354%
Loss D Fake: 0.8068 (0.8116) Acc D Fake: 6.667%
Loss D: 1.156
Loss G: 0.5965 (0.5939) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.3585 (0.3908) Acc D Real: 96.503%
Loss D Fake: 0.8066 (0.8109) Acc D Fake: 6.667%
Loss D: 1.165
Loss G: 0.5969 (0.5944) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.4388 (0.3968) Acc D Real: 96.432%
Loss D Fake: 0.8061 (0.8103) Acc D Fake: 6.667%
Loss D: 1.245
Loss G: 0.5971 (0.5947) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.3632 (0.3931) Acc D Real: 96.395%
Loss D Fake: 0.8060 (0.8098) Acc D Fake: 6.667%
Loss D: 1.169
Loss G: 0.5973 (0.5950) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3455 (0.3883) Acc D Real: 96.443%
Loss D Fake: 0.8054 (0.8094) Acc D Fake: 6.667%
Loss D: 1.151
Loss G: 0.5981 (0.5953) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.3579 (0.3856) Acc D Real: 96.435%
Loss D Fake: 0.8041 (0.8089) Acc D Fake: 6.667%
Loss D: 1.162
Loss G: 0.5993 (0.5957) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.3500 (0.3826) Acc D Real: 96.489%
Loss D Fake: 0.8024 (0.8084) Acc D Fake: 6.667%
Loss D: 1.152
Loss G: 0.6009 (0.5961) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3909 (0.3832) Acc D Real: 96.386%
Loss D Fake: 0.8004 (0.8078) Acc D Fake: 6.667%
Loss D: 1.191
Loss G: 0.6024 (0.5966) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.4640 (0.3890) Acc D Real: 96.343%
Loss D Fake: 0.7988 (0.8071) Acc D Fake: 6.667%
Loss D: 1.263
Loss G: 0.6034 (0.5971) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.4508 (0.3931) Acc D Real: 96.243%
Loss D Fake: 0.7982 (0.8065) Acc D Fake: 6.667%
Loss D: 1.249
Loss G: 0.6032 (0.5975) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3779 (0.3922) Acc D Real: 96.263%
Loss D Fake: 0.7987 (0.8060) Acc D Fake: 6.667%
Loss D: 1.177
Loss G: 0.6030 (0.5978) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3539 (0.3899) Acc D Real: 96.278%
Loss D Fake: 0.7988 (0.8056) Acc D Fake: 6.667%
Loss D: 1.153
Loss G: 0.6031 (0.5982) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.4219 (0.3917) Acc D Real: 96.282%
Loss D Fake: 0.7989 (0.8052) Acc D Fake: 6.667%
Loss D: 1.221
Loss G: 0.6028 (0.5984) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3452 (0.3893) Acc D Real: 96.291%
Loss D Fake: 0.7993 (0.8049) Acc D Fake: 6.667%
Loss D: 1.145
Loss G: 0.6030 (0.5987) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.4342 (0.3915) Acc D Real: 96.292%
Loss D Fake: 0.7992 (0.8046) Acc D Fake: 6.667%
Loss D: 1.233
Loss G: 0.6028 (0.5989) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3652 (0.3902) Acc D Real: 96.186%
Loss D Fake: 0.7996 (0.8044) Acc D Fake: 6.667%
Loss D: 1.165
Loss G: 0.6030 (0.5991) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.3729 (0.3895) Acc D Real: 96.184%
Loss D Fake: 0.7989 (0.8041) Acc D Fake: 6.667%
Loss D: 1.172
Loss G: 0.6039 (0.5993) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.4556 (0.3923) Acc D Real: 96.153%
Loss D Fake: 0.7978 (0.8039) Acc D Fake: 6.667%
Loss D: 1.253
Loss G: 0.6043 (0.5995) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4278 (0.3938) Acc D Real: 96.118%
Loss D Fake: 0.7977 (0.8036) Acc D Fake: 6.667%
Loss D: 1.226
Loss G: 0.6043 (0.5997) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.4335 (0.3954) Acc D Real: 96.119%
Loss D Fake: 0.7976 (0.8034) Acc D Fake: 6.667%
Loss D: 1.231
Loss G: 0.6044 (0.5999) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.4074 (0.3959) Acc D Real: 96.104%
Loss D Fake: 0.7972 (0.8031) Acc D Fake: 6.667%
Loss D: 1.205
Loss G: 0.6047 (0.6001) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3791 (0.3952) Acc D Real: 96.140%
Loss D Fake: 0.7967 (0.8029) Acc D Fake: 6.667%
Loss D: 1.176
Loss G: 0.6051 (0.6003) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.4132 (0.3959) Acc D Real: 96.142%
Loss D Fake: 0.7971 (0.8027) Acc D Fake: 6.667%
Loss D: 1.210
Loss G: 0.6032 (0.6004) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3854 (0.3955) Acc D Real: 96.139%
Loss D Fake: 0.8011 (0.8026) Acc D Fake: 6.667%
Loss D: 1.187
Loss G: 0.6002 (0.6004) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.4066 (0.3959) Acc D Real: 96.146%
Loss D Fake: 0.8056 (0.8027) Acc D Fake: 6.667%
Loss D: 1.212
Loss G: 0.5972 (0.6003) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3568 (0.3946) Acc D Real: 96.148%
Loss D Fake: 0.8095 (0.8030) Acc D Fake: 6.667%
Loss D: 1.166
Loss G: 0.5952 (0.6001) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.4114 (0.3952) Acc D Real: 96.178%
Loss D Fake: 0.8119 (0.8032) Acc D Fake: 6.667%
Loss D: 1.223
Loss G: 0.5945 (0.5999) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3783 (0.3946) Acc D Real: 96.185%
Loss D Fake: 0.8104 (0.8034) Acc D Fake: 6.667%
Loss D: 1.189
Loss G: 0.5976 (0.5998) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.4545 (0.3964) Acc D Real: 96.172%
Loss D Fake: 0.8045 (0.8035) Acc D Fake: 6.667%
Loss D: 1.259
Loss G: 0.6013 (0.5999) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3095 (0.3939) Acc D Real: 96.204%
Loss D Fake: 0.7997 (0.8034) Acc D Fake: 6.667%
Loss D: 1.109
Loss G: 0.6044 (0.6000) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.4557 (0.3956) Acc D Real: 96.201%
Loss D Fake: 0.7968 (0.8032) Acc D Fake: 6.667%
Loss D: 1.252
Loss G: 0.6051 (0.6002) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3582 (0.3946) Acc D Real: 96.212%
Loss D Fake: 0.7964 (0.8030) Acc D Fake: 6.667%
Loss D: 1.155
Loss G: 0.6059 (0.6003) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.4671 (0.3965) Acc D Real: 96.168%
Loss D Fake: 0.7955 (0.8028) Acc D Fake: 6.667%
Loss D: 1.263
Loss G: 0.6060 (0.6005) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.4646 (0.3983) Acc D Real: 96.139%
Loss D Fake: 0.7958 (0.8026) Acc D Fake: 6.667%
Loss D: 1.260
Loss G: 0.6056 (0.6006) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.4152 (0.3987) Acc D Real: 96.142%
Loss D Fake: 0.7964 (0.8025) Acc D Fake: 6.667%
Loss D: 1.212
Loss G: 0.6051 (0.6007) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.4799 (0.4007) Acc D Real: 96.146%
Loss D Fake: 0.7973 (0.8023) Acc D Fake: 6.667%
Loss D: 1.277
Loss G: 0.6040 (0.6008) Acc G: 93.333%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.4264 (0.4013) Acc D Real: 96.127%
Loss D Fake: 0.7993 (0.8023) Acc D Fake: 6.667%
Loss D: 1.226
Loss G: 0.6019 (0.6008) Acc G: 93.373%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.4796 (0.4031) Acc D Real: 96.093%
Loss D Fake: 0.8025 (0.8023) Acc D Fake: 6.628%
Loss D: 1.282
Loss G: 0.5991 (0.6008) Acc G: 93.411%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.4612 (0.4044) Acc D Real: 96.085%
Loss D Fake: 0.8064 (0.8024) Acc D Fake: 6.591%
Loss D: 1.268
Loss G: 0.5962 (0.6007) Acc G: 93.447%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3463 (0.4031) Acc D Real: 96.123%
Loss D Fake: 0.8096 (0.8025) Acc D Fake: 6.556%
Loss D: 1.156
Loss G: 0.5944 (0.6005) Acc G: 93.481%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.4091 (0.4033) Acc D Real: 96.139%
Loss D Fake: 0.8110 (0.8027) Acc D Fake: 6.522%
Loss D: 1.220
Loss G: 0.5939 (0.6004) Acc G: 93.514%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.3522 (0.4022) Acc D Real: 96.165%
Loss D Fake: 0.8107 (0.8029) Acc D Fake: 6.489%
Loss D: 1.163
Loss G: 0.5946 (0.6003) Acc G: 93.546%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3804 (0.4017) Acc D Real: 96.184%
Loss D Fake: 0.8115 (0.8031) Acc D Fake: 6.458%
Loss D: 1.192
Loss G: 0.5906 (0.6001) Acc G: 93.576%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3789 (0.4013) Acc D Real: 96.200%
Loss D Fake: 0.8207 (0.8034) Acc D Fake: 6.429%
Loss D: 1.200
Loss G: 0.5851 (0.5998) Acc G: 93.605%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.4303 (0.4018) Acc D Real: 96.206%
Loss D Fake: 0.8290 (0.8039) Acc D Fake: 6.400%
Loss D: 1.259
Loss G: 0.5826 (0.5994) Acc G: 93.633%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.4173 (0.4021) Acc D Real: 96.206%
Loss D Fake: 0.8254 (0.8044) Acc D Fake: 6.373%
Loss D: 1.243
Loss G: 0.5879 (0.5992) Acc G: 93.660%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3673 (0.4015) Acc D Real: 96.228%
Loss D Fake: 0.8157 (0.8046) Acc D Fake: 6.346%
Loss D: 1.183
Loss G: 0.5931 (0.5991) Acc G: 93.686%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3749 (0.4010) Acc D Real: 96.236%
Loss D Fake: 0.8094 (0.8047) Acc D Fake: 6.321%
Loss D: 1.184
Loss G: 0.5967 (0.5990) Acc G: 93.711%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3856 (0.4007) Acc D Real: 96.256%
Loss D Fake: 0.8055 (0.8047) Acc D Fake: 6.296%
Loss D: 1.191
Loss G: 0.5987 (0.5990) Acc G: 93.735%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4321 (0.4013) Acc D Real: 96.264%
Loss D Fake: 0.8036 (0.8047) Acc D Fake: 6.273%
Loss D: 1.236
Loss G: 0.5996 (0.5990) Acc G: 93.758%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.4147 (0.4015) Acc D Real: 96.270%
Loss D Fake: 0.8026 (0.8046) Acc D Fake: 6.250%
Loss D: 1.217
Loss G: 0.6002 (0.5991) Acc G: 93.780%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3528 (0.4006) Acc D Real: 96.282%
Loss D Fake: 0.8017 (0.8046) Acc D Fake: 6.228%
Loss D: 1.155
Loss G: 0.6010 (0.5991) Acc G: 93.801%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3896 (0.4005) Acc D Real: 96.298%
Loss D Fake: 0.8007 (0.8045) Acc D Fake: 6.207%
Loss D: 1.190
Loss G: 0.6016 (0.5991) Acc G: 93.822%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4088 (0.4006) Acc D Real: 96.309%
Loss D Fake: 0.8000 (0.8044) Acc D Fake: 6.186%
Loss D: 1.209
Loss G: 0.6021 (0.5992) Acc G: 93.842%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.4076 (0.4007) Acc D Real: 96.317%
Loss D Fake: 0.7994 (0.8044) Acc D Fake: 6.167%
Loss D: 1.207
Loss G: 0.6025 (0.5992) Acc G: 93.861%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.4022 (0.4007) Acc D Real: 96.323%
Loss D Fake: 0.7997 (0.8043) Acc D Fake: 6.148%
Loss D: 1.202
Loss G: 0.6011 (0.5993) Acc G: 93.880%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3738 (0.4003) Acc D Real: 96.343%
Loss D Fake: 0.8027 (0.8043) Acc D Fake: 6.129%
Loss D: 1.177
Loss G: 0.5991 (0.5993) Acc G: 93.898%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3956 (0.4002) Acc D Real: 96.351%
Loss D Fake: 0.8056 (0.8043) Acc D Fake: 6.111%
Loss D: 1.201
Loss G: 0.5970 (0.5992) Acc G: 93.915%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3968 (0.4002) Acc D Real: 96.365%
Loss D Fake: 0.8088 (0.8043) Acc D Fake: 6.094%
Loss D: 1.206
Loss G: 0.5951 (0.5992) Acc G: 93.932%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.4053 (0.4003) Acc D Real: 96.373%
Loss D Fake: 0.8111 (0.8044) Acc D Fake: 6.077%
Loss D: 1.216
Loss G: 0.5940 (0.5991) Acc G: 93.949%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3598 (0.3996) Acc D Real: 96.384%
Loss D Fake: 0.8121 (0.8046) Acc D Fake: 6.061%
Loss D: 1.172
Loss G: 0.5939 (0.5990) Acc G: 93.965%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.3239 (0.3985) Acc D Real: 96.397%
Loss D Fake: 0.8110 (0.8047) Acc D Fake: 6.045%
Loss D: 1.135
Loss G: 0.5961 (0.5990) Acc G: 93.980%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.3534 (0.3979) Acc D Real: 96.410%
Loss D Fake: 0.8068 (0.8047) Acc D Fake: 6.029%
Loss D: 1.160
Loss G: 0.5993 (0.5990) Acc G: 93.995%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4272 (0.3983) Acc D Real: 96.405%
Loss D Fake: 0.8025 (0.8047) Acc D Fake: 6.014%
Loss D: 1.230
Loss G: 0.6024 (0.5990) Acc G: 94.010%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3553 (0.3977) Acc D Real: 96.407%
Loss D Fake: 0.7985 (0.8046) Acc D Fake: 6.000%
Loss D: 1.154
Loss G: 0.6054 (0.5991) Acc G: 94.024%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.4083 (0.3978) Acc D Real: 96.402%
Loss D Fake: 0.7947 (0.8044) Acc D Fake: 5.986%
Loss D: 1.203
Loss G: 0.6080 (0.5992) Acc G: 94.038%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.4752 (0.3989) Acc D Real: 96.385%
Loss D Fake: 0.7918 (0.8043) Acc D Fake: 5.972%
Loss D: 1.267
Loss G: 0.6096 (0.5994) Acc G: 94.051%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.4946 (0.4002) Acc D Real: 96.374%
Loss D Fake: 0.7903 (0.8041) Acc D Fake: 5.959%
Loss D: 1.285
Loss G: 0.6101 (0.5995) Acc G: 94.064%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.4009 (0.4002) Acc D Real: 96.382%
Loss D Fake: 0.7899 (0.8039) Acc D Fake: 5.946%
Loss D: 1.191
Loss G: 0.6102 (0.5997) Acc G: 94.077%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3956 (0.4001) Acc D Real: 96.385%
Loss D Fake: 0.7896 (0.8037) Acc D Fake: 5.933%
Loss D: 1.185
Loss G: 0.6102 (0.5998) Acc G: 94.089%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4390 (0.4007) Acc D Real: 96.388%
Loss D Fake: 0.7897 (0.8035) Acc D Fake: 5.921%
Loss D: 1.229
Loss G: 0.6098 (0.5999) Acc G: 94.101%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3978 (0.4006) Acc D Real: 96.394%
Loss D Fake: 0.7904 (0.8033) Acc D Fake: 5.909%
Loss D: 1.188
Loss G: 0.6093 (0.6001) Acc G: 94.113%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.4682 (0.4015) Acc D Real: 96.391%
Loss D Fake: 0.7912 (0.8032) Acc D Fake: 5.897%
Loss D: 1.259
Loss G: 0.6081 (0.6002) Acc G: 94.124%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3178 (0.4004) Acc D Real: 96.410%
Loss D Fake: 0.7931 (0.8030) Acc D Fake: 5.886%
Loss D: 1.111
Loss G: 0.6065 (0.6002) Acc G: 94.135%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3976 (0.4004) Acc D Real: 96.424%
Loss D Fake: 0.7966 (0.8030) Acc D Fake: 5.875%
Loss D: 1.194
Loss G: 0.6018 (0.6003) Acc G: 94.146%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3982 (0.4004) Acc D Real: 96.431%
Loss D Fake: 0.8063 (0.8030) Acc D Fake: 5.864%
Loss D: 1.205
Loss G: 0.5927 (0.6002) Acc G: 94.156%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3891 (0.4002) Acc D Real: 96.442%
Loss D Fake: 0.8277 (0.8033) Acc D Fake: 5.854%
Loss D: 1.217
Loss G: 0.5827 (0.6000) Acc G: 94.167%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3917 (0.4001) Acc D Real: 96.448%
Loss D Fake: 0.8279 (0.8036) Acc D Fake: 5.843%
Loss D: 1.220
Loss G: 0.5922 (0.5999) Acc G: 94.177%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3874 (0.4000) Acc D Real: 96.458%
Loss D Fake: 0.8081 (0.8037) Acc D Fake: 5.833%
Loss D: 1.196
Loss G: 0.5998 (0.5999) Acc G: 94.187%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3767 (0.3997) Acc D Real: 96.464%
Loss D Fake: 0.8005 (0.8036) Acc D Fake: 5.824%
Loss D: 1.177
Loss G: 0.6039 (0.5999) Acc G: 94.196%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4300 (0.4001) Acc D Real: 96.469%
Loss D Fake: 0.7962 (0.8035) Acc D Fake: 5.814%
Loss D: 1.226
Loss G: 0.6062 (0.6000) Acc G: 94.205%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3641 (0.3996) Acc D Real: 96.478%
Loss D Fake: 0.7936 (0.8034) Acc D Fake: 5.805%
Loss D: 1.158
Loss G: 0.6080 (0.6001) Acc G: 94.215%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.4338 (0.4000) Acc D Real: 96.483%
Loss D Fake: 0.7915 (0.8033) Acc D Fake: 5.795%
Loss D: 1.225
Loss G: 0.6091 (0.6002) Acc G: 94.223%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.4242 (0.4003) Acc D Real: 96.488%
Loss D Fake: 0.7922 (0.8032) Acc D Fake: 5.787%
Loss D: 1.216
Loss G: 0.6055 (0.6002) Acc G: 94.232%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3617 (0.3999) Acc D Real: 96.491%
Loss D Fake: 0.8004 (0.8031) Acc D Fake: 5.778%
Loss D: 1.162
Loss G: 0.5979 (0.6002) Acc G: 94.241%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3970 (0.3998) Acc D Real: 96.496%
Loss D Fake: 0.8173 (0.8033) Acc D Fake: 5.769%
Loss D: 1.214
Loss G: 0.5809 (0.6000) Acc G: 94.249%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3227 (0.3990) Acc D Real: 96.508%
Loss D Fake: 1.1348 (0.8069) Acc D Fake: 5.761%
Loss D: 1.458
Loss G: 0.6009 (0.6000) Acc G: 94.257%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3681 (0.3987) Acc D Real: 96.519%
Loss D Fake: 0.7929 (0.8067) Acc D Fake: 5.753%
Loss D: 1.161
Loss G: 0.6118 (0.6001) Acc G: 94.265%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3983 (0.3987) Acc D Real: 96.528%
Loss D Fake: 0.7842 (0.8065) Acc D Fake: 5.745%
Loss D: 1.183
Loss G: 0.6163 (0.6003) Acc G: 94.273%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3891 (0.3986) Acc D Real: 96.532%
Loss D Fake: 0.7797 (0.8062) Acc D Fake: 5.737%
Loss D: 1.169
Loss G: 0.6190 (0.6005) Acc G: 94.281%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4928 (0.3995) Acc D Real: 96.536%
Loss D Fake: 0.7771 (0.8059) Acc D Fake: 5.729%
Loss D: 1.270
Loss G: 0.6202 (0.6007) Acc G: 94.288%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4165 (0.3997) Acc D Real: 96.537%
Loss D Fake: 0.7760 (0.8056) Acc D Fake: 5.722%
Loss D: 1.193
Loss G: 0.6207 (0.6009) Acc G: 94.296%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.5308 (0.4011) Acc D Real: 96.525%
Loss D Fake: 0.7758 (0.8053) Acc D Fake: 5.714%
Loss D: 1.307
Loss G: 0.6201 (0.6011) Acc G: 94.303%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.4480 (0.4015) Acc D Real: 96.528%
Loss D Fake: 0.7770 (0.8050) Acc D Fake: 5.707%
Loss D: 1.225
Loss G: 0.6188 (0.6013) Acc G: 94.310%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.4232 (0.4017) Acc D Real: 96.531%
Loss D Fake: 0.7786 (0.8047) Acc D Fake: 5.700%
Loss D: 1.202
Loss G: 0.6175 (0.6015) Acc G: 94.317%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3877 (0.4016) Acc D Real: 96.533%
Loss D Fake: 0.7799 (0.8045) Acc D Fake: 5.693%
Loss D: 1.168
Loss G: 0.6165 (0.6016) Acc G: 94.323%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3789 (0.4014) Acc D Real: 96.546%
Loss D Fake: 0.7808 (0.8043) Acc D Fake: 5.686%
Loss D: 1.160
Loss G: 0.6158 (0.6017) Acc G: 94.330%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4346 (0.4017) Acc D Real: 96.549%
Loss D Fake: 0.7815 (0.8040) Acc D Fake: 5.680%
Loss D: 1.216
Loss G: 0.6151 (0.6019) Acc G: 94.337%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.4599 (0.4023) Acc D Real: 96.548%
Loss D Fake: 0.7825 (0.8038) Acc D Fake: 5.673%
Loss D: 1.242
Loss G: 0.6140 (0.6020) Acc G: 94.343%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3778 (0.4020) Acc D Real: 96.554%
Loss D Fake: 0.7837 (0.8037) Acc D Fake: 5.667%
Loss D: 1.162
Loss G: 0.6132 (0.6021) Acc G: 94.349%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.4815 (0.4028) Acc D Real: 96.558%
Loss D Fake: 0.7848 (0.8035) Acc D Fake: 5.660%
Loss D: 1.266
Loss G: 0.6119 (0.6022) Acc G: 94.355%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4402 (0.4031) Acc D Real: 96.565%
Loss D Fake: 0.7867 (0.8033) Acc D Fake: 5.654%
Loss D: 1.227
Loss G: 0.6102 (0.6023) Acc G: 94.361%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.4090 (0.4032) Acc D Real: 96.570%
Loss D Fake: 0.7886 (0.8032) Acc D Fake: 5.648%
Loss D: 1.198
Loss G: 0.6086 (0.6023) Acc G: 94.367%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.4431 (0.4036) Acc D Real: 96.573%
Loss D Fake: 0.7905 (0.8031) Acc D Fake: 5.642%
Loss D: 1.234
Loss G: 0.6070 (0.6024) Acc G: 94.373%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.4413 (0.4039) Acc D Real: 96.581%
Loss D Fake: 0.7926 (0.8030) Acc D Fake: 5.636%
Loss D: 1.234
Loss G: 0.6052 (0.6024) Acc G: 94.379%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.4077 (0.4039) Acc D Real: 96.585%
Loss D Fake: 0.7947 (0.8029) Acc D Fake: 5.631%
Loss D: 1.202
Loss G: 0.6036 (0.6024) Acc G: 94.384%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.4955 (0.4048) Acc D Real: 96.589%
Loss D Fake: 0.7968 (0.8028) Acc D Fake: 5.625%
Loss D: 1.292
Loss G: 0.6015 (0.6024) Acc G: 94.390%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4225 (0.4049) Acc D Real: 96.596%
Loss D Fake: 0.7994 (0.8028) Acc D Fake: 5.619%
Loss D: 1.222
Loss G: 0.5995 (0.6024) Acc G: 94.395%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3869 (0.4048) Acc D Real: 96.604%
Loss D Fake: 0.8017 (0.8028) Acc D Fake: 5.614%
Loss D: 1.189
Loss G: 0.5979 (0.6023) Acc G: 94.401%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4518 (0.4052) Acc D Real: 96.608%
Loss D Fake: 0.8036 (0.8028) Acc D Fake: 5.609%
Loss D: 1.255
Loss G: 0.5962 (0.6023) Acc G: 94.406%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4385 (0.4054) Acc D Real: 96.614%
Loss D Fake: 0.8058 (0.8028) Acc D Fake: 5.603%
Loss D: 1.244
Loss G: 0.5945 (0.6022) Acc G: 94.411%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.4309 (0.4057) Acc D Real: 96.619%
Loss D Fake: 0.8079 (0.8029) Acc D Fake: 5.598%
Loss D: 1.239
Loss G: 0.5927 (0.6021) Acc G: 94.416%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3985 (0.4056) Acc D Real: 96.627%
Loss D Fake: 0.8100 (0.8029) Acc D Fake: 5.593%
Loss D: 1.209
Loss G: 0.5912 (0.6020) Acc G: 94.421%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3697 (0.4053) Acc D Real: 96.632%
Loss D Fake: 0.8116 (0.8030) Acc D Fake: 5.588%
Loss D: 1.181
Loss G: 0.5902 (0.6019) Acc G: 94.426%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3856 (0.4051) Acc D Real: 96.635%
Loss D Fake: 0.8126 (0.8031) Acc D Fake: 5.583%
Loss D: 1.198
Loss G: 0.5895 (0.6018) Acc G: 94.431%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2661 (0.4040) Acc D Real: 96.652%
Loss D Fake: 0.8129 (0.8032) Acc D Fake: 5.579%
Loss D: 1.079
Loss G: 0.5900 (0.6017) Acc G: 94.435%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4511 (0.4044) Acc D Real: 96.657%
Loss D Fake: 0.8121 (0.8032) Acc D Fake: 5.574%
Loss D: 1.263
Loss G: 0.5903 (0.6016) Acc G: 94.440%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3772 (0.4042) Acc D Real: 96.663%
Loss D Fake: 0.8118 (0.8033) Acc D Fake: 5.569%
Loss D: 1.189
Loss G: 0.5906 (0.6015) Acc G: 94.444%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3974 (0.4041) Acc D Real: 96.667%
Loss D Fake: 0.8115 (0.8034) Acc D Fake: 5.565%
Loss D: 1.209
Loss G: 0.5909 (0.6015) Acc G: 94.449%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3500 (0.4037) Acc D Real: 96.674%
Loss D Fake: 0.8110 (0.8034) Acc D Fake: 5.560%
Loss D: 1.161
Loss G: 0.5915 (0.6014) Acc G: 94.453%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.4270 (0.4039) Acc D Real: 96.680%
Loss D Fake: 0.8103 (0.8035) Acc D Fake: 5.556%
Loss D: 1.237
Loss G: 0.5918 (0.6013) Acc G: 94.458%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3646 (0.4035) Acc D Real: 96.686%
Loss D Fake: 0.8100 (0.8035) Acc D Fake: 5.551%
Loss D: 1.175
Loss G: 0.5922 (0.6012) Acc G: 94.462%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3315 (0.4030) Acc D Real: 96.698%
Loss D Fake: 0.8093 (0.8036) Acc D Fake: 5.547%
Loss D: 1.141
Loss G: 0.5931 (0.6012) Acc G: 94.466%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.4749 (0.4035) Acc D Real: 96.704%
Loss D Fake: 0.8084 (0.8036) Acc D Fake: 5.543%
Loss D: 1.283
Loss G: 0.5933 (0.6011) Acc G: 94.470%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3810 (0.4034) Acc D Real: 96.713%
Loss D Fake: 0.8084 (0.8037) Acc D Fake: 5.538%
Loss D: 1.189
Loss G: 0.5934 (0.6011) Acc G: 94.474%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.4149 (0.4035) Acc D Real: 96.716%
Loss D Fake: 0.8083 (0.8037) Acc D Fake: 5.534%
Loss D: 1.223
Loss G: 0.5934 (0.6010) Acc G: 94.478%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.5021 (0.4042) Acc D Real: 96.722%
Loss D Fake: 0.8087 (0.8037) Acc D Fake: 5.530%
Loss D: 1.311
Loss G: 0.5926 (0.6009) Acc G: 94.482%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.4610 (0.4046) Acc D Real: 96.728%
Loss D Fake: 0.8101 (0.8038) Acc D Fake: 5.526%
Loss D: 1.271
Loss G: 0.5912 (0.6009) Acc G: 94.486%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3787 (0.4044) Acc D Real: 96.734%
Loss D Fake: 0.8118 (0.8038) Acc D Fake: 5.522%
Loss D: 1.191
Loss G: 0.5901 (0.6008) Acc G: 94.490%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.4463 (0.4047) Acc D Real: 96.737%
Loss D Fake: 0.8132 (0.8039) Acc D Fake: 5.519%
Loss D: 1.259
Loss G: 0.5889 (0.6007) Acc G: 94.494%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.2976 (0.4040) Acc D Real: 96.748%
Loss D Fake: 0.8144 (0.8040) Acc D Fake: 5.515%
Loss D: 1.112
Loss G: 0.5885 (0.6006) Acc G: 94.498%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3527 (0.4036) Acc D Real: 96.757%
Loss D Fake: 0.8143 (0.8041) Acc D Fake: 5.511%
Loss D: 1.167
Loss G: 0.5888 (0.6005) Acc G: 94.501%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.3498 (0.4032) Acc D Real: 96.767%
Loss D Fake: 0.8138 (0.8041) Acc D Fake: 5.507%
Loss D: 1.164
Loss G: 0.5895 (0.6004) Acc G: 94.505%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3499 (0.4028) Acc D Real: 96.774%
Loss D Fake: 0.8127 (0.8042) Acc D Fake: 5.504%
Loss D: 1.163
Loss G: 0.5905 (0.6004) Acc G: 94.508%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3652 (0.4025) Acc D Real: 96.783%
Loss D Fake: 0.8113 (0.8043) Acc D Fake: 5.500%
Loss D: 1.177
Loss G: 0.5916 (0.6003) Acc G: 94.512%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.4010 (0.4025) Acc D Real: 96.787%
Loss D Fake: 0.8099 (0.8043) Acc D Fake: 5.496%
Loss D: 1.211
Loss G: 0.5927 (0.6002) Acc G: 94.515%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3844 (0.4024) Acc D Real: 96.792%
Loss D Fake: 0.8087 (0.8043) Acc D Fake: 5.493%
Loss D: 1.193
Loss G: 0.5936 (0.6002) Acc G: 94.519%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.3582 (0.4021) Acc D Real: 96.801%
Loss D Fake: 0.8075 (0.8043) Acc D Fake: 5.490%
Loss D: 1.166
Loss G: 0.5945 (0.6002) Acc G: 94.522%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.4058 (0.4021) Acc D Real: 96.806%
Loss D Fake: 0.8066 (0.8044) Acc D Fake: 5.486%
Loss D: 1.212
Loss G: 0.5952 (0.6001) Acc G: 94.525%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.4619 (0.4025) Acc D Real: 96.809%
Loss D Fake: 0.8061 (0.8044) Acc D Fake: 5.483%
Loss D: 1.268
Loss G: 0.5953 (0.6001) Acc G: 94.529%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.4182 (0.4026) Acc D Real: 96.814%
Loss D Fake: 0.8063 (0.8044) Acc D Fake: 5.479%
Loss D: 1.225
Loss G: 0.5950 (0.6001) Acc G: 94.532%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.4007 (0.4026) Acc D Real: 96.817%
Loss D Fake: 0.8068 (0.8044) Acc D Fake: 5.476%
Loss D: 1.207
Loss G: 0.5947 (0.6000) Acc G: 94.535%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3615 (0.4023) Acc D Real: 96.823%
Loss D Fake: 0.8071 (0.8044) Acc D Fake: 5.473%
Loss D: 1.169
Loss G: 0.5946 (0.6000) Acc G: 94.538%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.4830 (0.4029) Acc D Real: 96.825%
Loss D Fake: 0.8075 (0.8044) Acc D Fake: 5.470%
Loss D: 1.291
Loss G: 0.5939 (0.5999) Acc G: 94.541%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4008 (0.4029) Acc D Real: 96.831%
Loss D Fake: 0.8085 (0.8045) Acc D Fake: 5.467%
Loss D: 1.209
Loss G: 0.5932 (0.5999) Acc G: 94.544%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3772 (0.4027) Acc D Real: 96.838%
Loss D Fake: 0.8093 (0.8045) Acc D Fake: 5.464%
Loss D: 1.187
Loss G: 0.5927 (0.5998) Acc G: 94.547%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3615 (0.4024) Acc D Real: 96.846%
Loss D Fake: 0.8098 (0.8045) Acc D Fake: 5.461%
Loss D: 1.171
Loss G: 0.5926 (0.5998) Acc G: 94.550%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3596 (0.4022) Acc D Real: 96.852%
Loss D Fake: 0.8097 (0.8046) Acc D Fake: 5.458%
Loss D: 1.169
Loss G: 0.5929 (0.5998) Acc G: 94.553%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3908 (0.4021) Acc D Real: 96.856%
Loss D Fake: 0.8093 (0.8046) Acc D Fake: 5.455%
Loss D: 1.200
Loss G: 0.5932 (0.5997) Acc G: 94.556%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3279 (0.4016) Acc D Real: 96.862%
Loss D Fake: 0.8086 (0.8046) Acc D Fake: 5.452%
Loss D: 1.137
Loss G: 0.5941 (0.5997) Acc G: 94.559%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4400 (0.4018) Acc D Real: 96.865%
Loss D Fake: 0.8076 (0.8046) Acc D Fake: 5.449%
Loss D: 1.248
Loss G: 0.5946 (0.5996) Acc G: 94.562%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.4470 (0.4021) Acc D Real: 96.866%
Loss D Fake: 0.8073 (0.8047) Acc D Fake: 5.446%
Loss D: 1.254
Loss G: 0.5946 (0.5996) Acc G: 94.565%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3595 (0.4019) Acc D Real: 96.874%
Loss D Fake: 0.8073 (0.8047) Acc D Fake: 5.443%
Loss D: 1.167
Loss G: 0.5948 (0.5996) Acc G: 94.568%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3807 (0.4017) Acc D Real: 96.880%
Loss D Fake: 0.8069 (0.8047) Acc D Fake: 5.440%
Loss D: 1.188
Loss G: 0.5952 (0.5996) Acc G: 94.570%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.4047 (0.4018) Acc D Real: 96.882%
Loss D Fake: 0.8064 (0.8047) Acc D Fake: 5.438%
Loss D: 1.211
Loss G: 0.5956 (0.5995) Acc G: 94.573%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.4260 (0.4019) Acc D Real: 96.887%
Loss D Fake: 0.8062 (0.8047) Acc D Fake: 5.435%
Loss D: 1.232
Loss G: 0.5956 (0.5995) Acc G: 94.576%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3330 (0.4015) Acc D Real: 96.898%
Loss D Fake: 0.8060 (0.8047) Acc D Fake: 5.432%
Loss D: 1.139
Loss G: 0.5961 (0.5995) Acc G: 94.578%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.3544 (0.4012) Acc D Real: 96.905%
Loss D Fake: 0.8053 (0.8047) Acc D Fake: 5.429%
Loss D: 1.160
Loss G: 0.5968 (0.5995) Acc G: 94.581%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3390 (0.4008) Acc D Real: 96.913%
Loss D Fake: 0.8042 (0.8047) Acc D Fake: 5.427%
Loss D: 1.143
Loss G: 0.5978 (0.5995) Acc G: 94.583%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2965 (0.4002) Acc D Real: 96.920%
Loss D Fake: 0.8026 (0.8047) Acc D Fake: 5.424%
Loss D: 1.099
Loss G: 0.5995 (0.5995) Acc G: 94.586%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3287 (0.3997) Acc D Real: 96.926%
Loss D Fake: 0.8003 (0.8047) Acc D Fake: 5.422%
Loss D: 1.129
Loss G: 0.6015 (0.5995) Acc G: 94.588%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.4138 (0.3998) Acc D Real: 96.930%
Loss D Fake: 0.7980 (0.8046) Acc D Fake: 5.419%
Loss D: 1.212
Loss G: 0.6032 (0.5995) Acc G: 94.591%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.3452 (0.3995) Acc D Real: 96.937%
Loss D Fake: 0.7961 (0.8046) Acc D Fake: 5.417%
Loss D: 1.141
Loss G: 0.6048 (0.5995) Acc G: 94.593%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3550 (0.3992) Acc D Real: 96.942%
Loss D Fake: 0.7942 (0.8045) Acc D Fake: 5.414%
Loss D: 1.149
Loss G: 0.6064 (0.5996) Acc G: 94.596%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3862 (0.3992) Acc D Real: 96.946%
Loss D Fake: 0.7925 (0.8045) Acc D Fake: 5.412%
Loss D: 1.179
Loss G: 0.6077 (0.5996) Acc G: 94.598%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.4053 (0.3992) Acc D Real: 96.948%
Loss D Fake: 0.7910 (0.8044) Acc D Fake: 5.409%
Loss D: 1.196
Loss G: 0.6088 (0.5997) Acc G: 94.600%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3220 (0.3988) Acc D Real: 96.955%
Loss D Fake: 0.7897 (0.8043) Acc D Fake: 5.407%
Loss D: 1.112
Loss G: 0.6102 (0.5997) Acc G: 94.603%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.2844 (0.3981) Acc D Real: 96.962%
Loss D Fake: 0.7877 (0.8042) Acc D Fake: 5.405%
Loss D: 1.072
Loss G: 0.6122 (0.5998) Acc G: 94.605%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3776 (0.3980) Acc D Real: 96.966%
Loss D Fake: 0.7853 (0.8041) Acc D Fake: 5.402%
Loss D: 1.163
Loss G: 0.6140 (0.5999) Acc G: 94.607%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3414 (0.3977) Acc D Real: 96.971%
Loss D Fake: 0.7833 (0.8040) Acc D Fake: 5.400%
Loss D: 1.125
Loss G: 0.6158 (0.6000) Acc G: 94.610%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3407 (0.3973) Acc D Real: 96.977%
Loss D Fake: 0.7812 (0.8038) Acc D Fake: 5.398%
Loss D: 1.122
Loss G: 0.6177 (0.6001) Acc G: 94.612%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4043 (0.3974) Acc D Real: 96.980%
Loss D Fake: 0.7791 (0.8037) Acc D Fake: 5.395%
Loss D: 1.183
Loss G: 0.6193 (0.6002) Acc G: 94.614%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.4974 (0.3979) Acc D Real: 96.981%
Loss D Fake: 0.7779 (0.8036) Acc D Fake: 5.393%
Loss D: 1.275
Loss G: 0.6197 (0.6003) Acc G: 94.616%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3606 (0.3977) Acc D Real: 96.984%
Loss D Fake: 0.7777 (0.8034) Acc D Fake: 5.391%
Loss D: 1.138
Loss G: 0.6200 (0.6004) Acc G: 94.618%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.4517 (0.3980) Acc D Real: 96.985%
Loss D Fake: 0.7775 (0.8033) Acc D Fake: 5.389%
Loss D: 1.229
Loss G: 0.6199 (0.6005) Acc G: 94.620%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3539 (0.3978) Acc D Real: 96.990%
Loss D Fake: 0.7777 (0.8031) Acc D Fake: 5.387%
Loss D: 1.132
Loss G: 0.6200 (0.6006) Acc G: 94.622%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3429 (0.3975) Acc D Real: 96.995%
Loss D Fake: 0.7774 (0.8030) Acc D Fake: 5.385%
Loss D: 1.120
Loss G: 0.6204 (0.6007) Acc G: 94.625%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.3766 (0.3974) Acc D Real: 97.000%
Loss D Fake: 0.7768 (0.8028) Acc D Fake: 5.383%
Loss D: 1.153
Loss G: 0.6209 (0.6008) Acc G: 94.627%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.4154 (0.3975) Acc D Real: 97.001%
Loss D Fake: 0.7765 (0.8027) Acc D Fake: 5.380%
Loss D: 1.192
Loss G: 0.6210 (0.6009) Acc G: 94.629%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.3432 (0.3972) Acc D Real: 97.003%
Loss D Fake: 0.7764 (0.8026) Acc D Fake: 5.378%
Loss D: 1.120
Loss G: 0.6214 (0.6011) Acc G: 94.631%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.4086 (0.3972) Acc D Real: 97.003%
Loss D Fake: 0.7760 (0.8024) Acc D Fake: 5.376%
Loss D: 1.185
Loss G: 0.6217 (0.6012) Acc G: 94.633%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3038 (0.3967) Acc D Real: 97.010%
Loss D Fake: 0.7755 (0.8023) Acc D Fake: 5.374%
Loss D: 1.079
Loss G: 0.6225 (0.6013) Acc G: 94.635%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3947 (0.3967) Acc D Real: 97.014%
Loss D Fake: 0.7744 (0.8021) Acc D Fake: 5.372%
Loss D: 1.169
Loss G: 0.6234 (0.6014) Acc G: 94.637%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.4409 (0.3970) Acc D Real: 97.017%
Loss D Fake: 0.7737 (0.8020) Acc D Fake: 5.370%
Loss D: 1.215
Loss G: 0.6237 (0.6015) Acc G: 94.638%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3301 (0.3966) Acc D Real: 97.022%
Loss D Fake: 0.7736 (0.8018) Acc D Fake: 5.368%
Loss D: 1.104
Loss G: 0.6240 (0.6016) Acc G: 94.640%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.4412 (0.3968) Acc D Real: 97.022%
Loss D Fake: 0.7734 (0.8017) Acc D Fake: 5.366%
Loss D: 1.215
Loss G: 0.6239 (0.6018) Acc G: 94.642%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.3221 (0.3964) Acc D Real: 97.027%
Loss D Fake: 0.7738 (0.8015) Acc D Fake: 5.365%
Loss D: 1.096
Loss G: 0.6237 (0.6019) Acc G: 94.644%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.4290 (0.3966) Acc D Real: 97.029%
Loss D Fake: 0.7742 (0.8014) Acc D Fake: 5.363%
Loss D: 1.203
Loss G: 0.6233 (0.6020) Acc G: 94.646%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.4197 (0.3967) Acc D Real: 97.030%
Loss D Fake: 0.7750 (0.8013) Acc D Fake: 5.361%
Loss D: 1.195
Loss G: 0.6226 (0.6021) Acc G: 94.648%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3254 (0.3964) Acc D Real: 97.035%
Loss D Fake: 0.7758 (0.8011) Acc D Fake: 5.359%
Loss D: 1.101
Loss G: 0.6225 (0.6022) Acc G: 94.650%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.3884 (0.3963) Acc D Real: 97.034%
Loss D Fake: 0.7757 (0.8010) Acc D Fake: 5.357%
Loss D: 1.164
Loss G: 0.6227 (0.6023) Acc G: 94.651%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4018 (0.3964) Acc D Real: 97.035%
Loss D Fake: 0.7754 (0.8009) Acc D Fake: 5.355%
Loss D: 1.177
Loss G: 0.6230 (0.6024) Acc G: 94.653%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2972 (0.3959) Acc D Real: 97.040%
Loss D Fake: 0.7748 (0.8007) Acc D Fake: 5.354%
Loss D: 1.072
Loss G: 0.6240 (0.6025) Acc G: 94.655%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3229 (0.3955) Acc D Real: 97.044%
Loss D Fake: 0.7733 (0.8006) Acc D Fake: 5.352%
Loss D: 1.096
Loss G: 0.6257 (0.6026) Acc G: 94.657%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.3415 (0.3952) Acc D Real: 97.048%
Loss D Fake: 0.7710 (0.8004) Acc D Fake: 5.350%
Loss D: 1.113
Loss G: 0.6277 (0.6027) Acc G: 94.658%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2894 (0.3947) Acc D Real: 97.053%
Loss D Fake: 0.7689 (0.8003) Acc D Fake: 5.348%
Loss D: 1.058
Loss G: 0.6292 (0.6029) Acc G: 94.660%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.3538 (0.3945) Acc D Real: 97.043%
Loss D Fake: 0.7676 (0.8001) Acc D Fake: 5.347%
Loss D: 1.121
Loss G: 0.6305 (0.6030) Acc G: 94.662%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.3785 (0.3944) Acc D Real: 97.035%
Loss D Fake: 0.7663 (0.8000) Acc D Fake: 5.345%
Loss D: 1.145
Loss G: 0.6318 (0.6032) Acc G: 94.663%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3306 (0.3941) Acc D Real: 97.029%
Loss D Fake: 0.7648 (0.7998) Acc D Fake: 5.343%
Loss D: 1.095
Loss G: 0.6335 (0.6033) Acc G: 94.665%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.3597 (0.3939) Acc D Real: 97.017%
Loss D Fake: 0.7630 (0.7996) Acc D Fake: 5.341%
Loss D: 1.123
Loss G: 0.6347 (0.6035) Acc G: 94.667%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.4701 (0.3943) Acc D Real: 96.978%
Loss D Fake: 0.7621 (0.7994) Acc D Fake: 5.340%
Loss D: 1.232
Loss G: 0.6353 (0.6036) Acc G: 94.668%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4516 (0.3946) Acc D Real: 96.961%
Loss D Fake: 0.7617 (0.7992) Acc D Fake: 5.338%
Loss D: 1.213
Loss G: 0.6355 (0.6038) Acc G: 94.670%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.4524 (0.3949) Acc D Real: 96.912%
Loss D Fake: 0.7617 (0.7991) Acc D Fake: 5.337%
Loss D: 1.214
Loss G: 0.6354 (0.6039) Acc G: 94.671%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.4776 (0.3952) Acc D Real: 96.856%
Loss D Fake: 0.7627 (0.7989) Acc D Fake: 5.335%
Loss D: 1.240
Loss G: 0.6332 (0.6041) Acc G: 94.673%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3786 (0.3952) Acc D Real: 96.853%
Loss D Fake: 0.7661 (0.7987) Acc D Fake: 5.333%
Loss D: 1.145
Loss G: 0.6309 (0.6042) Acc G: 94.675%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3668 (0.3950) Acc D Real: 96.851%
Loss D Fake: 0.7683 (0.7986) Acc D Fake: 5.332%
Loss D: 1.135
Loss G: 0.6298 (0.6043) Acc G: 94.676%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3949 (0.3950) Acc D Real: 96.834%
Loss D Fake: 0.7690 (0.7984) Acc D Fake: 5.330%
Loss D: 1.164
Loss G: 0.6296 (0.6044) Acc G: 94.678%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3599 (0.3949) Acc D Real: 96.817%
Loss D Fake: 0.7685 (0.7983) Acc D Fake: 5.329%
Loss D: 1.128
Loss G: 0.6304 (0.6046) Acc G: 94.679%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.4772 (0.3953) Acc D Real: 96.789%
Loss D Fake: 0.7674 (0.7982) Acc D Fake: 5.327%
Loss D: 1.245
Loss G: 0.6307 (0.6047) Acc G: 94.681%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3487 (0.3950) Acc D Real: 96.778%
Loss D Fake: 0.7670 (0.7980) Acc D Fake: 5.326%
Loss D: 1.116
Loss G: 0.6312 (0.6048) Acc G: 94.682%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3945 (0.3950) Acc D Real: 96.758%
Loss D Fake: 0.7669 (0.7979) Acc D Fake: 5.324%
Loss D: 1.161
Loss G: 0.6300 (0.6049) Acc G: 94.684%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3616 (0.3949) Acc D Real: 96.747%
Loss D Fake: 0.7691 (0.7977) Acc D Fake: 5.323%
Loss D: 1.131
Loss G: 0.6288 (0.6050) Acc G: 94.685%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.4282 (0.3950) Acc D Real: 96.722%
Loss D Fake: 0.7702 (0.7976) Acc D Fake: 5.321%
Loss D: 1.198
Loss G: 0.6282 (0.6051) Acc G: 94.687%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.3735 (0.3949) Acc D Real: 96.712%
Loss D Fake: 0.7704 (0.7975) Acc D Fake: 5.320%
Loss D: 1.144
Loss G: 0.6284 (0.6052) Acc G: 94.688%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3538 (0.3947) Acc D Real: 96.710%
Loss D Fake: 0.7698 (0.7974) Acc D Fake: 5.318%
Loss D: 1.124
Loss G: 0.6292 (0.6053) Acc G: 94.689%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.3812 (0.3947) Acc D Real: 96.711%
Loss D Fake: 0.7684 (0.7972) Acc D Fake: 5.317%
Loss D: 1.150
Loss G: 0.6304 (0.6055) Acc G: 94.691%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2719 (0.3941) Acc D Real: 96.714%
Loss D Fake: 0.7665 (0.7971) Acc D Fake: 5.315%
Loss D: 1.038
Loss G: 0.6325 (0.6056) Acc G: 94.692%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2695 (0.3936) Acc D Real: 96.705%
Loss D Fake: 0.7633 (0.7969) Acc D Fake: 5.314%
Loss D: 1.033
Loss G: 0.6355 (0.6057) Acc G: 94.694%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3556 (0.3934) Acc D Real: 96.697%
Loss D Fake: 0.7595 (0.7968) Acc D Fake: 5.312%
Loss D: 1.115
Loss G: 0.6386 (0.6059) Acc G: 94.695%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.4908 (0.3938) Acc D Real: 96.672%
Loss D Fake: 0.7565 (0.7966) Acc D Fake: 5.311%
Loss D: 1.247
Loss G: 0.6404 (0.6060) Acc G: 94.696%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.4743 (0.3942) Acc D Real: 96.664%
Loss D Fake: 0.7551 (0.7964) Acc D Fake: 5.311%
Loss D: 1.229
Loss G: 0.6410 (0.6062) Acc G: 94.697%
LR: 2.000e-04
Epoch: 13/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3519 (0.4482) Acc D Real: 89.974%
Loss D Fake: 0.7563 (0.7557) Acc D Fake: 5.000%
Loss D: 1.108
Loss G: 0.6392 (0.6396) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3457 (0.4140) Acc D Real: 90.729%
Loss D Fake: 0.7569 (0.7561) Acc D Fake: 5.000%
Loss D: 1.103
Loss G: 0.6390 (0.6394) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.4244 (0.4166) Acc D Real: 91.419%
Loss D Fake: 0.7570 (0.7563) Acc D Fake: 5.000%
Loss D: 1.181
Loss G: 0.6387 (0.6392) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3857 (0.4104) Acc D Real: 91.708%
Loss D Fake: 0.7581 (0.7567) Acc D Fake: 5.000%
Loss D: 1.144
Loss G: 0.6365 (0.6387) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.4146 (0.4111) Acc D Real: 92.101%
Loss D Fake: 0.7618 (0.7575) Acc D Fake: 5.000%
Loss D: 1.176
Loss G: 0.6335 (0.6378) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.4018 (0.4098) Acc D Real: 92.567%
Loss D Fake: 0.7656 (0.7587) Acc D Fake: 5.000%
Loss D: 1.167
Loss G: 0.6307 (0.6368) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3704 (0.4049) Acc D Real: 93.008%
Loss D Fake: 0.7690 (0.7600) Acc D Fake: 5.000%
Loss D: 1.139
Loss G: 0.6285 (0.6357) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.4009 (0.4044) Acc D Real: 93.218%
Loss D Fake: 0.7712 (0.7612) Acc D Fake: 5.000%
Loss D: 1.172
Loss G: 0.6272 (0.6348) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3604 (0.4000) Acc D Real: 93.302%
Loss D Fake: 0.7722 (0.7623) Acc D Fake: 5.000%
Loss D: 1.133
Loss G: 0.6273 (0.6340) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.3702 (0.3973) Acc D Real: 93.665%
Loss D Fake: 0.7714 (0.7632) Acc D Fake: 5.000%
Loss D: 1.142
Loss G: 0.6281 (0.6335) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2942 (0.3887) Acc D Real: 93.902%
Loss D Fake: 0.7698 (0.7637) Acc D Fake: 5.000%
Loss D: 1.064
Loss G: 0.6300 (0.6332) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3984 (0.3895) Acc D Real: 93.802%
Loss D Fake: 0.7670 (0.7640) Acc D Fake: 5.000%
Loss D: 1.165
Loss G: 0.6320 (0.6331) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.3887 (0.3894) Acc D Real: 93.813%
Loss D Fake: 0.7647 (0.7640) Acc D Fake: 5.000%
Loss D: 1.153
Loss G: 0.6337 (0.6332) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3813 (0.3889) Acc D Real: 93.889%
Loss D Fake: 0.7628 (0.7639) Acc D Fake: 5.000%
Loss D: 1.144
Loss G: 0.6352 (0.6333) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3079 (0.3838) Acc D Real: 93.923%
Loss D Fake: 0.7610 (0.7637) Acc D Fake: 5.000%
Loss D: 1.069
Loss G: 0.6369 (0.6335) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3635 (0.3826) Acc D Real: 93.882%
Loss D Fake: 0.7591 (0.7635) Acc D Fake: 5.000%
Loss D: 1.123
Loss G: 0.6380 (0.6338) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.4077 (0.3840) Acc D Real: 93.791%
Loss D Fake: 0.7582 (0.7632) Acc D Fake: 5.000%
Loss D: 1.166
Loss G: 0.6387 (0.6341) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.4236 (0.3861) Acc D Real: 93.684%
Loss D Fake: 0.7577 (0.7629) Acc D Fake: 5.000%
Loss D: 1.181
Loss G: 0.6391 (0.6343) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.3414 (0.3839) Acc D Real: 93.612%
Loss D Fake: 0.7579 (0.7627) Acc D Fake: 5.000%
Loss D: 1.099
Loss G: 0.6382 (0.6345) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3412 (0.3818) Acc D Real: 93.222%
Loss D Fake: 0.7598 (0.7625) Acc D Fake: 5.000%
Loss D: 1.101
Loss G: 0.6369 (0.6346) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.3392 (0.3799) Acc D Real: 93.075%
Loss D Fake: 0.7615 (0.7625) Acc D Fake: 5.000%
Loss D: 1.101
Loss G: 0.6364 (0.6347) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3140 (0.3770) Acc D Real: 93.096%
Loss D Fake: 0.7617 (0.7624) Acc D Fake: 5.000%
Loss D: 1.076
Loss G: 0.6371 (0.6348) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4555 (0.3803) Acc D Real: 92.702%
Loss D Fake: 0.7605 (0.7624) Acc D Fake: 5.000%
Loss D: 1.216
Loss G: 0.6386 (0.6350) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.3370 (0.3786) Acc D Real: 92.379%
Loss D Fake: 0.7579 (0.7622) Acc D Fake: 5.000%
Loss D: 1.095
Loss G: 0.6413 (0.6352) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2766 (0.3746) Acc D Real: 92.292%
Loss D Fake: 0.7538 (0.7619) Acc D Fake: 5.000%
Loss D: 1.030
Loss G: 0.6451 (0.6356) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3627 (0.3742) Acc D Real: 91.943%
Loss D Fake: 0.7492 (0.7614) Acc D Fake: 5.000%
Loss D: 1.112
Loss G: 0.6487 (0.6361) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3748 (0.3742) Acc D Real: 91.501%
Loss D Fake: 0.7453 (0.7608) Acc D Fake: 5.000%
Loss D: 1.120
Loss G: 0.6516 (0.6366) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.4206 (0.3758) Acc D Real: 90.894%
Loss D Fake: 0.7423 (0.7602) Acc D Fake: 5.000%
Loss D: 1.163
Loss G: 0.6537 (0.6372) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.3236 (0.3741) Acc D Real: 90.460%
Loss D Fake: 0.7400 (0.7595) Acc D Fake: 5.000%
Loss D: 1.064
Loss G: 0.6559 (0.6379) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3800 (0.3743) Acc D Real: 90.190%
Loss D Fake: 0.7376 (0.7588) Acc D Fake: 5.000%
Loss D: 1.118
Loss G: 0.6577 (0.6385) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3314 (0.3729) Acc D Real: 89.871%
Loss D Fake: 0.7358 (0.7581) Acc D Fake: 5.000%
Loss D: 1.067
Loss G: 0.6591 (0.6391) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.4525 (0.3753) Acc D Real: 89.171%
Loss D Fake: 0.7347 (0.7574) Acc D Fake: 5.000%
Loss D: 1.187
Loss G: 0.6598 (0.6398) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.4374 (0.3772) Acc D Real: 88.611%
Loss D Fake: 0.7343 (0.7567) Acc D Fake: 5.000%
Loss D: 1.172
Loss G: 0.6598 (0.6404) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3447 (0.3762) Acc D Real: 88.265%
Loss D Fake: 0.7345 (0.7561) Acc D Fake: 5.000%
Loss D: 1.079
Loss G: 0.6597 (0.6409) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.4359 (0.3779) Acc D Real: 87.593%
Loss D Fake: 0.7347 (0.7555) Acc D Fake: 5.000%
Loss D: 1.171
Loss G: 0.6593 (0.6414) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.4277 (0.3792) Acc D Real: 87.085%
Loss D Fake: 0.7354 (0.7549) Acc D Fake: 5.000%
Loss D: 1.163
Loss G: 0.6586 (0.6419) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.4407 (0.3809) Acc D Real: 86.676%
Loss D Fake: 0.7363 (0.7544) Acc D Fake: 5.000%
Loss D: 1.177
Loss G: 0.6577 (0.6423) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3836 (0.3809) Acc D Real: 86.406%
Loss D Fake: 0.7373 (0.7540) Acc D Fake: 5.000%
Loss D: 1.121
Loss G: 0.6569 (0.6427) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3491 (0.3801) Acc D Real: 86.120%
Loss D Fake: 0.7378 (0.7536) Acc D Fake: 5.000%
Loss D: 1.087
Loss G: 0.6567 (0.6430) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3550 (0.3795) Acc D Real: 85.911%
Loss D Fake: 0.7379 (0.7532) Acc D Fake: 5.000%
Loss D: 1.093
Loss G: 0.6566 (0.6434) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3405 (0.3786) Acc D Real: 85.725%
Loss D Fake: 0.7378 (0.7528) Acc D Fake: 5.000%
Loss D: 1.078
Loss G: 0.6568 (0.6437) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.3174 (0.3772) Acc D Real: 85.560%
Loss D Fake: 0.7373 (0.7525) Acc D Fake: 5.000%
Loss D: 1.055
Loss G: 0.6577 (0.6440) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3938 (0.3775) Acc D Real: 85.232%
Loss D Fake: 0.7365 (0.7521) Acc D Fake: 5.000%
Loss D: 1.130
Loss G: 0.6581 (0.6443) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3773 (0.3775) Acc D Real: 85.084%
Loss D Fake: 0.7363 (0.7518) Acc D Fake: 5.000%
Loss D: 1.114
Loss G: 0.6584 (0.6446) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3765 (0.3775) Acc D Real: 84.825%
Loss D Fake: 0.7360 (0.7514) Acc D Fake: 5.000%
Loss D: 1.113
Loss G: 0.6587 (0.6449) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4010 (0.3780) Acc D Real: 84.583%
Loss D Fake: 0.7358 (0.7511) Acc D Fake: 5.000%
Loss D: 1.137
Loss G: 0.6589 (0.6452) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.4023 (0.3785) Acc D Real: 84.243%
Loss D Fake: 0.7358 (0.7508) Acc D Fake: 5.000%
Loss D: 1.138
Loss G: 0.6589 (0.6455) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3723 (0.3784) Acc D Real: 84.095%
Loss D Fake: 0.7363 (0.7505) Acc D Fake: 5.000%
Loss D: 1.109
Loss G: 0.6576 (0.6458) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3056 (0.3769) Acc D Real: 84.035%
Loss D Fake: 0.7382 (0.7502) Acc D Fake: 5.000%
Loss D: 1.044
Loss G: 0.6567 (0.6460) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.4196 (0.3778) Acc D Real: 83.798%
Loss D Fake: 0.7394 (0.7500) Acc D Fake: 5.000%
Loss D: 1.159
Loss G: 0.6557 (0.6462) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4187 (0.3786) Acc D Real: 83.590%
Loss D Fake: 0.7408 (0.7498) Acc D Fake: 5.000%
Loss D: 1.160
Loss G: 0.6545 (0.6463) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3471 (0.3780) Acc D Real: 83.477%
Loss D Fake: 0.7423 (0.7497) Acc D Fake: 5.000%
Loss D: 1.089
Loss G: 0.6536 (0.6465) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3754 (0.3779) Acc D Real: 83.275%
Loss D Fake: 0.7430 (0.7496) Acc D Fake: 5.000%
Loss D: 1.118
Loss G: 0.6537 (0.6466) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.3438 (0.3773) Acc D Real: 83.030%
Loss D Fake: 0.7420 (0.7494) Acc D Fake: 5.000%
Loss D: 1.086
Loss G: 0.6552 (0.6468) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3734 (0.3772) Acc D Real: 82.879%
Loss D Fake: 0.7398 (0.7493) Acc D Fake: 5.000%
Loss D: 1.113
Loss G: 0.6571 (0.6470) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2927 (0.3758) Acc D Real: 82.913%
Loss D Fake: 0.7372 (0.7490) Acc D Fake: 5.000%
Loss D: 1.030
Loss G: 0.6597 (0.6472) Acc G: 94.035%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3085 (0.3746) Acc D Real: 82.840%
Loss D Fake: 0.7338 (0.7488) Acc D Fake: 6.034%
Loss D: 1.042
Loss G: 0.6627 (0.6474) Acc G: 92.931%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4132 (0.3752) Acc D Real: 82.631%
Loss D Fake: 0.7305 (0.7485) Acc D Fake: 7.147%
Loss D: 1.144
Loss G: 0.6652 (0.6477) Acc G: 91.808%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3641 (0.3751) Acc D Real: 82.503%
Loss D Fake: 0.7281 (0.7481) Acc D Fake: 8.250%
Loss D: 1.092
Loss G: 0.6671 (0.6481) Acc G: 90.694%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3651 (0.3749) Acc D Real: 82.453%
Loss D Fake: 0.7261 (0.7478) Acc D Fake: 9.344%
Loss D: 1.091
Loss G: 0.6688 (0.6484) Acc G: 89.617%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3988 (0.3753) Acc D Real: 82.198%
Loss D Fake: 0.7243 (0.7474) Acc D Fake: 10.403%
Loss D: 1.123
Loss G: 0.6701 (0.6488) Acc G: 88.548%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.4062 (0.3758) Acc D Real: 81.926%
Loss D Fake: 0.7244 (0.7470) Acc D Fake: 11.429%
Loss D: 1.131
Loss G: 0.6678 (0.6491) Acc G: 87.540%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3601 (0.3755) Acc D Real: 81.789%
Loss D Fake: 0.7291 (0.7468) Acc D Fake: 12.396%
Loss D: 1.089
Loss G: 0.6637 (0.6493) Acc G: 86.615%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3277 (0.3748) Acc D Real: 81.677%
Loss D Fake: 0.7347 (0.7466) Acc D Fake: 13.308%
Loss D: 1.062
Loss G: 0.6595 (0.6494) Acc G: 85.744%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3241 (0.3740) Acc D Real: 81.604%
Loss D Fake: 0.7400 (0.7465) Acc D Fake: 14.141%
Loss D: 1.064
Loss G: 0.6570 (0.6496) Acc G: 84.924%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.4198 (0.3747) Acc D Real: 81.286%
Loss D Fake: 0.7414 (0.7464) Acc D Fake: 14.950%
Loss D: 1.161
Loss G: 0.6584 (0.6497) Acc G: 84.104%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.3399 (0.3742) Acc D Real: 81.140%
Loss D Fake: 0.7369 (0.7463) Acc D Fake: 15.760%
Loss D: 1.077
Loss G: 0.6628 (0.6499) Acc G: 83.284%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.3496 (0.3738) Acc D Real: 81.039%
Loss D Fake: 0.7315 (0.7460) Acc D Fake: 16.570%
Loss D: 1.081
Loss G: 0.6663 (0.6501) Acc G: 82.464%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3780 (0.3739) Acc D Real: 80.874%
Loss D Fake: 0.7277 (0.7458) Acc D Fake: 17.381%
Loss D: 1.106
Loss G: 0.6694 (0.6504) Acc G: 81.643%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2940 (0.3728) Acc D Real: 80.848%
Loss D Fake: 0.7240 (0.7455) Acc D Fake: 18.192%
Loss D: 1.018
Loss G: 0.6728 (0.6507) Acc G: 80.822%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3720 (0.3728) Acc D Real: 80.654%
Loss D Fake: 0.7201 (0.7451) Acc D Fake: 19.005%
Loss D: 1.092
Loss G: 0.6759 (0.6511) Acc G: 80.023%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.4189 (0.3734) Acc D Real: 80.406%
Loss D Fake: 0.7170 (0.7447) Acc D Fake: 19.817%
Loss D: 1.136
Loss G: 0.6782 (0.6514) Acc G: 79.224%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.2592 (0.3718) Acc D Real: 80.398%
Loss D Fake: 0.7146 (0.7443) Acc D Fake: 20.608%
Loss D: 0.974
Loss G: 0.6806 (0.6518) Acc G: 78.446%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3156 (0.3711) Acc D Real: 80.291%
Loss D Fake: 0.7118 (0.7439) Acc D Fake: 21.378%
Loss D: 1.027
Loss G: 0.6833 (0.6522) Acc G: 77.667%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4259 (0.3718) Acc D Real: 79.941%
Loss D Fake: 0.7091 (0.7434) Acc D Fake: 22.149%
Loss D: 1.135
Loss G: 0.6853 (0.6527) Acc G: 76.908%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3299 (0.3713) Acc D Real: 79.823%
Loss D Fake: 0.7072 (0.7430) Acc D Fake: 22.900%
Loss D: 1.037
Loss G: 0.6871 (0.6531) Acc G: 76.169%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3420 (0.3709) Acc D Real: 79.787%
Loss D Fake: 0.7058 (0.7425) Acc D Fake: 23.632%
Loss D: 1.048
Loss G: 0.6878 (0.6536) Acc G: 75.449%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3875 (0.3711) Acc D Real: 79.558%
Loss D Fake: 0.7058 (0.7420) Acc D Fake: 24.346%
Loss D: 1.093
Loss G: 0.6879 (0.6540) Acc G: 74.747%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.2379 (0.3694) Acc D Real: 79.596%
Loss D Fake: 0.7055 (0.7416) Acc D Fake: 25.042%
Loss D: 0.943
Loss G: 0.6891 (0.6544) Acc G: 74.062%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3887 (0.3697) Acc D Real: 79.377%
Loss D Fake: 0.7041 (0.7411) Acc D Fake: 25.720%
Loss D: 1.093
Loss G: 0.6905 (0.6549) Acc G: 73.395%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.4200 (0.3703) Acc D Real: 79.165%
Loss D Fake: 0.7032 (0.7406) Acc D Fake: 26.382%
Loss D: 1.123
Loss G: 0.6910 (0.6553) Acc G: 72.744%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3512 (0.3701) Acc D Real: 79.010%
Loss D Fake: 0.7031 (0.7402) Acc D Fake: 27.028%
Loss D: 1.054
Loss G: 0.6912 (0.6558) Acc G: 72.108%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3249 (0.3695) Acc D Real: 78.925%
Loss D Fake: 0.7029 (0.7397) Acc D Fake: 27.659%
Loss D: 1.028
Loss G: 0.6919 (0.6562) Acc G: 71.488%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3810 (0.3697) Acc D Real: 78.777%
Loss D Fake: 0.7021 (0.7393) Acc D Fake: 28.255%
Loss D: 1.083
Loss G: 0.6928 (0.6566) Acc G: 70.882%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.3705 (0.3697) Acc D Real: 78.631%
Loss D Fake: 0.7012 (0.7389) Acc D Fake: 28.857%
Loss D: 1.072
Loss G: 0.6938 (0.6571) Acc G: 70.291%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.2995 (0.3689) Acc D Real: 78.623%
Loss D Fake: 0.7000 (0.7384) Acc D Fake: 29.444%
Loss D: 1.000
Loss G: 0.6953 (0.6575) Acc G: 69.713%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3708 (0.3689) Acc D Real: 78.458%
Loss D Fake: 0.6982 (0.7380) Acc D Fake: 30.019%
Loss D: 1.069
Loss G: 0.6971 (0.6579) Acc G: 69.148%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3679 (0.3689) Acc D Real: 78.349%
Loss D Fake: 0.6964 (0.7375) Acc D Fake: 30.581%
Loss D: 1.064
Loss G: 0.6987 (0.6584) Acc G: 68.596%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3280 (0.3684) Acc D Real: 78.274%
Loss D Fake: 0.6952 (0.7370) Acc D Fake: 31.130%
Loss D: 1.023
Loss G: 0.6993 (0.6589) Acc G: 68.056%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.4674 (0.3695) Acc D Real: 78.022%
Loss D Fake: 0.6956 (0.7366) Acc D Fake: 31.667%
Loss D: 1.163
Loss G: 0.6981 (0.6593) Acc G: 67.527%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3723 (0.3695) Acc D Real: 77.868%
Loss D Fake: 0.6975 (0.7361) Acc D Fake: 32.174%
Loss D: 1.070
Loss G: 0.6968 (0.6597) Acc G: 67.029%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2783 (0.3686) Acc D Real: 77.905%
Loss D Fake: 0.6984 (0.7357) Acc D Fake: 32.670%
Loss D: 0.977
Loss G: 0.6970 (0.6601) Acc G: 66.541%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3290 (0.3681) Acc D Real: 77.805%
Loss D Fake: 0.6976 (0.7353) Acc D Fake: 33.156%
Loss D: 1.027
Loss G: 0.6983 (0.6605) Acc G: 66.064%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3619 (0.3681) Acc D Real: 77.672%
Loss D Fake: 0.6959 (0.7349) Acc D Fake: 33.632%
Loss D: 1.058
Loss G: 0.7001 (0.6609) Acc G: 65.579%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4406 (0.3688) Acc D Real: 77.444%
Loss D Fake: 0.6941 (0.7345) Acc D Fake: 34.115%
Loss D: 1.135
Loss G: 0.7015 (0.6613) Acc G: 65.104%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.3072 (0.3682) Acc D Real: 77.410%
Loss D Fake: 0.6925 (0.7341) Acc D Fake: 34.588%
Loss D: 1.000
Loss G: 0.7032 (0.6618) Acc G: 64.639%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3608 (0.3681) Acc D Real: 77.301%
Loss D Fake: 0.6906 (0.7336) Acc D Fake: 35.051%
Loss D: 1.051
Loss G: 0.7049 (0.6622) Acc G: 64.184%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3780 (0.3682) Acc D Real: 77.234%
Loss D Fake: 0.6890 (0.7332) Acc D Fake: 35.522%
Loss D: 1.067
Loss G: 0.7063 (0.6627) Acc G: 63.721%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.3560 (0.3681) Acc D Real: 77.156%
Loss D Fake: 0.6875 (0.7327) Acc D Fake: 35.983%
Loss D: 1.044
Loss G: 0.7076 (0.6631) Acc G: 63.267%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.4881 (0.3693) Acc D Real: 76.892%
Loss D Fake: 0.6867 (0.7322) Acc D Fake: 36.436%
Loss D: 1.175
Loss G: 0.7074 (0.6636) Acc G: 62.822%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3396 (0.3690) Acc D Real: 76.814%
Loss D Fake: 0.6871 (0.7318) Acc D Fake: 36.879%
Loss D: 1.027
Loss G: 0.7072 (0.6640) Acc G: 62.386%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.5104 (0.3704) Acc D Real: 76.511%
Loss D Fake: 0.6875 (0.7314) Acc D Fake: 37.314%
Loss D: 1.198
Loss G: 0.7060 (0.6644) Acc G: 61.958%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.4360 (0.3710) Acc D Real: 76.317%
Loss D Fake: 0.6891 (0.7310) Acc D Fake: 37.740%
Loss D: 1.125
Loss G: 0.7042 (0.6648) Acc G: 61.538%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3790 (0.3711) Acc D Real: 76.189%
Loss D Fake: 0.6908 (0.7306) Acc D Fake: 38.159%
Loss D: 1.070
Loss G: 0.7025 (0.6651) Acc G: 61.127%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3394 (0.3708) Acc D Real: 76.140%
Loss D Fake: 0.6922 (0.7302) Acc D Fake: 38.569%
Loss D: 1.032
Loss G: 0.7015 (0.6655) Acc G: 60.723%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.4313 (0.3713) Acc D Real: 75.981%
Loss D Fake: 0.6931 (0.7299) Acc D Fake: 38.972%
Loss D: 1.124
Loss G: 0.7003 (0.6658) Acc G: 60.327%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2876 (0.3706) Acc D Real: 75.980%
Loss D Fake: 0.6939 (0.7295) Acc D Fake: 39.367%
Loss D: 0.982
Loss G: 0.7000 (0.6661) Acc G: 59.938%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3860 (0.3707) Acc D Real: 75.892%
Loss D Fake: 0.6941 (0.7292) Acc D Fake: 39.755%
Loss D: 1.080
Loss G: 0.6994 (0.6664) Acc G: 59.557%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3408 (0.3704) Acc D Real: 75.807%
Loss D Fake: 0.6947 (0.7289) Acc D Fake: 40.136%
Loss D: 1.036
Loss G: 0.6992 (0.6667) Acc G: 59.182%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3677 (0.3704) Acc D Real: 75.677%
Loss D Fake: 0.6948 (0.7286) Acc D Fake: 40.511%
Loss D: 1.063
Loss G: 0.6991 (0.6670) Acc G: 58.814%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3446 (0.3702) Acc D Real: 75.638%
Loss D Fake: 0.6947 (0.7283) Acc D Fake: 40.878%
Loss D: 1.039
Loss G: 0.6995 (0.6673) Acc G: 58.452%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3628 (0.3701) Acc D Real: 75.550%
Loss D Fake: 0.6943 (0.7280) Acc D Fake: 41.239%
Loss D: 1.057
Loss G: 0.7000 (0.6676) Acc G: 58.097%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.4184 (0.3705) Acc D Real: 75.427%
Loss D Fake: 0.6937 (0.7277) Acc D Fake: 41.594%
Loss D: 1.112
Loss G: 0.7003 (0.6679) Acc G: 57.749%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3687 (0.3705) Acc D Real: 75.322%
Loss D Fake: 0.6935 (0.7274) Acc D Fake: 41.942%
Loss D: 1.062
Loss G: 0.7005 (0.6682) Acc G: 57.406%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4659 (0.3713) Acc D Real: 75.129%
Loss D Fake: 0.6935 (0.7271) Acc D Fake: 42.284%
Loss D: 1.159
Loss G: 0.7000 (0.6684) Acc G: 57.069%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3772 (0.3714) Acc D Real: 75.033%
Loss D Fake: 0.6941 (0.7268) Acc D Fake: 42.621%
Loss D: 1.071
Loss G: 0.6994 (0.6687) Acc G: 56.738%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.4696 (0.3722) Acc D Real: 74.868%
Loss D Fake: 0.6948 (0.7266) Acc D Fake: 42.952%
Loss D: 1.164
Loss G: 0.6981 (0.6689) Acc G: 56.412%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.4030 (0.3725) Acc D Real: 74.747%
Loss D Fake: 0.6962 (0.7263) Acc D Fake: 43.277%
Loss D: 1.099
Loss G: 0.6966 (0.6692) Acc G: 56.092%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2987 (0.3719) Acc D Real: 74.740%
Loss D Fake: 0.6973 (0.7261) Acc D Fake: 43.597%
Loss D: 0.996
Loss G: 0.6959 (0.6694) Acc G: 55.778%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.3429 (0.3716) Acc D Real: 74.688%
Loss D Fake: 0.6977 (0.7258) Acc D Fake: 43.912%
Loss D: 1.041
Loss G: 0.6958 (0.6696) Acc G: 55.468%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.3883 (0.3718) Acc D Real: 74.579%
Loss D Fake: 0.6978 (0.7256) Acc D Fake: 44.221%
Loss D: 1.086
Loss G: 0.6954 (0.6698) Acc G: 55.164%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3112 (0.3713) Acc D Real: 74.598%
Loss D Fake: 0.6981 (0.7254) Acc D Fake: 44.526%
Loss D: 1.009
Loss G: 0.6955 (0.6700) Acc G: 54.864%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3311 (0.3710) Acc D Real: 74.534%
Loss D Fake: 0.6978 (0.7251) Acc D Fake: 44.825%
Loss D: 1.029
Loss G: 0.6961 (0.6703) Acc G: 54.570%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3353 (0.3707) Acc D Real: 74.505%
Loss D Fake: 0.6970 (0.7249) Acc D Fake: 45.120%
Loss D: 1.032
Loss G: 0.6971 (0.6705) Acc G: 54.280%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.4534 (0.3713) Acc D Real: 74.315%
Loss D Fake: 0.6962 (0.7247) Acc D Fake: 45.410%
Loss D: 1.150
Loss G: 0.6974 (0.6707) Acc G: 53.995%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3643 (0.3713) Acc D Real: 74.252%
Loss D Fake: 0.6961 (0.7245) Acc D Fake: 45.696%
Loss D: 1.060
Loss G: 0.6976 (0.6709) Acc G: 53.714%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3221 (0.3709) Acc D Real: 74.269%
Loss D Fake: 0.6958 (0.7242) Acc D Fake: 45.977%
Loss D: 1.018
Loss G: 0.6981 (0.6711) Acc G: 53.438%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3718 (0.3709) Acc D Real: 74.209%
Loss D Fake: 0.6953 (0.7240) Acc D Fake: 46.253%
Loss D: 1.067
Loss G: 0.6985 (0.6713) Acc G: 53.165%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3311 (0.3706) Acc D Real: 74.175%
Loss D Fake: 0.6948 (0.7238) Acc D Fake: 46.526%
Loss D: 1.026
Loss G: 0.6992 (0.6715) Acc G: 52.897%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3923 (0.3707) Acc D Real: 74.050%
Loss D Fake: 0.6942 (0.7236) Acc D Fake: 46.794%
Loss D: 1.086
Loss G: 0.6996 (0.6717) Acc G: 52.634%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.4194 (0.3711) Acc D Real: 73.917%
Loss D Fake: 0.6940 (0.7233) Acc D Fake: 47.058%
Loss D: 1.113
Loss G: 0.6997 (0.6720) Acc G: 52.374%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3862 (0.3712) Acc D Real: 73.837%
Loss D Fake: 0.6942 (0.7231) Acc D Fake: 47.318%
Loss D: 1.080
Loss G: 0.6993 (0.6722) Acc G: 52.118%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3779 (0.3713) Acc D Real: 73.766%
Loss D Fake: 0.6946 (0.7229) Acc D Fake: 47.575%
Loss D: 1.072
Loss G: 0.6989 (0.6724) Acc G: 51.866%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.4510 (0.3719) Acc D Real: 73.635%
Loss D Fake: 0.6953 (0.7227) Acc D Fake: 47.827%
Loss D: 1.146
Loss G: 0.6979 (0.6725) Acc G: 51.617%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3623 (0.3718) Acc D Real: 73.555%
Loss D Fake: 0.6964 (0.7225) Acc D Fake: 48.076%
Loss D: 1.059
Loss G: 0.6968 (0.6727) Acc G: 51.373%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.2917 (0.3712) Acc D Real: 73.558%
Loss D Fake: 0.6972 (0.7223) Acc D Fake: 48.321%
Loss D: 0.989
Loss G: 0.6966 (0.6729) Acc G: 51.131%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.3352 (0.3710) Acc D Real: 73.524%
Loss D Fake: 0.6971 (0.7221) Acc D Fake: 48.563%
Loss D: 1.032
Loss G: 0.6970 (0.6731) Acc G: 50.894%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3606 (0.3709) Acc D Real: 73.486%
Loss D Fake: 0.6966 (0.7220) Acc D Fake: 48.801%
Loss D: 1.057
Loss G: 0.6975 (0.6733) Acc G: 50.659%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3941 (0.3710) Acc D Real: 73.399%
Loss D Fake: 0.6962 (0.7218) Acc D Fake: 49.036%
Loss D: 1.090
Loss G: 0.6977 (0.6734) Acc G: 50.429%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3468 (0.3709) Acc D Real: 73.350%
Loss D Fake: 0.6959 (0.7216) Acc D Fake: 49.267%
Loss D: 1.043
Loss G: 0.6982 (0.6736) Acc G: 50.201%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3660 (0.3708) Acc D Real: 73.299%
Loss D Fake: 0.6955 (0.7214) Acc D Fake: 49.495%
Loss D: 1.061
Loss G: 0.6983 (0.6738) Acc G: 49.977%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.4379 (0.3713) Acc D Real: 73.149%
Loss D Fake: 0.6959 (0.7212) Acc D Fake: 49.720%
Loss D: 1.134
Loss G: 0.6975 (0.6739) Acc G: 49.755%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3725 (0.3713) Acc D Real: 73.084%
Loss D Fake: 0.6969 (0.7211) Acc D Fake: 49.942%
Loss D: 1.069
Loss G: 0.6967 (0.6741) Acc G: 49.537%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.4021 (0.3715) Acc D Real: 72.995%
Loss D Fake: 0.6978 (0.7209) Acc D Fake: 50.161%
Loss D: 1.100
Loss G: 0.6958 (0.6743) Acc G: 49.322%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3826 (0.3716) Acc D Real: 72.936%
Loss D Fake: 0.6988 (0.7208) Acc D Fake: 50.377%
Loss D: 1.081
Loss G: 0.6947 (0.6744) Acc G: 49.110%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3127 (0.3712) Acc D Real: 72.926%
Loss D Fake: 0.6998 (0.7206) Acc D Fake: 50.590%
Loss D: 1.012
Loss G: 0.6944 (0.6745) Acc G: 48.900%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3041 (0.3707) Acc D Real: 72.915%
Loss D Fake: 0.6997 (0.7205) Acc D Fake: 50.800%
Loss D: 1.004
Loss G: 0.6949 (0.6747) Acc G: 48.694%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3748 (0.3708) Acc D Real: 72.852%
Loss D Fake: 0.6991 (0.7203) Acc D Fake: 51.007%
Loss D: 1.074
Loss G: 0.6955 (0.6748) Acc G: 48.490%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4155 (0.3711) Acc D Real: 72.746%
Loss D Fake: 0.6988 (0.7202) Acc D Fake: 51.211%
Loss D: 1.114
Loss G: 0.6955 (0.6749) Acc G: 48.289%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3745 (0.3711) Acc D Real: 72.708%
Loss D Fake: 0.6988 (0.7200) Acc D Fake: 51.413%
Loss D: 1.073
Loss G: 0.6955 (0.6751) Acc G: 48.091%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2806 (0.3705) Acc D Real: 72.707%
Loss D Fake: 0.6987 (0.7199) Acc D Fake: 51.612%
Loss D: 0.979
Loss G: 0.6960 (0.6752) Acc G: 47.895%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3312 (0.3702) Acc D Real: 72.699%
Loss D Fake: 0.6980 (0.7198) Acc D Fake: 51.808%
Loss D: 1.029
Loss G: 0.6970 (0.6754) Acc G: 47.702%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3248 (0.3700) Acc D Real: 72.660%
Loss D Fake: 0.6969 (0.7196) Acc D Fake: 52.002%
Loss D: 1.022
Loss G: 0.6982 (0.6755) Acc G: 47.511%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3244 (0.3697) Acc D Real: 72.654%
Loss D Fake: 0.6956 (0.7195) Acc D Fake: 52.194%
Loss D: 1.020
Loss G: 0.6997 (0.6757) Acc G: 47.323%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3124 (0.3693) Acc D Real: 72.664%
Loss D Fake: 0.6940 (0.7193) Acc D Fake: 52.382%
Loss D: 1.006
Loss G: 0.7017 (0.6758) Acc G: 47.137%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3139 (0.3689) Acc D Real: 72.666%
Loss D Fake: 0.6918 (0.7191) Acc D Fake: 52.569%
Loss D: 1.006
Loss G: 0.7041 (0.6760) Acc G: 46.953%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3486 (0.3688) Acc D Real: 72.627%
Loss D Fake: 0.6895 (0.7189) Acc D Fake: 52.753%
Loss D: 1.038
Loss G: 0.7062 (0.6762) Acc G: 46.772%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3704 (0.3688) Acc D Real: 72.582%
Loss D Fake: 0.6876 (0.7187) Acc D Fake: 52.935%
Loss D: 1.058
Loss G: 0.7080 (0.6764) Acc G: 46.593%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3783 (0.3689) Acc D Real: 72.535%
Loss D Fake: 0.6861 (0.7185) Acc D Fake: 53.115%
Loss D: 1.064
Loss G: 0.7093 (0.6766) Acc G: 46.417%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3367 (0.3687) Acc D Real: 72.513%
Loss D Fake: 0.6849 (0.7183) Acc D Fake: 53.292%
Loss D: 1.022
Loss G: 0.7106 (0.6768) Acc G: 46.242%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3400 (0.3685) Acc D Real: 72.494%
Loss D Fake: 0.6837 (0.7181) Acc D Fake: 53.467%
Loss D: 1.024
Loss G: 0.7118 (0.6770) Acc G: 46.070%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.3721 (0.3685) Acc D Real: 72.439%
Loss D Fake: 0.6826 (0.7179) Acc D Fake: 53.640%
Loss D: 1.055
Loss G: 0.7129 (0.6773) Acc G: 45.900%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3470 (0.3684) Acc D Real: 72.418%
Loss D Fake: 0.6815 (0.7177) Acc D Fake: 53.821%
Loss D: 1.029
Loss G: 0.7140 (0.6775) Acc G: 45.722%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2791 (0.3679) Acc D Real: 72.458%
Loss D Fake: 0.6802 (0.7174) Acc D Fake: 54.000%
Loss D: 0.959
Loss G: 0.7158 (0.6777) Acc G: 45.545%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.2920 (0.3674) Acc D Real: 72.488%
Loss D Fake: 0.6795 (0.7172) Acc D Fake: 54.167%
Loss D: 0.971
Loss G: 0.7144 (0.6779) Acc G: 45.382%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.3466 (0.3673) Acc D Real: 72.464%
Loss D Fake: 0.6836 (0.7170) Acc D Fake: 54.321%
Loss D: 1.030
Loss G: 0.7105 (0.6781) Acc G: 45.240%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.3629 (0.3672) Acc D Real: 72.415%
Loss D Fake: 0.6898 (0.7168) Acc D Fake: 54.464%
Loss D: 1.053
Loss G: 0.7053 (0.6783) Acc G: 45.109%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.4480 (0.3677) Acc D Real: 72.332%
Loss D Fake: 0.6965 (0.7167) Acc D Fake: 54.586%
Loss D: 1.145
Loss G: 0.7034 (0.6784) Acc G: 44.990%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3885 (0.3678) Acc D Real: 72.308%
Loss D Fake: 0.6937 (0.7166) Acc D Fake: 54.706%
Loss D: 1.082
Loss G: 0.7083 (0.6786) Acc G: 44.863%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3906 (0.3680) Acc D Real: 72.286%
Loss D Fake: 0.6865 (0.7164) Acc D Fake: 54.844%
Loss D: 1.077
Loss G: 0.7134 (0.6788) Acc G: 44.717%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3389 (0.3678) Acc D Real: 72.276%
Loss D Fake: 0.6813 (0.7162) Acc D Fake: 54.990%
Loss D: 1.020
Loss G: 0.7173 (0.6790) Acc G: 44.564%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3346 (0.3676) Acc D Real: 72.258%
Loss D Fake: 0.6773 (0.7160) Acc D Fake: 55.145%
Loss D: 1.012
Loss G: 0.7204 (0.6793) Acc G: 44.412%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.2863 (0.3671) Acc D Real: 72.284%
Loss D Fake: 0.6739 (0.7157) Acc D Fake: 55.307%
Loss D: 0.960
Loss G: 0.7236 (0.6795) Acc G: 44.253%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3814 (0.3672) Acc D Real: 72.262%
Loss D Fake: 0.6709 (0.7155) Acc D Fake: 55.467%
Loss D: 1.052
Loss G: 0.7259 (0.6798) Acc G: 44.095%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3538 (0.3672) Acc D Real: 72.240%
Loss D Fake: 0.6688 (0.7152) Acc D Fake: 55.625%
Loss D: 1.023
Loss G: 0.7278 (0.6801) Acc G: 43.939%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.4784 (0.3678) Acc D Real: 72.147%
Loss D Fake: 0.6674 (0.7150) Acc D Fake: 55.791%
Loss D: 1.146
Loss G: 0.7282 (0.6803) Acc G: 43.776%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2667 (0.3672) Acc D Real: 72.185%
Loss D Fake: 0.6670 (0.7147) Acc D Fake: 55.955%
Loss D: 0.934
Loss G: 0.7291 (0.6806) Acc G: 43.614%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3872 (0.3673) Acc D Real: 72.135%
Loss D Fake: 0.6660 (0.7144) Acc D Fake: 56.117%
Loss D: 1.053
Loss G: 0.7298 (0.6809) Acc G: 43.454%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.3425 (0.3672) Acc D Real: 72.110%
Loss D Fake: 0.6654 (0.7141) Acc D Fake: 56.278%
Loss D: 1.008
Loss G: 0.7303 (0.6812) Acc G: 43.296%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3894 (0.3673) Acc D Real: 72.048%
Loss D Fake: 0.6650 (0.7139) Acc D Fake: 56.436%
Loss D: 1.054
Loss G: 0.7305 (0.6814) Acc G: 43.140%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3947 (0.3675) Acc D Real: 71.981%
Loss D Fake: 0.6650 (0.7136) Acc D Fake: 56.593%
Loss D: 1.060
Loss G: 0.7304 (0.6817) Acc G: 42.985%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.3231 (0.3672) Acc D Real: 71.993%
Loss D Fake: 0.6651 (0.7133) Acc D Fake: 56.749%
Loss D: 0.988
Loss G: 0.7303 (0.6820) Acc G: 42.832%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3637 (0.3672) Acc D Real: 71.959%
Loss D Fake: 0.6652 (0.7131) Acc D Fake: 56.902%
Loss D: 1.029
Loss G: 0.7302 (0.6822) Acc G: 42.681%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.4397 (0.3676) Acc D Real: 71.881%
Loss D Fake: 0.6656 (0.7128) Acc D Fake: 57.054%
Loss D: 1.105
Loss G: 0.7294 (0.6825) Acc G: 42.532%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3825 (0.3677) Acc D Real: 71.846%
Loss D Fake: 0.6665 (0.7126) Acc D Fake: 57.204%
Loss D: 1.049
Loss G: 0.7283 (0.6827) Acc G: 42.384%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3990 (0.3678) Acc D Real: 71.784%
Loss D Fake: 0.6675 (0.7123) Acc D Fake: 57.353%
Loss D: 1.066
Loss G: 0.7272 (0.6830) Acc G: 42.237%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3691 (0.3678) Acc D Real: 71.763%
Loss D Fake: 0.6686 (0.7121) Acc D Fake: 57.500%
Loss D: 1.038
Loss G: 0.7260 (0.6832) Acc G: 42.092%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.4185 (0.3681) Acc D Real: 71.686%
Loss D Fake: 0.6697 (0.7119) Acc D Fake: 57.646%
Loss D: 1.088
Loss G: 0.7246 (0.6834) Acc G: 41.949%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3601 (0.3681) Acc D Real: 71.654%
Loss D Fake: 0.6711 (0.7117) Acc D Fake: 57.789%
Loss D: 1.031
Loss G: 0.7233 (0.6836) Acc G: 41.807%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.3765 (0.3681) Acc D Real: 71.605%
Loss D Fake: 0.6722 (0.7114) Acc D Fake: 57.932%
Loss D: 1.049
Loss G: 0.7222 (0.6838) Acc G: 41.667%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.3551 (0.3680) Acc D Real: 71.573%
Loss D Fake: 0.6731 (0.7112) Acc D Fake: 58.073%
Loss D: 1.028
Loss G: 0.7213 (0.6840) Acc G: 41.528%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3017 (0.3677) Acc D Real: 71.570%
Loss D Fake: 0.6736 (0.7111) Acc D Fake: 58.212%
Loss D: 0.975
Loss G: 0.7212 (0.6842) Acc G: 41.390%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.4203 (0.3680) Acc D Real: 71.491%
Loss D Fake: 0.6736 (0.7109) Acc D Fake: 58.351%
Loss D: 1.094
Loss G: 0.7208 (0.6844) Acc G: 41.254%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.4195 (0.3682) Acc D Real: 71.408%
Loss D Fake: 0.6743 (0.7107) Acc D Fake: 58.487%
Loss D: 1.094
Loss G: 0.7198 (0.6846) Acc G: 41.120%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.4137 (0.3685) Acc D Real: 71.354%
Loss D Fake: 0.6754 (0.7105) Acc D Fake: 58.622%
Loss D: 1.089
Loss G: 0.7184 (0.6848) Acc G: 40.986%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4111 (0.3687) Acc D Real: 71.300%
Loss D Fake: 0.6770 (0.7103) Acc D Fake: 58.756%
Loss D: 1.088
Loss G: 0.7165 (0.6849) Acc G: 40.854%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.3537 (0.3686) Acc D Real: 71.268%
Loss D Fake: 0.6787 (0.7102) Acc D Fake: 58.880%
Loss D: 1.032
Loss G: 0.7150 (0.6851) Acc G: 40.732%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2288 (0.3679) Acc D Real: 71.331%
Loss D Fake: 0.6796 (0.7100) Acc D Fake: 59.003%
Loss D: 0.908
Loss G: 0.7151 (0.6852) Acc G: 40.611%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.4031 (0.3681) Acc D Real: 71.282%
Loss D Fake: 0.6792 (0.7099) Acc D Fake: 59.125%
Loss D: 1.082
Loss G: 0.7151 (0.6854) Acc G: 40.492%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3584 (0.3680) Acc D Real: 71.261%
Loss D Fake: 0.6797 (0.7097) Acc D Fake: 59.245%
Loss D: 1.038
Loss G: 0.7141 (0.6855) Acc G: 40.373%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.3751 (0.3681) Acc D Real: 71.234%
Loss D Fake: 0.6812 (0.7096) Acc D Fake: 59.365%
Loss D: 1.056
Loss G: 0.7129 (0.6857) Acc G: 40.256%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2957 (0.3677) Acc D Real: 71.247%
Loss D Fake: 0.6825 (0.7094) Acc D Fake: 59.475%
Loss D: 0.978
Loss G: 0.7123 (0.6858) Acc G: 40.148%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3678 (0.3677) Acc D Real: 71.211%
Loss D Fake: 0.6830 (0.7093) Acc D Fake: 59.575%
Loss D: 1.051
Loss G: 0.7121 (0.6859) Acc G: 40.049%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.3529 (0.3676) Acc D Real: 71.173%
Loss D Fake: 0.6832 (0.7092) Acc D Fake: 59.667%
Loss D: 1.036
Loss G: 0.7122 (0.6860) Acc G: 39.959%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3838 (0.3677) Acc D Real: 71.150%
Loss D Fake: 0.6832 (0.7090) Acc D Fake: 59.757%
Loss D: 1.067
Loss G: 0.7123 (0.6862) Acc G: 39.871%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.3222 (0.3675) Acc D Real: 71.138%
Loss D Fake: 0.6830 (0.7089) Acc D Fake: 59.847%
Loss D: 1.005
Loss G: 0.7129 (0.6863) Acc G: 39.783%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.3717 (0.3675) Acc D Real: 71.125%
Loss D Fake: 0.6823 (0.7088) Acc D Fake: 59.936%
Loss D: 1.054
Loss G: 0.7134 (0.6864) Acc G: 39.696%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.2404 (0.3669) Acc D Real: 71.166%
Loss D Fake: 0.6823 (0.7087) Acc D Fake: 60.024%
Loss D: 0.923
Loss G: 0.7128 (0.6866) Acc G: 39.609%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.3698 (0.3669) Acc D Real: 71.140%
Loss D Fake: 0.6841 (0.7085) Acc D Fake: 60.111%
Loss D: 1.054
Loss G: 0.7116 (0.6867) Acc G: 39.532%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3838 (0.3670) Acc D Real: 71.124%
Loss D Fake: 0.6859 (0.7084) Acc D Fake: 60.190%
Loss D: 1.070
Loss G: 0.7104 (0.6868) Acc G: 39.455%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.2780 (0.3666) Acc D Real: 71.155%
Loss D Fake: 0.6874 (0.7083) Acc D Fake: 60.267%
Loss D: 0.965
Loss G: 0.7098 (0.6869) Acc G: 39.387%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3122 (0.3663) Acc D Real: 71.168%
Loss D Fake: 0.6876 (0.7082) Acc D Fake: 60.336%
Loss D: 1.000
Loss G: 0.7111 (0.6870) Acc G: 39.319%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3238 (0.3661) Acc D Real: 71.179%
Loss D Fake: 0.6853 (0.7081) Acc D Fake: 60.405%
Loss D: 1.009
Loss G: 0.7144 (0.6871) Acc G: 39.245%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.4239 (0.3664) Acc D Real: 71.121%
Loss D Fake: 0.6817 (0.7080) Acc D Fake: 60.481%
Loss D: 1.106
Loss G: 0.7171 (0.6873) Acc G: 39.171%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2910 (0.3661) Acc D Real: 71.168%
Loss D Fake: 0.6788 (0.7079) Acc D Fake: 60.556%
Loss D: 0.970
Loss G: 0.7203 (0.6874) Acc G: 39.090%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2818 (0.3657) Acc D Real: 71.179%
Loss D Fake: 0.6750 (0.7077) Acc D Fake: 60.637%
Loss D: 0.957
Loss G: 0.7242 (0.6876) Acc G: 39.009%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2214 (0.3650) Acc D Real: 71.237%
Loss D Fake: 0.6706 (0.7076) Acc D Fake: 60.719%
Loss D: 0.892
Loss G: 0.7292 (0.6878) Acc G: 38.922%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2677 (0.3646) Acc D Real: 71.250%
Loss D Fake: 0.6653 (0.7074) Acc D Fake: 60.807%
Loss D: 0.933
Loss G: 0.7347 (0.6880) Acc G: 38.836%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3274 (0.3644) Acc D Real: 71.249%
Loss D Fake: 0.6601 (0.7071) Acc D Fake: 60.894%
Loss D: 0.988
Loss G: 0.7398 (0.6882) Acc G: 38.750%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.3189 (0.3642) Acc D Real: 71.250%
Loss D Fake: 0.6567 (0.7069) Acc D Fake: 60.980%
Loss D: 0.976
Loss G: 0.7411 (0.6885) Acc G: 38.665%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3511 (0.3641) Acc D Real: 71.234%
Loss D Fake: 0.6577 (0.7067) Acc D Fake: 61.066%
Loss D: 1.009
Loss G: 0.7406 (0.6887) Acc G: 38.589%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3594 (0.3641) Acc D Real: 71.221%
Loss D Fake: 0.6593 (0.7065) Acc D Fake: 61.143%
Loss D: 1.019
Loss G: 0.7398 (0.6889) Acc G: 38.513%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3227 (0.3639) Acc D Real: 71.210%
Loss D Fake: 0.6604 (0.7063) Acc D Fake: 61.213%
Loss D: 0.983
Loss G: 0.7400 (0.6892) Acc G: 38.445%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.4505 (0.3643) Acc D Real: 71.167%
Loss D Fake: 0.6606 (0.7061) Acc D Fake: 61.281%
Loss D: 1.111
Loss G: 0.7400 (0.6894) Acc G: 38.378%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.1715 (0.3635) Acc D Real: 71.186%
Loss D Fake: 0.6596 (0.7059) Acc D Fake: 61.299%
Loss D: 0.831
Loss G: 0.7436 (0.6896) Acc G: 38.361%
LR: 2.000e-04
Epoch: 14/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2872 (0.2983) Acc D Real: 76.536%
Loss D Fake: 0.6521 (0.6538) Acc D Fake: 77.500%
Loss D: 0.939
Loss G: 0.7516 (0.7493) Acc G: 21.667%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3485 (0.3150) Acc D Real: 75.469%
Loss D Fake: 0.6509 (0.6528) Acc D Fake: 77.778%
Loss D: 0.999
Loss G: 0.7451 (0.7479) Acc G: 22.222%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2907 (0.3089) Acc D Real: 76.419%
Loss D Fake: 0.6741 (0.6581) Acc D Fake: 75.833%
Loss D: 0.965
Loss G: 0.7467 (0.7476) Acc G: 22.500%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3728 (0.3217) Acc D Real: 75.198%
Loss D Fake: 0.6487 (0.6562) Acc D Fake: 76.333%
Loss D: 1.022
Loss G: 0.7598 (0.7500) Acc G: 22.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.3480 (0.3261) Acc D Real: 73.958%
Loss D Fake: 0.6385 (0.6533) Acc D Fake: 77.222%
Loss D: 0.986
Loss G: 0.7675 (0.7530) Acc G: 21.389%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.3476 (0.3292) Acc D Real: 73.512%
Loss D Fake: 0.6326 (0.6503) Acc D Fake: 77.857%
Loss D: 0.980
Loss G: 0.7717 (0.7556) Acc G: 20.714%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.4416 (0.3432) Acc D Real: 71.712%
Loss D Fake: 0.6296 (0.6477) Acc D Fake: 78.542%
Loss D: 1.071
Loss G: 0.7739 (0.7579) Acc G: 20.208%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2885 (0.3371) Acc D Real: 72.714%
Loss D Fake: 0.6277 (0.6455) Acc D Fake: 79.074%
Loss D: 0.916
Loss G: 0.7760 (0.7599) Acc G: 19.815%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.3967 (0.3431) Acc D Real: 72.073%
Loss D Fake: 0.6259 (0.6435) Acc D Fake: 79.500%
Loss D: 1.023
Loss G: 0.7773 (0.7617) Acc G: 19.500%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.2820 (0.3375) Acc D Real: 72.836%
Loss D Fake: 0.6248 (0.6418) Acc D Fake: 79.848%
Loss D: 0.907
Loss G: 0.7788 (0.7632) Acc G: 19.091%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.3957 (0.3424) Acc D Real: 71.940%
Loss D Fake: 0.6234 (0.6403) Acc D Fake: 80.278%
Loss D: 1.019
Loss G: 0.7798 (0.7646) Acc G: 18.750%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2956 (0.3388) Acc D Real: 72.436%
Loss D Fake: 0.6226 (0.6389) Acc D Fake: 80.641%
Loss D: 0.918
Loss G: 0.7810 (0.7659) Acc G: 18.462%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.3080 (0.3366) Acc D Real: 72.493%
Loss D Fake: 0.6214 (0.6377) Acc D Fake: 80.952%
Loss D: 0.929
Loss G: 0.7825 (0.7670) Acc G: 18.214%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3545 (0.3378) Acc D Real: 72.149%
Loss D Fake: 0.6200 (0.6365) Acc D Fake: 81.222%
Loss D: 0.975
Loss G: 0.7839 (0.7682) Acc G: 18.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.4927 (0.3475) Acc D Real: 71.090%
Loss D Fake: 0.6194 (0.6354) Acc D Fake: 81.458%
Loss D: 1.112
Loss G: 0.7834 (0.7691) Acc G: 17.812%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3875 (0.3498) Acc D Real: 70.613%
Loss D Fake: 0.6203 (0.6345) Acc D Fake: 81.667%
Loss D: 1.008
Loss G: 0.7822 (0.7699) Acc G: 17.647%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2808 (0.3460) Acc D Real: 71.169%
Loss D Fake: 0.6211 (0.6338) Acc D Fake: 81.852%
Loss D: 0.902
Loss G: 0.7818 (0.7706) Acc G: 17.500%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3233 (0.3448) Acc D Real: 71.261%
Loss D Fake: 0.6211 (0.6331) Acc D Fake: 82.018%
Loss D: 0.944
Loss G: 0.7820 (0.7712) Acc G: 17.368%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.3896 (0.3470) Acc D Real: 70.826%
Loss D Fake: 0.6210 (0.6325) Acc D Fake: 82.167%
Loss D: 1.011
Loss G: 0.7817 (0.7717) Acc G: 17.250%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.4693 (0.3529) Acc D Real: 70.064%
Loss D Fake: 0.6217 (0.6320) Acc D Fake: 82.302%
Loss D: 1.091
Loss G: 0.7801 (0.7721) Acc G: 17.143%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.3294 (0.3518) Acc D Real: 70.066%
Loss D Fake: 0.6231 (0.6316) Acc D Fake: 82.424%
Loss D: 0.952
Loss G: 0.7787 (0.7724) Acc G: 17.045%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3569 (0.3520) Acc D Real: 70.068%
Loss D Fake: 0.6241 (0.6313) Acc D Fake: 82.536%
Loss D: 0.981
Loss G: 0.7776 (0.7726) Acc G: 16.957%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.4259 (0.3551) Acc D Real: 69.648%
Loss D Fake: 0.6251 (0.6310) Acc D Fake: 82.639%
Loss D: 1.051
Loss G: 0.7759 (0.7727) Acc G: 16.875%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.4545 (0.3591) Acc D Real: 69.152%
Loss D Fake: 0.6269 (0.6309) Acc D Fake: 82.733%
Loss D: 1.081
Loss G: 0.7731 (0.7728) Acc G: 16.800%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.3249 (0.3578) Acc D Real: 69.375%
Loss D Fake: 0.6293 (0.6308) Acc D Fake: 82.756%
Loss D: 0.954
Loss G: 0.7707 (0.7727) Acc G: 16.795%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.2929 (0.3554) Acc D Real: 69.614%
Loss D Fake: 0.6309 (0.6308) Acc D Fake: 82.778%
Loss D: 0.924
Loss G: 0.7694 (0.7726) Acc G: 16.790%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.2781 (0.3526) Acc D Real: 69.827%
Loss D Fake: 0.6315 (0.6308) Acc D Fake: 82.798%
Loss D: 0.910
Loss G: 0.7693 (0.7724) Acc G: 16.786%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3342 (0.3520) Acc D Real: 69.774%
Loss D Fake: 0.6314 (0.6308) Acc D Fake: 82.816%
Loss D: 0.966
Loss G: 0.7695 (0.7723) Acc G: 16.782%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.4053 (0.3537) Acc D Real: 69.542%
Loss D Fake: 0.6314 (0.6309) Acc D Fake: 82.833%
Loss D: 1.037
Loss G: 0.7690 (0.7722) Acc G: 16.778%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2934 (0.3518) Acc D Real: 69.829%
Loss D Fake: 0.6326 (0.6309) Acc D Fake: 82.849%
Loss D: 0.926
Loss G: 0.7669 (0.7721) Acc G: 16.828%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3274 (0.3510) Acc D Real: 69.945%
Loss D Fake: 0.6356 (0.6311) Acc D Fake: 82.812%
Loss D: 0.963
Loss G: 0.7641 (0.7718) Acc G: 16.875%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3481 (0.3509) Acc D Real: 69.998%
Loss D Fake: 0.6395 (0.6313) Acc D Fake: 82.727%
Loss D: 0.988
Loss G: 0.7599 (0.7714) Acc G: 16.970%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.3942 (0.3522) Acc D Real: 69.874%
Loss D Fake: 0.6454 (0.6317) Acc D Fake: 82.598%
Loss D: 1.040
Loss G: 0.7537 (0.7709) Acc G: 17.157%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3235 (0.3514) Acc D Real: 70.030%
Loss D Fake: 0.6531 (0.6323) Acc D Fake: 82.429%
Loss D: 0.977
Loss G: 0.7481 (0.7703) Acc G: 17.381%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.2775 (0.3493) Acc D Real: 70.269%
Loss D Fake: 0.6570 (0.6330) Acc D Fake: 82.222%
Loss D: 0.935
Loss G: 0.7499 (0.7697) Acc G: 17.593%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2380 (0.3463) Acc D Real: 70.564%
Loss D Fake: 0.6503 (0.6335) Acc D Fake: 82.072%
Loss D: 0.888
Loss G: 0.7579 (0.7694) Acc G: 17.703%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2860 (0.3447) Acc D Real: 70.696%
Loss D Fake: 0.6414 (0.6337) Acc D Fake: 81.974%
Loss D: 0.927
Loss G: 0.7661 (0.7693) Acc G: 17.763%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3492 (0.3449) Acc D Real: 70.637%
Loss D Fake: 0.6340 (0.6337) Acc D Fake: 81.923%
Loss D: 0.983
Loss G: 0.7725 (0.7694) Acc G: 17.821%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2388 (0.3422) Acc D Real: 70.906%
Loss D Fake: 0.6283 (0.6336) Acc D Fake: 81.917%
Loss D: 0.867
Loss G: 0.7786 (0.7696) Acc G: 17.833%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.4072 (0.3438) Acc D Real: 70.761%
Loss D Fake: 0.6230 (0.6333) Acc D Fake: 81.911%
Loss D: 1.030
Loss G: 0.7832 (0.7700) Acc G: 17.846%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.4680 (0.3468) Acc D Real: 70.444%
Loss D Fake: 0.6200 (0.6330) Acc D Fake: 81.905%
Loss D: 1.088
Loss G: 0.7848 (0.7703) Acc G: 17.817%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.3048 (0.3458) Acc D Real: 70.566%
Loss D Fake: 0.6191 (0.6327) Acc D Fake: 81.938%
Loss D: 0.924
Loss G: 0.7861 (0.7707) Acc G: 17.791%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.2991 (0.3447) Acc D Real: 70.631%
Loss D Fake: 0.6178 (0.6323) Acc D Fake: 81.970%
Loss D: 0.917
Loss G: 0.7878 (0.7711) Acc G: 17.765%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3759 (0.3454) Acc D Real: 70.573%
Loss D Fake: 0.6163 (0.6320) Acc D Fake: 82.000%
Loss D: 0.992
Loss G: 0.7891 (0.7715) Acc G: 17.741%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3535 (0.3456) Acc D Real: 70.503%
Loss D Fake: 0.6153 (0.6316) Acc D Fake: 82.029%
Loss D: 0.969
Loss G: 0.7900 (0.7719) Acc G: 17.717%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4036 (0.3468) Acc D Real: 70.331%
Loss D Fake: 0.6147 (0.6313) Acc D Fake: 82.057%
Loss D: 1.018
Loss G: 0.7902 (0.7723) Acc G: 17.695%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.4788 (0.3496) Acc D Real: 69.990%
Loss D Fake: 0.6152 (0.6309) Acc D Fake: 82.083%
Loss D: 1.094
Loss G: 0.7884 (0.7726) Acc G: 17.674%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3888 (0.3504) Acc D Real: 69.889%
Loss D Fake: 0.6171 (0.6306) Acc D Fake: 82.109%
Loss D: 1.006
Loss G: 0.7858 (0.7729) Acc G: 17.653%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.4096 (0.3516) Acc D Real: 69.694%
Loss D Fake: 0.6194 (0.6304) Acc D Fake: 82.133%
Loss D: 1.029
Loss G: 0.7826 (0.7731) Acc G: 17.633%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3798 (0.3521) Acc D Real: 69.610%
Loss D Fake: 0.6221 (0.6303) Acc D Fake: 82.157%
Loss D: 1.002
Loss G: 0.7794 (0.7732) Acc G: 17.614%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.2692 (0.3505) Acc D Real: 69.822%
Loss D Fake: 0.6243 (0.6301) Acc D Fake: 82.179%
Loss D: 0.894
Loss G: 0.7777 (0.7733) Acc G: 17.596%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.4542 (0.3525) Acc D Real: 69.533%
Loss D Fake: 0.6270 (0.6301) Acc D Fake: 82.201%
Loss D: 1.081
Loss G: 0.7711 (0.7732) Acc G: 17.610%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3157 (0.3518) Acc D Real: 69.603%
Loss D Fake: 0.6364 (0.6302) Acc D Fake: 82.160%
Loss D: 0.952
Loss G: 0.7606 (0.7730) Acc G: 17.685%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2704 (0.3503) Acc D Real: 69.799%
Loss D Fake: 0.6512 (0.6306) Acc D Fake: 82.061%
Loss D: 0.922
Loss G: 0.7411 (0.7724) Acc G: 17.848%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.2809 (0.3491) Acc D Real: 69.913%
Loss D Fake: 1.2346 (0.6414) Acc D Fake: 80.625%
Loss D: 1.515
Loss G: 0.7579 (0.7722) Acc G: 17.917%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3580 (0.3492) Acc D Real: 69.837%
Loss D Fake: 0.6359 (0.6413) Acc D Fake: 80.614%
Loss D: 0.994
Loss G: 0.7720 (0.7721) Acc G: 17.924%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.2937 (0.3483) Acc D Real: 69.944%
Loss D Fake: 0.6268 (0.6410) Acc D Fake: 80.661%
Loss D: 0.921
Loss G: 0.7784 (0.7723) Acc G: 17.902%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4178 (0.3494) Acc D Real: 69.816%
Loss D Fake: 0.6218 (0.6407) Acc D Fake: 80.706%
Loss D: 1.040
Loss G: 0.7818 (0.7724) Acc G: 17.853%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.5375 (0.3526) Acc D Real: 69.424%
Loss D Fake: 0.6200 (0.6404) Acc D Fake: 80.778%
Loss D: 1.158
Loss G: 0.7813 (0.7726) Acc G: 17.806%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3536 (0.3526) Acc D Real: 69.385%
Loss D Fake: 0.6208 (0.6400) Acc D Fake: 80.847%
Loss D: 0.974
Loss G: 0.7798 (0.7727) Acc G: 17.760%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3719 (0.3529) Acc D Real: 69.396%
Loss D Fake: 0.6218 (0.6397) Acc D Fake: 80.914%
Loss D: 0.994
Loss G: 0.7781 (0.7728) Acc G: 17.715%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3937 (0.3536) Acc D Real: 69.243%
Loss D Fake: 0.6232 (0.6395) Acc D Fake: 80.979%
Loss D: 1.017
Loss G: 0.7759 (0.7728) Acc G: 17.672%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3208 (0.3530) Acc D Real: 69.252%
Loss D Fake: 0.6248 (0.6392) Acc D Fake: 81.042%
Loss D: 0.946
Loss G: 0.7742 (0.7728) Acc G: 17.630%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3930 (0.3537) Acc D Real: 69.150%
Loss D Fake: 0.6261 (0.6390) Acc D Fake: 81.103%
Loss D: 1.019
Loss G: 0.7723 (0.7728) Acc G: 17.590%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.4845 (0.3556) Acc D Real: 68.845%
Loss D Fake: 0.6281 (0.6389) Acc D Fake: 81.162%
Loss D: 1.113
Loss G: 0.7687 (0.7728) Acc G: 17.551%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.4204 (0.3566) Acc D Real: 68.670%
Loss D Fake: 0.6316 (0.6388) Acc D Fake: 81.219%
Loss D: 1.052
Loss G: 0.7642 (0.7726) Acc G: 17.512%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4241 (0.3576) Acc D Real: 68.456%
Loss D Fake: 0.6356 (0.6387) Acc D Fake: 81.275%
Loss D: 1.060
Loss G: 0.7592 (0.7724) Acc G: 17.475%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.3851 (0.3580) Acc D Real: 68.370%
Loss D Fake: 0.6399 (0.6387) Acc D Fake: 81.329%
Loss D: 1.025
Loss G: 0.7543 (0.7722) Acc G: 17.440%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3821 (0.3583) Acc D Real: 68.244%
Loss D Fake: 0.6440 (0.6388) Acc D Fake: 81.381%
Loss D: 1.026
Loss G: 0.7496 (0.7719) Acc G: 17.405%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.4626 (0.3598) Acc D Real: 67.992%
Loss D Fake: 0.6484 (0.6389) Acc D Fake: 81.432%
Loss D: 1.111
Loss G: 0.7440 (0.7715) Acc G: 17.371%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3790 (0.3601) Acc D Real: 67.880%
Loss D Fake: 0.6535 (0.6392) Acc D Fake: 81.481%
Loss D: 1.032
Loss G: 0.7386 (0.7710) Acc G: 17.338%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3332 (0.3597) Acc D Real: 67.847%
Loss D Fake: 0.6579 (0.6394) Acc D Fake: 81.530%
Loss D: 0.991
Loss G: 0.7343 (0.7705) Acc G: 17.306%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.2719 (0.3585) Acc D Real: 67.960%
Loss D Fake: 0.6609 (0.6397) Acc D Fake: 81.577%
Loss D: 0.933
Loss G: 0.7322 (0.7700) Acc G: 17.275%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.2332 (0.3569) Acc D Real: 68.102%
Loss D Fake: 0.6617 (0.6400) Acc D Fake: 81.622%
Loss D: 0.895
Loss G: 0.7327 (0.7695) Acc G: 17.244%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3656 (0.3570) Acc D Real: 68.035%
Loss D Fake: 0.6608 (0.6403) Acc D Fake: 81.667%
Loss D: 1.026
Loss G: 0.7335 (0.7690) Acc G: 17.215%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3695 (0.3571) Acc D Real: 67.944%
Loss D Fake: 0.6603 (0.6405) Acc D Fake: 81.710%
Loss D: 1.030
Loss G: 0.7338 (0.7686) Acc G: 17.186%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3455 (0.3570) Acc D Real: 67.897%
Loss D Fake: 0.6601 (0.6408) Acc D Fake: 81.752%
Loss D: 1.006
Loss G: 0.7341 (0.7681) Acc G: 17.158%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3353 (0.3567) Acc D Real: 67.871%
Loss D Fake: 0.6599 (0.6410) Acc D Fake: 81.772%
Loss D: 0.995
Loss G: 0.7345 (0.7677) Acc G: 17.152%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.4672 (0.3581) Acc D Real: 67.656%
Loss D Fake: 0.6600 (0.6413) Acc D Fake: 81.792%
Loss D: 1.127
Loss G: 0.7333 (0.7673) Acc G: 17.146%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3253 (0.3577) Acc D Real: 67.642%
Loss D Fake: 0.6615 (0.6415) Acc D Fake: 81.811%
Loss D: 0.987
Loss G: 0.7320 (0.7668) Acc G: 17.140%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3731 (0.3579) Acc D Real: 67.579%
Loss D Fake: 0.6627 (0.6418) Acc D Fake: 81.829%
Loss D: 1.036
Loss G: 0.7308 (0.7664) Acc G: 17.134%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.2401 (0.3564) Acc D Real: 67.718%
Loss D Fake: 0.6633 (0.6420) Acc D Fake: 81.847%
Loss D: 0.903
Loss G: 0.7315 (0.7660) Acc G: 17.129%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3650 (0.3566) Acc D Real: 67.641%
Loss D Fake: 0.6623 (0.6423) Acc D Fake: 81.865%
Loss D: 1.027
Loss G: 0.7325 (0.7656) Acc G: 17.123%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3748 (0.3568) Acc D Real: 67.591%
Loss D Fake: 0.6618 (0.6425) Acc D Fake: 81.882%
Loss D: 1.037
Loss G: 0.7328 (0.7652) Acc G: 17.118%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4119 (0.3574) Acc D Real: 67.483%
Loss D Fake: 0.6621 (0.6427) Acc D Fake: 81.899%
Loss D: 1.074
Loss G: 0.7319 (0.7648) Acc G: 17.132%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3776 (0.3576) Acc D Real: 67.402%
Loss D Fake: 0.6635 (0.6430) Acc D Fake: 81.897%
Loss D: 1.041
Loss G: 0.7304 (0.7644) Acc G: 17.146%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3249 (0.3573) Acc D Real: 67.412%
Loss D Fake: 0.6649 (0.6432) Acc D Fake: 81.894%
Loss D: 0.990
Loss G: 0.7295 (0.7640) Acc G: 17.159%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3158 (0.3568) Acc D Real: 67.425%
Loss D Fake: 0.6656 (0.6435) Acc D Fake: 81.891%
Loss D: 0.981
Loss G: 0.7294 (0.7636) Acc G: 17.172%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4490 (0.3578) Acc D Real: 67.256%
Loss D Fake: 0.6662 (0.6437) Acc D Fake: 81.870%
Loss D: 1.115
Loss G: 0.7279 (0.7632) Acc G: 17.204%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3284 (0.3575) Acc D Real: 67.269%
Loss D Fake: 0.6680 (0.6440) Acc D Fake: 81.850%
Loss D: 0.996
Loss G: 0.7266 (0.7628) Acc G: 17.234%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3694 (0.3576) Acc D Real: 67.219%
Loss D Fake: 0.6691 (0.6443) Acc D Fake: 81.830%
Loss D: 1.039
Loss G: 0.7255 (0.7624) Acc G: 17.264%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2869 (0.3569) Acc D Real: 67.293%
Loss D Fake: 0.6698 (0.6445) Acc D Fake: 81.810%
Loss D: 0.957
Loss G: 0.7259 (0.7620) Acc G: 17.294%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3136 (0.3564) Acc D Real: 67.314%
Loss D Fake: 0.6690 (0.6448) Acc D Fake: 81.791%
Loss D: 0.983
Loss G: 0.7274 (0.7617) Acc G: 17.323%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3079 (0.3559) Acc D Real: 67.390%
Loss D Fake: 0.6673 (0.6450) Acc D Fake: 81.772%
Loss D: 0.975
Loss G: 0.7299 (0.7613) Acc G: 17.351%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.3837 (0.3562) Acc D Real: 67.323%
Loss D Fake: 0.6652 (0.6452) Acc D Fake: 81.753%
Loss D: 1.049
Loss G: 0.7317 (0.7610) Acc G: 17.378%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.2937 (0.3555) Acc D Real: 67.390%
Loss D Fake: 0.6637 (0.6454) Acc D Fake: 81.735%
Loss D: 0.957
Loss G: 0.7340 (0.7607) Acc G: 17.405%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3549 (0.3555) Acc D Real: 67.371%
Loss D Fake: 0.6616 (0.6456) Acc D Fake: 81.718%
Loss D: 1.016
Loss G: 0.7364 (0.7605) Acc G: 17.432%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2755 (0.3547) Acc D Real: 67.458%
Loss D Fake: 0.6593 (0.6457) Acc D Fake: 81.700%
Loss D: 0.935
Loss G: 0.7401 (0.7603) Acc G: 17.458%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2859 (0.3540) Acc D Real: 67.576%
Loss D Fake: 0.6555 (0.6458) Acc D Fake: 81.683%
Loss D: 0.941
Loss G: 0.7453 (0.7601) Acc G: 17.483%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.2135 (0.3526) Acc D Real: 67.743%
Loss D Fake: 0.6503 (0.6459) Acc D Fake: 81.667%
Loss D: 0.864
Loss G: 0.7529 (0.7601) Acc G: 17.508%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.2471 (0.3516) Acc D Real: 67.873%
Loss D Fake: 0.6432 (0.6458) Acc D Fake: 81.650%
Loss D: 0.890
Loss G: 0.7624 (0.7601) Acc G: 17.533%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3399 (0.3515) Acc D Real: 67.849%
Loss D Fake: 0.6355 (0.6457) Acc D Fake: 81.634%
Loss D: 0.975
Loss G: 0.7710 (0.7602) Acc G: 17.557%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2366 (0.3504) Acc D Real: 68.002%
Loss D Fake: 0.6285 (0.6456) Acc D Fake: 81.619%
Loss D: 0.865
Loss G: 0.7807 (0.7604) Acc G: 17.580%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3051 (0.3500) Acc D Real: 68.053%
Loss D Fake: 0.6207 (0.6453) Acc D Fake: 81.603%
Loss D: 0.926
Loss G: 0.7903 (0.7607) Acc G: 17.603%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.4901 (0.3513) Acc D Real: 67.883%
Loss D Fake: 0.6146 (0.6451) Acc D Fake: 81.588%
Loss D: 1.105
Loss G: 0.7949 (0.7610) Acc G: 17.626%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.3684 (0.3514) Acc D Real: 67.868%
Loss D Fake: 0.6125 (0.6448) Acc D Fake: 81.573%
Loss D: 0.981
Loss G: 0.7970 (0.7613) Acc G: 17.648%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.3952 (0.3519) Acc D Real: 67.821%
Loss D Fake: 0.6117 (0.6444) Acc D Fake: 81.559%
Loss D: 1.007
Loss G: 0.7970 (0.7617) Acc G: 17.670%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.2786 (0.3512) Acc D Real: 67.918%
Loss D Fake: 0.6116 (0.6441) Acc D Fake: 81.544%
Loss D: 0.890
Loss G: 0.7981 (0.7620) Acc G: 17.691%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.2737 (0.3505) Acc D Real: 67.995%
Loss D Fake: 0.6100 (0.6438) Acc D Fake: 81.530%
Loss D: 0.884
Loss G: 0.8010 (0.7623) Acc G: 17.712%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3441 (0.3504) Acc D Real: 68.018%
Loss D Fake: 0.6077 (0.6435) Acc D Fake: 81.517%
Loss D: 0.952
Loss G: 0.8035 (0.7627) Acc G: 17.733%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.2532 (0.3496) Acc D Real: 68.115%
Loss D Fake: 0.6054 (0.6432) Acc D Fake: 81.503%
Loss D: 0.859
Loss G: 0.8075 (0.7631) Acc G: 17.753%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3957 (0.3500) Acc D Real: 68.094%
Loss D Fake: 0.6025 (0.6428) Acc D Fake: 81.490%
Loss D: 0.998
Loss G: 0.8099 (0.7635) Acc G: 17.758%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3799 (0.3502) Acc D Real: 68.072%
Loss D Fake: 0.6016 (0.6424) Acc D Fake: 81.491%
Loss D: 0.982
Loss G: 0.8097 (0.7639) Acc G: 17.763%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4221 (0.3508) Acc D Real: 68.047%
Loss D Fake: 0.6030 (0.6421) Acc D Fake: 81.493%
Loss D: 1.025
Loss G: 0.8059 (0.7643) Acc G: 17.768%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.3352 (0.3507) Acc D Real: 68.070%
Loss D Fake: 0.6067 (0.6418) Acc D Fake: 81.480%
Loss D: 0.942
Loss G: 0.8012 (0.7646) Acc G: 17.787%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3755 (0.3509) Acc D Real: 68.046%
Loss D Fake: 0.6108 (0.6415) Acc D Fake: 81.467%
Loss D: 0.986
Loss G: 0.7952 (0.7649) Acc G: 17.806%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3230 (0.3507) Acc D Real: 68.088%
Loss D Fake: 0.6155 (0.6413) Acc D Fake: 81.455%
Loss D: 0.939
Loss G: 0.7905 (0.7651) Acc G: 17.825%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3009 (0.3503) Acc D Real: 68.129%
Loss D Fake: 0.6182 (0.6411) Acc D Fake: 81.443%
Loss D: 0.919
Loss G: 0.7889 (0.7653) Acc G: 17.843%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3850 (0.3506) Acc D Real: 68.127%
Loss D Fake: 0.6193 (0.6409) Acc D Fake: 81.431%
Loss D: 1.004
Loss G: 0.7869 (0.7655) Acc G: 17.861%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.3113 (0.3502) Acc D Real: 68.151%
Loss D Fake: 0.6210 (0.6408) Acc D Fake: 81.419%
Loss D: 0.932
Loss G: 0.7861 (0.7656) Acc G: 17.879%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4501 (0.3511) Acc D Real: 68.037%
Loss D Fake: 0.6235 (0.6406) Acc D Fake: 81.407%
Loss D: 1.074
Loss G: 0.7791 (0.7658) Acc G: 17.896%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2908 (0.3506) Acc D Real: 68.084%
Loss D Fake: 0.6307 (0.6405) Acc D Fake: 81.396%
Loss D: 0.921
Loss G: 0.7741 (0.7658) Acc G: 17.927%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3369 (0.3505) Acc D Real: 68.089%
Loss D Fake: 0.6339 (0.6405) Acc D Fake: 81.371%
Loss D: 0.971
Loss G: 0.7725 (0.7659) Acc G: 17.957%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.2740 (0.3498) Acc D Real: 68.168%
Loss D Fake: 0.6332 (0.6404) Acc D Fake: 81.347%
Loss D: 0.907
Loss G: 0.7796 (0.7660) Acc G: 17.987%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3472 (0.3498) Acc D Real: 68.187%
Loss D Fake: 0.6259 (0.6403) Acc D Fake: 81.323%
Loss D: 0.973
Loss G: 0.7889 (0.7662) Acc G: 18.016%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.2167 (0.3488) Acc D Real: 68.293%
Loss D Fake: 0.6175 (0.6401) Acc D Fake: 81.299%
Loss D: 0.834
Loss G: 0.8031 (0.7665) Acc G: 18.045%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.2704 (0.3482) Acc D Real: 68.383%
Loss D Fake: 0.6054 (0.6399) Acc D Fake: 81.276%
Loss D: 0.876
Loss G: 0.8185 (0.7669) Acc G: 18.073%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3161 (0.3479) Acc D Real: 68.421%
Loss D Fake: 0.5953 (0.6395) Acc D Fake: 81.253%
Loss D: 0.911
Loss G: 0.8280 (0.7673) Acc G: 18.101%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3397 (0.3478) Acc D Real: 68.444%
Loss D Fake: 0.5909 (0.6392) Acc D Fake: 81.231%
Loss D: 0.931
Loss G: 0.8303 (0.7678) Acc G: 18.115%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3030 (0.3475) Acc D Real: 68.523%
Loss D Fake: 0.5913 (0.6388) Acc D Fake: 81.221%
Loss D: 0.894
Loss G: 0.8293 (0.7683) Acc G: 18.130%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.4493 (0.3483) Acc D Real: 68.466%
Loss D Fake: 0.5962 (0.6385) Acc D Fake: 81.212%
Loss D: 1.045
Loss G: 0.8145 (0.7686) Acc G: 18.144%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.2666 (0.3477) Acc D Real: 68.562%
Loss D Fake: 0.6134 (0.6383) Acc D Fake: 81.190%
Loss D: 0.880
Loss G: 0.8026 (0.7689) Acc G: 18.170%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2386 (0.3468) Acc D Real: 68.678%
Loss D Fake: 0.6119 (0.6381) Acc D Fake: 81.169%
Loss D: 0.851
Loss G: 0.8229 (0.7693) Acc G: 18.184%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.2614 (0.3462) Acc D Real: 68.800%
Loss D Fake: 0.5851 (0.6377) Acc D Fake: 81.160%
Loss D: 0.847
Loss G: 0.8554 (0.7699) Acc G: 18.198%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3081 (0.3459) Acc D Real: 68.870%
Loss D Fake: 0.5654 (0.6372) Acc D Fake: 81.152%
Loss D: 0.873
Loss G: 0.8722 (0.7707) Acc G: 18.199%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3935 (0.3463) Acc D Real: 68.859%
Loss D Fake: 0.5602 (0.6366) Acc D Fake: 81.144%
Loss D: 0.954
Loss G: 0.8691 (0.7714) Acc G: 18.212%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.2670 (0.3457) Acc D Real: 68.952%
Loss D Fake: 0.5708 (0.6361) Acc D Fake: 81.135%
Loss D: 0.838
Loss G: 0.8472 (0.7720) Acc G: 18.225%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3367 (0.3456) Acc D Real: 69.045%
Loss D Fake: 0.6058 (0.6359) Acc D Fake: 81.127%
Loss D: 0.943
Loss G: 0.8349 (0.7724) Acc G: 18.237%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3415 (0.3456) Acc D Real: 69.182%
Loss D Fake: 0.5812 (0.6355) Acc D Fake: 81.119%
Loss D: 0.923
Loss G: 0.8746 (0.7731) Acc G: 18.238%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3440 (0.3456) Acc D Real: 69.268%
Loss D Fake: 0.5539 (0.6349) Acc D Fake: 81.123%
Loss D: 0.898
Loss G: 0.8892 (0.7740) Acc G: 18.239%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3028 (0.3453) Acc D Real: 69.339%
Loss D Fake: 0.5607 (0.6344) Acc D Fake: 81.127%
Loss D: 0.863
Loss G: 0.8406 (0.7744) Acc G: 18.251%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2992 (0.3450) Acc D Real: 69.499%
Loss D Fake: 0.6943 (0.6348) Acc D Fake: 81.119%
Loss D: 0.994
Loss G: 0.8820 (0.7752) Acc G: 18.252%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3050 (0.3447) Acc D Real: 69.616%
Loss D Fake: 0.5337 (0.6341) Acc D Fake: 81.123%
Loss D: 0.839
Loss G: 0.9301 (0.7763) Acc G: 18.252%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3640 (0.3448) Acc D Real: 69.679%
Loss D Fake: 0.5167 (0.6333) Acc D Fake: 81.138%
Loss D: 0.881
Loss G: 0.9445 (0.7774) Acc G: 18.241%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3853 (0.3451) Acc D Real: 69.697%
Loss D Fake: 0.5118 (0.6325) Acc D Fake: 81.153%
Loss D: 0.897
Loss G: 0.9461 (0.7786) Acc G: 18.231%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3516 (0.3452) Acc D Real: 69.732%
Loss D Fake: 0.5181 (0.6317) Acc D Fake: 81.168%
Loss D: 0.870
Loss G: 0.9139 (0.7795) Acc G: 18.231%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.2525 (0.3445) Acc D Real: 69.876%
Loss D Fake: 0.5707 (0.6313) Acc D Fake: 81.171%
Loss D: 0.823
Loss G: 0.8131 (0.7797) Acc G: 18.243%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3433 (0.3445) Acc D Real: 70.002%
Loss D Fake: 0.7229 (0.6319) Acc D Fake: 80.861%
Loss D: 1.066
Loss G: 0.8733 (0.7804) Acc G: 18.244%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3860 (0.3448) Acc D Real: 70.068%
Loss D Fake: 0.5393 (0.6313) Acc D Fake: 80.867%
Loss D: 0.925
Loss G: 0.9242 (0.7813) Acc G: 18.244%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3359 (0.3447) Acc D Real: 70.155%
Loss D Fake: 0.5212 (0.6306) Acc D Fake: 80.872%
Loss D: 0.857
Loss G: 0.9375 (0.7823) Acc G: 18.234%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3160 (0.3445) Acc D Real: 70.237%
Loss D Fake: 0.5169 (0.6298) Acc D Fake: 80.888%
Loss D: 0.833
Loss G: 0.9394 (0.7834) Acc G: 18.224%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3359 (0.3445) Acc D Real: 70.298%
Loss D Fake: 0.5183 (0.6291) Acc D Fake: 80.904%
Loss D: 0.854
Loss G: 0.9338 (0.7844) Acc G: 18.224%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.1818 (0.3434) Acc D Real: 70.476%
Loss D Fake: 0.5233 (0.6284) Acc D Fake: 80.909%
Loss D: 0.705
Loss G: 0.9276 (0.7853) Acc G: 18.225%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3145 (0.3432) Acc D Real: 70.575%
Loss D Fake: 0.5301 (0.6278) Acc D Fake: 80.914%
Loss D: 0.845
Loss G: 0.9108 (0.7861) Acc G: 18.226%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.2809 (0.3429) Acc D Real: 70.697%
Loss D Fake: 0.5475 (0.6272) Acc D Fake: 80.919%
Loss D: 0.828
Loss G: 0.8925 (0.7868) Acc G: 18.226%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.2951 (0.3425) Acc D Real: 70.845%
Loss D Fake: 0.5574 (0.6268) Acc D Fake: 80.913%
Loss D: 0.853
Loss G: 0.9009 (0.7875) Acc G: 18.227%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.2993 (0.3423) Acc D Real: 70.926%
Loss D Fake: 0.5384 (0.6262) Acc D Fake: 80.918%
Loss D: 0.838
Loss G: 0.9238 (0.7884) Acc G: 18.228%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.2409 (0.3416) Acc D Real: 71.048%
Loss D Fake: 0.5229 (0.6256) Acc D Fake: 80.922%
Loss D: 0.764
Loss G: 0.9427 (0.7893) Acc G: 18.229%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.2492 (0.3411) Acc D Real: 71.174%
Loss D Fake: 0.5115 (0.6249) Acc D Fake: 80.927%
Loss D: 0.761
Loss G: 0.9581 (0.7904) Acc G: 18.219%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2748 (0.3406) Acc D Real: 71.285%
Loss D Fake: 0.5123 (0.6242) Acc D Fake: 80.932%
Loss D: 0.787
Loss G: 0.9109 (0.7911) Acc G: 18.219%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.2186 (0.3399) Acc D Real: 71.410%
Loss D Fake: 0.8049 (0.6253) Acc D Fake: 80.432%
Loss D: 1.023
Loss G: 0.5801 (0.7898) Acc G: 18.724%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.2831 (0.3395) Acc D Real: 71.569%
Loss D Fake: 0.8428 (0.6266) Acc D Fake: 79.939%
Loss D: 1.126
Loss G: 0.5590 (0.7884) Acc G: 19.223%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2464 (0.3390) Acc D Real: 71.734%
Loss D Fake: 0.8584 (0.6280) Acc D Fake: 79.451%
Loss D: 1.105
Loss G: 0.5534 (0.7870) Acc G: 19.715%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2170 (0.3382) Acc D Real: 71.897%
Loss D Fake: 0.8594 (0.6294) Acc D Fake: 78.970%
Loss D: 1.076
Loss G: 0.5561 (0.7856) Acc G: 20.202%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.2141 (0.3375) Acc D Real: 72.059%
Loss D Fake: 0.8514 (0.6308) Acc D Fake: 78.494%
Loss D: 1.065
Loss G: 0.5643 (0.7843) Acc G: 20.683%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2845 (0.3372) Acc D Real: 72.217%
Loss D Fake: 0.8380 (0.6320) Acc D Fake: 78.024%
Loss D: 1.123
Loss G: 0.5751 (0.7830) Acc G: 21.158%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.1655 (0.3361) Acc D Real: 72.376%
Loss D Fake: 0.8219 (0.6332) Acc D Fake: 77.560%
Loss D: 0.987
Loss G: 0.5895 (0.7819) Acc G: 21.627%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3311 (0.3361) Acc D Real: 72.529%
Loss D Fake: 0.8029 (0.6342) Acc D Fake: 77.101%
Loss D: 1.134
Loss G: 0.6037 (0.7808) Acc G: 22.091%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.2127 (0.3354) Acc D Real: 72.682%
Loss D Fake: 0.7855 (0.6350) Acc D Fake: 76.647%
Loss D: 0.998
Loss G: 0.6188 (0.7799) Acc G: 22.549%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.2980 (0.3352) Acc D Real: 72.832%
Loss D Fake: 0.7679 (0.6358) Acc D Fake: 76.199%
Loss D: 1.066
Loss G: 0.6328 (0.7790) Acc G: 23.002%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.2855 (0.3349) Acc D Real: 72.981%
Loss D Fake: 0.7528 (0.6365) Acc D Fake: 75.756%
Loss D: 1.038
Loss G: 0.6457 (0.7782) Acc G: 23.450%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.2995 (0.3347) Acc D Real: 73.126%
Loss D Fake: 0.7392 (0.6371) Acc D Fake: 75.318%
Loss D: 1.039
Loss G: 0.6575 (0.7775) Acc G: 23.892%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3331 (0.3347) Acc D Real: 73.264%
Loss D Fake: 0.7274 (0.6376) Acc D Fake: 74.885%
Loss D: 1.061
Loss G: 0.6676 (0.7769) Acc G: 24.330%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3100 (0.3345) Acc D Real: 73.391%
Loss D Fake: 0.7175 (0.6381) Acc D Fake: 74.476%
Loss D: 1.028
Loss G: 0.6767 (0.7763) Acc G: 24.714%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3507 (0.3346) Acc D Real: 73.469%
Loss D Fake: 0.7089 (0.6385) Acc D Fake: 74.176%
Loss D: 1.060
Loss G: 0.6843 (0.7758) Acc G: 24.678%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2993 (0.3344) Acc D Real: 73.538%
Loss D Fake: 0.7019 (0.6388) Acc D Fake: 74.218%
Loss D: 1.001
Loss G: 0.6911 (0.7753) Acc G: 24.642%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2191 (0.3338) Acc D Real: 73.665%
Loss D Fake: 0.6949 (0.6391) Acc D Fake: 74.260%
Loss D: 0.914
Loss G: 0.6989 (0.7749) Acc G: 24.607%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3763 (0.3340) Acc D Real: 73.644%
Loss D Fake: 0.6875 (0.6394) Acc D Fake: 74.302%
Loss D: 1.064
Loss G: 0.7054 (0.7745) Acc G: 24.572%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.2680 (0.3336) Acc D Real: 73.703%
Loss D Fake: 0.6817 (0.6397) Acc D Fake: 74.343%
Loss D: 0.950
Loss G: 0.7117 (0.7741) Acc G: 24.537%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.2579 (0.3332) Acc D Real: 73.721%
Loss D Fake: 0.6755 (0.6399) Acc D Fake: 74.383%
Loss D: 0.933
Loss G: 0.7187 (0.7738) Acc G: 24.494%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3942 (0.3336) Acc D Real: 73.686%
Loss D Fake: 0.6694 (0.6400) Acc D Fake: 74.432%
Loss D: 1.064
Loss G: 0.7238 (0.7736) Acc G: 24.451%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2982 (0.3334) Acc D Real: 73.690%
Loss D Fake: 0.6654 (0.6402) Acc D Fake: 74.481%
Loss D: 0.964
Loss G: 0.7281 (0.7733) Acc G: 24.408%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3028 (0.3332) Acc D Real: 73.690%
Loss D Fake: 0.6614 (0.6403) Acc D Fake: 74.529%
Loss D: 0.964
Loss G: 0.7326 (0.7731) Acc G: 24.366%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2523 (0.3328) Acc D Real: 73.722%
Loss D Fake: 0.6571 (0.6404) Acc D Fake: 74.577%
Loss D: 0.909
Loss G: 0.7380 (0.7729) Acc G: 24.324%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3133 (0.3327) Acc D Real: 73.705%
Loss D Fake: 0.6520 (0.6404) Acc D Fake: 74.624%
Loss D: 0.965
Loss G: 0.7434 (0.7727) Acc G: 24.283%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.2324 (0.3321) Acc D Real: 73.737%
Loss D Fake: 0.6471 (0.6405) Acc D Fake: 74.670%
Loss D: 0.880
Loss G: 0.7497 (0.7726) Acc G: 24.242%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3621 (0.3323) Acc D Real: 73.683%
Loss D Fake: 0.6419 (0.6405) Acc D Fake: 74.716%
Loss D: 1.004
Loss G: 0.7543 (0.7725) Acc G: 24.202%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.3259 (0.3323) Acc D Real: 73.669%
Loss D Fake: 0.6386 (0.6405) Acc D Fake: 74.762%
Loss D: 0.965
Loss G: 0.7576 (0.7725) Acc G: 24.162%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.2530 (0.3318) Acc D Real: 73.700%
Loss D Fake: 0.6359 (0.6404) Acc D Fake: 74.807%
Loss D: 0.889
Loss G: 0.7613 (0.7724) Acc G: 24.123%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.3772 (0.3321) Acc D Real: 73.634%
Loss D Fake: 0.6334 (0.6404) Acc D Fake: 74.852%
Loss D: 1.011
Loss G: 0.7624 (0.7723) Acc G: 24.084%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.2647 (0.3317) Acc D Real: 73.661%
Loss D Fake: 0.6328 (0.6404) Acc D Fake: 74.896%
Loss D: 0.898
Loss G: 0.7640 (0.7723) Acc G: 24.045%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3634 (0.3319) Acc D Real: 73.621%
Loss D Fake: 0.6318 (0.6403) Acc D Fake: 74.940%
Loss D: 0.995
Loss G: 0.7637 (0.7723) Acc G: 24.007%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2950 (0.3317) Acc D Real: 73.615%
Loss D Fake: 0.6330 (0.6403) Acc D Fake: 74.983%
Loss D: 0.928
Loss G: 0.7620 (0.7722) Acc G: 23.969%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.2553 (0.3313) Acc D Real: 73.648%
Loss D Fake: 0.6342 (0.6402) Acc D Fake: 75.026%
Loss D: 0.890
Loss G: 0.7620 (0.7721) Acc G: 23.932%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.4471 (0.3319) Acc D Real: 73.585%
Loss D Fake: 0.6356 (0.6402) Acc D Fake: 75.068%
Loss D: 1.083
Loss G: 0.7563 (0.7721) Acc G: 23.895%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3109 (0.3318) Acc D Real: 73.582%
Loss D Fake: 0.6425 (0.6402) Acc D Fake: 75.110%
Loss D: 0.953
Loss G: 0.7499 (0.7720) Acc G: 23.858%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2750 (0.3315) Acc D Real: 73.608%
Loss D Fake: 0.6472 (0.6403) Acc D Fake: 75.152%
Loss D: 0.922
Loss G: 0.7465 (0.7718) Acc G: 23.822%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2310 (0.3310) Acc D Real: 73.685%
Loss D Fake: 0.6471 (0.6403) Acc D Fake: 75.193%
Loss D: 0.878
Loss G: 0.7528 (0.7717) Acc G: 23.786%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2967 (0.3308) Acc D Real: 73.733%
Loss D Fake: 0.6386 (0.6403) Acc D Fake: 75.233%
Loss D: 0.935
Loss G: 0.7635 (0.7717) Acc G: 23.750%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2243 (0.3303) Acc D Real: 73.797%
Loss D Fake: 0.6275 (0.6402) Acc D Fake: 75.274%
Loss D: 0.852
Loss G: 0.7797 (0.7717) Acc G: 23.715%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.3503 (0.3304) Acc D Real: 73.801%
Loss D Fake: 0.6142 (0.6401) Acc D Fake: 75.314%
Loss D: 0.964
Loss G: 0.7924 (0.7718) Acc G: 23.680%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2105 (0.3298) Acc D Real: 73.872%
Loss D Fake: 0.6041 (0.6399) Acc D Fake: 75.361%
Loss D: 0.815
Loss G: 0.8074 (0.7720) Acc G: 23.637%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.1803 (0.3291) Acc D Real: 73.937%
Loss D Fake: 0.5911 (0.6397) Acc D Fake: 75.417%
Loss D: 0.771
Loss G: 0.8246 (0.7723) Acc G: 23.587%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2960 (0.3289) Acc D Real: 73.950%
Loss D Fake: 0.5795 (0.6394) Acc D Fake: 75.472%
Loss D: 0.875
Loss G: 0.8361 (0.7726) Acc G: 23.537%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3803 (0.3292) Acc D Real: 73.940%
Loss D Fake: 0.5746 (0.6391) Acc D Fake: 75.526%
Loss D: 0.955
Loss G: 0.8368 (0.7729) Acc G: 23.487%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.3675 (0.3293) Acc D Real: 73.913%
Loss D Fake: 0.5790 (0.6388) Acc D Fake: 75.580%
Loss D: 0.946
Loss G: 0.8254 (0.7731) Acc G: 23.438%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.3485 (0.3294) Acc D Real: 73.918%
Loss D Fake: 0.5946 (0.6386) Acc D Fake: 75.633%
Loss D: 0.943
Loss G: 0.8035 (0.7733) Acc G: 23.389%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.2701 (0.3292) Acc D Real: 73.992%
Loss D Fake: 0.6147 (0.6385) Acc D Fake: 75.686%
Loss D: 0.885
Loss G: 0.8049 (0.7734) Acc G: 23.341%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.2554 (0.3288) Acc D Real: 74.045%
Loss D Fake: 0.5899 (0.6382) Acc D Fake: 75.738%
Loss D: 0.845
Loss G: 0.8418 (0.7738) Acc G: 23.294%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2665 (0.3285) Acc D Real: 74.101%
Loss D Fake: 0.5613 (0.6379) Acc D Fake: 75.790%
Loss D: 0.828
Loss G: 0.8697 (0.7742) Acc G: 23.246%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.2630 (0.3282) Acc D Real: 74.137%
Loss D Fake: 0.5465 (0.6374) Acc D Fake: 75.841%
Loss D: 0.810
Loss G: 0.8844 (0.7747) Acc G: 23.200%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2839 (0.3280) Acc D Real: 74.202%
Loss D Fake: 0.5399 (0.6370) Acc D Fake: 75.892%
Loss D: 0.824
Loss G: 0.8903 (0.7753) Acc G: 23.153%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.4080 (0.3284) Acc D Real: 74.177%
Loss D Fake: 0.5408 (0.6365) Acc D Fake: 75.942%
Loss D: 0.949
Loss G: 0.8790 (0.7758) Acc G: 23.107%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.2550 (0.3280) Acc D Real: 74.214%
Loss D Fake: 0.5553 (0.6361) Acc D Fake: 75.992%
Loss D: 0.810
Loss G: 0.8593 (0.7762) Acc G: 23.062%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.1717 (0.3273) Acc D Real: 74.296%
Loss D Fake: 0.5713 (0.6358) Acc D Fake: 76.034%
Loss D: 0.743
Loss G: 0.8692 (0.7766) Acc G: 23.017%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3115 (0.3272) Acc D Real: 74.351%
Loss D Fake: 0.5465 (0.6354) Acc D Fake: 76.083%
Loss D: 0.858
Loss G: 0.8930 (0.7771) Acc G: 22.972%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3481 (0.3273) Acc D Real: 74.378%
Loss D Fake: 0.5352 (0.6350) Acc D Fake: 76.131%
Loss D: 0.883
Loss G: 0.8988 (0.7777) Acc G: 22.928%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2765 (0.3271) Acc D Real: 74.432%
Loss D Fake: 0.5393 (0.6345) Acc D Fake: 76.180%
Loss D: 0.816
Loss G: 0.8847 (0.7782) Acc G: 22.884%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2452 (0.3267) Acc D Real: 74.506%
Loss D Fake: 0.5642 (0.6342) Acc D Fake: 76.227%
Loss D: 0.809
Loss G: 0.9245 (0.7788) Acc G: 22.841%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2355 (0.3263) Acc D Real: 74.570%
Loss D Fake: 0.4993 (0.6336) Acc D Fake: 76.275%
Loss D: 0.735
Loss G: 0.9793 (0.7797) Acc G: 22.790%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2712 (0.3261) Acc D Real: 74.623%
Loss D Fake: 0.4737 (0.6329) Acc D Fake: 76.329%
Loss D: 0.745
Loss G: 1.0531 (0.7810) Acc G: 22.740%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2622 (0.3258) Acc D Real: 74.680%
Loss D Fake: 0.4368 (0.6320) Acc D Fake: 76.383%
Loss D: 0.699
Loss G: 1.4010 (0.7838) Acc G: 22.683%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2529 (0.3254) Acc D Real: 74.737%
Loss D Fake: 0.3090 (0.6306) Acc D Fake: 76.443%
Loss D: 0.562
Loss G: 1.4573 (0.7868) Acc G: 22.619%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.2844 (0.3253) Acc D Real: 74.792%
Loss D Fake: 0.2979 (0.6291) Acc D Fake: 76.511%
Loss D: 0.582
Loss G: 1.4775 (0.7898) Acc G: 22.556%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.4227 (0.3257) Acc D Real: 74.795%
Loss D Fake: 0.2947 (0.6276) Acc D Fake: 76.528%
Loss D: 0.717
Loss G: 1.4858 (0.7929) Acc G: 22.540%
LR: 2.000e-04
Epoch: 15/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.4599 (0.5843) Acc D Real: 68.047%
Loss D Fake: 0.3013 (0.2982) Acc D Fake: 90.833%
Loss D: 0.761
Loss G: 1.4589 (1.4676) Acc G: 10.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.5070 (0.5585) Acc D Real: 68.993%
Loss D Fake: 0.3093 (0.3019) Acc D Fake: 90.000%
Loss D: 0.816
Loss G: 1.4351 (1.4568) Acc G: 11.111%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.4257 (0.5253) Acc D Real: 71.589%
Loss D Fake: 0.3201 (0.3065) Acc D Fake: 89.167%
Loss D: 0.746
Loss G: 1.4059 (1.4441) Acc G: 12.083%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.4114 (0.5025) Acc D Real: 72.833%
Loss D Fake: 0.3501 (0.3152) Acc D Fake: 88.000%
Loss D: 0.761
Loss G: 0.2681 (1.2089) Acc G: 29.667%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.3202 (0.4721) Acc D Real: 75.321%
Loss D Fake: 1.6642 (0.5400) Acc D Fake: 73.333%
Loss D: 1.984
Loss G: 0.2417 (1.0477) Acc G: 41.389%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2436 (0.4395) Acc D Real: 77.173%
Loss D Fake: 1.7092 (0.7070) Acc D Fake: 62.857%
Loss D: 1.953
Loss G: 0.2304 (0.9309) Acc G: 49.762%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.1493 (0.4032) Acc D Real: 79.824%
Loss D Fake: 1.7357 (0.8356) Acc D Fake: 55.000%
Loss D: 1.885
Loss G: 0.2254 (0.8427) Acc G: 56.042%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.1755 (0.3779) Acc D Real: 81.852%
Loss D Fake: 1.7410 (0.9362) Acc D Fake: 48.889%
Loss D: 1.916
Loss G: 0.2244 (0.7740) Acc G: 60.926%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.1780 (0.3579) Acc D Real: 83.536%
Loss D Fake: 1.7340 (1.0160) Acc D Fake: 44.000%
Loss D: 1.912
Loss G: 0.2253 (0.7192) Acc G: 64.833%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1419 (0.3383) Acc D Real: 84.910%
Loss D Fake: 1.7183 (1.0798) Acc D Fake: 40.000%
Loss D: 1.860
Loss G: 0.2284 (0.6745) Acc G: 68.030%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.1597 (0.3234) Acc D Real: 86.020%
Loss D Fake: 1.6944 (1.1311) Acc D Fake: 36.667%
Loss D: 1.854
Loss G: 0.2332 (0.6378) Acc G: 70.694%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.1111 (0.3071) Acc D Real: 87.003%
Loss D Fake: 1.6648 (1.1721) Acc D Fake: 33.846%
Loss D: 1.776
Loss G: 0.2395 (0.6071) Acc G: 72.949%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.1082 (0.2929) Acc D Real: 87.835%
Loss D Fake: 1.6303 (1.2048) Acc D Fake: 31.429%
Loss D: 1.738
Loss G: 0.2472 (0.5814) Acc G: 74.881%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.1273 (0.2818) Acc D Real: 88.562%
Loss D Fake: 1.5924 (1.2307) Acc D Fake: 29.333%
Loss D: 1.720
Loss G: 0.2559 (0.5597) Acc G: 76.556%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.1701 (0.2748) Acc D Real: 89.176%
Loss D Fake: 1.5530 (1.2508) Acc D Fake: 27.500%
Loss D: 1.723
Loss G: 0.2653 (0.5413) Acc G: 78.021%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.1490 (0.2674) Acc D Real: 89.724%
Loss D Fake: 1.5132 (1.2662) Acc D Fake: 25.882%
Loss D: 1.662
Loss G: 0.2754 (0.5257) Acc G: 79.314%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.1263 (0.2596) Acc D Real: 90.252%
Loss D Fake: 1.4730 (1.2777) Acc D Fake: 24.444%
Loss D: 1.599
Loss G: 0.2863 (0.5124) Acc G: 80.463%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.1321 (0.2529) Acc D Real: 90.707%
Loss D Fake: 1.4325 (1.2859) Acc D Fake: 23.158%
Loss D: 1.565
Loss G: 0.2978 (0.5011) Acc G: 81.491%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1169 (0.2461) Acc D Real: 91.112%
Loss D Fake: 1.3919 (1.2912) Acc D Fake: 22.000%
Loss D: 1.509
Loss G: 0.3101 (0.4915) Acc G: 82.417%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.1460 (0.2413) Acc D Real: 91.468%
Loss D Fake: 1.3517 (1.2941) Acc D Fake: 20.952%
Loss D: 1.498
Loss G: 0.3228 (0.4835) Acc G: 83.254%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.2032 (0.2396) Acc D Real: 91.785%
Loss D Fake: 1.3129 (1.2949) Acc D Fake: 20.000%
Loss D: 1.516
Loss G: 0.3355 (0.4768) Acc G: 84.015%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1597 (0.2361) Acc D Real: 92.072%
Loss D Fake: 1.2760 (1.2941) Acc D Fake: 19.130%
Loss D: 1.436
Loss G: 0.3485 (0.4712) Acc G: 84.710%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.1755 (0.2336) Acc D Real: 92.337%
Loss D Fake: 1.2404 (1.2919) Acc D Fake: 18.333%
Loss D: 1.416
Loss G: 0.3617 (0.4666) Acc G: 85.347%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.1866 (0.2317) Acc D Real: 92.575%
Loss D Fake: 1.2063 (1.2884) Acc D Fake: 17.600%
Loss D: 1.393
Loss G: 0.3749 (0.4630) Acc G: 85.933%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.1628 (0.2291) Acc D Real: 92.823%
Loss D Fake: 1.1738 (1.2840) Acc D Fake: 16.923%
Loss D: 1.337
Loss G: 0.3883 (0.4601) Acc G: 86.474%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.1791 (0.2272) Acc D Real: 93.042%
Loss D Fake: 1.1423 (1.2788) Acc D Fake: 16.296%
Loss D: 1.321
Loss G: 0.4019 (0.4579) Acc G: 86.975%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.1839 (0.2257) Acc D Real: 93.237%
Loss D Fake: 1.1121 (1.2728) Acc D Fake: 15.714%
Loss D: 1.296
Loss G: 0.4155 (0.4564) Acc G: 87.381%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.2132 (0.2252) Acc D Real: 93.409%
Loss D Fake: 1.0834 (1.2663) Acc D Fake: 15.287%
Loss D: 1.297
Loss G: 0.4289 (0.4555) Acc G: 87.701%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.2195 (0.2250) Acc D Real: 93.573%
Loss D Fake: 1.0565 (1.2593) Acc D Fake: 14.889%
Loss D: 1.276
Loss G: 0.4420 (0.4550) Acc G: 88.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2502 (0.2259) Acc D Real: 93.701%
Loss D Fake: 1.0313 (1.2519) Acc D Fake: 14.570%
Loss D: 1.281
Loss G: 0.4547 (0.4550) Acc G: 88.226%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1535 (0.2236) Acc D Real: 93.864%
Loss D Fake: 1.0077 (1.2443) Acc D Fake: 14.271%
Loss D: 1.161
Loss G: 0.4677 (0.4554) Acc G: 88.438%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2261 (0.2237) Acc D Real: 93.996%
Loss D Fake: 0.9844 (1.2364) Acc D Fake: 13.990%
Loss D: 1.210
Loss G: 0.4805 (0.4562) Acc G: 88.636%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.2168 (0.2235) Acc D Real: 94.115%
Loss D Fake: 0.9625 (1.2284) Acc D Fake: 13.725%
Loss D: 1.179
Loss G: 0.4931 (0.4573) Acc G: 88.824%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.1516 (0.2214) Acc D Real: 94.250%
Loss D Fake: 0.9414 (1.2202) Acc D Fake: 13.476%
Loss D: 1.093
Loss G: 0.5061 (0.4587) Acc G: 89.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.2425 (0.2220) Acc D Real: 94.349%
Loss D Fake: 0.9206 (1.2119) Acc D Fake: 13.241%
Loss D: 1.163
Loss G: 0.5189 (0.4603) Acc G: 89.167%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2193 (0.2219) Acc D Real: 94.441%
Loss D Fake: 0.9012 (1.2035) Acc D Fake: 13.018%
Loss D: 1.121
Loss G: 0.5313 (0.4622) Acc G: 89.324%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2668 (0.2231) Acc D Real: 94.526%
Loss D Fake: 0.8830 (1.1950) Acc D Fake: 12.807%
Loss D: 1.150
Loss G: 0.5432 (0.4644) Acc G: 89.474%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.2675 (0.2242) Acc D Real: 94.611%
Loss D Fake: 0.8665 (1.1866) Acc D Fake: 12.607%
Loss D: 1.134
Loss G: 0.5542 (0.4667) Acc G: 89.615%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2812 (0.2257) Acc D Real: 94.695%
Loss D Fake: 0.8515 (1.1782) Acc D Fake: 12.417%
Loss D: 1.133
Loss G: 0.5645 (0.4691) Acc G: 89.750%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.2699 (0.2268) Acc D Real: 94.771%
Loss D Fake: 0.8378 (1.1699) Acc D Fake: 12.236%
Loss D: 1.108
Loss G: 0.5744 (0.4717) Acc G: 89.878%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3404 (0.2295) Acc D Real: 94.823%
Loss D Fake: 0.8252 (1.1617) Acc D Fake: 12.063%
Loss D: 1.166
Loss G: 0.5832 (0.4743) Acc G: 90.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.3254 (0.2317) Acc D Real: 94.875%
Loss D Fake: 0.8143 (1.1536) Acc D Fake: 11.899%
Loss D: 1.140
Loss G: 0.5912 (0.4771) Acc G: 90.078%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3460 (0.2343) Acc D Real: 94.925%
Loss D Fake: 0.8047 (1.1457) Acc D Fake: 11.780%
Loss D: 1.151
Loss G: 0.5982 (0.4798) Acc G: 90.152%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.2659 (0.2350) Acc D Real: 94.985%
Loss D Fake: 0.7962 (1.1379) Acc D Fake: 11.667%
Loss D: 1.062
Loss G: 0.6051 (0.4826) Acc G: 90.222%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3346 (0.2372) Acc D Real: 95.026%
Loss D Fake: 0.7878 (1.1303) Acc D Fake: 11.558%
Loss D: 1.122
Loss G: 0.6115 (0.4854) Acc G: 90.290%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.1986 (0.2363) Acc D Real: 95.079%
Loss D Fake: 0.7800 (1.1229) Acc D Fake: 11.489%
Loss D: 0.979
Loss G: 0.6184 (0.4882) Acc G: 90.319%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3099 (0.2379) Acc D Real: 95.081%
Loss D Fake: 0.7716 (1.1156) Acc D Fake: 11.424%
Loss D: 1.082
Loss G: 0.6252 (0.4911) Acc G: 90.347%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2932 (0.2390) Acc D Real: 95.102%
Loss D Fake: 0.7639 (1.1084) Acc D Fake: 11.361%
Loss D: 1.057
Loss G: 0.6317 (0.4940) Acc G: 90.374%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.2622 (0.2395) Acc D Real: 95.126%
Loss D Fake: 0.7563 (1.1013) Acc D Fake: 11.300%
Loss D: 1.018
Loss G: 0.6383 (0.4968) Acc G: 90.400%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3565 (0.2418) Acc D Real: 95.111%
Loss D Fake: 0.7490 (1.0944) Acc D Fake: 11.242%
Loss D: 1.106
Loss G: 0.6443 (0.4997) Acc G: 90.392%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3955 (0.2447) Acc D Real: 95.063%
Loss D Fake: 0.7430 (1.0877) Acc D Fake: 11.250%
Loss D: 1.139
Loss G: 0.6489 (0.5026) Acc G: 90.353%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2453 (0.2447) Acc D Real: 95.084%
Loss D Fake: 0.7380 (1.0811) Acc D Fake: 11.258%
Loss D: 0.983
Loss G: 0.6539 (0.5055) Acc G: 90.283%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2669 (0.2451) Acc D Real: 95.083%
Loss D Fake: 0.7322 (1.0746) Acc D Fake: 11.296%
Loss D: 0.999
Loss G: 0.6593 (0.5083) Acc G: 90.216%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2455 (0.2451) Acc D Real: 95.108%
Loss D Fake: 0.7262 (1.0683) Acc D Fake: 11.364%
Loss D: 0.972
Loss G: 0.6651 (0.5112) Acc G: 90.121%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3872 (0.2477) Acc D Real: 95.052%
Loss D Fake: 0.7202 (1.0621) Acc D Fake: 11.458%
Loss D: 1.107
Loss G: 0.6701 (0.5140) Acc G: 89.970%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3111 (0.2488) Acc D Real: 95.007%
Loss D Fake: 0.7153 (1.0560) Acc D Fake: 11.579%
Loss D: 1.026
Loss G: 0.6746 (0.5168) Acc G: 89.795%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.2159 (0.2482) Acc D Real: 95.012%
Loss D Fake: 0.7103 (1.0500) Acc D Fake: 11.724%
Loss D: 0.926
Loss G: 0.6798 (0.5196) Acc G: 89.598%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3612 (0.2501) Acc D Real: 94.916%
Loss D Fake: 0.7049 (1.0442) Acc D Fake: 11.893%
Loss D: 1.066
Loss G: 0.6846 (0.5224) Acc G: 89.379%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.2804 (0.2506) Acc D Real: 94.832%
Loss D Fake: 0.7002 (1.0384) Acc D Fake: 12.111%
Loss D: 0.981
Loss G: 0.6893 (0.5252) Acc G: 89.111%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3812 (0.2528) Acc D Real: 94.624%
Loss D Fake: 0.6957 (1.0328) Acc D Fake: 12.377%
Loss D: 1.077
Loss G: 0.6932 (0.5280) Acc G: 88.798%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3877 (0.2550) Acc D Real: 94.267%
Loss D Fake: 0.6924 (1.0273) Acc D Fake: 12.688%
Loss D: 1.080
Loss G: 0.6960 (0.5307) Acc G: 88.414%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.2600 (0.2550) Acc D Real: 94.136%
Loss D Fake: 0.6896 (1.0220) Acc D Fake: 13.095%
Loss D: 0.950
Loss G: 0.6992 (0.5333) Acc G: 87.857%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.2598 (0.2551) Acc D Real: 94.032%
Loss D Fake: 0.6861 (1.0167) Acc D Fake: 13.802%
Loss D: 0.946
Loss G: 0.7030 (0.5360) Acc G: 86.536%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3314 (0.2563) Acc D Real: 93.753%
Loss D Fake: 0.6823 (1.0116) Acc D Fake: 15.077%
Loss D: 1.014
Loss G: 0.7067 (0.5386) Acc G: 85.256%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.4157 (0.2587) Acc D Real: 93.300%
Loss D Fake: 0.6792 (1.0065) Acc D Fake: 16.313%
Loss D: 1.095
Loss G: 0.7092 (0.5412) Acc G: 84.015%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.3268 (0.2597) Acc D Real: 93.048%
Loss D Fake: 0.6770 (1.0016) Acc D Fake: 17.512%
Loss D: 1.004
Loss G: 0.7114 (0.5437) Acc G: 82.836%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.2756 (0.2600) Acc D Real: 92.738%
Loss D Fake: 0.6747 (0.9968) Acc D Fake: 18.652%
Loss D: 0.950
Loss G: 0.7142 (0.5462) Acc G: 81.691%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.2066 (0.2592) Acc D Real: 92.610%
Loss D Fake: 0.6716 (0.9921) Acc D Fake: 19.758%
Loss D: 0.878
Loss G: 0.7181 (0.5487) Acc G: 80.580%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.3074 (0.2599) Acc D Real: 92.317%
Loss D Fake: 0.6675 (0.9875) Acc D Fake: 20.833%
Loss D: 0.975
Loss G: 0.7223 (0.5512) Acc G: 79.500%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2702 (0.2600) Acc D Real: 92.017%
Loss D Fake: 0.6635 (0.9829) Acc D Fake: 21.878%
Loss D: 0.934
Loss G: 0.7268 (0.5537) Acc G: 78.451%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3399 (0.2611) Acc D Real: 91.579%
Loss D Fake: 0.6595 (0.9784) Acc D Fake: 22.894%
Loss D: 0.999
Loss G: 0.7308 (0.5561) Acc G: 77.431%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.2724 (0.2613) Acc D Real: 91.255%
Loss D Fake: 0.6558 (0.9740) Acc D Fake: 23.881%
Loss D: 0.928
Loss G: 0.7349 (0.5586) Acc G: 76.438%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.4001 (0.2632) Acc D Real: 90.726%
Loss D Fake: 0.6527 (0.9696) Acc D Fake: 24.842%
Loss D: 1.053
Loss G: 0.7367 (0.5610) Acc G: 75.473%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3290 (0.2640) Acc D Real: 90.316%
Loss D Fake: 0.6518 (0.9654) Acc D Fake: 25.778%
Loss D: 0.981
Loss G: 0.7378 (0.5634) Acc G: 74.533%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4039 (0.2659) Acc D Real: 89.790%
Loss D Fake: 0.6510 (0.9613) Acc D Fake: 26.689%
Loss D: 1.055
Loss G: 0.7382 (0.5657) Acc G: 73.618%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2309 (0.2654) Acc D Real: 89.573%
Loss D Fake: 0.6505 (0.9572) Acc D Fake: 27.576%
Loss D: 0.881
Loss G: 0.7395 (0.5679) Acc G: 72.727%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3245 (0.2662) Acc D Real: 89.200%
Loss D Fake: 0.6490 (0.9533) Acc D Fake: 28.440%
Loss D: 0.973
Loss G: 0.7412 (0.5701) Acc G: 71.859%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3051 (0.2667) Acc D Real: 88.879%
Loss D Fake: 0.6474 (0.9494) Acc D Fake: 29.283%
Loss D: 0.952
Loss G: 0.7431 (0.5723) Acc G: 71.013%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3479 (0.2677) Acc D Real: 88.503%
Loss D Fake: 0.6457 (0.9456) Acc D Fake: 30.104%
Loss D: 0.994
Loss G: 0.7448 (0.5745) Acc G: 70.188%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.2812 (0.2678) Acc D Real: 88.241%
Loss D Fake: 0.6442 (0.9419) Acc D Fake: 30.905%
Loss D: 0.925
Loss G: 0.7469 (0.5766) Acc G: 69.383%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3853 (0.2693) Acc D Real: 87.812%
Loss D Fake: 0.6424 (0.9382) Acc D Fake: 31.646%
Loss D: 1.028
Loss G: 0.7484 (0.5787) Acc G: 68.638%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3271 (0.2700) Acc D Real: 87.478%
Loss D Fake: 0.6413 (0.9347) Acc D Fake: 32.369%
Loss D: 0.968
Loss G: 0.7497 (0.5808) Acc G: 67.912%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3520 (0.2710) Acc D Real: 87.129%
Loss D Fake: 0.6402 (0.9312) Acc D Fake: 33.075%
Loss D: 0.992
Loss G: 0.7508 (0.5828) Acc G: 67.202%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3910 (0.2724) Acc D Real: 86.721%
Loss D Fake: 0.6395 (0.9277) Acc D Fake: 33.765%
Loss D: 1.030
Loss G: 0.7512 (0.5848) Acc G: 66.510%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.3263 (0.2730) Acc D Real: 86.424%
Loss D Fake: 0.6392 (0.9244) Acc D Fake: 34.438%
Loss D: 0.966
Loss G: 0.7517 (0.5867) Acc G: 65.833%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3499 (0.2739) Acc D Real: 86.097%
Loss D Fake: 0.6388 (0.9211) Acc D Fake: 35.096%
Loss D: 0.989
Loss G: 0.7520 (0.5886) Acc G: 65.172%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.4604 (0.2760) Acc D Real: 85.620%
Loss D Fake: 0.6391 (0.9179) Acc D Fake: 35.739%
Loss D: 1.100
Loss G: 0.7506 (0.5905) Acc G: 64.527%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.2828 (0.2761) Acc D Real: 85.408%
Loss D Fake: 0.6406 (0.9148) Acc D Fake: 36.367%
Loss D: 0.923
Loss G: 0.7495 (0.5922) Acc G: 63.895%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4206 (0.2777) Acc D Real: 85.001%
Loss D Fake: 0.6415 (0.9117) Acc D Fake: 36.963%
Loss D: 1.062
Loss G: 0.7481 (0.5940) Acc G: 63.296%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.2789 (0.2777) Acc D Real: 84.803%
Loss D Fake: 0.6427 (0.9088) Acc D Fake: 37.546%
Loss D: 0.922
Loss G: 0.7474 (0.5957) Acc G: 62.711%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.2334 (0.2772) Acc D Real: 84.674%
Loss D Fake: 0.6427 (0.9059) Acc D Fake: 38.098%
Loss D: 0.876
Loss G: 0.7482 (0.5973) Acc G: 62.156%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2835 (0.2773) Acc D Real: 84.477%
Loss D Fake: 0.6416 (0.9030) Acc D Fake: 38.638%
Loss D: 0.925
Loss G: 0.7498 (0.5990) Acc G: 61.613%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3728 (0.2783) Acc D Real: 84.162%
Loss D Fake: 0.6402 (0.9003) Acc D Fake: 39.167%
Loss D: 1.013
Loss G: 0.7510 (0.6006) Acc G: 61.082%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.2622 (0.2781) Acc D Real: 84.008%
Loss D Fake: 0.6391 (0.8975) Acc D Fake: 39.684%
Loss D: 0.901
Loss G: 0.7528 (0.6022) Acc G: 60.561%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4445 (0.2799) Acc D Real: 83.610%
Loss D Fake: 0.6377 (0.8948) Acc D Fake: 40.191%
Loss D: 1.082
Loss G: 0.7535 (0.6038) Acc G: 60.052%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.2840 (0.2799) Acc D Real: 83.439%
Loss D Fake: 0.6373 (0.8921) Acc D Fake: 40.687%
Loss D: 0.921
Loss G: 0.7544 (0.6053) Acc G: 59.553%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3265 (0.2804) Acc D Real: 83.223%
Loss D Fake: 0.6364 (0.8895) Acc D Fake: 41.173%
Loss D: 0.963
Loss G: 0.7555 (0.6068) Acc G: 59.065%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2359 (0.2799) Acc D Real: 83.130%
Loss D Fake: 0.6351 (0.8870) Acc D Fake: 41.650%
Loss D: 0.871
Loss G: 0.7575 (0.6084) Acc G: 58.586%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2429 (0.2796) Acc D Real: 83.032%
Loss D Fake: 0.6329 (0.8844) Acc D Fake: 42.117%
Loss D: 0.876
Loss G: 0.7606 (0.6099) Acc G: 58.117%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.2626 (0.2794) Acc D Real: 82.902%
Loss D Fake: 0.6300 (0.8819) Acc D Fake: 42.574%
Loss D: 0.893
Loss G: 0.7642 (0.6114) Acc G: 57.657%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3074 (0.2797) Acc D Real: 82.724%
Loss D Fake: 0.6268 (0.8794) Acc D Fake: 43.023%
Loss D: 0.934
Loss G: 0.7678 (0.6129) Acc G: 57.206%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.2666 (0.2795) Acc D Real: 82.604%
Loss D Fake: 0.6237 (0.8769) Acc D Fake: 43.463%
Loss D: 0.890
Loss G: 0.7716 (0.6145) Acc G: 56.764%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3266 (0.2800) Acc D Real: 82.416%
Loss D Fake: 0.6206 (0.8745) Acc D Fake: 43.894%
Loss D: 0.947
Loss G: 0.7751 (0.6160) Acc G: 56.330%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.2929 (0.2801) Acc D Real: 82.268%
Loss D Fake: 0.6178 (0.8720) Acc D Fake: 44.317%
Loss D: 0.911
Loss G: 0.7785 (0.6176) Acc G: 55.905%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3868 (0.2811) Acc D Real: 82.014%
Loss D Fake: 0.6153 (0.8696) Acc D Fake: 44.733%
Loss D: 1.002
Loss G: 0.7809 (0.6191) Acc G: 55.487%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2196 (0.2805) Acc D Real: 81.963%
Loss D Fake: 0.6132 (0.8672) Acc D Fake: 45.140%
Loss D: 0.833
Loss G: 0.7840 (0.6207) Acc G: 55.078%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2977 (0.2807) Acc D Real: 81.822%
Loss D Fake: 0.6105 (0.8648) Acc D Fake: 45.540%
Loss D: 0.908
Loss G: 0.7873 (0.6222) Acc G: 54.676%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3739 (0.2816) Acc D Real: 81.595%
Loss D Fake: 0.6080 (0.8625) Acc D Fake: 45.933%
Loss D: 0.982
Loss G: 0.7897 (0.6237) Acc G: 54.281%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3167 (0.2819) Acc D Real: 81.440%
Loss D Fake: 0.6063 (0.8601) Acc D Fake: 46.318%
Loss D: 0.923
Loss G: 0.7918 (0.6253) Acc G: 53.894%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.2760 (0.2818) Acc D Real: 81.336%
Loss D Fake: 0.6046 (0.8578) Acc D Fake: 46.697%
Loss D: 0.881
Loss G: 0.7942 (0.6268) Acc G: 53.514%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3440 (0.2824) Acc D Real: 81.148%
Loss D Fake: 0.6027 (0.8555) Acc D Fake: 47.068%
Loss D: 0.947
Loss G: 0.7962 (0.6283) Acc G: 53.140%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.2726 (0.2823) Acc D Real: 81.047%
Loss D Fake: 0.6011 (0.8533) Acc D Fake: 47.434%
Loss D: 0.874
Loss G: 0.7984 (0.6298) Acc G: 52.773%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3488 (0.2829) Acc D Real: 80.864%
Loss D Fake: 0.5994 (0.8511) Acc D Fake: 47.792%
Loss D: 0.948
Loss G: 0.8004 (0.6313) Acc G: 52.412%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3459 (0.2834) Acc D Real: 80.697%
Loss D Fake: 0.5981 (0.8489) Acc D Fake: 48.145%
Loss D: 0.944
Loss G: 0.8016 (0.6328) Acc G: 52.058%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.2512 (0.2831) Acc D Real: 80.628%
Loss D Fake: 0.5972 (0.8467) Acc D Fake: 48.491%
Loss D: 0.848
Loss G: 0.8033 (0.6343) Acc G: 51.710%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.2751 (0.2831) Acc D Real: 80.536%
Loss D Fake: 0.5955 (0.8446) Acc D Fake: 48.832%
Loss D: 0.871
Loss G: 0.8056 (0.6357) Acc G: 51.368%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.2473 (0.2828) Acc D Real: 80.480%
Loss D Fake: 0.5935 (0.8424) Acc D Fake: 49.167%
Loss D: 0.841
Loss G: 0.8085 (0.6372) Acc G: 51.031%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3745 (0.2835) Acc D Real: 80.288%
Loss D Fake: 0.5914 (0.8403) Acc D Fake: 49.496%
Loss D: 0.966
Loss G: 0.8108 (0.6386) Acc G: 50.700%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2036 (0.2829) Acc D Real: 80.280%
Loss D Fake: 0.5896 (0.8382) Acc D Fake: 49.819%
Loss D: 0.793
Loss G: 0.8136 (0.6401) Acc G: 50.375%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2747 (0.2828) Acc D Real: 80.201%
Loss D Fake: 0.5872 (0.8362) Acc D Fake: 50.138%
Loss D: 0.862
Loss G: 0.8168 (0.6416) Acc G: 50.055%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.3085 (0.2830) Acc D Real: 80.082%
Loss D Fake: 0.5848 (0.8341) Acc D Fake: 50.451%
Loss D: 0.893
Loss G: 0.8199 (0.6430) Acc G: 49.740%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3311 (0.2834) Acc D Real: 79.946%
Loss D Fake: 0.5827 (0.8321) Acc D Fake: 50.759%
Loss D: 0.914
Loss G: 0.8224 (0.6445) Acc G: 49.431%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3136 (0.2837) Acc D Real: 79.835%
Loss D Fake: 0.5810 (0.8300) Acc D Fake: 51.062%
Loss D: 0.895
Loss G: 0.8246 (0.6459) Acc G: 49.126%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.2098 (0.2831) Acc D Real: 79.832%
Loss D Fake: 0.5791 (0.8280) Acc D Fake: 51.360%
Loss D: 0.789
Loss G: 0.8278 (0.6474) Acc G: 48.827%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3423 (0.2835) Acc D Real: 79.698%
Loss D Fake: 0.5767 (0.8260) Acc D Fake: 51.653%
Loss D: 0.919
Loss G: 0.8306 (0.6488) Acc G: 48.532%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3639 (0.2842) Acc D Real: 79.541%
Loss D Fake: 0.5749 (0.8240) Acc D Fake: 51.942%
Loss D: 0.939
Loss G: 0.8325 (0.6503) Acc G: 48.241%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.2379 (0.2838) Acc D Real: 79.506%
Loss D Fake: 0.5735 (0.8221) Acc D Fake: 52.227%
Loss D: 0.811
Loss G: 0.8349 (0.6517) Acc G: 47.956%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.2601 (0.2836) Acc D Real: 79.459%
Loss D Fake: 0.5716 (0.8201) Acc D Fake: 52.506%
Loss D: 0.832
Loss G: 0.8378 (0.6532) Acc G: 47.674%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3192 (0.2839) Acc D Real: 79.357%
Loss D Fake: 0.5694 (0.8182) Acc D Fake: 52.782%
Loss D: 0.889
Loss G: 0.8405 (0.6546) Acc G: 47.397%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.2378 (0.2835) Acc D Real: 79.327%
Loss D Fake: 0.5674 (0.8163) Acc D Fake: 53.053%
Loss D: 0.805
Loss G: 0.8437 (0.6561) Acc G: 47.125%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3156 (0.2838) Acc D Real: 79.233%
Loss D Fake: 0.5651 (0.8144) Acc D Fake: 53.321%
Loss D: 0.881
Loss G: 0.8466 (0.6575) Acc G: 46.856%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.2901 (0.2838) Acc D Real: 79.158%
Loss D Fake: 0.5631 (0.8125) Acc D Fake: 53.584%
Loss D: 0.853
Loss G: 0.8494 (0.6589) Acc G: 46.591%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2643 (0.2837) Acc D Real: 79.113%
Loss D Fake: 0.5610 (0.8106) Acc D Fake: 53.843%
Loss D: 0.825
Loss G: 0.8524 (0.6604) Acc G: 46.331%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3505 (0.2842) Acc D Real: 78.992%
Loss D Fake: 0.5591 (0.8088) Acc D Fake: 54.099%
Loss D: 0.910
Loss G: 0.8547 (0.6618) Acc G: 46.074%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3455 (0.2846) Acc D Real: 78.873%
Loss D Fake: 0.5577 (0.8069) Acc D Fake: 54.350%
Loss D: 0.903
Loss G: 0.8563 (0.6633) Acc G: 45.821%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3682 (0.2852) Acc D Real: 78.737%
Loss D Fake: 0.5569 (0.8051) Acc D Fake: 54.599%
Loss D: 0.925
Loss G: 0.8570 (0.6647) Acc G: 45.572%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.4246 (0.2863) Acc D Real: 78.562%
Loss D Fake: 0.5569 (0.8033) Acc D Fake: 54.843%
Loss D: 0.982
Loss G: 0.8563 (0.6661) Acc G: 45.326%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.3209 (0.2865) Acc D Real: 78.476%
Loss D Fake: 0.5576 (0.8015) Acc D Fake: 55.084%
Loss D: 0.879
Loss G: 0.8555 (0.6674) Acc G: 45.084%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2961 (0.2866) Acc D Real: 78.414%
Loss D Fake: 0.5581 (0.7998) Acc D Fake: 55.321%
Loss D: 0.854
Loss G: 0.8550 (0.6688) Acc G: 44.845%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.2988 (0.2867) Acc D Real: 78.350%
Loss D Fake: 0.5583 (0.7981) Acc D Fake: 55.556%
Loss D: 0.857
Loss G: 0.8551 (0.6701) Acc G: 44.610%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.1406 (0.2856) Acc D Real: 78.413%
Loss D Fake: 0.5578 (0.7964) Acc D Fake: 55.786%
Loss D: 0.698
Loss G: 0.8571 (0.6714) Acc G: 44.378%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.4727 (0.2869) Acc D Real: 78.203%
Loss D Fake: 0.5564 (0.7947) Acc D Fake: 56.014%
Loss D: 1.029
Loss G: 0.8577 (0.6727) Acc G: 44.149%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3460 (0.2873) Acc D Real: 78.101%
Loss D Fake: 0.5566 (0.7931) Acc D Fake: 56.238%
Loss D: 0.903
Loss G: 0.8574 (0.6740) Acc G: 43.924%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2912 (0.2874) Acc D Real: 78.047%
Loss D Fake: 0.5569 (0.7914) Acc D Fake: 56.460%
Loss D: 0.848
Loss G: 0.8573 (0.6752) Acc G: 43.701%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3564 (0.2878) Acc D Real: 77.942%
Loss D Fake: 0.5570 (0.7898) Acc D Fake: 56.678%
Loss D: 0.913
Loss G: 0.8570 (0.6765) Acc G: 43.482%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.2931 (0.2879) Acc D Real: 77.879%
Loss D Fake: 0.5572 (0.7882) Acc D Fake: 56.893%
Loss D: 0.850
Loss G: 0.8569 (0.6777) Acc G: 43.265%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3502 (0.2883) Acc D Real: 77.782%
Loss D Fake: 0.5573 (0.7867) Acc D Fake: 57.106%
Loss D: 0.908
Loss G: 0.8566 (0.6789) Acc G: 43.052%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3279 (0.2886) Acc D Real: 77.704%
Loss D Fake: 0.5576 (0.7851) Acc D Fake: 57.315%
Loss D: 0.885
Loss G: 0.8563 (0.6801) Acc G: 42.841%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3442 (0.2889) Acc D Real: 77.608%
Loss D Fake: 0.5580 (0.7836) Acc D Fake: 57.522%
Loss D: 0.902
Loss G: 0.8557 (0.6813) Acc G: 42.633%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3168 (0.2891) Acc D Real: 77.539%
Loss D Fake: 0.5584 (0.7821) Acc D Fake: 57.726%
Loss D: 0.875
Loss G: 0.8552 (0.6824) Acc G: 42.428%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.3203 (0.2893) Acc D Real: 77.466%
Loss D Fake: 0.5588 (0.7807) Acc D Fake: 57.917%
Loss D: 0.879
Loss G: 0.8548 (0.6836) Acc G: 42.237%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3219 (0.2895) Acc D Real: 77.394%
Loss D Fake: 0.5591 (0.7792) Acc D Fake: 58.105%
Loss D: 0.881
Loss G: 0.8545 (0.6847) Acc G: 42.048%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.4010 (0.2903) Acc D Real: 77.266%
Loss D Fake: 0.5648 (0.7778) Acc D Fake: 58.290%
Loss D: 0.966
Loss G: 0.8360 (0.6857) Acc G: 41.872%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.1888 (0.2896) Acc D Real: 77.299%
Loss D Fake: 0.5870 (0.7766) Acc D Fake: 58.462%
Loss D: 0.776
Loss G: 0.8083 (0.6865) Acc G: 41.710%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3539 (0.2900) Acc D Real: 77.202%
Loss D Fake: 0.6279 (0.7756) Acc D Fake: 58.600%
Loss D: 0.982
Loss G: 0.7358 (0.6868) Acc G: 41.624%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3623 (0.2905) Acc D Real: 77.097%
Loss D Fake: 5.0388 (0.8028) Acc D Fake: 58.227%
Loss D: 5.401
Loss G: 0.0685 (0.6828) Acc G: 41.996%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.2267 (0.2901) Acc D Real: 77.102%
Loss D Fake: 5.2434 (0.8309) Acc D Fake: 57.859%
Loss D: 5.470
Loss G: 0.0624 (0.6789) Acc G: 42.363%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3774 (0.2906) Acc D Real: 76.986%
Loss D Fake: 5.2685 (0.8588) Acc D Fake: 57.495%
Loss D: 5.646
Loss G: 0.0589 (0.6750) Acc G: 42.725%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.2126 (0.2901) Acc D Real: 76.995%
Loss D Fake: 5.2524 (0.8863) Acc D Fake: 57.135%
Loss D: 5.465
Loss G: 0.0566 (0.6712) Acc G: 43.083%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2932 (0.2902) Acc D Real: 76.938%
Loss D Fake: 5.2154 (0.9132) Acc D Fake: 56.781%
Loss D: 5.509
Loss G: 0.0551 (0.6673) Acc G: 43.437%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3751 (0.2907) Acc D Real: 76.817%
Loss D Fake: 5.1642 (0.9394) Acc D Fake: 56.430%
Loss D: 5.539
Loss G: 0.0541 (0.6635) Acc G: 43.786%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.2122 (0.2902) Acc D Real: 76.825%
Loss D Fake: 5.1022 (0.9649) Acc D Fake: 56.084%
Loss D: 5.314
Loss G: 0.0535 (0.6598) Acc G: 44.131%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.1049 (0.2891) Acc D Real: 76.917%
Loss D Fake: 5.0320 (0.9897) Acc D Fake: 55.742%
Loss D: 5.137
Loss G: 0.0531 (0.6561) Acc G: 44.472%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1289 (0.2881) Acc D Real: 76.991%
Loss D Fake: 4.9558 (1.0138) Acc D Fake: 55.404%
Loss D: 5.085
Loss G: 0.0530 (0.6524) Acc G: 44.808%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.1821 (0.2875) Acc D Real: 77.059%
Loss D Fake: 4.8763 (1.0370) Acc D Fake: 55.070%
Loss D: 5.058
Loss G: 0.0531 (0.6488) Acc G: 45.141%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.1184 (0.2865) Acc D Real: 77.164%
Loss D Fake: 4.7964 (1.0596) Acc D Fake: 54.741%
Loss D: 4.915
Loss G: 0.0533 (0.6453) Acc G: 45.469%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.1547 (0.2857) Acc D Real: 77.267%
Loss D Fake: 4.7179 (1.0813) Acc D Fake: 54.415%
Loss D: 4.873
Loss G: 0.0536 (0.6417) Acc G: 45.794%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.2116 (0.2852) Acc D Real: 77.360%
Loss D Fake: 4.6415 (1.1024) Acc D Fake: 54.093%
Loss D: 4.853
Loss G: 0.0539 (0.6383) Acc G: 46.114%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.1431 (0.2844) Acc D Real: 77.469%
Loss D Fake: 4.5674 (1.1228) Acc D Fake: 53.775%
Loss D: 4.710
Loss G: 0.0543 (0.6348) Acc G: 46.431%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.0663 (0.2831) Acc D Real: 77.596%
Loss D Fake: 4.4952 (1.1425) Acc D Fake: 53.460%
Loss D: 4.561
Loss G: 0.0548 (0.6314) Acc G: 46.745%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.1041 (0.2821) Acc D Real: 77.716%
Loss D Fake: 4.4250 (1.1616) Acc D Fake: 53.149%
Loss D: 4.529
Loss G: 0.0554 (0.6281) Acc G: 47.054%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.1357 (0.2812) Acc D Real: 77.837%
Loss D Fake: 4.3568 (1.1801) Acc D Fake: 52.842%
Loss D: 4.493
Loss G: 0.0560 (0.6248) Acc G: 47.360%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1229 (0.2803) Acc D Real: 77.954%
Loss D Fake: 4.2909 (1.1979) Acc D Fake: 52.538%
Loss D: 4.414
Loss G: 0.0566 (0.6215) Acc G: 47.663%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.0814 (0.2792) Acc D Real: 78.074%
Loss D Fake: 4.2265 (1.2152) Acc D Fake: 52.238%
Loss D: 4.308
Loss G: 0.0573 (0.6183) Acc G: 47.962%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.0643 (0.2780) Acc D Real: 78.193%
Loss D Fake: 4.1638 (1.2320) Acc D Fake: 51.941%
Loss D: 4.228
Loss G: 0.0581 (0.6151) Acc G: 48.258%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.0818 (0.2769) Acc D Real: 78.309%
Loss D Fake: 4.1024 (1.2482) Acc D Fake: 51.648%
Loss D: 4.184
Loss G: 0.0589 (0.6120) Acc G: 48.550%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.0691 (0.2757) Acc D Real: 78.423%
Loss D Fake: 4.0425 (1.2639) Acc D Fake: 51.358%
Loss D: 4.112
Loss G: 0.0597 (0.6089) Acc G: 48.839%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.0576 (0.2745) Acc D Real: 78.540%
Loss D Fake: 3.9837 (1.2791) Acc D Fake: 51.071%
Loss D: 4.041
Loss G: 0.0606 (0.6058) Acc G: 49.125%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.0538 (0.2732) Acc D Real: 78.655%
Loss D Fake: 3.9261 (1.2938) Acc D Fake: 50.787%
Loss D: 3.980
Loss G: 0.0616 (0.6028) Acc G: 49.407%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.0687 (0.2721) Acc D Real: 78.766%
Loss D Fake: 3.8695 (1.3080) Acc D Fake: 50.506%
Loss D: 3.938
Loss G: 0.0626 (0.5998) Acc G: 49.687%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.0641 (0.2710) Acc D Real: 78.878%
Loss D Fake: 3.8139 (1.3218) Acc D Fake: 50.229%
Loss D: 3.878
Loss G: 0.0637 (0.5969) Acc G: 49.963%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.0602 (0.2698) Acc D Real: 78.987%
Loss D Fake: 3.7592 (1.3351) Acc D Fake: 49.954%
Loss D: 3.819
Loss G: 0.0648 (0.5939) Acc G: 50.237%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.0679 (0.2687) Acc D Real: 79.095%
Loss D Fake: 3.7053 (1.3480) Acc D Fake: 49.683%
Loss D: 3.773
Loss G: 0.0659 (0.5911) Acc G: 50.507%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.0657 (0.2676) Acc D Real: 79.202%
Loss D Fake: 3.6521 (1.3605) Acc D Fake: 49.414%
Loss D: 3.718
Loss G: 0.0671 (0.5882) Acc G: 50.766%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.0643 (0.2665) Acc D Real: 79.308%
Loss D Fake: 3.5997 (1.3725) Acc D Fake: 49.158%
Loss D: 3.664
Loss G: 0.0684 (0.5854) Acc G: 51.022%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.0675 (0.2655) Acc D Real: 79.411%
Loss D Fake: 3.5479 (1.3841) Acc D Fake: 48.904%
Loss D: 3.615
Loss G: 0.0697 (0.5827) Acc G: 51.275%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.0631 (0.2644) Acc D Real: 79.516%
Loss D Fake: 3.4967 (1.3954) Acc D Fake: 48.652%
Loss D: 3.560
Loss G: 0.0710 (0.5800) Acc G: 51.525%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.0662 (0.2633) Acc D Real: 79.618%
Loss D Fake: 3.4461 (1.4062) Acc D Fake: 48.404%
Loss D: 3.512
Loss G: 0.0724 (0.5773) Acc G: 51.772%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.0706 (0.2623) Acc D Real: 79.720%
Loss D Fake: 3.3960 (1.4167) Acc D Fake: 48.158%
Loss D: 3.467
Loss G: 0.0739 (0.5746) Acc G: 52.018%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.0719 (0.2613) Acc D Real: 79.818%
Loss D Fake: 3.3464 (1.4268) Acc D Fake: 47.914%
Loss D: 3.418
Loss G: 0.0755 (0.5720) Acc G: 52.260%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.0764 (0.2604) Acc D Real: 79.915%
Loss D Fake: 3.2972 (1.4365) Acc D Fake: 47.674%
Loss D: 3.374
Loss G: 0.0771 (0.5694) Acc G: 52.500%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.0671 (0.2594) Acc D Real: 80.016%
Loss D Fake: 3.2484 (1.4459) Acc D Fake: 47.435%
Loss D: 3.316
Loss G: 0.0787 (0.5669) Acc G: 52.737%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.0729 (0.2584) Acc D Real: 80.113%
Loss D Fake: 3.2001 (1.4550) Acc D Fake: 47.199%
Loss D: 3.273
Loss G: 0.0805 (0.5644) Acc G: 52.973%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.0760 (0.2575) Acc D Real: 80.209%
Loss D Fake: 3.1521 (1.4637) Acc D Fake: 46.966%
Loss D: 3.228
Loss G: 0.0823 (0.5619) Acc G: 53.205%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.0820 (0.2566) Acc D Real: 80.304%
Loss D Fake: 3.1045 (1.4720) Acc D Fake: 46.735%
Loss D: 3.187
Loss G: 0.0842 (0.5595) Acc G: 53.435%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.0815 (0.2557) Acc D Real: 80.397%
Loss D Fake: 3.0572 (1.4801) Acc D Fake: 46.506%
Loss D: 3.139
Loss G: 0.0862 (0.5571) Acc G: 53.663%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.0791 (0.2548) Acc D Real: 80.490%
Loss D Fake: 3.0103 (1.4878) Acc D Fake: 46.279%
Loss D: 3.089
Loss G: 0.0883 (0.5547) Acc G: 53.889%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.0880 (0.2540) Acc D Real: 80.581%
Loss D Fake: 2.9637 (1.4952) Acc D Fake: 46.055%
Loss D: 3.052
Loss G: 0.0904 (0.5524) Acc G: 54.112%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.0859 (0.2531) Acc D Real: 80.670%
Loss D Fake: 2.9174 (1.5023) Acc D Fake: 45.833%
Loss D: 3.003
Loss G: 0.0927 (0.5501) Acc G: 54.333%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.0811 (0.2523) Acc D Real: 80.762%
Loss D Fake: 2.8715 (1.5092) Acc D Fake: 45.614%
Loss D: 2.953
Loss G: 0.0951 (0.5478) Acc G: 54.552%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.0947 (0.2515) Acc D Real: 80.847%
Loss D Fake: 2.8259 (1.5157) Acc D Fake: 45.396%
Loss D: 2.921
Loss G: 0.0975 (0.5456) Acc G: 54.769%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.0925 (0.2507) Acc D Real: 80.933%
Loss D Fake: 2.7806 (1.5219) Acc D Fake: 45.181%
Loss D: 2.873
Loss G: 0.1001 (0.5434) Acc G: 54.984%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.0980 (0.2499) Acc D Real: 81.018%
Loss D Fake: 2.7357 (1.5279) Acc D Fake: 44.967%
Loss D: 2.834
Loss G: 0.1027 (0.5412) Acc G: 55.196%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.0954 (0.2492) Acc D Real: 81.104%
Loss D Fake: 2.6911 (1.5335) Acc D Fake: 44.756%
Loss D: 2.786
Loss G: 0.1055 (0.5391) Acc G: 55.407%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.1054 (0.2485) Acc D Real: 81.187%
Loss D Fake: 2.6469 (1.5389) Acc D Fake: 44.547%
Loss D: 2.752
Loss G: 0.1084 (0.5370) Acc G: 55.615%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.1023 (0.2478) Acc D Real: 81.271%
Loss D Fake: 2.6031 (1.5441) Acc D Fake: 44.340%
Loss D: 2.705
Loss G: 0.1114 (0.5350) Acc G: 55.821%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.1090 (0.2471) Acc D Real: 81.352%
Loss D Fake: 2.5597 (1.5490) Acc D Fake: 44.135%
Loss D: 2.669
Loss G: 0.1145 (0.5329) Acc G: 56.026%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1096 (0.2465) Acc D Real: 81.433%
Loss D Fake: 2.5166 (1.5536) Acc D Fake: 43.931%
Loss D: 2.626
Loss G: 0.1178 (0.5310) Acc G: 56.228%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.1104 (0.2458) Acc D Real: 81.515%
Loss D Fake: 2.4740 (1.5580) Acc D Fake: 43.730%
Loss D: 2.584
Loss G: 0.1212 (0.5290) Acc G: 56.429%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.1150 (0.2452) Acc D Real: 81.594%
Loss D Fake: 2.4319 (1.5621) Acc D Fake: 43.531%
Loss D: 2.547
Loss G: 0.1247 (0.5271) Acc G: 56.627%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.1220 (0.2446) Acc D Real: 81.672%
Loss D Fake: 2.3902 (1.5660) Acc D Fake: 43.333%
Loss D: 2.512
Loss G: 0.1283 (0.5252) Acc G: 56.824%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.1261 (0.2441) Acc D Real: 81.748%
Loss D Fake: 2.3490 (1.5697) Acc D Fake: 43.138%
Loss D: 2.475
Loss G: 0.1321 (0.5234) Acc G: 57.019%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.1321 (0.2435) Acc D Real: 81.823%
Loss D Fake: 2.3083 (1.5731) Acc D Fake: 42.944%
Loss D: 2.440
Loss G: 0.1360 (0.5216) Acc G: 57.212%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.1325 (0.2430) Acc D Real: 81.898%
Loss D Fake: 2.2680 (1.5764) Acc D Fake: 42.752%
Loss D: 2.400
Loss G: 0.1401 (0.5198) Acc G: 57.403%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.1381 (0.2425) Acc D Real: 81.972%
Loss D Fake: 2.2282 (1.5794) Acc D Fake: 42.562%
Loss D: 2.366
Loss G: 0.1443 (0.5180) Acc G: 57.593%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.1365 (0.2420) Acc D Real: 82.048%
Loss D Fake: 2.1889 (1.5822) Acc D Fake: 42.373%
Loss D: 2.325
Loss G: 0.1486 (0.5163) Acc G: 57.780%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.1458 (0.2416) Acc D Real: 82.119%
Loss D Fake: 2.1502 (1.5848) Acc D Fake: 42.187%
Loss D: 2.296
Loss G: 0.1531 (0.5147) Acc G: 57.966%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.1472 (0.2412) Acc D Real: 82.191%
Loss D Fake: 2.1121 (1.5872) Acc D Fake: 42.002%
Loss D: 2.259
Loss G: 0.1577 (0.5130) Acc G: 58.151%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.1480 (0.2408) Acc D Real: 82.263%
Loss D Fake: 2.0746 (1.5894) Acc D Fake: 41.818%
Loss D: 2.223
Loss G: 0.1624 (0.5114) Acc G: 58.333%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.1525 (0.2404) Acc D Real: 82.334%
Loss D Fake: 2.0378 (1.5915) Acc D Fake: 41.637%
Loss D: 2.190
Loss G: 0.1673 (0.5099) Acc G: 58.514%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.1630 (0.2400) Acc D Real: 82.404%
Loss D Fake: 2.0016 (1.5933) Acc D Fake: 41.456%
Loss D: 2.165
Loss G: 0.1722 (0.5084) Acc G: 58.694%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.1635 (0.2397) Acc D Real: 82.472%
Loss D Fake: 1.9661 (1.5950) Acc D Fake: 41.278%
Loss D: 2.130
Loss G: 0.1774 (0.5069) Acc G: 58.871%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.1689 (0.2393) Acc D Real: 82.542%
Loss D Fake: 1.9314 (1.5965) Acc D Fake: 41.101%
Loss D: 2.100
Loss G: 0.1826 (0.5054) Acc G: 59.048%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.1746 (0.2391) Acc D Real: 82.609%
Loss D Fake: 1.8973 (1.5978) Acc D Fake: 40.926%
Loss D: 2.072
Loss G: 0.1879 (0.5040) Acc G: 59.222%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.1778 (0.2388) Acc D Real: 82.626%
Loss D Fake: 1.8639 (1.5990) Acc D Fake: 40.882%
Loss D: 2.042
Loss G: 0.1934 (0.5026) Acc G: 59.266%
LR: 2.000e-04
Epoch: 16/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.1865 (0.1853) Acc D Real: 97.578%
Loss D Fake: 1.7993 (1.8153) Acc D Fake: 1.667%
Loss D: 1.986
Loss G: 0.2046 (0.2018) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.1950 (0.1885) Acc D Real: 97.604%
Loss D Fake: 1.7682 (1.7996) Acc D Fake: 1.667%
Loss D: 1.963
Loss G: 0.2103 (0.2046) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.1986 (0.1910) Acc D Real: 97.656%
Loss D Fake: 1.7377 (1.7841) Acc D Fake: 1.667%
Loss D: 1.936
Loss G: 0.2162 (0.2075) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2032 (0.1935) Acc D Real: 97.719%
Loss D Fake: 1.7079 (1.7689) Acc D Fake: 1.667%
Loss D: 1.911
Loss G: 0.2221 (0.2104) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2104 (0.1963) Acc D Real: 97.717%
Loss D Fake: 1.6790 (1.7539) Acc D Fake: 1.667%
Loss D: 1.889
Loss G: 0.2281 (0.2134) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.2177 (0.1993) Acc D Real: 97.686%
Loss D Fake: 1.6508 (1.7392) Acc D Fake: 1.667%
Loss D: 1.868
Loss G: 0.2341 (0.2163) Acc G: 98.333%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.2178 (0.2017) Acc D Real: 97.734%
Loss D Fake: 1.6234 (1.7247) Acc D Fake: 1.875%
Loss D: 1.841
Loss G: 0.2402 (0.2193) Acc G: 98.125%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2243 (0.2042) Acc D Real: 97.737%
Loss D Fake: 1.5967 (1.7105) Acc D Fake: 2.037%
Loss D: 1.821
Loss G: 0.2463 (0.2223) Acc G: 97.963%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2322 (0.2070) Acc D Real: 97.719%
Loss D Fake: 1.5709 (1.6965) Acc D Fake: 2.167%
Loss D: 1.803
Loss G: 0.2525 (0.2253) Acc G: 97.833%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.2358 (0.2096) Acc D Real: 97.727%
Loss D Fake: 1.5457 (1.6828) Acc D Fake: 2.273%
Loss D: 1.782
Loss G: 0.2587 (0.2284) Acc G: 97.727%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2425 (0.2123) Acc D Real: 97.730%
Loss D Fake: 1.5214 (1.6694) Acc D Fake: 2.361%
Loss D: 1.764
Loss G: 0.2649 (0.2314) Acc G: 97.639%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2461 (0.2149) Acc D Real: 97.732%
Loss D Fake: 1.4979 (1.6562) Acc D Fake: 2.436%
Loss D: 1.744
Loss G: 0.2711 (0.2345) Acc G: 97.564%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2540 (0.2177) Acc D Real: 97.731%
Loss D Fake: 1.4751 (1.6432) Acc D Fake: 2.500%
Loss D: 1.729
Loss G: 0.2772 (0.2375) Acc G: 97.500%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2590 (0.2205) Acc D Real: 97.722%
Loss D Fake: 1.4532 (1.6306) Acc D Fake: 2.556%
Loss D: 1.712
Loss G: 0.2834 (0.2406) Acc G: 97.444%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.2705 (0.2236) Acc D Real: 97.721%
Loss D Fake: 1.4321 (1.6182) Acc D Fake: 2.604%
Loss D: 1.703
Loss G: 0.2894 (0.2436) Acc G: 97.396%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.2713 (0.2264) Acc D Real: 97.733%
Loss D Fake: 1.4120 (1.6060) Acc D Fake: 2.647%
Loss D: 1.683
Loss G: 0.2953 (0.2467) Acc G: 97.353%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2708 (0.2289) Acc D Real: 97.740%
Loss D Fake: 1.3926 (1.5942) Acc D Fake: 2.685%
Loss D: 1.663
Loss G: 0.3012 (0.2497) Acc G: 97.315%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.2774 (0.2314) Acc D Real: 97.736%
Loss D Fake: 1.3738 (1.5826) Acc D Fake: 2.719%
Loss D: 1.651
Loss G: 0.3071 (0.2527) Acc G: 97.281%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.2914 (0.2344) Acc D Real: 97.719%
Loss D Fake: 1.3556 (1.5712) Acc D Fake: 2.750%
Loss D: 1.647
Loss G: 0.3129 (0.2557) Acc G: 97.250%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3004 (0.2376) Acc D Real: 97.698%
Loss D Fake: 1.3383 (1.5601) Acc D Fake: 2.778%
Loss D: 1.639
Loss G: 0.3186 (0.2587) Acc G: 97.222%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.2994 (0.2404) Acc D Real: 97.685%
Loss D Fake: 1.3218 (1.5493) Acc D Fake: 2.803%
Loss D: 1.621
Loss G: 0.3241 (0.2617) Acc G: 97.197%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3081 (0.2433) Acc D Real: 97.683%
Loss D Fake: 1.3060 (1.5387) Acc D Fake: 2.826%
Loss D: 1.614
Loss G: 0.3295 (0.2646) Acc G: 97.174%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.3059 (0.2459) Acc D Real: 97.689%
Loss D Fake: 1.2911 (1.5284) Acc D Fake: 2.847%
Loss D: 1.597
Loss G: 0.3348 (0.2676) Acc G: 97.153%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.3159 (0.2487) Acc D Real: 97.681%
Loss D Fake: 1.2768 (1.5183) Acc D Fake: 2.867%
Loss D: 1.593
Loss G: 0.3399 (0.2705) Acc G: 97.133%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.3213 (0.2515) Acc D Real: 97.676%
Loss D Fake: 1.2631 (1.5085) Acc D Fake: 2.885%
Loss D: 1.584
Loss G: 0.3448 (0.2733) Acc G: 97.115%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3205 (0.2541) Acc D Real: 97.679%
Loss D Fake: 1.2502 (1.4990) Acc D Fake: 2.901%
Loss D: 1.571
Loss G: 0.3496 (0.2761) Acc G: 97.099%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3301 (0.2568) Acc D Real: 97.669%
Loss D Fake: 1.2379 (1.4896) Acc D Fake: 2.917%
Loss D: 1.568
Loss G: 0.3543 (0.2789) Acc G: 97.083%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3308 (0.2593) Acc D Real: 97.660%
Loss D Fake: 1.2261 (1.4805) Acc D Fake: 2.931%
Loss D: 1.557
Loss G: 0.3589 (0.2817) Acc G: 97.069%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.3238 (0.2615) Acc D Real: 97.656%
Loss D Fake: 1.2148 (1.4717) Acc D Fake: 2.944%
Loss D: 1.539
Loss G: 0.3634 (0.2844) Acc G: 97.056%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3391 (0.2640) Acc D Real: 97.650%
Loss D Fake: 1.2038 (1.4630) Acc D Fake: 2.957%
Loss D: 1.543
Loss G: 0.3678 (0.2871) Acc G: 97.043%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3451 (0.2665) Acc D Real: 97.650%
Loss D Fake: 1.1933 (1.4546) Acc D Fake: 2.969%
Loss D: 1.538
Loss G: 0.3720 (0.2898) Acc G: 97.031%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3416 (0.2688) Acc D Real: 97.648%
Loss D Fake: 1.1835 (1.4464) Acc D Fake: 2.980%
Loss D: 1.525
Loss G: 0.3760 (0.2924) Acc G: 97.020%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.3446 (0.2710) Acc D Real: 97.644%
Loss D Fake: 1.1741 (1.4384) Acc D Fake: 2.990%
Loss D: 1.519
Loss G: 0.3800 (0.2950) Acc G: 97.010%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3502 (0.2733) Acc D Real: 97.634%
Loss D Fake: 1.1650 (1.4306) Acc D Fake: 3.000%
Loss D: 1.515
Loss G: 0.3839 (0.2975) Acc G: 97.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3460 (0.2753) Acc D Real: 97.632%
Loss D Fake: 1.1562 (1.4230) Acc D Fake: 3.009%
Loss D: 1.502
Loss G: 0.3877 (0.3000) Acc G: 96.991%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3588 (0.2776) Acc D Real: 97.631%
Loss D Fake: 1.1477 (1.4155) Acc D Fake: 3.018%
Loss D: 1.507
Loss G: 0.3914 (0.3025) Acc G: 96.982%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.3608 (0.2798) Acc D Real: 97.626%
Loss D Fake: 1.1398 (1.4083) Acc D Fake: 3.026%
Loss D: 1.501
Loss G: 0.3948 (0.3049) Acc G: 96.974%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3781 (0.2823) Acc D Real: 97.619%
Loss D Fake: 1.1325 (1.4012) Acc D Fake: 3.034%
Loss D: 1.511
Loss G: 0.3980 (0.3073) Acc G: 96.966%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3673 (0.2844) Acc D Real: 97.612%
Loss D Fake: 1.1258 (1.3943) Acc D Fake: 3.042%
Loss D: 1.493
Loss G: 0.4010 (0.3096) Acc G: 96.958%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3703 (0.2865) Acc D Real: 97.611%
Loss D Fake: 1.1195 (1.3876) Acc D Fake: 3.049%
Loss D: 1.490
Loss G: 0.4039 (0.3119) Acc G: 96.951%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3817 (0.2888) Acc D Real: 97.602%
Loss D Fake: 1.1135 (1.3811) Acc D Fake: 3.056%
Loss D: 1.495
Loss G: 0.4066 (0.3142) Acc G: 96.944%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.3731 (0.2907) Acc D Real: 97.604%
Loss D Fake: 1.1080 (1.3747) Acc D Fake: 3.062%
Loss D: 1.481
Loss G: 0.4092 (0.3164) Acc G: 96.938%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3801 (0.2928) Acc D Real: 97.601%
Loss D Fake: 1.1027 (1.3685) Acc D Fake: 3.068%
Loss D: 1.483
Loss G: 0.4116 (0.3186) Acc G: 96.932%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3775 (0.2946) Acc D Real: 97.596%
Loss D Fake: 1.0978 (1.3625) Acc D Fake: 3.074%
Loss D: 1.475
Loss G: 0.4140 (0.3207) Acc G: 96.926%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3911 (0.2967) Acc D Real: 97.588%
Loss D Fake: 1.0930 (1.3567) Acc D Fake: 3.080%
Loss D: 1.484
Loss G: 0.4162 (0.3228) Acc G: 96.920%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.3688 (0.2983) Acc D Real: 97.594%
Loss D Fake: 1.0886 (1.3510) Acc D Fake: 3.085%
Loss D: 1.457
Loss G: 0.4183 (0.3248) Acc G: 96.915%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3788 (0.3000) Acc D Real: 97.595%
Loss D Fake: 1.0842 (1.3454) Acc D Fake: 3.090%
Loss D: 1.463
Loss G: 0.4205 (0.3268) Acc G: 96.910%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3906 (0.3018) Acc D Real: 97.587%
Loss D Fake: 1.0800 (1.3400) Acc D Fake: 3.095%
Loss D: 1.471
Loss G: 0.4225 (0.3287) Acc G: 96.905%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3837 (0.3034) Acc D Real: 97.588%
Loss D Fake: 1.0761 (1.3347) Acc D Fake: 3.100%
Loss D: 1.460
Loss G: 0.4244 (0.3306) Acc G: 96.900%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.3814 (0.3050) Acc D Real: 97.588%
Loss D Fake: 1.0724 (1.3296) Acc D Fake: 3.105%
Loss D: 1.454
Loss G: 0.4263 (0.3325) Acc G: 96.895%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3899 (0.3066) Acc D Real: 97.589%
Loss D Fake: 1.0687 (1.3246) Acc D Fake: 3.109%
Loss D: 1.459
Loss G: 0.4281 (0.3344) Acc G: 96.891%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.4054 (0.3085) Acc D Real: 97.583%
Loss D Fake: 1.0654 (1.3197) Acc D Fake: 3.113%
Loss D: 1.471
Loss G: 0.4296 (0.3362) Acc G: 96.887%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3992 (0.3101) Acc D Real: 97.578%
Loss D Fake: 1.0625 (1.3149) Acc D Fake: 3.117%
Loss D: 1.462
Loss G: 0.4310 (0.3379) Acc G: 96.883%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.3956 (0.3117) Acc D Real: 97.577%
Loss D Fake: 1.0599 (1.3103) Acc D Fake: 3.121%
Loss D: 1.456
Loss G: 0.4323 (0.3396) Acc G: 96.879%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.3835 (0.3130) Acc D Real: 97.572%
Loss D Fake: 1.0574 (1.3057) Acc D Fake: 3.125%
Loss D: 1.441
Loss G: 0.4337 (0.3413) Acc G: 96.875%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3978 (0.3145) Acc D Real: 97.569%
Loss D Fake: 1.0547 (1.3013) Acc D Fake: 3.129%
Loss D: 1.453
Loss G: 0.4350 (0.3430) Acc G: 96.871%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3938 (0.3158) Acc D Real: 97.566%
Loss D Fake: 1.0522 (1.2970) Acc D Fake: 3.132%
Loss D: 1.446
Loss G: 0.4363 (0.3446) Acc G: 96.868%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.3999 (0.3173) Acc D Real: 97.564%
Loss D Fake: 1.0497 (1.2929) Acc D Fake: 3.136%
Loss D: 1.450
Loss G: 0.4376 (0.3461) Acc G: 96.864%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3945 (0.3186) Acc D Real: 97.566%
Loss D Fake: 1.0474 (1.2888) Acc D Fake: 3.139%
Loss D: 1.442
Loss G: 0.4388 (0.3477) Acc G: 96.861%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3841 (0.3196) Acc D Real: 97.564%
Loss D Fake: 1.0451 (1.2848) Acc D Fake: 3.142%
Loss D: 1.429
Loss G: 0.4400 (0.3492) Acc G: 96.858%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.3804 (0.3206) Acc D Real: 97.562%
Loss D Fake: 1.0426 (1.2809) Acc D Fake: 3.145%
Loss D: 1.423
Loss G: 0.4414 (0.3507) Acc G: 96.855%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3693 (0.3214) Acc D Real: 97.560%
Loss D Fake: 1.0398 (1.2770) Acc D Fake: 3.148%
Loss D: 1.409
Loss G: 0.4431 (0.3522) Acc G: 96.852%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3783 (0.3223) Acc D Real: 97.558%
Loss D Fake: 1.0366 (1.2733) Acc D Fake: 3.151%
Loss D: 1.415
Loss G: 0.4448 (0.3536) Acc G: 96.849%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3863 (0.3233) Acc D Real: 97.551%
Loss D Fake: 1.0333 (1.2696) Acc D Fake: 3.154%
Loss D: 1.420
Loss G: 0.4466 (0.3550) Acc G: 96.846%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.4008 (0.3244) Acc D Real: 97.546%
Loss D Fake: 1.0300 (1.2660) Acc D Fake: 3.157%
Loss D: 1.431
Loss G: 0.4483 (0.3564) Acc G: 96.843%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.3993 (0.3255) Acc D Real: 97.547%
Loss D Fake: 1.0271 (1.2624) Acc D Fake: 3.159%
Loss D: 1.426
Loss G: 0.4498 (0.3578) Acc G: 96.841%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.4255 (0.3270) Acc D Real: 97.543%
Loss D Fake: 1.0246 (1.2589) Acc D Fake: 3.162%
Loss D: 1.450
Loss G: 0.4510 (0.3592) Acc G: 96.838%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.4189 (0.3283) Acc D Real: 97.541%
Loss D Fake: 1.0227 (1.2555) Acc D Fake: 3.164%
Loss D: 1.442
Loss G: 0.4519 (0.3605) Acc G: 96.836%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.4021 (0.3294) Acc D Real: 97.537%
Loss D Fake: 1.0212 (1.2521) Acc D Fake: 3.167%
Loss D: 1.423
Loss G: 0.4527 (0.3619) Acc G: 96.833%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.3798 (0.3301) Acc D Real: 97.534%
Loss D Fake: 1.0196 (1.2489) Acc D Fake: 3.169%
Loss D: 1.399
Loss G: 0.4537 (0.3632) Acc G: 96.831%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3706 (0.3307) Acc D Real: 97.532%
Loss D Fake: 1.0175 (1.2456) Acc D Fake: 3.171%
Loss D: 1.388
Loss G: 0.4550 (0.3644) Acc G: 96.829%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3798 (0.3313) Acc D Real: 97.529%
Loss D Fake: 1.0150 (1.2425) Acc D Fake: 3.174%
Loss D: 1.395
Loss G: 0.4565 (0.3657) Acc G: 96.826%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3881 (0.3321) Acc D Real: 97.525%
Loss D Fake: 1.0123 (1.2394) Acc D Fake: 3.176%
Loss D: 1.400
Loss G: 0.4580 (0.3669) Acc G: 96.824%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3732 (0.3327) Acc D Real: 97.522%
Loss D Fake: 1.0094 (1.2363) Acc D Fake: 3.178%
Loss D: 1.383
Loss G: 0.4597 (0.3682) Acc G: 96.822%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3940 (0.3335) Acc D Real: 97.519%
Loss D Fake: 1.0064 (1.2333) Acc D Fake: 3.180%
Loss D: 1.400
Loss G: 0.4614 (0.3694) Acc G: 96.820%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3777 (0.3340) Acc D Real: 97.519%
Loss D Fake: 1.0035 (1.2303) Acc D Fake: 3.182%
Loss D: 1.381
Loss G: 0.4631 (0.3706) Acc G: 96.818%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.4040 (0.3349) Acc D Real: 97.519%
Loss D Fake: 1.0007 (1.2274) Acc D Fake: 3.184%
Loss D: 1.405
Loss G: 0.4646 (0.3718) Acc G: 96.816%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3912 (0.3357) Acc D Real: 97.514%
Loss D Fake: 0.9982 (1.2244) Acc D Fake: 3.186%
Loss D: 1.389
Loss G: 0.4660 (0.3730) Acc G: 96.814%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3746 (0.3361) Acc D Real: 97.513%
Loss D Fake: 0.9957 (1.2216) Acc D Fake: 3.188%
Loss D: 1.370
Loss G: 0.4675 (0.3742) Acc G: 96.812%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.4139 (0.3371) Acc D Real: 97.512%
Loss D Fake: 0.9931 (1.2188) Acc D Fake: 3.189%
Loss D: 1.407
Loss G: 0.4688 (0.3754) Acc G: 96.811%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3505 (0.3373) Acc D Real: 97.509%
Loss D Fake: 0.9909 (1.2160) Acc D Fake: 3.191%
Loss D: 1.341
Loss G: 0.4703 (0.3765) Acc G: 96.809%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3952 (0.3380) Acc D Real: 97.504%
Loss D Fake: 0.9882 (1.2132) Acc D Fake: 3.193%
Loss D: 1.383
Loss G: 0.4718 (0.3777) Acc G: 96.807%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3836 (0.3385) Acc D Real: 97.504%
Loss D Fake: 0.9857 (1.2105) Acc D Fake: 3.194%
Loss D: 1.369
Loss G: 0.4733 (0.3788) Acc G: 96.806%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3806 (0.3390) Acc D Real: 97.502%
Loss D Fake: 0.9831 (1.2079) Acc D Fake: 3.196%
Loss D: 1.364
Loss G: 0.4748 (0.3799) Acc G: 96.804%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4066 (0.3398) Acc D Real: 97.501%
Loss D Fake: 0.9807 (1.2052) Acc D Fake: 3.198%
Loss D: 1.387
Loss G: 0.4761 (0.3811) Acc G: 96.802%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3603 (0.3400) Acc D Real: 97.501%
Loss D Fake: 0.9786 (1.2026) Acc D Fake: 3.199%
Loss D: 1.339
Loss G: 0.4775 (0.3822) Acc G: 96.801%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3888 (0.3406) Acc D Real: 97.499%
Loss D Fake: 0.9762 (1.2000) Acc D Fake: 3.201%
Loss D: 1.365
Loss G: 0.4789 (0.3833) Acc G: 96.799%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3948 (0.3412) Acc D Real: 97.494%
Loss D Fake: 0.9739 (1.1975) Acc D Fake: 3.202%
Loss D: 1.369
Loss G: 0.4802 (0.3844) Acc G: 96.798%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3855 (0.3417) Acc D Real: 97.493%
Loss D Fake: 0.9717 (1.1950) Acc D Fake: 3.204%
Loss D: 1.357
Loss G: 0.4815 (0.3854) Acc G: 96.796%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3731 (0.3420) Acc D Real: 97.491%
Loss D Fake: 0.9695 (1.1925) Acc D Fake: 3.205%
Loss D: 1.343
Loss G: 0.4829 (0.3865) Acc G: 96.795%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3833 (0.3425) Acc D Real: 97.490%
Loss D Fake: 0.9671 (1.1901) Acc D Fake: 3.207%
Loss D: 1.350
Loss G: 0.4843 (0.3876) Acc G: 96.793%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3396 (0.3424) Acc D Real: 97.489%
Loss D Fake: 0.9647 (1.1876) Acc D Fake: 3.208%
Loss D: 1.304
Loss G: 0.4860 (0.3886) Acc G: 96.792%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.4031 (0.3431) Acc D Real: 97.488%
Loss D Fake: 0.9619 (1.1852) Acc D Fake: 3.209%
Loss D: 1.365
Loss G: 0.4876 (0.3897) Acc G: 96.791%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3555 (0.3432) Acc D Real: 97.485%
Loss D Fake: 0.9593 (1.1829) Acc D Fake: 3.211%
Loss D: 1.315
Loss G: 0.4892 (0.3907) Acc G: 96.789%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.4362 (0.3442) Acc D Real: 97.483%
Loss D Fake: 0.9568 (1.1805) Acc D Fake: 3.212%
Loss D: 1.393
Loss G: 0.4905 (0.3918) Acc G: 96.788%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4186 (0.3450) Acc D Real: 97.483%
Loss D Fake: 0.9551 (1.1782) Acc D Fake: 3.213%
Loss D: 1.374
Loss G: 0.4914 (0.3928) Acc G: 96.787%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3990 (0.3455) Acc D Real: 97.482%
Loss D Fake: 0.9537 (1.1759) Acc D Fake: 3.214%
Loss D: 1.353
Loss G: 0.4923 (0.3938) Acc G: 96.786%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.3568 (0.3456) Acc D Real: 97.483%
Loss D Fake: 0.9523 (1.1736) Acc D Fake: 3.215%
Loss D: 1.309
Loss G: 0.4933 (0.3948) Acc G: 96.785%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.3587 (0.3458) Acc D Real: 97.483%
Loss D Fake: 0.9503 (1.1714) Acc D Fake: 3.217%
Loss D: 1.309
Loss G: 0.4947 (0.3958) Acc G: 96.783%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3750 (0.3460) Acc D Real: 97.481%
Loss D Fake: 0.9480 (1.1692) Acc D Fake: 3.218%
Loss D: 1.323
Loss G: 0.4961 (0.3968) Acc G: 96.782%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3836 (0.3464) Acc D Real: 97.481%
Loss D Fake: 0.9457 (1.1670) Acc D Fake: 3.219%
Loss D: 1.329
Loss G: 0.4976 (0.3978) Acc G: 96.781%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.3508 (0.3465) Acc D Real: 97.479%
Loss D Fake: 0.9432 (1.1648) Acc D Fake: 3.220%
Loss D: 1.294
Loss G: 0.4993 (0.3988) Acc G: 96.780%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3632 (0.3466) Acc D Real: 97.477%
Loss D Fake: 0.9404 (1.1627) Acc D Fake: 3.221%
Loss D: 1.304
Loss G: 0.5012 (0.3998) Acc G: 96.779%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3390 (0.3465) Acc D Real: 97.475%
Loss D Fake: 0.9373 (1.1605) Acc D Fake: 3.222%
Loss D: 1.276
Loss G: 0.5033 (0.4008) Acc G: 96.778%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.3362 (0.3464) Acc D Real: 97.474%
Loss D Fake: 0.9337 (1.1584) Acc D Fake: 3.223%
Loss D: 1.270
Loss G: 0.5057 (0.4017) Acc G: 96.777%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.3536 (0.3465) Acc D Real: 97.476%
Loss D Fake: 0.9298 (1.1562) Acc D Fake: 3.224%
Loss D: 1.283
Loss G: 0.5083 (0.4027) Acc G: 96.776%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.4022 (0.3470) Acc D Real: 97.475%
Loss D Fake: 0.9261 (1.1541) Acc D Fake: 3.225%
Loss D: 1.328
Loss G: 0.5105 (0.4037) Acc G: 96.775%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3702 (0.3472) Acc D Real: 97.474%
Loss D Fake: 0.9230 (1.1520) Acc D Fake: 3.226%
Loss D: 1.293
Loss G: 0.5125 (0.4047) Acc G: 96.774%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3598 (0.3474) Acc D Real: 97.475%
Loss D Fake: 0.9199 (1.1499) Acc D Fake: 3.227%
Loss D: 1.280
Loss G: 0.5146 (0.4057) Acc G: 96.773%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3838 (0.3477) Acc D Real: 97.474%
Loss D Fake: 0.9168 (1.1478) Acc D Fake: 3.228%
Loss D: 1.301
Loss G: 0.5165 (0.4067) Acc G: 96.772%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.4220 (0.3483) Acc D Real: 97.472%
Loss D Fake: 0.9141 (1.1457) Acc D Fake: 3.229%
Loss D: 1.336
Loss G: 0.5180 (0.4077) Acc G: 96.771%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3887 (0.3487) Acc D Real: 97.470%
Loss D Fake: 0.9121 (1.1436) Acc D Fake: 3.230%
Loss D: 1.301
Loss G: 0.5193 (0.4087) Acc G: 96.770%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3691 (0.3489) Acc D Real: 97.472%
Loss D Fake: 0.9101 (1.1416) Acc D Fake: 3.231%
Loss D: 1.279
Loss G: 0.5207 (0.4097) Acc G: 96.769%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.3532 (0.3489) Acc D Real: 97.471%
Loss D Fake: 0.9080 (1.1395) Acc D Fake: 3.232%
Loss D: 1.261
Loss G: 0.5223 (0.4107) Acc G: 96.768%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.4237 (0.3496) Acc D Real: 97.471%
Loss D Fake: 0.9057 (1.1375) Acc D Fake: 3.233%
Loss D: 1.329
Loss G: 0.5235 (0.4116) Acc G: 96.767%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3590 (0.3496) Acc D Real: 97.474%
Loss D Fake: 0.9040 (1.1355) Acc D Fake: 3.234%
Loss D: 1.263
Loss G: 0.5247 (0.4126) Acc G: 96.766%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3780 (0.3499) Acc D Real: 97.474%
Loss D Fake: 0.9021 (1.1336) Acc D Fake: 3.234%
Loss D: 1.280
Loss G: 0.5260 (0.4136) Acc G: 96.766%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3886 (0.3502) Acc D Real: 97.474%
Loss D Fake: 0.9004 (1.1316) Acc D Fake: 3.235%
Loss D: 1.289
Loss G: 0.5272 (0.4145) Acc G: 96.765%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3744 (0.3504) Acc D Real: 97.473%
Loss D Fake: 0.8987 (1.1297) Acc D Fake: 3.236%
Loss D: 1.273
Loss G: 0.5283 (0.4155) Acc G: 96.764%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.4351 (0.3511) Acc D Real: 97.472%
Loss D Fake: 0.8972 (1.1277) Acc D Fake: 3.237%
Loss D: 1.332
Loss G: 0.5291 (0.4164) Acc G: 96.763%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.3786 (0.3513) Acc D Real: 97.469%
Loss D Fake: 0.8962 (1.1258) Acc D Fake: 3.238%
Loss D: 1.275
Loss G: 0.5298 (0.4173) Acc G: 96.762%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.4631 (0.3522) Acc D Real: 97.470%
Loss D Fake: 0.8955 (1.1240) Acc D Fake: 3.238%
Loss D: 1.359
Loss G: 0.5299 (0.4183) Acc G: 96.762%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3860 (0.3525) Acc D Real: 97.470%
Loss D Fake: 0.8956 (1.1221) Acc D Fake: 3.239%
Loss D: 1.282
Loss G: 0.5299 (0.4192) Acc G: 96.761%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3643 (0.3526) Acc D Real: 97.472%
Loss D Fake: 0.8954 (1.1203) Acc D Fake: 3.240%
Loss D: 1.260
Loss G: 0.5302 (0.4200) Acc G: 96.760%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3896 (0.3529) Acc D Real: 97.471%
Loss D Fake: 0.8949 (1.1185) Acc D Fake: 3.241%
Loss D: 1.284
Loss G: 0.5305 (0.4209) Acc G: 96.759%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.4261 (0.3535) Acc D Real: 97.472%
Loss D Fake: 0.8945 (1.1168) Acc D Fake: 3.241%
Loss D: 1.321
Loss G: 0.5306 (0.4218) Acc G: 96.759%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.4246 (0.3540) Acc D Real: 97.473%
Loss D Fake: 0.8946 (1.1150) Acc D Fake: 3.242%
Loss D: 1.319
Loss G: 0.5304 (0.4226) Acc G: 96.758%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3514 (0.3540) Acc D Real: 97.472%
Loss D Fake: 0.8948 (1.1133) Acc D Fake: 3.243%
Loss D: 1.246
Loss G: 0.5306 (0.4235) Acc G: 96.757%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3534 (0.3540) Acc D Real: 97.472%
Loss D Fake: 0.8942 (1.1116) Acc D Fake: 3.244%
Loss D: 1.248
Loss G: 0.5311 (0.4243) Acc G: 96.756%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3761 (0.3542) Acc D Real: 97.471%
Loss D Fake: 0.8932 (1.1100) Acc D Fake: 3.244%
Loss D: 1.269
Loss G: 0.5319 (0.4251) Acc G: 96.756%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3843 (0.3544) Acc D Real: 97.472%
Loss D Fake: 0.8921 (1.1083) Acc D Fake: 3.245%
Loss D: 1.276
Loss G: 0.5327 (0.4259) Acc G: 96.755%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3503 (0.3544) Acc D Real: 97.472%
Loss D Fake: 0.8908 (1.1067) Acc D Fake: 3.246%
Loss D: 1.241
Loss G: 0.5338 (0.4268) Acc G: 96.754%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3793 (0.3546) Acc D Real: 97.471%
Loss D Fake: 0.8891 (1.1051) Acc D Fake: 3.246%
Loss D: 1.268
Loss G: 0.5349 (0.4276) Acc G: 96.754%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.4162 (0.3550) Acc D Real: 97.469%
Loss D Fake: 0.8876 (1.1034) Acc D Fake: 3.247%
Loss D: 1.304
Loss G: 0.5358 (0.4284) Acc G: 96.753%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3687 (0.3551) Acc D Real: 97.469%
Loss D Fake: 0.8865 (1.1018) Acc D Fake: 3.248%
Loss D: 1.255
Loss G: 0.5367 (0.4292) Acc G: 96.752%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3866 (0.3553) Acc D Real: 97.471%
Loss D Fake: 0.8852 (1.1003) Acc D Fake: 3.248%
Loss D: 1.272
Loss G: 0.5375 (0.4299) Acc G: 96.752%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.3668 (0.3554) Acc D Real: 97.471%
Loss D Fake: 0.8840 (1.0987) Acc D Fake: 3.249%
Loss D: 1.251
Loss G: 0.5384 (0.4307) Acc G: 96.751%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.4047 (0.3558) Acc D Real: 97.470%
Loss D Fake: 0.8827 (1.0971) Acc D Fake: 3.249%
Loss D: 1.287
Loss G: 0.5392 (0.4315) Acc G: 96.751%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.4688 (0.3566) Acc D Real: 97.468%
Loss D Fake: 0.8820 (1.0956) Acc D Fake: 3.250%
Loss D: 1.351
Loss G: 0.5392 (0.4323) Acc G: 96.750%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.4372 (0.3572) Acc D Real: 97.469%
Loss D Fake: 0.8825 (1.0941) Acc D Fake: 3.251%
Loss D: 1.320
Loss G: 0.5387 (0.4330) Acc G: 96.749%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3703 (0.3573) Acc D Real: 97.468%
Loss D Fake: 0.8833 (1.0926) Acc D Fake: 3.251%
Loss D: 1.254
Loss G: 0.5384 (0.4338) Acc G: 96.749%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.3965 (0.3575) Acc D Real: 97.470%
Loss D Fake: 0.8836 (1.0911) Acc D Fake: 3.252%
Loss D: 1.280
Loss G: 0.5381 (0.4345) Acc G: 96.748%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.3249 (0.3573) Acc D Real: 97.472%
Loss D Fake: 0.8836 (1.0897) Acc D Fake: 3.252%
Loss D: 1.209
Loss G: 0.5385 (0.4352) Acc G: 96.748%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.3962 (0.3576) Acc D Real: 97.472%
Loss D Fake: 0.8828 (1.0883) Acc D Fake: 3.253%
Loss D: 1.279
Loss G: 0.5390 (0.4359) Acc G: 96.747%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3781 (0.3577) Acc D Real: 97.470%
Loss D Fake: 0.8821 (1.0869) Acc D Fake: 3.253%
Loss D: 1.260
Loss G: 0.5396 (0.4367) Acc G: 96.747%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3678 (0.3578) Acc D Real: 97.470%
Loss D Fake: 0.8812 (1.0855) Acc D Fake: 3.254%
Loss D: 1.249
Loss G: 0.5403 (0.4374) Acc G: 96.746%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3645 (0.3578) Acc D Real: 97.469%
Loss D Fake: 0.8800 (1.0841) Acc D Fake: 3.255%
Loss D: 1.245
Loss G: 0.5412 (0.4381) Acc G: 96.745%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3437 (0.3577) Acc D Real: 97.467%
Loss D Fake: 0.8785 (1.0827) Acc D Fake: 3.255%
Loss D: 1.222
Loss G: 0.5425 (0.4388) Acc G: 96.745%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.4332 (0.3582) Acc D Real: 97.468%
Loss D Fake: 0.8768 (1.0813) Acc D Fake: 3.256%
Loss D: 1.310
Loss G: 0.5434 (0.4395) Acc G: 96.744%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3277 (0.3580) Acc D Real: 97.468%
Loss D Fake: 0.8756 (1.0800) Acc D Fake: 3.256%
Loss D: 1.203
Loss G: 0.5445 (0.4402) Acc G: 96.744%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.4127 (0.3584) Acc D Real: 97.467%
Loss D Fake: 0.8739 (1.0786) Acc D Fake: 3.257%
Loss D: 1.287
Loss G: 0.5455 (0.4409) Acc G: 96.743%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3820 (0.3585) Acc D Real: 97.466%
Loss D Fake: 0.8727 (1.0773) Acc D Fake: 3.257%
Loss D: 1.255
Loss G: 0.5464 (0.4415) Acc G: 96.743%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.4315 (0.3590) Acc D Real: 97.466%
Loss D Fake: 0.8717 (1.0759) Acc D Fake: 3.258%
Loss D: 1.303
Loss G: 0.5469 (0.4422) Acc G: 96.742%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3978 (0.3593) Acc D Real: 97.465%
Loss D Fake: 0.8713 (1.0746) Acc D Fake: 3.258%
Loss D: 1.269
Loss G: 0.5471 (0.4429) Acc G: 96.742%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.4327 (0.3597) Acc D Real: 97.466%
Loss D Fake: 0.8712 (1.0733) Acc D Fake: 3.259%
Loss D: 1.304
Loss G: 0.5469 (0.4436) Acc G: 96.741%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.4863 (0.3605) Acc D Real: 97.465%
Loss D Fake: 0.8720 (1.0720) Acc D Fake: 3.259%
Loss D: 1.358
Loss G: 0.5459 (0.4442) Acc G: 96.741%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.4455 (0.3611) Acc D Real: 97.465%
Loss D Fake: 0.8739 (1.0708) Acc D Fake: 3.259%
Loss D: 1.319
Loss G: 0.5443 (0.4449) Acc G: 96.741%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3768 (0.3612) Acc D Real: 97.465%
Loss D Fake: 0.8759 (1.0695) Acc D Fake: 3.260%
Loss D: 1.253
Loss G: 0.5431 (0.4455) Acc G: 96.740%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.4306 (0.3616) Acc D Real: 97.464%
Loss D Fake: 0.8776 (1.0683) Acc D Fake: 3.260%
Loss D: 1.308
Loss G: 0.5418 (0.4461) Acc G: 96.740%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3846 (0.3618) Acc D Real: 97.464%
Loss D Fake: 0.8793 (1.0672) Acc D Fake: 3.261%
Loss D: 1.264
Loss G: 0.5408 (0.4467) Acc G: 96.739%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3961 (0.3620) Acc D Real: 97.465%
Loss D Fake: 0.8806 (1.0660) Acc D Fake: 3.261%
Loss D: 1.277
Loss G: 0.5399 (0.4472) Acc G: 96.739%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.4028 (0.3622) Acc D Real: 97.465%
Loss D Fake: 0.8818 (1.0649) Acc D Fake: 3.262%
Loss D: 1.285
Loss G: 0.5391 (0.4478) Acc G: 96.738%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.3811 (0.3623) Acc D Real: 97.464%
Loss D Fake: 0.8828 (1.0638) Acc D Fake: 3.262%
Loss D: 1.264
Loss G: 0.5386 (0.4484) Acc G: 96.738%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.4008 (0.3626) Acc D Real: 97.464%
Loss D Fake: 0.8834 (1.0627) Acc D Fake: 3.263%
Loss D: 1.284
Loss G: 0.5381 (0.4489) Acc G: 96.737%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3487 (0.3625) Acc D Real: 97.466%
Loss D Fake: 0.8838 (1.0616) Acc D Fake: 3.263%
Loss D: 1.232
Loss G: 0.5381 (0.4494) Acc G: 96.737%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.3379 (0.3623) Acc D Real: 97.467%
Loss D Fake: 0.8834 (1.0605) Acc D Fake: 3.263%
Loss D: 1.221
Loss G: 0.5387 (0.4500) Acc G: 96.737%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.4044 (0.3626) Acc D Real: 97.467%
Loss D Fake: 0.8824 (1.0595) Acc D Fake: 3.264%
Loss D: 1.287
Loss G: 0.5393 (0.4505) Acc G: 96.736%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3631 (0.3626) Acc D Real: 97.466%
Loss D Fake: 0.8816 (1.0584) Acc D Fake: 3.264%
Loss D: 1.245
Loss G: 0.5400 (0.4510) Acc G: 96.736%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.4012 (0.3628) Acc D Real: 97.467%
Loss D Fake: 0.8806 (1.0574) Acc D Fake: 3.265%
Loss D: 1.282
Loss G: 0.5406 (0.4516) Acc G: 96.735%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3729 (0.3629) Acc D Real: 97.468%
Loss D Fake: 0.8798 (1.0563) Acc D Fake: 3.265%
Loss D: 1.253
Loss G: 0.5412 (0.4521) Acc G: 96.735%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.4342 (0.3633) Acc D Real: 97.468%
Loss D Fake: 0.8791 (1.0553) Acc D Fake: 3.266%
Loss D: 1.313
Loss G: 0.5414 (0.4526) Acc G: 96.734%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3359 (0.3631) Acc D Real: 97.468%
Loss D Fake: 0.8789 (1.0543) Acc D Fake: 3.266%
Loss D: 1.215
Loss G: 0.5419 (0.4531) Acc G: 96.734%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3162 (0.3629) Acc D Real: 97.468%
Loss D Fake: 0.8777 (1.0533) Acc D Fake: 3.266%
Loss D: 1.194
Loss G: 0.5431 (0.4536) Acc G: 96.734%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3868 (0.3630) Acc D Real: 97.468%
Loss D Fake: 0.8758 (1.0523) Acc D Fake: 3.267%
Loss D: 1.263
Loss G: 0.5443 (0.4542) Acc G: 96.733%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3893 (0.3632) Acc D Real: 97.468%
Loss D Fake: 0.8742 (1.0513) Acc D Fake: 3.267%
Loss D: 1.264
Loss G: 0.5454 (0.4547) Acc G: 96.733%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.3760 (0.3632) Acc D Real: 97.469%
Loss D Fake: 0.8729 (1.0502) Acc D Fake: 3.267%
Loss D: 1.249
Loss G: 0.5464 (0.4552) Acc G: 96.733%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.3992 (0.3634) Acc D Real: 97.470%
Loss D Fake: 0.8716 (1.0492) Acc D Fake: 3.268%
Loss D: 1.271
Loss G: 0.5471 (0.4557) Acc G: 96.732%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.4140 (0.3637) Acc D Real: 97.470%
Loss D Fake: 0.8708 (1.0482) Acc D Fake: 3.268%
Loss D: 1.285
Loss G: 0.5475 (0.4562) Acc G: 96.732%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.3766 (0.3638) Acc D Real: 97.470%
Loss D Fake: 0.8704 (1.0473) Acc D Fake: 3.269%
Loss D: 1.247
Loss G: 0.5478 (0.4567) Acc G: 96.731%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3317 (0.3636) Acc D Real: 97.472%
Loss D Fake: 0.8697 (1.0463) Acc D Fake: 3.269%
Loss D: 1.201
Loss G: 0.5487 (0.4572) Acc G: 96.731%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3930 (0.3638) Acc D Real: 97.472%
Loss D Fake: 0.8684 (1.0453) Acc D Fake: 3.269%
Loss D: 1.261
Loss G: 0.5494 (0.4577) Acc G: 96.731%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.3871 (0.3639) Acc D Real: 97.472%
Loss D Fake: 0.8674 (1.0443) Acc D Fake: 3.270%
Loss D: 1.255
Loss G: 0.5501 (0.4582) Acc G: 96.730%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3638 (0.3639) Acc D Real: 97.472%
Loss D Fake: 0.8665 (1.0434) Acc D Fake: 3.270%
Loss D: 1.230
Loss G: 0.5509 (0.4588) Acc G: 96.730%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.4053 (0.3641) Acc D Real: 97.472%
Loss D Fake: 0.8655 (1.0424) Acc D Fake: 3.270%
Loss D: 1.271
Loss G: 0.5515 (0.4593) Acc G: 96.730%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3903 (0.3643) Acc D Real: 97.472%
Loss D Fake: 0.8648 (1.0414) Acc D Fake: 3.271%
Loss D: 1.255
Loss G: 0.5519 (0.4597) Acc G: 96.729%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3128 (0.3640) Acc D Real: 97.474%
Loss D Fake: 0.8640 (1.0405) Acc D Fake: 3.271%
Loss D: 1.177
Loss G: 0.5528 (0.4602) Acc G: 96.729%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3978 (0.3642) Acc D Real: 97.473%
Loss D Fake: 0.8626 (1.0395) Acc D Fake: 3.271%
Loss D: 1.260
Loss G: 0.5538 (0.4607) Acc G: 96.729%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.3920 (0.3643) Acc D Real: 97.474%
Loss D Fake: 0.8615 (1.0386) Acc D Fake: 3.272%
Loss D: 1.254
Loss G: 0.5544 (0.4612) Acc G: 96.728%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.4244 (0.3646) Acc D Real: 97.474%
Loss D Fake: 0.8608 (1.0377) Acc D Fake: 3.272%
Loss D: 1.285
Loss G: 0.5547 (0.4617) Acc G: 96.728%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.3547 (0.3646) Acc D Real: 97.475%
Loss D Fake: 0.8605 (1.0367) Acc D Fake: 3.272%
Loss D: 1.215
Loss G: 0.5551 (0.4622) Acc G: 96.728%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.4225 (0.3649) Acc D Real: 97.475%
Loss D Fake: 0.8600 (1.0358) Acc D Fake: 3.273%
Loss D: 1.283
Loss G: 0.5552 (0.4627) Acc G: 96.727%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3648 (0.3649) Acc D Real: 97.475%
Loss D Fake: 0.8599 (1.0349) Acc D Fake: 3.273%
Loss D: 1.225
Loss G: 0.5554 (0.4632) Acc G: 96.727%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.3956 (0.3650) Acc D Real: 97.475%
Loss D Fake: 0.8596 (1.0340) Acc D Fake: 3.273%
Loss D: 1.255
Loss G: 0.5556 (0.4637) Acc G: 96.727%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3541 (0.3650) Acc D Real: 97.474%
Loss D Fake: 0.8593 (1.0331) Acc D Fake: 3.274%
Loss D: 1.213
Loss G: 0.5561 (0.4641) Acc G: 96.726%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.4764 (0.3655) Acc D Real: 97.474%
Loss D Fake: 0.8586 (1.0322) Acc D Fake: 3.274%
Loss D: 1.335
Loss G: 0.5571 (0.4646) Acc G: 96.726%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.4223 (0.3658) Acc D Real: 97.475%
Loss D Fake: 0.8601 (1.0313) Acc D Fake: 3.274%
Loss D: 1.282
Loss G: 0.5564 (0.4651) Acc G: 96.726%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.3402 (0.3657) Acc D Real: 97.476%
Loss D Fake: 0.8668 (1.0305) Acc D Fake: 3.274%
Loss D: 1.207
Loss G: 0.5529 (0.4655) Acc G: 96.726%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3311 (0.3655) Acc D Real: 97.476%
Loss D Fake: 0.8867 (1.0298) Acc D Fake: 3.275%
Loss D: 1.218
Loss G: 0.5339 (0.4659) Acc G: 96.725%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.3995 (0.3657) Acc D Real: 97.475%
Loss D Fake: 2.8203 (1.0387) Acc D Fake: 3.275%
Loss D: 3.220
Loss G: 0.1122 (0.4641) Acc G: 96.725%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3167 (0.3655) Acc D Real: 97.475%
Loss D Fake: 2.7270 (1.0471) Acc D Fake: 3.275%
Loss D: 3.044
Loss G: 0.1124 (0.4623) Acc G: 96.725%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.3406 (0.3653) Acc D Real: 97.475%
Loss D Fake: 2.6956 (1.0553) Acc D Fake: 3.276%
Loss D: 3.036
Loss G: 0.1147 (0.4606) Acc G: 96.724%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.3790 (0.3654) Acc D Real: 97.475%
Loss D Fake: 2.6303 (1.0631) Acc D Fake: 3.276%
Loss D: 3.009
Loss G: 0.1211 (0.4590) Acc G: 96.724%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.3467 (0.3653) Acc D Real: 97.475%
Loss D Fake: 2.5086 (1.0701) Acc D Fake: 3.276%
Loss D: 2.855
Loss G: 0.1277 (0.4573) Acc G: 96.724%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2943 (0.3650) Acc D Real: 97.475%
Loss D Fake: 2.4439 (1.0768) Acc D Fake: 3.276%
Loss D: 2.738
Loss G: 0.1319 (0.4557) Acc G: 96.724%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3552 (0.3649) Acc D Real: 97.475%
Loss D Fake: 2.3925 (1.0832) Acc D Fake: 3.277%
Loss D: 2.748
Loss G: 0.1363 (0.4542) Acc G: 96.723%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.3710 (0.3649) Acc D Real: 97.474%
Loss D Fake: 2.3399 (1.0893) Acc D Fake: 3.277%
Loss D: 2.711
Loss G: 0.1412 (0.4527) Acc G: 96.723%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.3618 (0.3649) Acc D Real: 97.474%
Loss D Fake: 2.2860 (1.0951) Acc D Fake: 3.277%
Loss D: 2.648
Loss G: 0.1465 (0.4512) Acc G: 96.723%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.3435 (0.3648) Acc D Real: 97.475%
Loss D Fake: 2.2313 (1.1005) Acc D Fake: 3.278%
Loss D: 2.575
Loss G: 0.1522 (0.4498) Acc G: 96.722%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.2961 (0.3645) Acc D Real: 97.475%
Loss D Fake: 2.1763 (1.1056) Acc D Fake: 3.278%
Loss D: 2.472
Loss G: 0.1584 (0.4484) Acc G: 96.722%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3472 (0.3644) Acc D Real: 97.476%
Loss D Fake: 2.1214 (1.1104) Acc D Fake: 3.278%
Loss D: 2.469
Loss G: 0.1648 (0.4470) Acc G: 96.722%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3451 (0.3643) Acc D Real: 97.475%
Loss D Fake: 2.0672 (1.1149) Acc D Fake: 3.278%
Loss D: 2.412
Loss G: 0.1717 (0.4457) Acc G: 96.722%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3044 (0.3640) Acc D Real: 97.474%
Loss D Fake: 2.0140 (1.1192) Acc D Fake: 3.279%
Loss D: 2.318
Loss G: 0.1788 (0.4445) Acc G: 96.721%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3401 (0.3639) Acc D Real: 97.474%
Loss D Fake: 1.9621 (1.1231) Acc D Fake: 3.279%
Loss D: 2.302
Loss G: 0.1862 (0.4433) Acc G: 96.721%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3009 (0.3636) Acc D Real: 97.473%
Loss D Fake: 1.9114 (1.1268) Acc D Fake: 3.279%
Loss D: 2.212
Loss G: 0.1939 (0.4421) Acc G: 96.721%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3223 (0.3635) Acc D Real: 97.472%
Loss D Fake: 1.8620 (1.1302) Acc D Fake: 3.279%
Loss D: 2.184
Loss G: 0.2019 (0.4410) Acc G: 96.721%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.3236 (0.3633) Acc D Real: 97.471%
Loss D Fake: 1.8140 (1.1333) Acc D Fake: 3.280%
Loss D: 2.138
Loss G: 0.2101 (0.4399) Acc G: 96.720%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3220 (0.3631) Acc D Real: 97.470%
Loss D Fake: 1.7673 (1.1362) Acc D Fake: 3.287%
Loss D: 2.089
Loss G: 0.2185 (0.4389) Acc G: 96.713%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.3300 (0.3629) Acc D Real: 97.466%
Loss D Fake: 1.7219 (1.1389) Acc D Fake: 3.295%
Loss D: 2.052
Loss G: 0.2273 (0.4380) Acc G: 96.705%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3138 (0.3627) Acc D Real: 97.463%
Loss D Fake: 1.6779 (1.1414) Acc D Fake: 3.303%
Loss D: 1.992
Loss G: 0.2362 (0.4370) Acc G: 96.697%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.3062 (0.3624) Acc D Real: 97.462%
Loss D Fake: 1.6353 (1.1436) Acc D Fake: 3.311%
Loss D: 1.942
Loss G: 0.2454 (0.4362) Acc G: 96.689%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3253 (0.3623) Acc D Real: 97.458%
Loss D Fake: 1.5939 (1.1456) Acc D Fake: 3.318%
Loss D: 1.919
Loss G: 0.2548 (0.4354) Acc G: 96.682%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2890 (0.3620) Acc D Real: 97.456%
Loss D Fake: 1.5538 (1.1475) Acc D Fake: 3.326%
Loss D: 1.843
Loss G: 0.2644 (0.4346) Acc G: 96.674%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2993 (0.3617) Acc D Real: 97.450%
Loss D Fake: 1.5151 (1.1491) Acc D Fake: 3.333%
Loss D: 1.814
Loss G: 0.2741 (0.4339) Acc G: 96.667%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3131 (0.3615) Acc D Real: 97.446%
Loss D Fake: 1.4777 (1.1506) Acc D Fake: 3.341%
Loss D: 1.791
Loss G: 0.2840 (0.4332) Acc G: 96.659%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.3203 (0.3613) Acc D Real: 97.446%
Loss D Fake: 1.4412 (1.1518) Acc D Fake: 3.343%
Loss D: 1.762
Loss G: 0.2942 (0.4326) Acc G: 96.657%
LR: 2.000e-04
Epoch: 17/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2875 (0.2962) Acc D Real: 96.719%
Loss D Fake: 1.3712 (1.3885) Acc D Fake: 5.000%
Loss D: 1.659
Loss G: 0.3153 (0.3100) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3045 (0.2990) Acc D Real: 96.684%
Loss D Fake: 1.3377 (1.3716) Acc D Fake: 5.000%
Loss D: 1.642
Loss G: 0.3262 (0.3154) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.3058 (0.3007) Acc D Real: 96.719%
Loss D Fake: 1.3052 (1.3550) Acc D Fake: 5.000%
Loss D: 1.611
Loss G: 0.3372 (0.3208) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3036 (0.3013) Acc D Real: 96.677%
Loss D Fake: 1.2737 (1.3387) Acc D Fake: 5.000%
Loss D: 1.577
Loss G: 0.3483 (0.3263) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2965 (0.3005) Acc D Real: 96.675%
Loss D Fake: 1.2434 (1.3228) Acc D Fake: 5.000%
Loss D: 1.540
Loss G: 0.3595 (0.3319) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.3036 (0.3009) Acc D Real: 96.637%
Loss D Fake: 1.2143 (1.3073) Acc D Fake: 5.000%
Loss D: 1.518
Loss G: 0.3708 (0.3374) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.3087 (0.3019) Acc D Real: 96.582%
Loss D Fake: 1.1862 (1.2922) Acc D Fake: 5.000%
Loss D: 1.495
Loss G: 0.3822 (0.3430) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.3030 (0.3020) Acc D Real: 96.522%
Loss D Fake: 1.1592 (1.2774) Acc D Fake: 5.000%
Loss D: 1.462
Loss G: 0.3934 (0.3486) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2908 (0.3009) Acc D Real: 96.547%
Loss D Fake: 1.1337 (1.2630) Acc D Fake: 5.000%
Loss D: 1.425
Loss G: 0.4046 (0.3542) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.2967 (0.3005) Acc D Real: 96.548%
Loss D Fake: 1.1090 (1.2490) Acc D Fake: 5.000%
Loss D: 1.406
Loss G: 0.4157 (0.3598) Acc G: 95.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.3029 (0.3007) Acc D Real: 96.519%
Loss D Fake: 1.0856 (1.2354) Acc D Fake: 5.139%
Loss D: 1.388
Loss G: 0.4269 (0.3654) Acc G: 94.861%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.3162 (0.3019) Acc D Real: 96.474%
Loss D Fake: 1.0630 (1.2222) Acc D Fake: 5.256%
Loss D: 1.379
Loss G: 0.4379 (0.3710) Acc G: 94.744%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.3163 (0.3029) Acc D Real: 96.432%
Loss D Fake: 1.0412 (1.2092) Acc D Fake: 5.357%
Loss D: 1.357
Loss G: 0.4491 (0.3766) Acc G: 94.643%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3183 (0.3040) Acc D Real: 96.406%
Loss D Fake: 1.0204 (1.1966) Acc D Fake: 5.444%
Loss D: 1.339
Loss G: 0.4597 (0.3821) Acc G: 94.556%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3022 (0.3039) Acc D Real: 96.403%
Loss D Fake: 1.0012 (1.1844) Acc D Fake: 5.521%
Loss D: 1.303
Loss G: 0.4704 (0.3876) Acc G: 94.479%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3157 (0.3046) Acc D Real: 96.366%
Loss D Fake: 0.9823 (1.1725) Acc D Fake: 5.588%
Loss D: 1.298
Loss G: 0.4810 (0.3931) Acc G: 94.412%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.3331 (0.3061) Acc D Real: 96.334%
Loss D Fake: 0.9642 (1.1610) Acc D Fake: 5.648%
Loss D: 1.297
Loss G: 0.4914 (0.3986) Acc G: 94.352%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3102 (0.3064) Acc D Real: 96.305%
Loss D Fake: 0.9474 (1.1497) Acc D Fake: 5.702%
Loss D: 1.258
Loss G: 0.5014 (0.4040) Acc G: 94.298%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.3317 (0.3076) Acc D Real: 96.284%
Loss D Fake: 0.9318 (1.1388) Acc D Fake: 5.750%
Loss D: 1.264
Loss G: 0.5107 (0.4093) Acc G: 94.250%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.3048 (0.3075) Acc D Real: 96.272%
Loss D Fake: 0.9175 (1.1283) Acc D Fake: 5.794%
Loss D: 1.222
Loss G: 0.5201 (0.4146) Acc G: 94.206%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.3131 (0.3077) Acc D Real: 96.257%
Loss D Fake: 0.9032 (1.1180) Acc D Fake: 5.833%
Loss D: 1.216
Loss G: 0.5295 (0.4198) Acc G: 94.167%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.3320 (0.3088) Acc D Real: 96.214%
Loss D Fake: 0.8908 (1.1082) Acc D Fake: 5.870%
Loss D: 1.223
Loss G: 0.5359 (0.4249) Acc G: 94.130%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.3221 (0.3094) Acc D Real: 96.204%
Loss D Fake: 0.8867 (1.0989) Acc D Fake: 5.903%
Loss D: 1.209
Loss G: 0.5385 (0.4296) Acc G: 94.097%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2960 (0.3088) Acc D Real: 96.183%
Loss D Fake: 0.9070 (1.0913) Acc D Fake: 5.933%
Loss D: 1.203
Loss G: 0.5532 (0.4345) Acc G: 94.067%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.3074 (0.3088) Acc D Real: 96.150%
Loss D Fake: 0.8503 (1.0820) Acc D Fake: 5.962%
Loss D: 1.158
Loss G: 0.5693 (0.4397) Acc G: 94.038%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.3221 (0.3093) Acc D Real: 96.132%
Loss D Fake: 0.8306 (1.0727) Acc D Fake: 5.988%
Loss D: 1.153
Loss G: 0.5819 (0.4450) Acc G: 94.012%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3229 (0.3097) Acc D Real: 96.097%
Loss D Fake: 0.8146 (1.0635) Acc D Fake: 6.012%
Loss D: 1.137
Loss G: 0.5934 (0.4503) Acc G: 93.988%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3191 (0.3101) Acc D Real: 96.076%
Loss D Fake: 0.8001 (1.0544) Acc D Fake: 6.034%
Loss D: 1.119
Loss G: 0.6045 (0.4556) Acc G: 93.908%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.3103 (0.3101) Acc D Real: 96.062%
Loss D Fake: 0.7864 (1.0454) Acc D Fake: 6.111%
Loss D: 1.097
Loss G: 0.6155 (0.4609) Acc G: 93.833%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.3461 (0.3112) Acc D Real: 96.040%
Loss D Fake: 0.7733 (1.0367) Acc D Fake: 6.183%
Loss D: 1.119
Loss G: 0.6260 (0.4663) Acc G: 93.763%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.3078 (0.3111) Acc D Real: 96.021%
Loss D Fake: 0.7609 (1.0281) Acc D Fake: 6.250%
Loss D: 1.069
Loss G: 0.6367 (0.4716) Acc G: 93.698%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.3414 (0.3120) Acc D Real: 95.974%
Loss D Fake: 0.7486 (1.0196) Acc D Fake: 6.313%
Loss D: 1.090
Loss G: 0.6476 (0.4769) Acc G: 93.636%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.3092 (0.3120) Acc D Real: 95.965%
Loss D Fake: 0.7361 (1.0112) Acc D Fake: 6.373%
Loss D: 1.045
Loss G: 0.6597 (0.4823) Acc G: 93.578%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.3046 (0.3118) Acc D Real: 95.952%
Loss D Fake: 0.7219 (1.0030) Acc D Fake: 6.429%
Loss D: 1.027
Loss G: 0.6752 (0.4878) Acc G: 93.286%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3394 (0.3125) Acc D Real: 95.874%
Loss D Fake: 0.7032 (0.9947) Acc D Fake: 6.944%
Loss D: 1.043
Loss G: 0.6988 (0.4937) Acc G: 91.806%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.3529 (0.3136) Acc D Real: 95.776%
Loss D Fake: 0.6741 (0.9860) Acc D Fake: 9.189%
Loss D: 1.027
Loss G: 0.7400 (0.5003) Acc G: 89.595%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.3723 (0.3152) Acc D Real: 95.524%
Loss D Fake: 0.6320 (0.9767) Acc D Fake: 11.360%
Loss D: 1.004
Loss G: 0.7811 (0.5077) Acc G: 87.456%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3263 (0.3154) Acc D Real: 95.294%
Loss D Fake: 0.6031 (0.9671) Acc D Fake: 13.419%
Loss D: 0.929
Loss G: 0.8074 (0.5154) Acc G: 85.385%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.4121 (0.3179) Acc D Real: 94.758%
Loss D Fake: 0.5871 (0.9576) Acc D Fake: 15.417%
Loss D: 0.999
Loss G: 0.8220 (0.5231) Acc G: 83.417%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.3647 (0.3190) Acc D Real: 94.452%
Loss D Fake: 0.5782 (0.9483) Acc D Fake: 17.317%
Loss D: 0.943
Loss G: 0.8316 (0.5306) Acc G: 81.545%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3636 (0.3201) Acc D Real: 94.153%
Loss D Fake: 0.5720 (0.9394) Acc D Fake: 19.127%
Loss D: 0.936
Loss G: 0.8389 (0.5379) Acc G: 79.762%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.3438 (0.3206) Acc D Real: 93.973%
Loss D Fake: 0.5670 (0.9307) Acc D Fake: 20.891%
Loss D: 0.911
Loss G: 0.8452 (0.5451) Acc G: 78.023%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.3986 (0.3224) Acc D Real: 93.567%
Loss D Fake: 0.5629 (0.9224) Acc D Fake: 22.576%
Loss D: 0.961
Loss G: 0.8502 (0.5520) Acc G: 76.364%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.3344 (0.3227) Acc D Real: 93.411%
Loss D Fake: 0.5596 (0.9143) Acc D Fake: 24.185%
Loss D: 0.894
Loss G: 0.8548 (0.5587) Acc G: 74.778%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.2876 (0.3219) Acc D Real: 93.399%
Loss D Fake: 0.5563 (0.9065) Acc D Fake: 25.725%
Loss D: 0.844
Loss G: 0.8597 (0.5653) Acc G: 73.261%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.3639 (0.3228) Acc D Real: 93.201%
Loss D Fake: 0.5529 (0.8990) Acc D Fake: 27.199%
Loss D: 0.917
Loss G: 0.8640 (0.5716) Acc G: 71.809%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3522 (0.3234) Acc D Real: 92.999%
Loss D Fake: 0.5502 (0.8917) Acc D Fake: 28.611%
Loss D: 0.902
Loss G: 0.8676 (0.5778) Acc G: 70.417%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3353 (0.3236) Acc D Real: 92.872%
Loss D Fake: 0.5480 (0.8847) Acc D Fake: 29.966%
Loss D: 0.883
Loss G: 0.8707 (0.5838) Acc G: 69.082%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3188 (0.3235) Acc D Real: 92.755%
Loss D Fake: 0.5461 (0.8779) Acc D Fake: 31.267%
Loss D: 0.865
Loss G: 0.8738 (0.5896) Acc G: 67.800%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.2748 (0.3226) Acc D Real: 92.745%
Loss D Fake: 0.5438 (0.8714) Acc D Fake: 32.516%
Loss D: 0.819
Loss G: 0.8778 (0.5952) Acc G: 66.569%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.3619 (0.3233) Acc D Real: 92.549%
Loss D Fake: 0.5413 (0.8650) Acc D Fake: 33.718%
Loss D: 0.903
Loss G: 0.8811 (0.6007) Acc G: 65.385%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.3547 (0.3239) Acc D Real: 92.371%
Loss D Fake: 0.5395 (0.8589) Acc D Fake: 34.874%
Loss D: 0.894
Loss G: 0.8836 (0.6061) Acc G: 64.245%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2969 (0.3234) Acc D Real: 92.353%
Loss D Fake: 0.5382 (0.8530) Acc D Fake: 35.957%
Loss D: 0.835
Loss G: 0.8861 (0.6113) Acc G: 63.179%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.3198 (0.3234) Acc D Real: 92.286%
Loss D Fake: 0.5369 (0.8472) Acc D Fake: 37.000%
Loss D: 0.857
Loss G: 0.8881 (0.6163) Acc G: 62.152%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.2644 (0.3223) Acc D Real: 92.288%
Loss D Fake: 0.5358 (0.8417) Acc D Fake: 38.006%
Loss D: 0.800
Loss G: 0.8907 (0.6212) Acc G: 61.161%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3457 (0.3227) Acc D Real: 92.135%
Loss D Fake: 0.5347 (0.8363) Acc D Fake: 38.977%
Loss D: 0.880
Loss G: 0.8923 (0.6260) Acc G: 60.205%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.3597 (0.3234) Acc D Real: 91.936%
Loss D Fake: 0.5347 (0.8311) Acc D Fake: 39.914%
Loss D: 0.894
Loss G: 0.8923 (0.6305) Acc G: 59.282%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.2907 (0.3228) Acc D Real: 91.891%
Loss D Fake: 0.5358 (0.8261) Acc D Fake: 40.791%
Loss D: 0.827
Loss G: 0.8917 (0.6350) Acc G: 58.418%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3611 (0.3235) Acc D Real: 91.697%
Loss D Fake: 0.5373 (0.8213) Acc D Fake: 41.639%
Loss D: 0.898
Loss G: 0.8903 (0.6392) Acc G: 57.583%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.3304 (0.3236) Acc D Real: 91.543%
Loss D Fake: 0.5411 (0.8167) Acc D Fake: 42.432%
Loss D: 0.872
Loss G: 0.8820 (0.6432) Acc G: 56.803%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.2640 (0.3226) Acc D Real: 91.543%
Loss D Fake: 0.5543 (0.8124) Acc D Fake: 43.172%
Loss D: 0.818
Loss G: 0.8692 (0.6468) Acc G: 56.102%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.2985 (0.3222) Acc D Real: 91.489%
Loss D Fake: 0.5838 (0.8088) Acc D Fake: 43.783%
Loss D: 0.882
Loss G: 0.8629 (0.6503) Acc G: 55.450%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.3632 (0.3229) Acc D Real: 91.275%
Loss D Fake: 0.5847 (0.8053) Acc D Fake: 44.349%
Loss D: 0.948
Loss G: 0.8816 (0.6539) Acc G: 54.766%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.2837 (0.3223) Acc D Real: 91.237%
Loss D Fake: 0.5444 (0.8013) Acc D Fake: 45.026%
Loss D: 0.828
Loss G: 0.9002 (0.6577) Acc G: 54.077%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.2854 (0.3217) Acc D Real: 91.193%
Loss D Fake: 0.5290 (0.7972) Acc D Fake: 45.732%
Loss D: 0.814
Loss G: 0.9128 (0.6615) Acc G: 53.359%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.2694 (0.3209) Acc D Real: 91.196%
Loss D Fake: 0.5193 (0.7930) Acc D Fake: 46.443%
Loss D: 0.789
Loss G: 0.9236 (0.6655) Acc G: 52.637%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.2782 (0.3203) Acc D Real: 91.172%
Loss D Fake: 0.5110 (0.7889) Acc D Fake: 47.157%
Loss D: 0.789
Loss G: 0.9350 (0.6694) Acc G: 51.936%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.2949 (0.3199) Acc D Real: 91.093%
Loss D Fake: 0.5033 (0.7847) Acc D Fake: 47.874%
Loss D: 0.798
Loss G: 0.9449 (0.6734) Acc G: 51.208%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.2949 (0.3196) Acc D Real: 91.050%
Loss D Fake: 0.4973 (0.7806) Acc D Fake: 48.595%
Loss D: 0.792
Loss G: 0.9529 (0.6774) Acc G: 50.500%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.3121 (0.3195) Acc D Real: 90.950%
Loss D Fake: 0.4928 (0.7766) Acc D Fake: 49.296%
Loss D: 0.805
Loss G: 0.9587 (0.6814) Acc G: 49.789%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.2756 (0.3189) Acc D Real: 90.905%
Loss D Fake: 0.4897 (0.7726) Acc D Fake: 50.000%
Loss D: 0.765
Loss G: 0.9631 (0.6853) Acc G: 49.097%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.2770 (0.3183) Acc D Real: 90.850%
Loss D Fake: 0.4872 (0.7687) Acc D Fake: 50.685%
Loss D: 0.764
Loss G: 0.9671 (0.6891) Acc G: 48.425%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.2777 (0.3177) Acc D Real: 90.819%
Loss D Fake: 0.4849 (0.7648) Acc D Fake: 51.351%
Loss D: 0.763
Loss G: 0.9711 (0.6930) Acc G: 47.770%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3321 (0.3179) Acc D Real: 90.680%
Loss D Fake: 0.4830 (0.7611) Acc D Fake: 52.000%
Loss D: 0.815
Loss G: 0.9734 (0.6967) Acc G: 47.133%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3325 (0.3181) Acc D Real: 90.563%
Loss D Fake: 0.4828 (0.7574) Acc D Fake: 52.632%
Loss D: 0.815
Loss G: 0.9726 (0.7003) Acc G: 46.513%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2714 (0.3175) Acc D Real: 90.520%
Loss D Fake: 0.4836 (0.7539) Acc D Fake: 53.247%
Loss D: 0.755
Loss G: 0.9747 (0.7039) Acc G: 45.909%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.3096 (0.3174) Acc D Real: 90.395%
Loss D Fake: 0.4812 (0.7504) Acc D Fake: 53.846%
Loss D: 0.791
Loss G: 0.9799 (0.7074) Acc G: 45.321%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.2697 (0.3168) Acc D Real: 90.376%
Loss D Fake: 0.4772 (0.7469) Acc D Fake: 54.430%
Loss D: 0.747
Loss G: 0.9880 (0.7110) Acc G: 44.747%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3172 (0.3168) Acc D Real: 90.278%
Loss D Fake: 0.4728 (0.7435) Acc D Fake: 55.000%
Loss D: 0.790
Loss G: 0.9942 (0.7145) Acc G: 44.188%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3770 (0.3176) Acc D Real: 90.101%
Loss D Fake: 0.4701 (0.7401) Acc D Fake: 55.556%
Loss D: 0.847
Loss G: 0.9977 (0.7180) Acc G: 43.642%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3273 (0.3177) Acc D Real: 89.978%
Loss D Fake: 0.4685 (0.7368) Acc D Fake: 56.098%
Loss D: 0.796
Loss G: 1.0002 (0.7215) Acc G: 43.110%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.2195 (0.3165) Acc D Real: 90.009%
Loss D Fake: 0.4668 (0.7335) Acc D Fake: 56.627%
Loss D: 0.686
Loss G: 1.0041 (0.7249) Acc G: 42.590%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.2871 (0.3161) Acc D Real: 89.957%
Loss D Fake: 0.4642 (0.7303) Acc D Fake: 57.143%
Loss D: 0.751
Loss G: 1.0087 (0.7282) Acc G: 42.083%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3008 (0.3160) Acc D Real: 89.878%
Loss D Fake: 0.4614 (0.7272) Acc D Fake: 57.647%
Loss D: 0.762
Loss G: 1.0137 (0.7316) Acc G: 41.588%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.2556 (0.3153) Acc D Real: 89.890%
Loss D Fake: 0.4583 (0.7240) Acc D Fake: 58.140%
Loss D: 0.714
Loss G: 1.0194 (0.7349) Acc G: 41.105%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.2775 (0.3148) Acc D Real: 89.843%
Loss D Fake: 0.4555 (0.7210) Acc D Fake: 58.621%
Loss D: 0.733
Loss G: 1.0215 (0.7382) Acc G: 40.632%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.2804 (0.3144) Acc D Real: 89.777%
Loss D Fake: 0.4566 (0.7180) Acc D Fake: 59.091%
Loss D: 0.737
Loss G: 1.0154 (0.7414) Acc G: 40.170%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.2663 (0.3139) Acc D Real: 89.786%
Loss D Fake: 0.4620 (0.7151) Acc D Fake: 59.551%
Loss D: 0.728
Loss G: 1.0289 (0.7446) Acc G: 39.719%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4061 (0.3149) Acc D Real: 89.578%
Loss D Fake: 0.4483 (0.7121) Acc D Fake: 60.000%
Loss D: 0.854
Loss G: 1.0368 (0.7479) Acc G: 39.278%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3645 (0.3155) Acc D Real: 89.428%
Loss D Fake: 0.4466 (0.7092) Acc D Fake: 60.440%
Loss D: 0.811
Loss G: 1.0384 (0.7511) Acc G: 38.846%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.2882 (0.3152) Acc D Real: 89.383%
Loss D Fake: 0.4462 (0.7063) Acc D Fake: 60.870%
Loss D: 0.734
Loss G: 1.0395 (0.7542) Acc G: 38.424%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.2689 (0.3147) Acc D Real: 89.368%
Loss D Fake: 0.4456 (0.7035) Acc D Fake: 61.290%
Loss D: 0.715
Loss G: 1.0405 (0.7573) Acc G: 38.011%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3778 (0.3153) Acc D Real: 89.204%
Loss D Fake: 0.4458 (0.7008) Acc D Fake: 61.702%
Loss D: 0.824
Loss G: 1.0381 (0.7603) Acc G: 37.606%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.2913 (0.3151) Acc D Real: 89.184%
Loss D Fake: 0.4490 (0.6981) Acc D Fake: 62.105%
Loss D: 0.740
Loss G: 1.0316 (0.7631) Acc G: 37.211%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2850 (0.3148) Acc D Real: 89.143%
Loss D Fake: 0.4526 (0.6956) Acc D Fake: 62.500%
Loss D: 0.738
Loss G: 1.0382 (0.7660) Acc G: 36.823%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4003 (0.3157) Acc D Real: 88.933%
Loss D Fake: 0.4443 (0.6930) Acc D Fake: 62.887%
Loss D: 0.845
Loss G: 1.0458 (0.7689) Acc G: 36.443%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.3508 (0.3160) Acc D Real: 88.789%
Loss D Fake: 0.4417 (0.6904) Acc D Fake: 63.265%
Loss D: 0.792
Loss G: 1.0493 (0.7717) Acc G: 36.071%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2935 (0.3158) Acc D Real: 88.732%
Loss D Fake: 0.4400 (0.6879) Acc D Fake: 63.636%
Loss D: 0.733
Loss G: 1.0519 (0.7746) Acc G: 35.707%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.3383 (0.3160) Acc D Real: 88.616%
Loss D Fake: 0.4389 (0.6854) Acc D Fake: 64.000%
Loss D: 0.777
Loss G: 1.0527 (0.7773) Acc G: 35.350%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3226 (0.3161) Acc D Real: 88.526%
Loss D Fake: 0.4393 (0.6830) Acc D Fake: 64.356%
Loss D: 0.762
Loss G: 1.0479 (0.7800) Acc G: 35.000%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.2774 (0.3157) Acc D Real: 88.496%
Loss D Fake: 0.4498 (0.6807) Acc D Fake: 64.706%
Loss D: 0.727
Loss G: 1.0445 (0.7826) Acc G: 34.657%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.2643 (0.3152) Acc D Real: 88.474%
Loss D Fake: 0.4396 (0.6783) Acc D Fake: 65.049%
Loss D: 0.704
Loss G: 1.0559 (0.7853) Acc G: 34.320%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2992 (0.3150) Acc D Real: 88.414%
Loss D Fake: 0.4353 (0.6760) Acc D Fake: 65.385%
Loss D: 0.734
Loss G: 1.0616 (0.7879) Acc G: 33.990%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3443 (0.3153) Acc D Real: 88.311%
Loss D Fake: 0.4334 (0.6737) Acc D Fake: 65.714%
Loss D: 0.778
Loss G: 1.0628 (0.7905) Acc G: 33.667%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.2482 (0.3147) Acc D Real: 88.316%
Loss D Fake: 0.4333 (0.6714) Acc D Fake: 66.038%
Loss D: 0.682
Loss G: 1.0633 (0.7931) Acc G: 33.349%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2233 (0.3138) Acc D Real: 88.346%
Loss D Fake: 0.4329 (0.6692) Acc D Fake: 66.355%
Loss D: 0.656
Loss G: 1.0650 (0.7957) Acc G: 33.037%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.2922 (0.3136) Acc D Real: 88.295%
Loss D Fake: 0.4322 (0.6670) Acc D Fake: 66.667%
Loss D: 0.724
Loss G: 1.0651 (0.7981) Acc G: 32.731%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3024 (0.3135) Acc D Real: 88.259%
Loss D Fake: 0.4329 (0.6649) Acc D Fake: 66.972%
Loss D: 0.735
Loss G: 1.0649 (0.8006) Acc G: 32.431%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3093 (0.3135) Acc D Real: 88.185%
Loss D Fake: 0.4372 (0.6628) Acc D Fake: 67.273%
Loss D: 0.747
Loss G: 1.0570 (0.8029) Acc G: 32.136%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3164 (0.3135) Acc D Real: 88.109%
Loss D Fake: 0.4349 (0.6607) Acc D Fake: 67.568%
Loss D: 0.751
Loss G: 1.0743 (0.8054) Acc G: 31.847%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.2964 (0.3134) Acc D Real: 88.039%
Loss D Fake: 0.4245 (0.6586) Acc D Fake: 67.857%
Loss D: 0.721
Loss G: 1.0832 (0.8078) Acc G: 31.562%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.2812 (0.3131) Acc D Real: 88.018%
Loss D Fake: 0.4214 (0.6565) Acc D Fake: 68.142%
Loss D: 0.703
Loss G: 1.0877 (0.8103) Acc G: 31.283%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3054 (0.3130) Acc D Real: 87.949%
Loss D Fake: 0.4194 (0.6544) Acc D Fake: 68.421%
Loss D: 0.725
Loss G: 1.0912 (0.8128) Acc G: 31.009%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.2596 (0.3125) Acc D Real: 87.934%
Loss D Fake: 0.4177 (0.6524) Acc D Fake: 68.696%
Loss D: 0.677
Loss G: 1.0943 (0.8152) Acc G: 30.739%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.3188 (0.3126) Acc D Real: 87.865%
Loss D Fake: 0.4162 (0.6503) Acc D Fake: 68.966%
Loss D: 0.735
Loss G: 1.0969 (0.8177) Acc G: 30.474%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.3881 (0.3132) Acc D Real: 87.745%
Loss D Fake: 0.4151 (0.6483) Acc D Fake: 69.231%
Loss D: 0.803
Loss G: 1.0984 (0.8201) Acc G: 30.214%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3401 (0.3135) Acc D Real: 87.653%
Loss D Fake: 0.4145 (0.6464) Acc D Fake: 69.492%
Loss D: 0.755
Loss G: 1.0994 (0.8224) Acc G: 29.958%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.3541 (0.3138) Acc D Real: 87.561%
Loss D Fake: 0.4141 (0.6444) Acc D Fake: 69.748%
Loss D: 0.768
Loss G: 1.0998 (0.8248) Acc G: 29.706%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2808 (0.3135) Acc D Real: 87.528%
Loss D Fake: 0.4140 (0.6425) Acc D Fake: 70.000%
Loss D: 0.695
Loss G: 1.0999 (0.8271) Acc G: 29.458%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2783 (0.3133) Acc D Real: 87.497%
Loss D Fake: 0.4139 (0.6406) Acc D Fake: 70.248%
Loss D: 0.692
Loss G: 1.1002 (0.8293) Acc G: 29.215%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.2201 (0.3125) Acc D Real: 87.529%
Loss D Fake: 0.4137 (0.6387) Acc D Fake: 70.492%
Loss D: 0.634
Loss G: 1.1006 (0.8315) Acc G: 28.975%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.3090 (0.3125) Acc D Real: 87.487%
Loss D Fake: 0.4138 (0.6369) Acc D Fake: 70.732%
Loss D: 0.723
Loss G: 1.0989 (0.8337) Acc G: 28.740%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.2356 (0.3118) Acc D Real: 87.498%
Loss D Fake: 0.4160 (0.6351) Acc D Fake: 70.968%
Loss D: 0.652
Loss G: 1.0920 (0.8358) Acc G: 28.508%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.2796 (0.3116) Acc D Real: 87.463%
Loss D Fake: 0.4206 (0.6334) Acc D Fake: 71.200%
Loss D: 0.700
Loss G: 1.1025 (0.8379) Acc G: 28.280%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.3426 (0.3118) Acc D Real: 87.362%
Loss D Fake: 0.4103 (0.6316) Acc D Fake: 71.429%
Loss D: 0.753
Loss G: 1.1104 (0.8401) Acc G: 28.056%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3613 (0.3122) Acc D Real: 87.260%
Loss D Fake: 0.4085 (0.6299) Acc D Fake: 71.654%
Loss D: 0.770
Loss G: 1.1113 (0.8422) Acc G: 27.835%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.2934 (0.3121) Acc D Real: 87.222%
Loss D Fake: 0.4088 (0.6282) Acc D Fake: 71.875%
Loss D: 0.702
Loss G: 1.1111 (0.8443) Acc G: 27.617%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.2625 (0.3117) Acc D Real: 87.196%
Loss D Fake: 0.4089 (0.6265) Acc D Fake: 72.093%
Loss D: 0.671
Loss G: 1.1119 (0.8464) Acc G: 27.403%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.2011 (0.3108) Acc D Real: 87.231%
Loss D Fake: 0.4083 (0.6248) Acc D Fake: 72.308%
Loss D: 0.609
Loss G: 1.1141 (0.8485) Acc G: 27.192%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3223 (0.3109) Acc D Real: 87.151%
Loss D Fake: 0.4072 (0.6231) Acc D Fake: 72.519%
Loss D: 0.729
Loss G: 1.1163 (0.8505) Acc G: 26.985%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.1734 (0.3099) Acc D Real: 87.213%
Loss D Fake: 0.4060 (0.6215) Acc D Fake: 72.727%
Loss D: 0.579
Loss G: 1.1198 (0.8525) Acc G: 26.780%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3563 (0.3102) Acc D Real: 87.119%
Loss D Fake: 0.4049 (0.6198) Acc D Fake: 72.932%
Loss D: 0.761
Loss G: 1.1189 (0.8545) Acc G: 26.579%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3176 (0.3103) Acc D Real: 87.063%
Loss D Fake: 0.4127 (0.6183) Acc D Fake: 73.134%
Loss D: 0.730
Loss G: 0.9736 (0.8554) Acc G: 26.381%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.2118 (0.3096) Acc D Real: 87.084%
Loss D Fake: 0.4955 (0.6174) Acc D Fake: 73.333%
Loss D: 0.707
Loss G: 0.9399 (0.8561) Acc G: 26.185%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3424 (0.3098) Acc D Real: 86.992%
Loss D Fake: 0.5008 (0.6165) Acc D Fake: 73.529%
Loss D: 0.843
Loss G: 0.9356 (0.8566) Acc G: 25.993%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.3216 (0.3099) Acc D Real: 86.919%
Loss D Fake: 0.5012 (0.6157) Acc D Fake: 73.723%
Loss D: 0.823
Loss G: 0.9365 (0.8572) Acc G: 25.803%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.1939 (0.3090) Acc D Real: 86.958%
Loss D Fake: 0.4994 (0.6148) Acc D Fake: 73.913%
Loss D: 0.693
Loss G: 0.9419 (0.8578) Acc G: 25.616%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.2820 (0.3088) Acc D Real: 86.928%
Loss D Fake: 0.4965 (0.6140) Acc D Fake: 74.101%
Loss D: 0.778
Loss G: 0.9429 (0.8585) Acc G: 25.432%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2754 (0.3086) Acc D Real: 86.899%
Loss D Fake: 0.4959 (0.6132) Acc D Fake: 74.286%
Loss D: 0.771
Loss G: 0.9452 (0.8591) Acc G: 25.250%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3620 (0.3090) Acc D Real: 86.790%
Loss D Fake: 0.4940 (0.6123) Acc D Fake: 74.468%
Loss D: 0.856
Loss G: 0.9479 (0.8597) Acc G: 25.071%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.2212 (0.3084) Acc D Real: 86.808%
Loss D Fake: 0.4919 (0.6115) Acc D Fake: 74.648%
Loss D: 0.713
Loss G: 0.9525 (0.8604) Acc G: 24.894%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2811 (0.3082) Acc D Real: 86.781%
Loss D Fake: 0.4889 (0.6106) Acc D Fake: 74.825%
Loss D: 0.770
Loss G: 0.9569 (0.8610) Acc G: 24.720%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.2295 (0.3076) Acc D Real: 86.795%
Loss D Fake: 0.4864 (0.6097) Acc D Fake: 75.000%
Loss D: 0.716
Loss G: 0.9622 (0.8617) Acc G: 24.549%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2510 (0.3072) Acc D Real: 86.794%
Loss D Fake: 0.4830 (0.6089) Acc D Fake: 75.172%
Loss D: 0.734
Loss G: 0.9696 (0.8625) Acc G: 24.379%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.2719 (0.3070) Acc D Real: 86.769%
Loss D Fake: 0.4782 (0.6080) Acc D Fake: 75.342%
Loss D: 0.750
Loss G: 0.9821 (0.8633) Acc G: 24.212%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3481 (0.3073) Acc D Real: 86.685%
Loss D Fake: 0.4696 (0.6070) Acc D Fake: 75.510%
Loss D: 0.818
Loss G: 1.0111 (0.8643) Acc G: 24.048%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3269 (0.3074) Acc D Real: 86.621%
Loss D Fake: 0.4412 (0.6059) Acc D Fake: 75.676%
Loss D: 0.768
Loss G: 1.1742 (0.8664) Acc G: 23.885%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3280 (0.3075) Acc D Real: 86.572%
Loss D Fake: 0.3798 (0.6044) Acc D Fake: 75.839%
Loss D: 0.708
Loss G: 1.1832 (0.8685) Acc G: 23.725%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.2569 (0.3072) Acc D Real: 86.567%
Loss D Fake: 0.3774 (0.6029) Acc D Fake: 76.000%
Loss D: 0.634
Loss G: 1.1874 (0.8706) Acc G: 23.567%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3456 (0.3075) Acc D Real: 86.522%
Loss D Fake: 0.3757 (0.6014) Acc D Fake: 76.159%
Loss D: 0.721
Loss G: 1.1902 (0.8728) Acc G: 23.411%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2548 (0.3071) Acc D Real: 86.521%
Loss D Fake: 0.3744 (0.5999) Acc D Fake: 76.316%
Loss D: 0.629
Loss G: 1.1928 (0.8749) Acc G: 23.257%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2325 (0.3066) Acc D Real: 86.536%
Loss D Fake: 0.3732 (0.5984) Acc D Fake: 76.471%
Loss D: 0.606
Loss G: 1.1947 (0.8770) Acc G: 23.105%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3328 (0.3068) Acc D Real: 86.492%
Loss D Fake: 0.3729 (0.5969) Acc D Fake: 76.623%
Loss D: 0.706
Loss G: 1.1929 (0.8790) Acc G: 22.955%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3261 (0.3069) Acc D Real: 86.451%
Loss D Fake: 0.3749 (0.5955) Acc D Fake: 76.774%
Loss D: 0.701
Loss G: 1.1866 (0.8810) Acc G: 22.806%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3245 (0.3070) Acc D Real: 86.412%
Loss D Fake: 0.3778 (0.5941) Acc D Fake: 76.923%
Loss D: 0.702
Loss G: 1.1790 (0.8829) Acc G: 22.660%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3012 (0.3070) Acc D Real: 86.389%
Loss D Fake: 0.3794 (0.5927) Acc D Fake: 77.070%
Loss D: 0.681
Loss G: 1.1790 (0.8848) Acc G: 22.516%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.4027 (0.3076) Acc D Real: 86.318%
Loss D Fake: 0.3784 (0.5914) Acc D Fake: 77.215%
Loss D: 0.781
Loss G: 1.1797 (0.8867) Acc G: 22.373%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.2968 (0.3075) Acc D Real: 86.296%
Loss D Fake: 0.3775 (0.5900) Acc D Fake: 77.358%
Loss D: 0.674
Loss G: 1.1852 (0.8885) Acc G: 22.233%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3550 (0.3078) Acc D Real: 86.236%
Loss D Fake: 0.3748 (0.5887) Acc D Fake: 77.500%
Loss D: 0.730
Loss G: 1.1904 (0.8904) Acc G: 22.094%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2011 (0.3072) Acc D Real: 86.274%
Loss D Fake: 0.3715 (0.5873) Acc D Fake: 77.640%
Loss D: 0.573
Loss G: 1.2017 (0.8924) Acc G: 21.957%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.3738 (0.3076) Acc D Real: 86.216%
Loss D Fake: 0.3675 (0.5860) Acc D Fake: 77.778%
Loss D: 0.741
Loss G: 1.2058 (0.8943) Acc G: 21.821%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.2585 (0.3073) Acc D Real: 86.215%
Loss D Fake: 0.3669 (0.5846) Acc D Fake: 77.914%
Loss D: 0.625
Loss G: 1.2070 (0.8962) Acc G: 21.687%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2867 (0.3072) Acc D Real: 86.200%
Loss D Fake: 0.3666 (0.5833) Acc D Fake: 78.049%
Loss D: 0.653
Loss G: 1.2077 (0.8981) Acc G: 21.555%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1597 (0.3063) Acc D Real: 86.253%
Loss D Fake: 0.3662 (0.5820) Acc D Fake: 78.182%
Loss D: 0.526
Loss G: 1.2097 (0.9000) Acc G: 21.424%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.2376 (0.3059) Acc D Real: 86.258%
Loss D Fake: 0.3652 (0.5807) Acc D Fake: 78.313%
Loss D: 0.603
Loss G: 1.2125 (0.9019) Acc G: 21.295%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2521 (0.3055) Acc D Real: 86.260%
Loss D Fake: 0.3640 (0.5794) Acc D Fake: 78.443%
Loss D: 0.616
Loss G: 1.2155 (0.9038) Acc G: 21.168%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.2729 (0.3053) Acc D Real: 86.259%
Loss D Fake: 0.3629 (0.5781) Acc D Fake: 78.571%
Loss D: 0.636
Loss G: 1.2180 (0.9056) Acc G: 21.042%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.2436 (0.3050) Acc D Real: 86.272%
Loss D Fake: 0.3620 (0.5768) Acc D Fake: 78.698%
Loss D: 0.606
Loss G: 1.2202 (0.9075) Acc G: 20.917%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.2519 (0.3047) Acc D Real: 86.272%
Loss D Fake: 0.3613 (0.5756) Acc D Fake: 78.824%
Loss D: 0.613
Loss G: 1.2214 (0.9093) Acc G: 20.794%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3117 (0.3047) Acc D Real: 86.238%
Loss D Fake: 0.3614 (0.5743) Acc D Fake: 78.947%
Loss D: 0.673
Loss G: 1.2200 (0.9112) Acc G: 20.673%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3215 (0.3048) Acc D Real: 86.204%
Loss D Fake: 0.3653 (0.5731) Acc D Fake: 79.070%
Loss D: 0.687
Loss G: 1.1989 (0.9128) Acc G: 20.552%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3756 (0.3052) Acc D Real: 86.120%
Loss D Fake: 0.3971 (0.5721) Acc D Fake: 79.171%
Loss D: 0.773
Loss G: 1.0914 (0.9139) Acc G: 20.472%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.1881 (0.3045) Acc D Real: 86.150%
Loss D Fake: 1.3910 (0.5768) Acc D Fake: 78.793%
Loss D: 1.579
Loss G: 0.3440 (0.9106) Acc G: 20.852%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.2952 (0.3045) Acc D Real: 86.118%
Loss D Fake: 1.3059 (0.5809) Acc D Fake: 78.419%
Loss D: 1.601
Loss G: 0.8090 (0.9100) Acc G: 20.924%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3116 (0.3045) Acc D Real: 86.072%
Loss D Fake: 0.5397 (0.5807) Acc D Fake: 78.447%
Loss D: 0.851
Loss G: 1.2174 (0.9118) Acc G: 20.805%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2913 (0.3044) Acc D Real: 86.050%
Loss D Fake: 0.3573 (0.5794) Acc D Fake: 78.569%
Loss D: 0.649
Loss G: 1.2452 (0.9136) Acc G: 20.687%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2120 (0.3039) Acc D Real: 86.068%
Loss D Fake: 0.3482 (0.5782) Acc D Fake: 78.689%
Loss D: 0.560
Loss G: 1.2570 (0.9156) Acc G: 20.571%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.2332 (0.3035) Acc D Real: 86.082%
Loss D Fake: 0.3437 (0.5768) Acc D Fake: 78.808%
Loss D: 0.577
Loss G: 1.2641 (0.9175) Acc G: 20.456%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.2332 (0.3031) Acc D Real: 86.099%
Loss D Fake: 0.3412 (0.5755) Acc D Fake: 78.926%
Loss D: 0.574
Loss G: 1.2668 (0.9195) Acc G: 20.343%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3450 (0.3034) Acc D Real: 86.064%
Loss D Fake: 0.3409 (0.5742) Acc D Fake: 79.042%
Loss D: 0.686
Loss G: 1.2643 (0.9214) Acc G: 20.230%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3126 (0.3034) Acc D Real: 86.030%
Loss D Fake: 0.3428 (0.5730) Acc D Fake: 79.158%
Loss D: 0.655
Loss G: 1.2597 (0.9232) Acc G: 20.119%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2950 (0.3034) Acc D Real: 86.025%
Loss D Fake: 0.3428 (0.5717) Acc D Fake: 79.271%
Loss D: 0.638
Loss G: 1.2675 (0.9251) Acc G: 20.009%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3415 (0.3036) Acc D Real: 85.980%
Loss D Fake: 0.3391 (0.5704) Acc D Fake: 79.384%
Loss D: 0.681
Loss G: 1.2706 (0.9270) Acc G: 19.900%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.3589 (0.3039) Acc D Real: 85.936%
Loss D Fake: 0.3417 (0.5692) Acc D Fake: 79.495%
Loss D: 0.701
Loss G: 1.1941 (0.9284) Acc G: 19.793%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1905 (0.3033) Acc D Real: 85.960%
Loss D Fake: 0.3997 (0.5683) Acc D Fake: 79.606%
Loss D: 0.590
Loss G: 1.1539 (0.9296) Acc G: 19.686%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3203 (0.3034) Acc D Real: 85.913%
Loss D Fake: 0.3810 (0.5673) Acc D Fake: 79.715%
Loss D: 0.701
Loss G: 1.2705 (0.9315) Acc G: 19.581%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.2596 (0.3031) Acc D Real: 85.910%
Loss D Fake: 0.3364 (0.5661) Acc D Fake: 79.823%
Loss D: 0.596
Loss G: 1.2811 (0.9333) Acc G: 19.477%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.2226 (0.3027) Acc D Real: 85.928%
Loss D Fake: 0.3346 (0.5648) Acc D Fake: 79.929%
Loss D: 0.557
Loss G: 1.2830 (0.9352) Acc G: 19.374%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.2514 (0.3024) Acc D Real: 85.929%
Loss D Fake: 0.3379 (0.5636) Acc D Fake: 80.035%
Loss D: 0.589
Loss G: 1.2472 (0.9368) Acc G: 19.272%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.1790 (0.3018) Acc D Real: 85.966%
Loss D Fake: 0.3818 (0.5627) Acc D Fake: 80.140%
Loss D: 0.561
Loss G: 1.2555 (0.9385) Acc G: 19.171%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.2284 (0.3014) Acc D Real: 85.985%
Loss D Fake: 0.3349 (0.5615) Acc D Fake: 80.243%
Loss D: 0.563
Loss G: 1.2930 (0.9403) Acc G: 19.071%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3930 (0.3019) Acc D Real: 85.933%
Loss D Fake: 0.3311 (0.5603) Acc D Fake: 80.345%
Loss D: 0.724
Loss G: 1.2871 (0.9421) Acc G: 18.972%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2506 (0.3016) Acc D Real: 85.935%
Loss D Fake: 0.3435 (0.5592) Acc D Fake: 80.447%
Loss D: 0.594
Loss G: 1.2496 (0.9437) Acc G: 18.875%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.2960 (0.3016) Acc D Real: 85.919%
Loss D Fake: 0.3688 (0.5582) Acc D Fake: 80.547%
Loss D: 0.665
Loss G: 1.2697 (0.9454) Acc G: 18.778%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.2356 (0.3013) Acc D Real: 85.929%
Loss D Fake: 0.3337 (0.5571) Acc D Fake: 80.646%
Loss D: 0.569
Loss G: 1.2920 (0.9471) Acc G: 18.682%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3023 (0.3013) Acc D Real: 85.912%
Loss D Fake: 0.3326 (0.5559) Acc D Fake: 80.745%
Loss D: 0.635
Loss G: 1.2862 (0.9489) Acc G: 18.587%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2969 (0.3012) Acc D Real: 85.903%
Loss D Fake: 0.3386 (0.5548) Acc D Fake: 80.842%
Loss D: 0.636
Loss G: 1.2685 (0.9505) Acc G: 18.493%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2544 (0.3010) Acc D Real: 85.910%
Loss D Fake: 0.3526 (0.5538) Acc D Fake: 80.938%
Loss D: 0.607
Loss G: 1.2186 (0.9518) Acc G: 18.400%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.2011 (0.3005) Acc D Real: 85.942%
Loss D Fake: 0.4116 (0.5531) Acc D Fake: 80.992%
Loss D: 0.613
Loss G: 0.3943 (0.9490) Acc G: 18.742%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.1610 (0.2998) Acc D Real: 85.984%
Loss D Fake: 1.2080 (0.5564) Acc D Fake: 80.655%
Loss D: 1.369
Loss G: 0.3856 (0.9462) Acc G: 19.080%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1880 (0.2993) Acc D Real: 86.017%
Loss D Fake: 1.2097 (0.5596) Acc D Fake: 80.322%
Loss D: 1.398
Loss G: 0.3861 (0.9435) Acc G: 19.414%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2173 (0.2988) Acc D Real: 86.039%
Loss D Fake: 1.2022 (0.5628) Acc D Fake: 79.992%
Loss D: 1.420
Loss G: 0.3899 (0.9407) Acc G: 19.745%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.2483 (0.2986) Acc D Real: 86.048%
Loss D Fake: 1.1894 (0.5658) Acc D Fake: 79.665%
Loss D: 1.438
Loss G: 0.3957 (0.9381) Acc G: 20.074%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.1994 (0.2981) Acc D Real: 86.077%
Loss D Fake: 1.1732 (0.5688) Acc D Fake: 79.341%
Loss D: 1.373
Loss G: 0.4032 (0.9355) Acc G: 20.398%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.1801 (0.2975) Acc D Real: 86.104%
Loss D Fake: 1.1534 (0.5716) Acc D Fake: 79.021%
Loss D: 1.334
Loss G: 0.4128 (0.9329) Acc G: 20.720%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.2391 (0.2973) Acc D Real: 86.127%
Loss D Fake: 1.1304 (0.5743) Acc D Fake: 78.704%
Loss D: 1.370
Loss G: 0.4242 (0.9305) Acc G: 21.039%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2072 (0.2968) Acc D Real: 86.156%
Loss D Fake: 1.1043 (0.5769) Acc D Fake: 78.389%
Loss D: 1.312
Loss G: 0.4402 (0.9281) Acc G: 21.354%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1897 (0.2963) Acc D Real: 86.188%
Loss D Fake: 1.0608 (0.5792) Acc D Fake: 78.086%
Loss D: 1.251
Loss G: 0.6231 (0.9266) Acc G: 21.475%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.1859 (0.2958) Acc D Real: 86.224%
Loss D Fake: 1.3738 (0.5830) Acc D Fake: 77.952%
Loss D: 1.560
Loss G: 0.6309 (0.9252) Acc G: 21.603%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.1834 (0.2953) Acc D Real: 86.260%
Loss D Fake: 1.2492 (0.5861) Acc D Fake: 77.844%
Loss D: 1.433
Loss G: 0.7996 (0.9246) Acc G: 21.666%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.1843 (0.2947) Acc D Real: 86.289%
Loss D Fake: 0.7036 (0.5867) Acc D Fake: 77.838%
Loss D: 0.888
Loss G: 1.3233 (0.9265) Acc G: 21.564%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2055 (0.2943) Acc D Real: 86.313%
Loss D Fake: 0.2995 (0.5853) Acc D Fake: 77.942%
Loss D: 0.505
Loss G: 1.3968 (0.9287) Acc G: 21.463%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.2977 (0.2943) Acc D Real: 86.299%
Loss D Fake: 0.3082 (0.5841) Acc D Fake: 78.045%
Loss D: 0.606
Loss G: 1.3517 (0.9307) Acc G: 21.362%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.3553 (0.2946) Acc D Real: 86.289%
Loss D Fake: 0.3134 (0.5828) Acc D Fake: 78.147%
Loss D: 0.669
Loss G: 1.3533 (0.9327) Acc G: 21.263%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3293 (0.2948) Acc D Real: 86.283%
Loss D Fake: 0.3139 (0.5815) Acc D Fake: 78.248%
Loss D: 0.643
Loss G: 1.3543 (0.9346) Acc G: 21.165%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2554 (0.2946) Acc D Real: 86.296%
Loss D Fake: 0.3160 (0.5803) Acc D Fake: 78.349%
Loss D: 0.571
Loss G: 1.3533 (0.9366) Acc G: 21.067%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.3583 (0.2949) Acc D Real: 86.280%
Loss D Fake: 0.3187 (0.5791) Acc D Fake: 78.448%
Loss D: 0.677
Loss G: 1.3558 (0.9385) Acc G: 20.970%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.3120 (0.2950) Acc D Real: 86.278%
Loss D Fake: 0.3154 (0.5779) Acc D Fake: 78.546%
Loss D: 0.627
Loss G: 1.3702 (0.9404) Acc G: 20.875%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3883 (0.2954) Acc D Real: 86.264%
Loss D Fake: 0.3083 (0.5767) Acc D Fake: 78.644%
Loss D: 0.697
Loss G: 1.3831 (0.9425) Acc G: 20.780%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2443 (0.2952) Acc D Real: 86.284%
Loss D Fake: 0.3035 (0.5755) Acc D Fake: 78.741%
Loss D: 0.548
Loss G: 1.3923 (0.9445) Acc G: 20.686%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2453 (0.2949) Acc D Real: 86.306%
Loss D Fake: 0.3002 (0.5742) Acc D Fake: 78.836%
Loss D: 0.546
Loss G: 1.3998 (0.9465) Acc G: 20.593%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.3527 (0.2952) Acc D Real: 86.288%
Loss D Fake: 0.2977 (0.5730) Acc D Fake: 78.931%
Loss D: 0.650
Loss G: 1.4046 (0.9486) Acc G: 20.500%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.3908 (0.2956) Acc D Real: 86.255%
Loss D Fake: 0.2964 (0.5717) Acc D Fake: 79.025%
Loss D: 0.687
Loss G: 1.4063 (0.9506) Acc G: 20.409%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.2032 (0.2952) Acc D Real: 86.281%
Loss D Fake: 0.2958 (0.5705) Acc D Fake: 79.119%
Loss D: 0.499
Loss G: 1.4080 (0.9527) Acc G: 20.318%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.2155 (0.2949) Acc D Real: 86.289%
Loss D Fake: 0.2950 (0.5693) Acc D Fake: 79.142%
Loss D: 0.511
Loss G: 1.4102 (0.9547) Acc G: 20.296%
LR: 2.000e-04
Epoch: 18/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2715 (0.2898) Acc D Real: 86.901%
Loss D Fake: 0.2938 (0.2940) Acc D Fake: 100.000%
Loss D: 0.565
Loss G: 1.4126 (1.4121) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.3217 (0.3004) Acc D Real: 87.135%
Loss D Fake: 0.2936 (0.2939) Acc D Fake: 100.000%
Loss D: 0.615
Loss G: 1.4125 (1.4122) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2486 (0.2875) Acc D Real: 87.591%
Loss D Fake: 0.2937 (0.2938) Acc D Fake: 100.000%
Loss D: 0.542
Loss G: 1.4122 (1.4122) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2395 (0.2779) Acc D Real: 88.219%
Loss D Fake: 0.2937 (0.2938) Acc D Fake: 100.000%
Loss D: 0.533
Loss G: 1.4124 (1.4123) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.2644 (0.2756) Acc D Real: 88.177%
Loss D Fake: 0.2936 (0.2938) Acc D Fake: 100.000%
Loss D: 0.558
Loss G: 1.4124 (1.4123) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.3013 (0.2793) Acc D Real: 87.708%
Loss D Fake: 0.2936 (0.2938) Acc D Fake: 100.000%
Loss D: 0.595
Loss G: 1.4120 (1.4123) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.2163 (0.2714) Acc D Real: 87.930%
Loss D Fake: 0.2948 (0.2939) Acc D Fake: 100.000%
Loss D: 0.511
Loss G: 1.4014 (1.4109) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2092 (0.2645) Acc D Real: 88.328%
Loss D Fake: 0.3006 (0.2946) Acc D Fake: 100.000%
Loss D: 0.510
Loss G: 1.3773 (1.4072) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.1542 (0.2535) Acc D Real: 89.010%
Loss D Fake: 0.3140 (0.2966) Acc D Fake: 100.000%
Loss D: 0.468
Loss G: 1.2022 (1.3867) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.3608 (0.2632) Acc D Real: 87.969%
Loss D Fake: 0.3901 (0.3051) Acc D Fake: 100.000%
Loss D: 0.751
Loss G: 1.1390 (1.3642) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2594 (0.2629) Acc D Real: 88.108%
Loss D Fake: 0.3940 (0.3125) Acc D Fake: 100.000%
Loss D: 0.653
Loss G: 1.1393 (1.3454) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2808 (0.2643) Acc D Real: 88.013%
Loss D Fake: 0.3907 (0.3185) Acc D Fake: 100.000%
Loss D: 0.672
Loss G: 1.1687 (1.3318) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.2382 (0.2624) Acc D Real: 87.898%
Loss D Fake: 0.3178 (0.3184) Acc D Fake: 100.000%
Loss D: 0.556
Loss G: 1.3974 (1.3365) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.2551 (0.2619) Acc D Real: 87.913%
Loss D Fake: 0.2953 (0.3169) Acc D Fake: 100.000%
Loss D: 0.550
Loss G: 1.4159 (1.3418) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3584 (0.2680) Acc D Real: 87.562%
Loss D Fake: 0.2925 (0.3154) Acc D Fake: 100.000%
Loss D: 0.651
Loss G: 1.4209 (1.3467) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3701 (0.2740) Acc D Real: 87.089%
Loss D Fake: 0.2918 (0.3140) Acc D Fake: 100.000%
Loss D: 0.662
Loss G: 1.4214 (1.3511) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2872 (0.2747) Acc D Real: 87.066%
Loss D Fake: 0.2920 (0.3128) Acc D Fake: 100.000%
Loss D: 0.579
Loss G: 1.4207 (1.3550) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3442 (0.2784) Acc D Real: 86.812%
Loss D Fake: 0.2924 (0.3117) Acc D Fake: 100.000%
Loss D: 0.637
Loss G: 1.4190 (1.3584) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1450 (0.2717) Acc D Real: 87.201%
Loss D Fake: 0.2928 (0.3107) Acc D Fake: 100.000%
Loss D: 0.438
Loss G: 1.4188 (1.3614) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.2663 (0.2714) Acc D Real: 87.247%
Loss D Fake: 0.2928 (0.3099) Acc D Fake: 100.000%
Loss D: 0.559
Loss G: 1.4184 (1.3641) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.2233 (0.2693) Acc D Real: 87.434%
Loss D Fake: 0.2930 (0.3091) Acc D Fake: 100.000%
Loss D: 0.516
Loss G: 1.4183 (1.3666) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1558 (0.2643) Acc D Real: 87.740%
Loss D Fake: 0.2929 (0.3084) Acc D Fake: 100.000%
Loss D: 0.449
Loss G: 1.4194 (1.3689) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.2799 (0.2650) Acc D Real: 87.650%
Loss D Fake: 0.2927 (0.3078) Acc D Fake: 100.000%
Loss D: 0.573
Loss G: 1.4199 (1.3710) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.1795 (0.2616) Acc D Real: 87.817%
Loss D Fake: 0.2926 (0.3072) Acc D Fake: 100.000%
Loss D: 0.472
Loss G: 1.4211 (1.3730) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2782 (0.2622) Acc D Real: 87.730%
Loss D Fake: 0.2924 (0.3066) Acc D Fake: 100.000%
Loss D: 0.571
Loss G: 1.4217 (1.3749) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.1428 (0.2578) Acc D Real: 88.011%
Loss D Fake: 0.2923 (0.3061) Acc D Fake: 100.000%
Loss D: 0.435
Loss G: 1.4232 (1.3767) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.2694 (0.2582) Acc D Real: 87.887%
Loss D Fake: 0.2919 (0.3056) Acc D Fake: 100.000%
Loss D: 0.561
Loss G: 1.4247 (1.3784) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.2788 (0.2589) Acc D Real: 87.861%
Loss D Fake: 0.2916 (0.3051) Acc D Fake: 100.000%
Loss D: 0.570
Loss G: 1.4255 (1.3800) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.2041 (0.2571) Acc D Real: 87.946%
Loss D Fake: 0.2915 (0.3046) Acc D Fake: 100.000%
Loss D: 0.496
Loss G: 1.4264 (1.3815) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2799 (0.2578) Acc D Real: 87.880%
Loss D Fake: 0.2913 (0.3042) Acc D Fake: 100.000%
Loss D: 0.571
Loss G: 1.4271 (1.3830) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1768 (0.2553) Acc D Real: 88.016%
Loss D Fake: 0.2911 (0.3038) Acc D Fake: 100.000%
Loss D: 0.468
Loss G: 1.4283 (1.3844) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.1521 (0.2522) Acc D Real: 88.158%
Loss D Fake: 0.2908 (0.3034) Acc D Fake: 100.000%
Loss D: 0.443
Loss G: 1.4295 (1.3858) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.1886 (0.2503) Acc D Real: 88.312%
Loss D Fake: 0.2907 (0.3030) Acc D Fake: 100.000%
Loss D: 0.479
Loss G: 1.4308 (1.3871) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.2648 (0.2507) Acc D Real: 88.268%
Loss D Fake: 0.2905 (0.3027) Acc D Fake: 100.000%
Loss D: 0.555
Loss G: 1.4317 (1.3884) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.1732 (0.2485) Acc D Real: 88.397%
Loss D Fake: 0.2904 (0.3023) Acc D Fake: 100.000%
Loss D: 0.464
Loss G: 1.4330 (1.3896) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2685 (0.2491) Acc D Real: 88.363%
Loss D Fake: 0.2902 (0.3020) Acc D Fake: 100.000%
Loss D: 0.559
Loss G: 1.4330 (1.3908) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2094 (0.2480) Acc D Real: 88.502%
Loss D Fake: 0.2907 (0.3017) Acc D Fake: 100.000%
Loss D: 0.500
Loss G: 1.4319 (1.3919) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.3106 (0.2496) Acc D Real: 88.377%
Loss D Fake: 0.2915 (0.3014) Acc D Fake: 100.000%
Loss D: 0.602
Loss G: 1.4291 (1.3928) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.2072 (0.2486) Acc D Real: 88.419%
Loss D Fake: 0.2927 (0.3012) Acc D Fake: 100.000%
Loss D: 0.500
Loss G: 1.4262 (1.3937) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.2093 (0.2476) Acc D Real: 88.427%
Loss D Fake: 0.2937 (0.3010) Acc D Fake: 100.000%
Loss D: 0.503
Loss G: 1.4234 (1.3944) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.2143 (0.2468) Acc D Real: 88.408%
Loss D Fake: 0.2949 (0.3009) Acc D Fake: 100.000%
Loss D: 0.509
Loss G: 1.4183 (1.3950) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.1318 (0.2442) Acc D Real: 88.560%
Loss D Fake: 0.2968 (0.3008) Acc D Fake: 100.000%
Loss D: 0.429
Loss G: 1.4142 (1.3954) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.2180 (0.2436) Acc D Real: 88.633%
Loss D Fake: 0.2976 (0.3007) Acc D Fake: 100.000%
Loss D: 0.516
Loss G: 1.4226 (1.3960) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.2074 (0.2428) Acc D Real: 88.675%
Loss D Fake: 0.3077 (0.3009) Acc D Fake: 100.000%
Loss D: 0.515
Loss G: 1.4431 (1.3971) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.1390 (0.2405) Acc D Real: 88.790%
Loss D Fake: 0.2853 (0.3005) Acc D Fake: 100.000%
Loss D: 0.424
Loss G: 1.4574 (1.3984) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.1461 (0.2385) Acc D Real: 88.916%
Loss D Fake: 0.2822 (0.3001) Acc D Fake: 100.000%
Loss D: 0.428
Loss G: 1.4650 (1.3998) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.1667 (0.2370) Acc D Real: 88.993%
Loss D Fake: 0.2801 (0.2997) Acc D Fake: 100.000%
Loss D: 0.447
Loss G: 1.4710 (1.4013) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.1905 (0.2361) Acc D Real: 89.088%
Loss D Fake: 0.2783 (0.2993) Acc D Fake: 100.000%
Loss D: 0.469
Loss G: 1.4762 (1.4028) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.2433 (0.2362) Acc D Real: 89.073%
Loss D Fake: 0.2768 (0.2988) Acc D Fake: 100.000%
Loss D: 0.520
Loss G: 1.4805 (1.4044) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.1903 (0.2353) Acc D Real: 89.175%
Loss D Fake: 0.2756 (0.2984) Acc D Fake: 100.000%
Loss D: 0.466
Loss G: 1.4844 (1.4059) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.2666 (0.2359) Acc D Real: 89.144%
Loss D Fake: 0.2751 (0.2979) Acc D Fake: 100.000%
Loss D: 0.542
Loss G: 1.4828 (1.4074) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2787 (0.2367) Acc D Real: 89.053%
Loss D Fake: 0.2770 (0.2975) Acc D Fake: 100.000%
Loss D: 0.556
Loss G: 1.4784 (1.4088) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.2158 (0.2363) Acc D Real: 89.063%
Loss D Fake: 0.2793 (0.2972) Acc D Fake: 100.000%
Loss D: 0.495
Loss G: 1.4738 (1.4100) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.2336 (0.2363) Acc D Real: 89.063%
Loss D Fake: 0.2813 (0.2969) Acc D Fake: 100.000%
Loss D: 0.515
Loss G: 1.4709 (1.4111) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.2905 (0.2372) Acc D Real: 89.001%
Loss D Fake: 0.2824 (0.2966) Acc D Fake: 100.000%
Loss D: 0.573
Loss G: 1.4700 (1.4121) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.2557 (0.2376) Acc D Real: 88.991%
Loss D Fake: 0.2824 (0.2964) Acc D Fake: 100.000%
Loss D: 0.538
Loss G: 1.4716 (1.4132) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.1733 (0.2365) Acc D Real: 89.039%
Loss D Fake: 0.2811 (0.2961) Acc D Fake: 100.000%
Loss D: 0.454
Loss G: 1.4763 (1.4143) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.1589 (0.2351) Acc D Real: 89.100%
Loss D Fake: 0.2789 (0.2958) Acc D Fake: 100.000%
Loss D: 0.438
Loss G: 1.4825 (1.4154) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.1926 (0.2344) Acc D Real: 89.175%
Loss D Fake: 0.2766 (0.2955) Acc D Fake: 100.000%
Loss D: 0.469
Loss G: 1.4887 (1.4166) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.1376 (0.2328) Acc D Real: 89.270%
Loss D Fake: 0.2743 (0.2952) Acc D Fake: 100.000%
Loss D: 0.412
Loss G: 1.4951 (1.4179) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.2969 (0.2339) Acc D Real: 89.179%
Loss D Fake: 0.2723 (0.2948) Acc D Fake: 100.000%
Loss D: 0.569
Loss G: 1.5001 (1.4192) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3428 (0.2356) Acc D Real: 89.041%
Loss D Fake: 0.2709 (0.2944) Acc D Fake: 100.000%
Loss D: 0.614
Loss G: 1.5030 (1.4206) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.2648 (0.2361) Acc D Real: 89.033%
Loss D Fake: 0.2701 (0.2940) Acc D Fake: 100.000%
Loss D: 0.535
Loss G: 1.5047 (1.4219) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3416 (0.2377) Acc D Real: 88.930%
Loss D Fake: 0.2697 (0.2937) Acc D Fake: 100.000%
Loss D: 0.611
Loss G: 1.5048 (1.4232) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.1239 (0.2360) Acc D Real: 89.035%
Loss D Fake: 0.2694 (0.2933) Acc D Fake: 100.000%
Loss D: 0.393
Loss G: 1.5059 (1.4244) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.1995 (0.2354) Acc D Real: 89.078%
Loss D Fake: 0.2689 (0.2929) Acc D Fake: 100.000%
Loss D: 0.468
Loss G: 1.5077 (1.4257) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.2515 (0.2357) Acc D Real: 89.089%
Loss D Fake: 0.2683 (0.2926) Acc D Fake: 100.000%
Loss D: 0.520
Loss G: 1.5089 (1.4269) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.2300 (0.2356) Acc D Real: 89.086%
Loss D Fake: 0.2680 (0.2922) Acc D Fake: 100.000%
Loss D: 0.498
Loss G: 1.5100 (1.4281) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.2092 (0.2352) Acc D Real: 89.126%
Loss D Fake: 0.2676 (0.2919) Acc D Fake: 100.000%
Loss D: 0.477
Loss G: 1.5111 (1.4293) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.1757 (0.2344) Acc D Real: 89.196%
Loss D Fake: 0.2673 (0.2915) Acc D Fake: 100.000%
Loss D: 0.443
Loss G: 1.5126 (1.4305) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.1870 (0.2337) Acc D Real: 89.251%
Loss D Fake: 0.2668 (0.2912) Acc D Fake: 100.000%
Loss D: 0.454
Loss G: 1.5144 (1.4316) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.2504 (0.2339) Acc D Real: 89.235%
Loss D Fake: 0.2664 (0.2908) Acc D Fake: 100.000%
Loss D: 0.517
Loss G: 1.5157 (1.4328) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.1827 (0.2332) Acc D Real: 89.275%
Loss D Fake: 0.2660 (0.2905) Acc D Fake: 100.000%
Loss D: 0.449
Loss G: 1.5173 (1.4339) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.2188 (0.2330) Acc D Real: 89.272%
Loss D Fake: 0.2656 (0.2902) Acc D Fake: 100.000%
Loss D: 0.484
Loss G: 1.5190 (1.4350) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.2490 (0.2333) Acc D Real: 89.242%
Loss D Fake: 0.2651 (0.2898) Acc D Fake: 100.000%
Loss D: 0.514
Loss G: 1.5204 (1.4362) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.1780 (0.2325) Acc D Real: 89.274%
Loss D Fake: 0.2647 (0.2895) Acc D Fake: 100.000%
Loss D: 0.443
Loss G: 1.5222 (1.4373) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.1062 (0.2309) Acc D Real: 89.362%
Loss D Fake: 0.2641 (0.2892) Acc D Fake: 100.000%
Loss D: 0.370
Loss G: 1.5253 (1.4384) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3736 (0.2327) Acc D Real: 89.203%
Loss D Fake: 0.2635 (0.2889) Acc D Fake: 100.000%
Loss D: 0.637
Loss G: 1.5257 (1.4395) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.1225 (0.2313) Acc D Real: 89.273%
Loss D Fake: 0.2636 (0.2886) Acc D Fake: 100.000%
Loss D: 0.386
Loss G: 1.5263 (1.4406) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.2028 (0.2310) Acc D Real: 89.273%
Loss D Fake: 0.2635 (0.2882) Acc D Fake: 100.000%
Loss D: 0.466
Loss G: 1.5275 (1.4417) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.2575 (0.2313) Acc D Real: 89.224%
Loss D Fake: 0.2633 (0.2879) Acc D Fake: 100.000%
Loss D: 0.521
Loss G: 1.5275 (1.4427) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.2869 (0.2320) Acc D Real: 89.177%
Loss D Fake: 0.2636 (0.2876) Acc D Fake: 100.000%
Loss D: 0.551
Loss G: 1.5261 (1.4437) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.2590 (0.2323) Acc D Real: 89.105%
Loss D Fake: 0.2645 (0.2874) Acc D Fake: 100.000%
Loss D: 0.524
Loss G: 1.4358 (1.4436) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.1464 (0.2313) Acc D Real: 89.167%
Loss D Fake: 0.4424 (0.2892) Acc D Fake: 99.879%
Loss D: 0.589
Loss G: 1.3572 (1.4426) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.1276 (0.2301) Acc D Real: 89.233%
Loss D Fake: 0.2651 (0.2889) Acc D Fake: 99.880%
Loss D: 0.393
Loss G: 1.5271 (1.4436) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.1988 (0.2297) Acc D Real: 89.243%
Loss D Fake: 0.2633 (0.2886) Acc D Fake: 99.881%
Loss D: 0.462
Loss G: 1.5322 (1.4446) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.2053 (0.2295) Acc D Real: 89.239%
Loss D Fake: 0.2626 (0.2883) Acc D Fake: 99.883%
Loss D: 0.468
Loss G: 1.5349 (1.4456) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.1502 (0.2286) Acc D Real: 89.291%
Loss D Fake: 0.2622 (0.2880) Acc D Fake: 99.884%
Loss D: 0.412
Loss G: 1.5373 (1.4467) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.1865 (0.2281) Acc D Real: 89.314%
Loss D Fake: 0.2617 (0.2877) Acc D Fake: 99.885%
Loss D: 0.448
Loss G: 1.5398 (1.4477) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.1519 (0.2273) Acc D Real: 89.369%
Loss D Fake: 0.2612 (0.2874) Acc D Fake: 99.887%
Loss D: 0.413
Loss G: 1.5425 (1.4487) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.2449 (0.2274) Acc D Real: 89.342%
Loss D Fake: 0.2608 (0.2872) Acc D Fake: 99.888%
Loss D: 0.506
Loss G: 1.5439 (1.4498) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.1371 (0.2265) Acc D Real: 89.392%
Loss D Fake: 0.2606 (0.2869) Acc D Fake: 99.889%
Loss D: 0.398
Loss G: 1.5453 (1.4508) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.1497 (0.2257) Acc D Real: 89.423%
Loss D Fake: 0.2603 (0.2866) Acc D Fake: 99.890%
Loss D: 0.410
Loss G: 1.5474 (1.4518) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.1109 (0.2245) Acc D Real: 89.483%
Loss D Fake: 0.2597 (0.2863) Acc D Fake: 99.891%
Loss D: 0.371
Loss G: 1.5503 (1.4529) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2199 (0.2244) Acc D Real: 89.495%
Loss D Fake: 0.2590 (0.2860) Acc D Fake: 99.893%
Loss D: 0.479
Loss G: 1.5529 (1.4539) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.1039 (0.2232) Acc D Real: 89.562%
Loss D Fake: 0.2584 (0.2857) Acc D Fake: 99.894%
Loss D: 0.362
Loss G: 1.5561 (1.4550) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.1500 (0.2224) Acc D Real: 89.611%
Loss D Fake: 0.2575 (0.2854) Acc D Fake: 99.895%
Loss D: 0.407
Loss G: 1.5596 (1.4560) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2462 (0.2227) Acc D Real: 89.594%
Loss D Fake: 0.2567 (0.2852) Acc D Fake: 99.896%
Loss D: 0.503
Loss G: 1.5620 (1.4571) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.1594 (0.2220) Acc D Real: 89.641%
Loss D Fake: 0.2562 (0.2849) Acc D Fake: 99.897%
Loss D: 0.416
Loss G: 1.5644 (1.4582) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.1830 (0.2216) Acc D Real: 89.662%
Loss D Fake: 0.2557 (0.2846) Acc D Fake: 99.898%
Loss D: 0.439
Loss G: 1.5661 (1.4592) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.1689 (0.2211) Acc D Real: 89.704%
Loss D Fake: 0.2553 (0.2843) Acc D Fake: 99.899%
Loss D: 0.424
Loss G: 1.5678 (1.4603) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.1690 (0.2206) Acc D Real: 89.717%
Loss D Fake: 0.2551 (0.2840) Acc D Fake: 99.900%
Loss D: 0.424
Loss G: 1.5369 (1.4611) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.1399 (0.2198) Acc D Real: 89.754%
Loss D Fake: 0.2873 (0.2840) Acc D Fake: 99.901%
Loss D: 0.427
Loss G: 1.5771 (1.4622) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.1496 (0.2192) Acc D Real: 89.783%
Loss D Fake: 0.2516 (0.2837) Acc D Fake: 99.902%
Loss D: 0.401
Loss G: 1.5866 (1.4634) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.1398 (0.2184) Acc D Real: 89.830%
Loss D Fake: 0.2495 (0.2834) Acc D Fake: 99.903%
Loss D: 0.389
Loss G: 1.5943 (1.4646) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2599 (0.2188) Acc D Real: 89.809%
Loss D Fake: 0.2478 (0.2831) Acc D Fake: 99.904%
Loss D: 0.508
Loss G: 1.5998 (1.4659) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.1517 (0.2182) Acc D Real: 89.842%
Loss D Fake: 0.2467 (0.2827) Acc D Fake: 99.905%
Loss D: 0.398
Loss G: 1.6044 (1.4671) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.1231 (0.2173) Acc D Real: 89.896%
Loss D Fake: 0.2455 (0.2824) Acc D Fake: 99.905%
Loss D: 0.369
Loss G: 1.6091 (1.4684) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.1710 (0.2169) Acc D Real: 89.920%
Loss D Fake: 0.2445 (0.2821) Acc D Fake: 99.906%
Loss D: 0.415
Loss G: 1.6134 (1.4698) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.2108 (0.2168) Acc D Real: 89.916%
Loss D Fake: 0.2437 (0.2817) Acc D Fake: 99.907%
Loss D: 0.454
Loss G: 1.6164 (1.4711) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.1772 (0.2165) Acc D Real: 89.927%
Loss D Fake: 0.2439 (0.2814) Acc D Fake: 99.908%
Loss D: 0.421
Loss G: 1.6082 (1.4723) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.2140 (0.2165) Acc D Real: 89.923%
Loss D Fake: 0.3052 (0.2816) Acc D Fake: 99.909%
Loss D: 0.519
Loss G: 1.1611 (1.4696) Acc G: 0.124%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.2063 (0.2164) Acc D Real: 89.918%
Loss D Fake: 1.3007 (0.2905) Acc D Fake: 99.544%
Loss D: 1.507
Loss G: 1.6172 (1.4708) Acc G: 0.122%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.1873 (0.2161) Acc D Real: 89.922%
Loss D Fake: 0.2406 (0.2901) Acc D Fake: 99.548%
Loss D: 0.428
Loss G: 1.6319 (1.4722) Acc G: 0.121%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.2167 (0.2161) Acc D Real: 89.915%
Loss D Fake: 0.2388 (0.2896) Acc D Fake: 99.552%
Loss D: 0.456
Loss G: 1.6336 (1.4736) Acc G: 0.120%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.2332 (0.2163) Acc D Real: 89.904%
Loss D Fake: 0.2390 (0.2892) Acc D Fake: 99.556%
Loss D: 0.472
Loss G: 1.6306 (1.4750) Acc G: 0.119%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3258 (0.2172) Acc D Real: 89.845%
Loss D Fake: 0.2401 (0.2888) Acc D Fake: 99.559%
Loss D: 0.566
Loss G: 1.6244 (1.4762) Acc G: 0.118%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.1244 (0.2164) Acc D Real: 89.889%
Loss D Fake: 0.2420 (0.2884) Acc D Fake: 99.563%
Loss D: 0.366
Loss G: 1.6180 (1.4774) Acc G: 0.117%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2117 (0.2164) Acc D Real: 89.888%
Loss D Fake: 0.2437 (0.2880) Acc D Fake: 99.567%
Loss D: 0.455
Loss G: 1.6122 (1.4786) Acc G: 0.116%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2560 (0.2167) Acc D Real: 89.867%
Loss D Fake: 0.2452 (0.2877) Acc D Fake: 99.570%
Loss D: 0.501
Loss G: 1.6074 (1.4796) Acc G: 0.115%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.1988 (0.2166) Acc D Real: 89.876%
Loss D Fake: 0.2471 (0.2873) Acc D Fake: 99.574%
Loss D: 0.446
Loss G: 1.5970 (1.4806) Acc G: 0.114%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2415 (0.2168) Acc D Real: 89.877%
Loss D Fake: 0.2515 (0.2871) Acc D Fake: 99.577%
Loss D: 0.493
Loss G: 1.5859 (1.4814) Acc G: 0.113%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.2750 (0.2172) Acc D Real: 89.848%
Loss D Fake: 0.2546 (0.2868) Acc D Fake: 99.581%
Loss D: 0.530
Loss G: 1.5822 (1.4823) Acc G: 0.113%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3313 (0.2181) Acc D Real: 89.806%
Loss D Fake: 0.2541 (0.2865) Acc D Fake: 99.584%
Loss D: 0.585
Loss G: 1.5853 (1.4831) Acc G: 0.112%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.1649 (0.2177) Acc D Real: 89.833%
Loss D Fake: 0.2515 (0.2862) Acc D Fake: 99.587%
Loss D: 0.416
Loss G: 1.5929 (1.4840) Acc G: 0.111%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.3139 (0.2185) Acc D Real: 89.782%
Loss D Fake: 0.2484 (0.2860) Acc D Fake: 99.591%
Loss D: 0.562
Loss G: 1.5996 (1.4849) Acc G: 0.110%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.1929 (0.2183) Acc D Real: 89.810%
Loss D Fake: 0.2459 (0.2856) Acc D Fake: 99.594%
Loss D: 0.439
Loss G: 1.6051 (1.4858) Acc G: 0.109%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3723 (0.2195) Acc D Real: 89.744%
Loss D Fake: 0.2445 (0.2853) Acc D Fake: 99.597%
Loss D: 0.617
Loss G: 1.6050 (1.4867) Acc G: 0.108%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.2891 (0.2200) Acc D Real: 89.713%
Loss D Fake: 0.2447 (0.2850) Acc D Fake: 99.600%
Loss D: 0.534
Loss G: 1.6035 (1.4876) Acc G: 0.107%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.2383 (0.2201) Acc D Real: 89.699%
Loss D Fake: 0.2448 (0.2847) Acc D Fake: 99.603%
Loss D: 0.483
Loss G: 1.6026 (1.4885) Acc G: 0.107%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.2459 (0.2203) Acc D Real: 89.695%
Loss D Fake: 0.2447 (0.2844) Acc D Fake: 99.606%
Loss D: 0.491
Loss G: 1.6022 (1.4894) Acc G: 0.106%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3751 (0.2215) Acc D Real: 89.615%
Loss D Fake: 0.2448 (0.2841) Acc D Fake: 99.609%
Loss D: 0.620
Loss G: 1.5997 (1.4902) Acc G: 0.105%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2588 (0.2218) Acc D Real: 89.601%
Loss D Fake: 0.2446 (0.2838) Acc D Fake: 99.612%
Loss D: 0.503
Loss G: 1.6017 (1.4910) Acc G: 0.104%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.3356 (0.2226) Acc D Real: 89.547%
Loss D Fake: 0.2444 (0.2835) Acc D Fake: 99.615%
Loss D: 0.580
Loss G: 1.5980 (1.4918) Acc G: 0.103%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.2290 (0.2227) Acc D Real: 89.539%
Loss D Fake: 0.2459 (0.2832) Acc D Fake: 99.618%
Loss D: 0.475
Loss G: 1.5921 (1.4926) Acc G: 0.103%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.1912 (0.2224) Acc D Real: 89.546%
Loss D Fake: 0.2476 (0.2830) Acc D Fake: 99.621%
Loss D: 0.439
Loss G: 1.5865 (1.4932) Acc G: 0.102%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.1971 (0.2223) Acc D Real: 89.543%
Loss D Fake: 0.2492 (0.2827) Acc D Fake: 99.623%
Loss D: 0.446
Loss G: 1.5817 (1.4939) Acc G: 0.101%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.1744 (0.2219) Acc D Real: 89.557%
Loss D Fake: 0.2506 (0.2825) Acc D Fake: 99.626%
Loss D: 0.425
Loss G: 1.5782 (1.4945) Acc G: 0.100%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2230 (0.2219) Acc D Real: 89.550%
Loss D Fake: 0.2518 (0.2823) Acc D Fake: 99.629%
Loss D: 0.475
Loss G: 1.5751 (1.4951) Acc G: 0.100%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.1933 (0.2217) Acc D Real: 89.551%
Loss D Fake: 0.2537 (0.2821) Acc D Fake: 99.631%
Loss D: 0.447
Loss G: 1.5614 (1.4955) Acc G: 0.099%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.1221 (0.2210) Acc D Real: 89.590%
Loss D Fake: 0.2911 (0.2821) Acc D Fake: 99.634%
Loss D: 0.413
Loss G: 0.4634 (1.4883) Acc G: 0.542%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.1106 (0.2202) Acc D Real: 89.633%
Loss D Fake: 2.3936 (0.2969) Acc D Fake: 99.147%
Loss D: 2.504
Loss G: 0.3489 (1.4803) Acc G: 1.028%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.1925 (0.2201) Acc D Real: 89.636%
Loss D Fake: 2.3792 (0.3114) Acc D Fake: 98.667%
Loss D: 2.572
Loss G: 0.4502 (1.4731) Acc G: 1.461%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.1366 (0.2195) Acc D Real: 89.668%
Loss D Fake: 1.7963 (0.3216) Acc D Fake: 98.300%
Loss D: 1.933
Loss G: 1.5943 (1.4740) Acc G: 1.451%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.2009 (0.2194) Acc D Real: 89.663%
Loss D Fake: 0.2457 (0.3211) Acc D Fake: 98.311%
Loss D: 0.447
Loss G: 1.6103 (1.4749) Acc G: 1.441%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.1924 (0.2192) Acc D Real: 89.672%
Loss D Fake: 0.2432 (0.3206) Acc D Fake: 98.323%
Loss D: 0.436
Loss G: 1.6133 (1.4759) Acc G: 1.431%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.2310 (0.2193) Acc D Real: 89.663%
Loss D Fake: 0.2437 (0.3200) Acc D Fake: 98.334%
Loss D: 0.475
Loss G: 1.6081 (1.4767) Acc G: 1.421%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.2413 (0.2194) Acc D Real: 89.651%
Loss D Fake: 0.2469 (0.3196) Acc D Fake: 98.345%
Loss D: 0.488
Loss G: 1.5932 (1.4775) Acc G: 1.412%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.2137 (0.2194) Acc D Real: 89.647%
Loss D Fake: 0.2556 (0.3191) Acc D Fake: 98.356%
Loss D: 0.469
Loss G: 1.5510 (1.4780) Acc G: 1.402%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.1215 (0.2187) Acc D Real: 89.685%
Loss D Fake: 1.1617 (0.3247) Acc D Fake: 97.848%
Loss D: 1.283
Loss G: 1.5796 (1.4787) Acc G: 1.393%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2212 (0.2187) Acc D Real: 89.685%
Loss D Fake: 0.2463 (0.3242) Acc D Fake: 97.863%
Loss D: 0.467
Loss G: 1.6190 (1.4796) Acc G: 1.384%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.2043 (0.2186) Acc D Real: 89.693%
Loss D Fake: 0.2380 (0.3236) Acc D Fake: 97.876%
Loss D: 0.442
Loss G: 1.6372 (1.4806) Acc G: 1.375%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.3233 (0.2193) Acc D Real: 89.651%
Loss D Fake: 0.2337 (0.3230) Acc D Fake: 97.890%
Loss D: 0.557
Loss G: 1.6480 (1.4817) Acc G: 1.366%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.2966 (0.2198) Acc D Real: 89.622%
Loss D Fake: 0.2312 (0.3224) Acc D Fake: 97.904%
Loss D: 0.528
Loss G: 1.6546 (1.4828) Acc G: 1.357%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.1933 (0.2196) Acc D Real: 89.631%
Loss D Fake: 0.2295 (0.3219) Acc D Fake: 97.917%
Loss D: 0.423
Loss G: 1.6595 (1.4840) Acc G: 1.348%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3648 (0.2206) Acc D Real: 89.573%
Loss D Fake: 0.2283 (0.3213) Acc D Fake: 97.931%
Loss D: 0.593
Loss G: 1.6620 (1.4851) Acc G: 1.340%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.2418 (0.2207) Acc D Real: 89.561%
Loss D Fake: 0.2278 (0.3207) Acc D Fake: 97.944%
Loss D: 0.470
Loss G: 1.6631 (1.4862) Acc G: 1.331%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.2522 (0.2209) Acc D Real: 89.540%
Loss D Fake: 0.2275 (0.3201) Acc D Fake: 97.957%
Loss D: 0.480
Loss G: 1.6637 (1.4874) Acc G: 1.323%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3679 (0.2218) Acc D Real: 89.493%
Loss D Fake: 0.2274 (0.3195) Acc D Fake: 97.969%
Loss D: 0.595
Loss G: 1.6624 (1.4884) Acc G: 1.315%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2730 (0.2221) Acc D Real: 89.476%
Loss D Fake: 0.2278 (0.3189) Acc D Fake: 97.982%
Loss D: 0.501
Loss G: 1.6600 (1.4895) Acc G: 1.307%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.2775 (0.2225) Acc D Real: 89.454%
Loss D Fake: 0.2283 (0.3184) Acc D Fake: 97.994%
Loss D: 0.506
Loss G: 1.6571 (1.4905) Acc G: 1.299%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.3109 (0.2230) Acc D Real: 89.423%
Loss D Fake: 0.2290 (0.3178) Acc D Fake: 98.007%
Loss D: 0.540
Loss G: 1.6536 (1.4915) Acc G: 1.291%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2924 (0.2234) Acc D Real: 89.399%
Loss D Fake: 0.2298 (0.3173) Acc D Fake: 98.019%
Loss D: 0.522
Loss G: 1.6497 (1.4925) Acc G: 1.283%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.3538 (0.2242) Acc D Real: 89.343%
Loss D Fake: 0.2307 (0.3168) Acc D Fake: 98.031%
Loss D: 0.584
Loss G: 1.6452 (1.4934) Acc G: 1.275%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3887 (0.2252) Acc D Real: 89.277%
Loss D Fake: 0.2318 (0.3162) Acc D Fake: 98.043%
Loss D: 0.620
Loss G: 1.6395 (1.4943) Acc G: 1.267%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.2697 (0.2255) Acc D Real: 89.256%
Loss D Fake: 0.2332 (0.3158) Acc D Fake: 98.055%
Loss D: 0.503
Loss G: 1.6339 (1.4952) Acc G: 1.260%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.2245 (0.2255) Acc D Real: 89.252%
Loss D Fake: 0.2344 (0.3153) Acc D Fake: 98.066%
Loss D: 0.459
Loss G: 1.6292 (1.4960) Acc G: 1.252%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.2235 (0.2255) Acc D Real: 89.252%
Loss D Fake: 0.2354 (0.3148) Acc D Fake: 98.078%
Loss D: 0.459
Loss G: 1.6252 (1.4967) Acc G: 1.245%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.1841 (0.2252) Acc D Real: 89.262%
Loss D Fake: 0.2362 (0.3143) Acc D Fake: 98.089%
Loss D: 0.420
Loss G: 1.6221 (1.4975) Acc G: 1.237%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.2772 (0.2255) Acc D Real: 89.236%
Loss D Fake: 0.2370 (0.3139) Acc D Fake: 98.100%
Loss D: 0.514
Loss G: 1.6190 (1.4982) Acc G: 1.230%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3469 (0.2262) Acc D Real: 89.184%
Loss D Fake: 0.2378 (0.3134) Acc D Fake: 98.111%
Loss D: 0.585
Loss G: 1.6153 (1.4988) Acc G: 1.223%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.3222 (0.2268) Acc D Real: 89.147%
Loss D Fake: 0.2389 (0.3130) Acc D Fake: 98.122%
Loss D: 0.561
Loss G: 1.6109 (1.4995) Acc G: 1.216%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.3715 (0.2276) Acc D Real: 89.087%
Loss D Fake: 0.2401 (0.3126) Acc D Fake: 98.133%
Loss D: 0.612
Loss G: 1.6053 (1.5001) Acc G: 1.209%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.2381 (0.2277) Acc D Real: 89.080%
Loss D Fake: 0.2415 (0.3122) Acc D Fake: 98.143%
Loss D: 0.480
Loss G: 1.6000 (1.5007) Acc G: 1.202%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.2146 (0.2276) Acc D Real: 89.076%
Loss D Fake: 0.2428 (0.3118) Acc D Fake: 98.154%
Loss D: 0.457
Loss G: 1.5957 (1.5012) Acc G: 1.195%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2271 (0.2276) Acc D Real: 89.073%
Loss D Fake: 0.2438 (0.3114) Acc D Fake: 98.164%
Loss D: 0.471
Loss G: 1.5922 (1.5017) Acc G: 1.189%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2973 (0.2280) Acc D Real: 89.046%
Loss D Fake: 0.2447 (0.3110) Acc D Fake: 98.175%
Loss D: 0.542
Loss G: 1.5885 (1.5022) Acc G: 1.182%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.2810 (0.2283) Acc D Real: 89.024%
Loss D Fake: 0.2457 (0.3107) Acc D Fake: 98.185%
Loss D: 0.527
Loss G: 1.5846 (1.5027) Acc G: 1.175%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.2204 (0.2283) Acc D Real: 89.022%
Loss D Fake: 0.2467 (0.3103) Acc D Fake: 98.195%
Loss D: 0.467
Loss G: 1.5810 (1.5031) Acc G: 1.169%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3172 (0.2287) Acc D Real: 88.980%
Loss D Fake: 0.2477 (0.3100) Acc D Fake: 98.205%
Loss D: 0.565
Loss G: 1.5773 (1.5035) Acc G: 1.162%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.4019 (0.2297) Acc D Real: 88.914%
Loss D Fake: 0.2487 (0.3096) Acc D Fake: 98.215%
Loss D: 0.651
Loss G: 1.5725 (1.5039) Acc G: 1.156%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2338 (0.2297) Acc D Real: 88.909%
Loss D Fake: 0.2500 (0.3093) Acc D Fake: 98.225%
Loss D: 0.484
Loss G: 1.5680 (1.5043) Acc G: 1.150%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.1589 (0.2293) Acc D Real: 88.929%
Loss D Fake: 0.2510 (0.3090) Acc D Fake: 98.234%
Loss D: 0.410
Loss G: 1.5650 (1.5046) Acc G: 1.143%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2028 (0.2292) Acc D Real: 88.937%
Loss D Fake: 0.2517 (0.3087) Acc D Fake: 98.244%
Loss D: 0.455
Loss G: 1.5628 (1.5049) Acc G: 1.137%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.1130 (0.2286) Acc D Real: 88.967%
Loss D Fake: 0.2522 (0.3084) Acc D Fake: 98.253%
Loss D: 0.365
Loss G: 1.5621 (1.5052) Acc G: 1.131%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.1607 (0.2282) Acc D Real: 88.984%
Loss D Fake: 0.2522 (0.3081) Acc D Fake: 98.263%
Loss D: 0.413
Loss G: 1.5626 (1.5055) Acc G: 1.125%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3165 (0.2287) Acc D Real: 88.951%
Loss D Fake: 0.2522 (0.3078) Acc D Fake: 98.272%
Loss D: 0.569
Loss G: 1.5624 (1.5058) Acc G: 1.119%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.2340 (0.2287) Acc D Real: 88.946%
Loss D Fake: 0.2524 (0.3075) Acc D Fake: 98.281%
Loss D: 0.486
Loss G: 1.5618 (1.5061) Acc G: 1.113%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.3459 (0.2293) Acc D Real: 88.901%
Loss D Fake: 0.2526 (0.3072) Acc D Fake: 98.290%
Loss D: 0.599
Loss G: 1.5602 (1.5064) Acc G: 1.107%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.2948 (0.2297) Acc D Real: 88.872%
Loss D Fake: 0.2532 (0.3069) Acc D Fake: 98.299%
Loss D: 0.548
Loss G: 1.5579 (1.5067) Acc G: 1.101%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.1758 (0.2294) Acc D Real: 88.885%
Loss D Fake: 0.2538 (0.3066) Acc D Fake: 98.308%
Loss D: 0.430
Loss G: 1.5561 (1.5069) Acc G: 1.096%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.4105 (0.2303) Acc D Real: 88.815%
Loss D Fake: 0.2544 (0.3064) Acc D Fake: 98.317%
Loss D: 0.665
Loss G: 1.5530 (1.5072) Acc G: 1.090%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.1918 (0.2301) Acc D Real: 88.822%
Loss D Fake: 0.2553 (0.3061) Acc D Fake: 98.325%
Loss D: 0.447
Loss G: 1.5504 (1.5074) Acc G: 1.084%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.3055 (0.2305) Acc D Real: 88.787%
Loss D Fake: 0.2560 (0.3058) Acc D Fake: 98.334%
Loss D: 0.562
Loss G: 1.5469 (1.5076) Acc G: 1.079%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.2281 (0.2305) Acc D Real: 88.778%
Loss D Fake: 0.2571 (0.3056) Acc D Fake: 98.342%
Loss D: 0.485
Loss G: 1.5436 (1.5078) Acc G: 1.073%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3289 (0.2310) Acc D Real: 88.735%
Loss D Fake: 0.2581 (0.3054) Acc D Fake: 98.351%
Loss D: 0.587
Loss G: 1.5401 (1.5079) Acc G: 1.068%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2763 (0.2312) Acc D Real: 88.714%
Loss D Fake: 0.2592 (0.3051) Acc D Fake: 98.359%
Loss D: 0.535
Loss G: 1.5366 (1.5081) Acc G: 1.062%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.1614 (0.2309) Acc D Real: 88.734%
Loss D Fake: 0.2601 (0.3049) Acc D Fake: 98.367%
Loss D: 0.421
Loss G: 1.5342 (1.5082) Acc G: 1.057%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.1906 (0.2307) Acc D Real: 88.744%
Loss D Fake: 0.2607 (0.3047) Acc D Fake: 98.376%
Loss D: 0.451
Loss G: 1.5327 (1.5083) Acc G: 1.052%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2593 (0.2308) Acc D Real: 88.726%
Loss D Fake: 0.2611 (0.3045) Acc D Fake: 98.384%
Loss D: 0.520
Loss G: 1.5317 (1.5084) Acc G: 1.047%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.1394 (0.2304) Acc D Real: 88.754%
Loss D Fake: 0.2614 (0.3042) Acc D Fake: 98.392%
Loss D: 0.401
Loss G: 1.5315 (1.5086) Acc G: 1.041%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.1521 (0.2300) Acc D Real: 88.775%
Loss D Fake: 0.2614 (0.3040) Acc D Fake: 98.400%
Loss D: 0.414
Loss G: 1.5322 (1.5087) Acc G: 1.036%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.2399 (0.2300) Acc D Real: 88.765%
Loss D Fake: 0.2614 (0.3038) Acc D Fake: 98.407%
Loss D: 0.501
Loss G: 1.5328 (1.5088) Acc G: 1.031%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2664 (0.2302) Acc D Real: 88.748%
Loss D Fake: 0.2614 (0.3036) Acc D Fake: 98.415%
Loss D: 0.528
Loss G: 1.5329 (1.5089) Acc G: 1.026%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.2861 (0.2305) Acc D Real: 88.724%
Loss D Fake: 0.2616 (0.3034) Acc D Fake: 98.423%
Loss D: 0.548
Loss G: 1.5317 (1.5090) Acc G: 1.021%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.0932 (0.2298) Acc D Real: 88.762%
Loss D Fake: 0.2620 (0.3032) Acc D Fake: 98.430%
Loss D: 0.355
Loss G: 1.5317 (1.5091) Acc G: 1.016%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2230 (0.2298) Acc D Real: 88.754%
Loss D Fake: 0.2621 (0.3030) Acc D Fake: 98.438%
Loss D: 0.485
Loss G: 1.5323 (1.5092) Acc G: 1.011%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.1200 (0.2292) Acc D Real: 88.780%
Loss D Fake: 0.2620 (0.3028) Acc D Fake: 98.445%
Loss D: 0.382
Loss G: 1.5339 (1.5094) Acc G: 1.007%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.1681 (0.2290) Acc D Real: 88.792%
Loss D Fake: 0.2616 (0.3026) Acc D Fake: 98.453%
Loss D: 0.430
Loss G: 1.5360 (1.5095) Acc G: 1.002%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.1304 (0.2285) Acc D Real: 88.817%
Loss D Fake: 0.2611 (0.3024) Acc D Fake: 98.460%
Loss D: 0.391
Loss G: 1.5390 (1.5096) Acc G: 0.997%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.2546 (0.2286) Acc D Real: 88.804%
Loss D Fake: 0.2604 (0.3022) Acc D Fake: 98.467%
Loss D: 0.515
Loss G: 1.5413 (1.5098) Acc G: 0.992%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2545 (0.2287) Acc D Real: 88.791%
Loss D Fake: 0.2600 (0.3020) Acc D Fake: 98.475%
Loss D: 0.515
Loss G: 1.5429 (1.5099) Acc G: 0.988%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.2260 (0.2287) Acc D Real: 88.787%
Loss D Fake: 0.2598 (0.3018) Acc D Fake: 98.482%
Loss D: 0.486
Loss G: 1.5437 (1.5101) Acc G: 0.983%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.1836 (0.2285) Acc D Real: 88.793%
Loss D Fake: 0.2598 (0.3016) Acc D Fake: 98.489%
Loss D: 0.443
Loss G: 1.5447 (1.5103) Acc G: 0.978%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.1978 (0.2284) Acc D Real: 88.797%
Loss D Fake: 0.2596 (0.3014) Acc D Fake: 98.496%
Loss D: 0.457
Loss G: 1.5458 (1.5104) Acc G: 0.974%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2517 (0.2285) Acc D Real: 88.781%
Loss D Fake: 0.2595 (0.3012) Acc D Fake: 98.503%
Loss D: 0.511
Loss G: 1.5465 (1.5106) Acc G: 0.969%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.1820 (0.2283) Acc D Real: 88.788%
Loss D Fake: 0.2595 (0.3011) Acc D Fake: 98.510%
Loss D: 0.441
Loss G: 1.5475 (1.5108) Acc G: 0.965%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.0766 (0.2276) Acc D Real: 88.825%
Loss D Fake: 0.2591 (0.3009) Acc D Fake: 98.516%
Loss D: 0.336
Loss G: 1.5501 (1.5109) Acc G: 0.961%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.1631 (0.2273) Acc D Real: 88.837%
Loss D Fake: 0.2584 (0.3007) Acc D Fake: 98.523%
Loss D: 0.422
Loss G: 1.5534 (1.5111) Acc G: 0.956%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2386 (0.2273) Acc D Real: 88.825%
Loss D Fake: 0.2578 (0.3005) Acc D Fake: 98.530%
Loss D: 0.496
Loss G: 1.5560 (1.5113) Acc G: 0.952%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2601 (0.2275) Acc D Real: 88.812%
Loss D Fake: 0.2573 (0.3003) Acc D Fake: 98.537%
Loss D: 0.517
Loss G: 1.5577 (1.5115) Acc G: 0.948%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2163 (0.2274) Acc D Real: 88.806%
Loss D Fake: 0.2570 (0.3001) Acc D Fake: 98.543%
Loss D: 0.473
Loss G: 1.5592 (1.5118) Acc G: 0.943%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.1461 (0.2271) Acc D Real: 88.825%
Loss D Fake: 0.2567 (0.2999) Acc D Fake: 98.550%
Loss D: 0.403
Loss G: 1.5613 (1.5120) Acc G: 0.939%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.1787 (0.2268) Acc D Real: 88.832%
Loss D Fake: 0.2561 (0.2997) Acc D Fake: 98.556%
Loss D: 0.435
Loss G: 1.5636 (1.5122) Acc G: 0.935%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.2808 (0.2271) Acc D Real: 88.827%
Loss D Fake: 0.2557 (0.2995) Acc D Fake: 98.558%
Loss D: 0.536
Loss G: 1.5652 (1.5124) Acc G: 0.934%
LR: 2.000e-04
Epoch: 19/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.2062 (0.1761) Acc D Real: 91.719%
Loss D Fake: 0.2549 (0.2551) Acc D Fake: 100.000%
Loss D: 0.461
Loss G: 1.5688 (1.5680) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.1916 (0.1812) Acc D Real: 91.181%
Loss D Fake: 0.2546 (0.2549) Acc D Fake: 100.000%
Loss D: 0.446
Loss G: 1.5705 (1.5688) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.1617 (0.1764) Acc D Real: 91.419%
Loss D Fake: 0.2541 (0.2547) Acc D Fake: 100.000%
Loss D: 0.416
Loss G: 1.5726 (1.5698) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.2117 (0.1834) Acc D Real: 90.969%
Loss D Fake: 0.2537 (0.2545) Acc D Fake: 100.000%
Loss D: 0.465
Loss G: 1.5744 (1.5707) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.1461 (0.1772) Acc D Real: 91.293%
Loss D Fake: 0.2532 (0.2543) Acc D Fake: 100.000%
Loss D: 0.399
Loss G: 1.5767 (1.5717) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.1305 (0.1705) Acc D Real: 91.734%
Loss D Fake: 0.2526 (0.2541) Acc D Fake: 100.000%
Loss D: 0.383
Loss G: 1.5796 (1.5728) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.1257 (0.1649) Acc D Real: 92.064%
Loss D Fake: 0.2519 (0.2538) Acc D Fake: 100.000%
Loss D: 0.378
Loss G: 1.5831 (1.5741) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2848 (0.1782) Acc D Real: 91.175%
Loss D Fake: 0.2512 (0.2535) Acc D Fake: 100.000%
Loss D: 0.536
Loss G: 1.5856 (1.5754) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.2264 (0.1831) Acc D Real: 90.792%
Loss D Fake: 0.2507 (0.2532) Acc D Fake: 100.000%
Loss D: 0.477
Loss G: 1.5875 (1.5766) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.1999 (0.1846) Acc D Real: 90.587%
Loss D Fake: 0.2503 (0.2530) Acc D Fake: 100.000%
Loss D: 0.450
Loss G: 1.5890 (1.5777) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.1344 (0.1804) Acc D Real: 90.864%
Loss D Fake: 0.2501 (0.2527) Acc D Fake: 100.000%
Loss D: 0.384
Loss G: 1.5908 (1.5788) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.1039 (0.1745) Acc D Real: 91.242%
Loss D Fake: 0.2497 (0.2525) Acc D Fake: 100.000%
Loss D: 0.354
Loss G: 1.5935 (1.5800) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.1914 (0.1757) Acc D Real: 91.138%
Loss D Fake: 0.2491 (0.2522) Acc D Fake: 100.000%
Loss D: 0.441
Loss G: 1.5961 (1.5811) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.1623 (0.1748) Acc D Real: 91.198%
Loss D Fake: 0.2487 (0.2520) Acc D Fake: 100.000%
Loss D: 0.411
Loss G: 1.5985 (1.5823) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3061 (0.1830) Acc D Real: 90.684%
Loss D Fake: 0.2484 (0.2518) Acc D Fake: 100.000%
Loss D: 0.555
Loss G: 1.5995 (1.5833) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.1423 (0.1806) Acc D Real: 90.803%
Loss D Fake: 0.2483 (0.2516) Acc D Fake: 100.000%
Loss D: 0.391
Loss G: 1.6006 (1.5844) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.1801 (0.1806) Acc D Real: 90.755%
Loss D Fake: 0.2482 (0.2514) Acc D Fake: 100.000%
Loss D: 0.428
Loss G: 1.6020 (1.5853) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.1930 (0.1813) Acc D Real: 90.694%
Loss D Fake: 0.2482 (0.2512) Acc D Fake: 100.000%
Loss D: 0.441
Loss G: 1.6018 (1.5862) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.1485 (0.1796) Acc D Real: 90.807%
Loss D Fake: 0.2489 (0.2511) Acc D Fake: 100.000%
Loss D: 0.397
Loss G: 1.6015 (1.5870) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.1074 (0.1762) Acc D Real: 91.019%
Loss D Fake: 0.2494 (0.2510) Acc D Fake: 100.000%
Loss D: 0.357
Loss G: 1.6023 (1.5877) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1184 (0.1736) Acc D Real: 91.203%
Loss D Fake: 0.2496 (0.2510) Acc D Fake: 100.000%
Loss D: 0.368
Loss G: 1.6039 (1.5884) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.1335 (0.1718) Acc D Real: 91.293%
Loss D Fake: 0.2495 (0.2509) Acc D Fake: 100.000%
Loss D: 0.383
Loss G: 1.6061 (1.5892) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.1743 (0.1719) Acc D Real: 91.324%
Loss D Fake: 0.2493 (0.2508) Acc D Fake: 100.000%
Loss D: 0.424
Loss G: 1.6083 (1.5900) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.1201 (0.1698) Acc D Real: 91.467%
Loss D Fake: 0.2491 (0.2508) Acc D Fake: 100.000%
Loss D: 0.369
Loss G: 1.6111 (1.5908) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.1078 (0.1675) Acc D Real: 91.617%
Loss D Fake: 0.2486 (0.2507) Acc D Fake: 100.000%
Loss D: 0.356
Loss G: 1.6148 (1.5918) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.1518 (0.1669) Acc D Real: 91.657%
Loss D Fake: 0.2479 (0.2506) Acc D Fake: 100.000%
Loss D: 0.400
Loss G: 1.6188 (1.5928) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.1326 (0.1657) Acc D Real: 91.717%
Loss D Fake: 0.2471 (0.2505) Acc D Fake: 100.000%
Loss D: 0.380
Loss G: 1.6231 (1.5939) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.1611 (0.1655) Acc D Real: 91.749%
Loss D Fake: 0.2462 (0.2503) Acc D Fake: 100.000%
Loss D: 0.407
Loss G: 1.6274 (1.5950) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.1717 (0.1657) Acc D Real: 91.757%
Loss D Fake: 0.2453 (0.2501) Acc D Fake: 100.000%
Loss D: 0.417
Loss G: 1.6311 (1.5962) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.1050 (0.1638) Acc D Real: 91.880%
Loss D Fake: 0.2446 (0.2500) Acc D Fake: 100.000%
Loss D: 0.350
Loss G: 1.6353 (1.5975) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1036 (0.1619) Acc D Real: 92.005%
Loss D Fake: 0.2436 (0.2498) Acc D Fake: 100.000%
Loss D: 0.347
Loss G: 1.6402 (1.5988) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2577 (0.1648) Acc D Real: 91.848%
Loss D Fake: 0.2426 (0.2495) Acc D Fake: 100.000%
Loss D: 0.500
Loss G: 1.6440 (1.6002) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.1428 (0.1641) Acc D Real: 91.884%
Loss D Fake: 0.2418 (0.2493) Acc D Fake: 100.000%
Loss D: 0.385
Loss G: 1.6476 (1.6016) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.0893 (0.1620) Acc D Real: 92.013%
Loss D Fake: 0.2410 (0.2491) Acc D Fake: 100.000%
Loss D: 0.330
Loss G: 1.6521 (1.6030) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.1271 (0.1610) Acc D Real: 92.060%
Loss D Fake: 0.2399 (0.2488) Acc D Fake: 100.000%
Loss D: 0.367
Loss G: 1.6570 (1.6045) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.1509 (0.1608) Acc D Real: 92.044%
Loss D Fake: 0.2388 (0.2486) Acc D Fake: 100.000%
Loss D: 0.390
Loss G: 1.6616 (1.6061) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2545 (0.1632) Acc D Real: 91.902%
Loss D Fake: 0.2380 (0.2483) Acc D Fake: 100.000%
Loss D: 0.492
Loss G: 1.6646 (1.6076) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.1253 (0.1622) Acc D Real: 91.942%
Loss D Fake: 0.2375 (0.2480) Acc D Fake: 100.000%
Loss D: 0.363
Loss G: 1.6677 (1.6091) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.1799 (0.1627) Acc D Real: 91.915%
Loss D Fake: 0.2368 (0.2477) Acc D Fake: 100.000%
Loss D: 0.417
Loss G: 1.6709 (1.6107) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.2139 (0.1639) Acc D Real: 91.852%
Loss D Fake: 0.2362 (0.2474) Acc D Fake: 100.000%
Loss D: 0.450
Loss G: 1.6735 (1.6122) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.1570 (0.1638) Acc D Real: 91.855%
Loss D Fake: 0.2357 (0.2472) Acc D Fake: 100.000%
Loss D: 0.393
Loss G: 1.6759 (1.6137) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.1718 (0.1640) Acc D Real: 91.850%
Loss D Fake: 0.2353 (0.2469) Acc D Fake: 100.000%
Loss D: 0.407
Loss G: 1.6781 (1.6152) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.2151 (0.1651) Acc D Real: 91.776%
Loss D Fake: 0.2349 (0.2466) Acc D Fake: 100.000%
Loss D: 0.450
Loss G: 1.6799 (1.6167) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.1405 (0.1646) Acc D Real: 91.787%
Loss D Fake: 0.2345 (0.2463) Acc D Fake: 100.000%
Loss D: 0.375
Loss G: 1.6821 (1.6182) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.1887 (0.1651) Acc D Real: 91.770%
Loss D Fake: 0.2341 (0.2461) Acc D Fake: 100.000%
Loss D: 0.423
Loss G: 1.6841 (1.6196) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.1765 (0.1653) Acc D Real: 91.746%
Loss D Fake: 0.2336 (0.2458) Acc D Fake: 100.000%
Loss D: 0.410
Loss G: 1.6862 (1.6210) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.1014 (0.1640) Acc D Real: 91.836%
Loss D Fake: 0.2332 (0.2455) Acc D Fake: 100.000%
Loss D: 0.335
Loss G: 1.6890 (1.6224) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.2516 (0.1658) Acc D Real: 91.729%
Loss D Fake: 0.2326 (0.2453) Acc D Fake: 100.000%
Loss D: 0.484
Loss G: 1.6909 (1.6238) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.1240 (0.1650) Acc D Real: 91.756%
Loss D Fake: 0.2435 (0.2452) Acc D Fake: 100.000%
Loss D: 0.367
Loss G: 0.2742 (1.5968) Acc G: 1.733%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.1553 (0.1648) Acc D Real: 91.773%
Loss D Fake: 1.6103 (0.2720) Acc D Fake: 98.268%
Loss D: 1.766
Loss G: 0.2708 (1.5708) Acc G: 3.431%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.0915 (0.1634) Acc D Real: 91.859%
Loss D Fake: 1.5937 (0.2974) Acc D Fake: 96.603%
Loss D: 1.685
Loss G: 0.2772 (1.5459) Acc G: 5.096%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.1055 (0.1623) Acc D Real: 91.929%
Loss D Fake: 1.5640 (0.3213) Acc D Fake: 94.969%
Loss D: 1.669
Loss G: 0.2869 (1.5222) Acc G: 6.698%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.1374 (0.1618) Acc D Real: 91.976%
Loss D Fake: 1.5273 (0.3437) Acc D Fake: 93.395%
Loss D: 1.665
Loss G: 0.2986 (1.4995) Acc G: 8.241%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.1309 (0.1612) Acc D Real: 92.015%
Loss D Fake: 1.4865 (0.3644) Acc D Fake: 91.879%
Loss D: 1.617
Loss G: 0.3121 (1.4779) Acc G: 9.667%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.0972 (0.1601) Acc D Real: 92.082%
Loss D Fake: 1.4422 (0.3837) Acc D Fake: 90.506%
Loss D: 1.539
Loss G: 0.3273 (1.4574) Acc G: 11.012%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.1512 (0.1599) Acc D Real: 92.109%
Loss D Fake: 1.3951 (0.4014) Acc D Fake: 89.211%
Loss D: 1.546
Loss G: 0.3438 (1.4379) Acc G: 12.281%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.2013 (0.1607) Acc D Real: 92.053%
Loss D Fake: 1.3471 (0.4177) Acc D Fake: 87.960%
Loss D: 1.548
Loss G: 0.3614 (1.4193) Acc G: 13.477%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.1924 (0.1612) Acc D Real: 92.033%
Loss D Fake: 1.2992 (0.4327) Acc D Fake: 86.780%
Loss D: 1.492
Loss G: 0.3799 (1.4017) Acc G: 14.633%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.1482 (0.1610) Acc D Real: 92.046%
Loss D Fake: 1.2503 (0.4463) Acc D Fake: 85.667%
Loss D: 1.399
Loss G: 0.4003 (1.3850) Acc G: 15.722%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.1794 (0.1613) Acc D Real: 92.019%
Loss D Fake: 1.1988 (0.4586) Acc D Fake: 84.590%
Loss D: 1.378
Loss G: 0.4228 (1.3692) Acc G: 16.749%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.1841 (0.1616) Acc D Real: 91.983%
Loss D Fake: 1.1439 (0.4697) Acc D Fake: 83.575%
Loss D: 1.328
Loss G: 0.4488 (1.3544) Acc G: 17.715%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.0753 (0.1603) Acc D Real: 92.063%
Loss D Fake: 1.0786 (0.4794) Acc D Fake: 82.619%
Loss D: 1.154
Loss G: 0.4841 (1.3406) Acc G: 18.624%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.1395 (0.1600) Acc D Real: 92.091%
Loss D Fake: 1.0006 (0.4875) Acc D Fake: 81.719%
Loss D: 1.140
Loss G: 0.5201 (1.3277) Acc G: 19.479%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.2762 (0.1617) Acc D Real: 92.007%
Loss D Fake: 0.9437 (0.4945) Acc D Fake: 80.897%
Loss D: 1.220
Loss G: 0.5524 (1.3158) Acc G: 20.282%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.2086 (0.1625) Acc D Real: 91.986%
Loss D Fake: 0.8934 (0.5006) Acc D Fake: 80.126%
Loss D: 1.102
Loss G: 0.5867 (1.3048) Acc G: 21.010%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.2715 (0.1641) Acc D Real: 91.885%
Loss D Fake: 0.8440 (0.5057) Acc D Fake: 79.428%
Loss D: 1.115
Loss G: 0.6229 (1.2946) Acc G: 21.667%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.2428 (0.1652) Acc D Real: 91.821%
Loss D Fake: 0.7968 (0.5100) Acc D Fake: 78.799%
Loss D: 1.040
Loss G: 0.6614 (1.2853) Acc G: 22.230%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.2811 (0.1669) Acc D Real: 91.707%
Loss D Fake: 0.7506 (0.5135) Acc D Fake: 78.261%
Loss D: 1.032
Loss G: 0.7038 (1.2768) Acc G: 22.705%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.1618 (0.1668) Acc D Real: 91.702%
Loss D Fake: 0.7009 (0.5161) Acc D Fake: 77.833%
Loss D: 0.863
Loss G: 0.7627 (1.2695) Acc G: 23.024%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2636 (0.1682) Acc D Real: 91.563%
Loss D Fake: 0.6358 (0.5178) Acc D Fake: 77.582%
Loss D: 0.899
Loss G: 0.8511 (1.2636) Acc G: 23.099%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3428 (0.1706) Acc D Real: 91.348%
Loss D Fake: 0.5414 (0.5181) Acc D Fake: 77.662%
Loss D: 0.884
Loss G: 0.9986 (1.2599) Acc G: 22.778%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3687 (0.1733) Acc D Real: 91.109%
Loss D Fake: 0.4343 (0.5170) Acc D Fake: 77.968%
Loss D: 0.803
Loss G: 1.1338 (1.2582) Acc G: 22.466%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3934 (0.1763) Acc D Real: 90.838%
Loss D Fake: 0.3825 (0.5152) Acc D Fake: 78.266%
Loss D: 0.776
Loss G: 1.1757 (1.2571) Acc G: 22.162%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3374 (0.1785) Acc D Real: 90.636%
Loss D Fake: 0.3721 (0.5133) Acc D Fake: 78.556%
Loss D: 0.709
Loss G: 1.1934 (1.2562) Acc G: 21.867%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.3691 (0.1810) Acc D Real: 90.401%
Loss D Fake: 0.3663 (0.5113) Acc D Fake: 78.838%
Loss D: 0.735
Loss G: 1.2049 (1.2556) Acc G: 21.579%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.3440 (0.1831) Acc D Real: 90.183%
Loss D Fake: 0.3625 (0.5094) Acc D Fake: 79.113%
Loss D: 0.706
Loss G: 1.2125 (1.2550) Acc G: 21.299%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.4539 (0.1866) Acc D Real: 89.846%
Loss D Fake: 0.3605 (0.5075) Acc D Fake: 79.380%
Loss D: 0.814
Loss G: 1.2147 (1.2545) Acc G: 21.026%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.3604 (0.1888) Acc D Real: 89.630%
Loss D Fake: 0.3602 (0.5056) Acc D Fake: 79.641%
Loss D: 0.721
Loss G: 1.2147 (1.2540) Acc G: 20.759%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.4202 (0.1917) Acc D Real: 89.345%
Loss D Fake: 0.3606 (0.5038) Acc D Fake: 79.896%
Loss D: 0.781
Loss G: 1.2125 (1.2535) Acc G: 20.500%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.3828 (0.1940) Acc D Real: 89.104%
Loss D Fake: 0.3618 (0.5021) Acc D Fake: 80.144%
Loss D: 0.745
Loss G: 1.2088 (1.2529) Acc G: 20.247%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3808 (0.1963) Acc D Real: 88.892%
Loss D Fake: 0.3635 (0.5004) Acc D Fake: 80.386%
Loss D: 0.744
Loss G: 1.2046 (1.2523) Acc G: 20.000%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3876 (0.1986) Acc D Real: 88.667%
Loss D Fake: 0.3653 (0.4988) Acc D Fake: 80.622%
Loss D: 0.753
Loss G: 1.1996 (1.2517) Acc G: 19.759%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.2749 (0.1995) Acc D Real: 88.550%
Loss D Fake: 0.3672 (0.4972) Acc D Fake: 80.853%
Loss D: 0.642
Loss G: 1.1957 (1.2510) Acc G: 19.524%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.2407 (0.2000) Acc D Real: 88.468%
Loss D Fake: 0.3685 (0.4957) Acc D Fake: 81.078%
Loss D: 0.609
Loss G: 1.1938 (1.2503) Acc G: 19.294%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.4156 (0.2025) Acc D Real: 88.224%
Loss D Fake: 0.3693 (0.4942) Acc D Fake: 81.298%
Loss D: 0.785
Loss G: 1.1907 (1.2497) Acc G: 19.070%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.5334 (0.2063) Acc D Real: 87.872%
Loss D Fake: 0.3712 (0.4928) Acc D Fake: 81.513%
Loss D: 0.905
Loss G: 1.1839 (1.2489) Acc G: 18.851%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3581 (0.2080) Acc D Real: 87.696%
Loss D Fake: 0.3743 (0.4914) Acc D Fake: 81.723%
Loss D: 0.732
Loss G: 1.1769 (1.2481) Acc G: 18.636%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3825 (0.2100) Acc D Real: 87.495%
Loss D Fake: 0.3772 (0.4902) Acc D Fake: 81.929%
Loss D: 0.760
Loss G: 1.1699 (1.2472) Acc G: 18.427%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.3063 (0.2111) Acc D Real: 87.366%
Loss D Fake: 0.3801 (0.4889) Acc D Fake: 82.130%
Loss D: 0.686
Loss G: 1.1639 (1.2463) Acc G: 18.222%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3402 (0.2125) Acc D Real: 87.220%
Loss D Fake: 0.3825 (0.4878) Acc D Fake: 82.326%
Loss D: 0.723
Loss G: 1.1587 (1.2453) Acc G: 18.022%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.3744 (0.2142) Acc D Real: 87.033%
Loss D Fake: 0.3847 (0.4866) Acc D Fake: 82.518%
Loss D: 0.759
Loss G: 1.1533 (1.2443) Acc G: 17.826%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3542 (0.2157) Acc D Real: 86.858%
Loss D Fake: 0.3872 (0.4856) Acc D Fake: 82.706%
Loss D: 0.741
Loss G: 1.1476 (1.2433) Acc G: 17.634%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.5780 (0.2196) Acc D Real: 86.492%
Loss D Fake: 0.3903 (0.4846) Acc D Fake: 82.890%
Loss D: 0.968
Loss G: 1.1381 (1.2422) Acc G: 17.447%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.3469 (0.2209) Acc D Real: 86.332%
Loss D Fake: 0.3950 (0.4836) Acc D Fake: 83.070%
Loss D: 0.742
Loss G: 1.1279 (1.2410) Acc G: 17.263%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2172 (0.2209) Acc D Real: 86.306%
Loss D Fake: 0.3992 (0.4827) Acc D Fake: 83.247%
Loss D: 0.616
Loss G: 1.1203 (1.2397) Acc G: 17.083%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.4483 (0.2232) Acc D Real: 86.064%
Loss D Fake: 0.4026 (0.4819) Acc D Fake: 83.419%
Loss D: 0.851
Loss G: 1.1122 (1.2384) Acc G: 16.907%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2887 (0.2239) Acc D Real: 85.964%
Loss D Fake: 0.4064 (0.4811) Acc D Fake: 83.588%
Loss D: 0.695
Loss G: 1.1046 (1.2370) Acc G: 16.735%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2961 (0.2246) Acc D Real: 85.863%
Loss D Fake: 0.4096 (0.4804) Acc D Fake: 83.754%
Loss D: 0.706
Loss G: 1.0986 (1.2356) Acc G: 16.566%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.2482 (0.2249) Acc D Real: 85.806%
Loss D Fake: 0.4121 (0.4797) Acc D Fake: 83.917%
Loss D: 0.660
Loss G: 1.0943 (1.2342) Acc G: 16.400%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.2684 (0.2253) Acc D Real: 85.736%
Loss D Fake: 0.4145 (0.4791) Acc D Fake: 84.076%
Loss D: 0.683
Loss G: 1.0875 (1.2328) Acc G: 16.238%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.3024 (0.2261) Acc D Real: 85.639%
Loss D Fake: 0.4181 (0.4785) Acc D Fake: 84.232%
Loss D: 0.721
Loss G: 1.0802 (1.2313) Acc G: 16.078%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4160 (0.2279) Acc D Real: 85.437%
Loss D Fake: 0.4229 (0.4780) Acc D Fake: 84.385%
Loss D: 0.839
Loss G: 1.0645 (1.2296) Acc G: 15.922%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.3250 (0.2288) Acc D Real: 85.323%
Loss D Fake: 0.4374 (0.4776) Acc D Fake: 84.535%
Loss D: 0.762
Loss G: 1.0084 (1.2275) Acc G: 15.769%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.2457 (0.2290) Acc D Real: 85.285%
Loss D Fake: 0.4983 (0.4778) Acc D Fake: 84.683%
Loss D: 0.744
Loss G: 0.9757 (1.2251) Acc G: 15.619%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.2295 (0.2290) Acc D Real: 85.254%
Loss D Fake: 0.4712 (0.4777) Acc D Fake: 84.827%
Loss D: 0.701
Loss G: 1.0573 (1.2235) Acc G: 15.472%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.3299 (0.2299) Acc D Real: 85.151%
Loss D Fake: 0.4246 (0.4772) Acc D Fake: 84.969%
Loss D: 0.754
Loss G: 1.0775 (1.2222) Acc G: 15.327%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.4727 (0.2322) Acc D Real: 84.939%
Loss D Fake: 0.4194 (0.4767) Acc D Fake: 85.108%
Loss D: 0.892
Loss G: 1.0832 (1.2209) Acc G: 15.185%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3355 (0.2331) Acc D Real: 84.851%
Loss D Fake: 0.4182 (0.4761) Acc D Fake: 85.245%
Loss D: 0.754
Loss G: 1.0834 (1.2196) Acc G: 15.046%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.4480 (0.2351) Acc D Real: 84.668%
Loss D Fake: 0.4192 (0.4756) Acc D Fake: 85.379%
Loss D: 0.867
Loss G: 1.0791 (1.2183) Acc G: 14.909%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.5424 (0.2379) Acc D Real: 84.388%
Loss D Fake: 0.4233 (0.4751) Acc D Fake: 85.511%
Loss D: 0.966
Loss G: 1.0648 (1.2170) Acc G: 14.775%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3595 (0.2389) Acc D Real: 84.293%
Loss D Fake: 0.4334 (0.4748) Acc D Fake: 85.640%
Loss D: 0.793
Loss G: 1.0382 (1.2154) Acc G: 14.643%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.3853 (0.2402) Acc D Real: 84.168%
Loss D Fake: 0.4689 (0.4747) Acc D Fake: 85.767%
Loss D: 0.854
Loss G: 1.0437 (1.2138) Acc G: 14.513%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.2590 (0.2404) Acc D Real: 84.152%
Loss D Fake: 0.4335 (0.4744) Acc D Fake: 85.892%
Loss D: 0.692
Loss G: 1.0592 (1.2125) Acc G: 14.386%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.2880 (0.2408) Acc D Real: 84.106%
Loss D Fake: 0.4286 (0.4740) Acc D Fake: 86.014%
Loss D: 0.717
Loss G: 1.0656 (1.2112) Acc G: 14.261%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.2798 (0.2412) Acc D Real: 84.078%
Loss D Fake: 0.4262 (0.4735) Acc D Fake: 86.135%
Loss D: 0.706
Loss G: 1.0698 (1.2100) Acc G: 14.138%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.2303 (0.2411) Acc D Real: 84.075%
Loss D Fake: 0.4243 (0.4731) Acc D Fake: 86.254%
Loss D: 0.655
Loss G: 1.0732 (1.2088) Acc G: 14.017%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.3616 (0.2421) Acc D Real: 83.977%
Loss D Fake: 0.4234 (0.4727) Acc D Fake: 86.370%
Loss D: 0.785
Loss G: 1.0723 (1.2077) Acc G: 13.898%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.2467 (0.2421) Acc D Real: 83.974%
Loss D Fake: 0.4247 (0.4723) Acc D Fake: 86.485%
Loss D: 0.671
Loss G: 1.0693 (1.2065) Acc G: 13.782%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.3298 (0.2429) Acc D Real: 83.909%
Loss D Fake: 0.4268 (0.4719) Acc D Fake: 86.597%
Loss D: 0.757
Loss G: 1.0631 (1.2053) Acc G: 13.667%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.3222 (0.2435) Acc D Real: 83.827%
Loss D Fake: 0.4312 (0.4716) Acc D Fake: 86.708%
Loss D: 0.753
Loss G: 1.0522 (1.2040) Acc G: 13.554%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.3545 (0.2444) Acc D Real: 83.722%
Loss D Fake: 0.4397 (0.4713) Acc D Fake: 86.817%
Loss D: 0.794
Loss G: 1.0350 (1.2027) Acc G: 13.443%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2786 (0.2447) Acc D Real: 83.684%
Loss D Fake: 0.4568 (0.4712) Acc D Fake: 86.924%
Loss D: 0.735
Loss G: 1.0648 (1.2015) Acc G: 13.333%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.3716 (0.2457) Acc D Real: 83.585%
Loss D Fake: 0.4213 (0.4708) Acc D Fake: 87.030%
Loss D: 0.793
Loss G: 1.0860 (1.2006) Acc G: 13.226%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3284 (0.2464) Acc D Real: 83.517%
Loss D Fake: 0.4144 (0.4704) Acc D Fake: 87.133%
Loss D: 0.743
Loss G: 1.0968 (1.1998) Acc G: 13.120%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.2988 (0.2468) Acc D Real: 83.462%
Loss D Fake: 0.4104 (0.4699) Acc D Fake: 87.235%
Loss D: 0.709
Loss G: 1.1034 (1.1990) Acc G: 13.016%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.2721 (0.2470) Acc D Real: 83.432%
Loss D Fake: 0.4081 (0.4694) Acc D Fake: 87.336%
Loss D: 0.680
Loss G: 1.1070 (1.1983) Acc G: 12.913%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.3476 (0.2478) Acc D Real: 83.331%
Loss D Fake: 0.4070 (0.4689) Acc D Fake: 87.435%
Loss D: 0.755
Loss G: 1.1081 (1.1976) Acc G: 12.812%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3340 (0.2485) Acc D Real: 83.257%
Loss D Fake: 0.4080 (0.4684) Acc D Fake: 87.532%
Loss D: 0.742
Loss G: 1.1012 (1.1968) Acc G: 12.713%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3876 (0.2495) Acc D Real: 83.160%
Loss D Fake: 0.4131 (0.4680) Acc D Fake: 87.628%
Loss D: 0.801
Loss G: 1.0870 (1.1960) Acc G: 12.615%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3721 (0.2505) Acc D Real: 83.053%
Loss D Fake: 0.4242 (0.4677) Acc D Fake: 87.723%
Loss D: 0.796
Loss G: 0.9122 (1.1938) Acc G: 12.659%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.3404 (0.2511) Acc D Real: 83.009%
Loss D Fake: 0.7021 (0.4694) Acc D Fake: 87.462%
Loss D: 1.042
Loss G: 0.7173 (1.1902) Acc G: 12.942%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.2918 (0.2514) Acc D Real: 82.987%
Loss D Fake: 0.7451 (0.4715) Acc D Fake: 87.155%
Loss D: 1.037
Loss G: 0.6980 (1.1865) Acc G: 13.246%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.3949 (0.2525) Acc D Real: 82.910%
Loss D Fake: 0.7457 (0.4736) Acc D Fake: 86.847%
Loss D: 1.141
Loss G: 0.7099 (1.1830) Acc G: 13.532%
LR: 2.000e-04
TRAIN Iteration: [ 135/226]
Loss D Real: 0.2475 (0.2525) Acc D Real: 82.922%
Loss D Fake: 0.7196 (0.4754) Acc D Fake: 86.574%
Loss D: 0.967
Loss G: 0.7706 (1.1799) Acc G: 13.741%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.2249 (0.2523) Acc D Real: 82.933%
Loss D Fake: 0.6971 (0.4770) Acc D Fake: 86.329%
Loss D: 0.922
Loss G: 0.7108 (1.1764) Acc G: 14.020%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.2011 (0.2519) Acc D Real: 82.957%
Loss D Fake: 0.7482 (0.4790) Acc D Fake: 86.015%
Loss D: 0.949
Loss G: 0.6828 (1.1728) Acc G: 14.331%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.2939 (0.2522) Acc D Real: 82.962%
Loss D Fake: 0.7565 (0.4810) Acc D Fake: 85.694%
Loss D: 1.050
Loss G: 0.6785 (1.1693) Acc G: 14.650%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.4294 (0.2535) Acc D Real: 82.905%
Loss D Fake: 0.7546 (0.4830) Acc D Fake: 85.365%
Loss D: 1.184
Loss G: 0.6781 (1.1657) Acc G: 14.976%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.2070 (0.2532) Acc D Real: 82.955%
Loss D Fake: 0.7502 (0.4849) Acc D Fake: 85.041%
Loss D: 0.957
Loss G: 0.6845 (1.1623) Acc G: 15.286%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3516 (0.2538) Acc D Real: 82.916%
Loss D Fake: 0.7398 (0.4867) Acc D Fake: 84.734%
Loss D: 1.091
Loss G: 0.6940 (1.1590) Acc G: 15.591%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.3416 (0.2545) Acc D Real: 82.909%
Loss D Fake: 0.7287 (0.4884) Acc D Fake: 84.442%
Loss D: 1.070
Loss G: 0.7033 (1.1558) Acc G: 15.880%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2531 (0.2545) Acc D Real: 82.920%
Loss D Fake: 0.7173 (0.4900) Acc D Fake: 84.166%
Loss D: 0.970
Loss G: 0.7163 (1.1527) Acc G: 16.154%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.2867 (0.2547) Acc D Real: 82.920%
Loss D Fake: 0.7032 (0.4915) Acc D Fake: 83.906%
Loss D: 0.990
Loss G: 0.7289 (1.1497) Acc G: 16.400%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.1657 (0.2541) Acc D Real: 82.965%
Loss D Fake: 0.6887 (0.4928) Acc D Fake: 83.661%
Loss D: 0.854
Loss G: 0.7474 (1.1470) Acc G: 16.632%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.3319 (0.2546) Acc D Real: 82.929%
Loss D Fake: 0.6692 (0.4940) Acc D Fake: 83.441%
Loss D: 1.001
Loss G: 0.7679 (1.1444) Acc G: 16.826%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.3601 (0.2553) Acc D Real: 82.866%
Loss D Fake: 0.6506 (0.4951) Acc D Fake: 83.259%
Loss D: 1.011
Loss G: 0.7878 (1.1419) Acc G: 16.995%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.3319 (0.2558) Acc D Real: 82.822%
Loss D Fake: 0.6334 (0.4960) Acc D Fake: 83.102%
Loss D: 0.965
Loss G: 0.8047 (1.1397) Acc G: 17.128%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3225 (0.2563) Acc D Real: 82.770%
Loss D Fake: 0.6188 (0.4969) Acc D Fake: 82.969%
Loss D: 0.941
Loss G: 0.8211 (1.1375) Acc G: 17.237%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3499 (0.2569) Acc D Real: 82.727%
Loss D Fake: 0.6022 (0.4976) Acc D Fake: 82.872%
Loss D: 0.952
Loss G: 0.8445 (1.1356) Acc G: 17.300%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.3113 (0.2573) Acc D Real: 82.678%
Loss D Fake: 0.5793 (0.4981) Acc D Fake: 82.831%
Loss D: 0.891
Loss G: 0.8762 (1.1339) Acc G: 17.307%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2742 (0.2574) Acc D Real: 82.648%
Loss D Fake: 0.5486 (0.4984) Acc D Fake: 82.856%
Loss D: 0.823
Loss G: 0.9236 (1.1325) Acc G: 17.215%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.3399 (0.2579) Acc D Real: 82.587%
Loss D Fake: 0.5085 (0.4985) Acc D Fake: 82.968%
Loss D: 0.848
Loss G: 0.9726 (1.1314) Acc G: 17.102%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.4709 (0.2593) Acc D Real: 82.460%
Loss D Fake: 0.4761 (0.4984) Acc D Fake: 83.079%
Loss D: 0.947
Loss G: 1.0420 (1.1308) Acc G: 16.991%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.3612 (0.2600) Acc D Real: 82.383%
Loss D Fake: 0.4182 (0.4978) Acc D Fake: 83.188%
Loss D: 0.779
Loss G: 1.1562 (1.1310) Acc G: 16.882%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3913 (0.2608) Acc D Real: 82.294%
Loss D Fake: 0.3799 (0.4971) Acc D Fake: 83.296%
Loss D: 0.771
Loss G: 1.1728 (1.1313) Acc G: 16.774%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.3060 (0.2611) Acc D Real: 82.242%
Loss D Fake: 0.3761 (0.4963) Acc D Fake: 83.402%
Loss D: 0.682
Loss G: 1.1784 (1.1316) Acc G: 16.667%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3392 (0.2616) Acc D Real: 82.176%
Loss D Fake: 0.3746 (0.4956) Acc D Fake: 83.507%
Loss D: 0.714
Loss G: 1.1805 (1.1319) Acc G: 16.561%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.3657 (0.2622) Acc D Real: 82.108%
Loss D Fake: 0.3742 (0.4948) Acc D Fake: 83.611%
Loss D: 0.740
Loss G: 1.1805 (1.1322) Acc G: 16.457%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.3881 (0.2630) Acc D Real: 82.041%
Loss D Fake: 0.3752 (0.4940) Acc D Fake: 83.713%
Loss D: 0.763
Loss G: 1.1749 (1.1325) Acc G: 16.354%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.3429 (0.2635) Acc D Real: 81.981%
Loss D Fake: 0.3785 (0.4933) Acc D Fake: 83.814%
Loss D: 0.721
Loss G: 1.1657 (1.1327) Acc G: 16.253%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.2359 (0.2634) Acc D Real: 81.974%
Loss D Fake: 0.3827 (0.4926) Acc D Fake: 83.914%
Loss D: 0.619
Loss G: 1.1558 (1.1328) Acc G: 16.152%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.3558 (0.2639) Acc D Real: 81.900%
Loss D Fake: 0.3876 (0.4920) Acc D Fake: 84.013%
Loss D: 0.743
Loss G: 1.1417 (1.1329) Acc G: 16.053%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2794 (0.2640) Acc D Real: 81.876%
Loss D Fake: 0.3971 (0.4914) Acc D Fake: 84.110%
Loss D: 0.676
Loss G: 1.0981 (1.1327) Acc G: 15.955%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.1589 (0.2634) Acc D Real: 81.924%
Loss D Fake: 0.4367 (0.4911) Acc D Fake: 84.207%
Loss D: 0.596
Loss G: 1.0984 (1.1324) Acc G: 15.859%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.3036 (0.2636) Acc D Real: 81.888%
Loss D Fake: 0.3991 (0.4905) Acc D Fake: 84.302%
Loss D: 0.703
Loss G: 1.1405 (1.1325) Acc G: 15.763%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.4004 (0.2644) Acc D Real: 81.798%
Loss D Fake: 0.3891 (0.4899) Acc D Fake: 84.396%
Loss D: 0.789
Loss G: 1.1497 (1.1326) Acc G: 15.669%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.3085 (0.2647) Acc D Real: 81.771%
Loss D Fake: 0.3867 (0.4893) Acc D Fake: 84.489%
Loss D: 0.695
Loss G: 1.1531 (1.1327) Acc G: 15.575%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.3956 (0.2655) Acc D Real: 81.691%
Loss D Fake: 0.3859 (0.4887) Acc D Fake: 84.581%
Loss D: 0.782
Loss G: 1.1535 (1.1328) Acc G: 15.483%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.3703 (0.2661) Acc D Real: 81.637%
Loss D Fake: 0.3864 (0.4881) Acc D Fake: 84.671%
Loss D: 0.757
Loss G: 1.1512 (1.1330) Acc G: 15.392%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.3127 (0.2664) Acc D Real: 81.606%
Loss D Fake: 0.3878 (0.4875) Acc D Fake: 84.761%
Loss D: 0.700
Loss G: 1.1474 (1.1330) Acc G: 15.302%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3745 (0.2670) Acc D Real: 81.536%
Loss D Fake: 0.3899 (0.4869) Acc D Fake: 84.850%
Loss D: 0.764
Loss G: 1.1412 (1.1331) Acc G: 15.213%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.2736 (0.2670) Acc D Real: 81.519%
Loss D Fake: 0.3931 (0.4864) Acc D Fake: 84.937%
Loss D: 0.667
Loss G: 1.1344 (1.1331) Acc G: 15.125%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.2714 (0.2671) Acc D Real: 81.504%
Loss D Fake: 0.3963 (0.4859) Acc D Fake: 85.024%
Loss D: 0.668
Loss G: 1.1272 (1.1331) Acc G: 15.038%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3817 (0.2677) Acc D Real: 81.449%
Loss D Fake: 0.4017 (0.4854) Acc D Fake: 85.109%
Loss D: 0.783
Loss G: 1.1167 (1.1330) Acc G: 14.952%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3573 (0.2682) Acc D Real: 81.403%
Loss D Fake: 0.4060 (0.4849) Acc D Fake: 85.194%
Loss D: 0.763
Loss G: 1.1256 (1.1329) Acc G: 14.867%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2634 (0.2682) Acc D Real: 81.399%
Loss D Fake: 0.3963 (0.4844) Acc D Fake: 85.277%
Loss D: 0.660
Loss G: 1.1340 (1.1329) Acc G: 14.783%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2695 (0.2682) Acc D Real: 81.400%
Loss D Fake: 0.3937 (0.4839) Acc D Fake: 85.360%
Loss D: 0.663
Loss G: 1.1384 (1.1330) Acc G: 14.700%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.4720 (0.2693) Acc D Real: 81.289%
Loss D Fake: 0.3932 (0.4834) Acc D Fake: 85.442%
Loss D: 0.865
Loss G: 1.1342 (1.1330) Acc G: 14.618%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.2984 (0.2695) Acc D Real: 81.271%
Loss D Fake: 0.3966 (0.4829) Acc D Fake: 85.523%
Loss D: 0.695
Loss G: 1.1256 (1.1329) Acc G: 14.537%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.3716 (0.2701) Acc D Real: 81.212%
Loss D Fake: 0.4040 (0.4825) Acc D Fake: 85.603%
Loss D: 0.776
Loss G: 1.0756 (1.1326) Acc G: 14.457%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.3557 (0.2705) Acc D Real: 81.173%
Loss D Fake: 0.4666 (0.4824) Acc D Fake: 85.682%
Loss D: 0.822
Loss G: 1.1128 (1.1325) Acc G: 14.377%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2465 (0.2704) Acc D Real: 81.181%
Loss D Fake: 0.3990 (0.4820) Acc D Fake: 85.760%
Loss D: 0.646
Loss G: 1.1312 (1.1325) Acc G: 14.299%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.3004 (0.2706) Acc D Real: 81.161%
Loss D Fake: 0.3948 (0.4815) Acc D Fake: 85.838%
Loss D: 0.695
Loss G: 1.1359 (1.1325) Acc G: 14.221%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.3225 (0.2708) Acc D Real: 81.133%
Loss D Fake: 0.3939 (0.4810) Acc D Fake: 85.914%
Loss D: 0.716
Loss G: 1.1363 (1.1325) Acc G: 14.144%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.3428 (0.2712) Acc D Real: 81.095%
Loss D Fake: 0.3944 (0.4806) Acc D Fake: 85.990%
Loss D: 0.737
Loss G: 1.1344 (1.1325) Acc G: 14.068%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.2604 (0.2712) Acc D Real: 81.092%
Loss D Fake: 0.3957 (0.4801) Acc D Fake: 86.065%
Loss D: 0.656
Loss G: 1.1307 (1.1325) Acc G: 13.993%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.2735 (0.2712) Acc D Real: 81.090%
Loss D Fake: 0.3976 (0.4797) Acc D Fake: 86.139%
Loss D: 0.671
Loss G: 1.1264 (1.1325) Acc G: 13.918%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.3557 (0.2716) Acc D Real: 81.048%
Loss D Fake: 0.4005 (0.4792) Acc D Fake: 86.212%
Loss D: 0.756
Loss G: 1.1150 (1.1324) Acc G: 13.845%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.4099 (0.2724) Acc D Real: 80.994%
Loss D Fake: 0.4225 (0.4789) Acc D Fake: 86.285%
Loss D: 0.832
Loss G: 1.1214 (1.1323) Acc G: 13.772%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.2799 (0.2724) Acc D Real: 80.991%
Loss D Fake: 0.3972 (0.4785) Acc D Fake: 86.357%
Loss D: 0.677
Loss G: 1.1350 (1.1324) Acc G: 13.700%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.2816 (0.2724) Acc D Real: 80.986%
Loss D Fake: 0.3933 (0.4781) Acc D Fake: 86.428%
Loss D: 0.675
Loss G: 1.1407 (1.1324) Acc G: 13.628%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.1944 (0.2720) Acc D Real: 81.028%
Loss D Fake: 0.3914 (0.4776) Acc D Fake: 86.498%
Loss D: 0.586
Loss G: 1.1450 (1.1325) Acc G: 13.558%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.2986 (0.2722) Acc D Real: 81.019%
Loss D Fake: 0.3899 (0.4772) Acc D Fake: 86.568%
Loss D: 0.689
Loss G: 1.1470 (1.1325) Acc G: 13.488%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.2178 (0.2719) Acc D Real: 81.050%
Loss D Fake: 0.3893 (0.4767) Acc D Fake: 86.636%
Loss D: 0.607
Loss G: 1.1490 (1.1326) Acc G: 13.419%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.1747 (0.2714) Acc D Real: 81.086%
Loss D Fake: 0.3882 (0.4763) Acc D Fake: 86.705%
Loss D: 0.563
Loss G: 1.1523 (1.1327) Acc G: 13.350%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3869 (0.2720) Acc D Real: 81.039%
Loss D Fake: 0.3870 (0.4758) Acc D Fake: 86.772%
Loss D: 0.774
Loss G: 1.1535 (1.1328) Acc G: 13.283%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.2272 (0.2718) Acc D Real: 81.056%
Loss D Fake: 0.3871 (0.4754) Acc D Fake: 86.839%
Loss D: 0.614
Loss G: 1.1527 (1.1329) Acc G: 13.215%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.2095 (0.2715) Acc D Real: 81.092%
Loss D Fake: 0.3875 (0.4749) Acc D Fake: 86.905%
Loss D: 0.597
Loss G: 1.1520 (1.1330) Acc G: 13.149%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.3736 (0.2720) Acc D Real: 81.041%
Loss D Fake: 0.3886 (0.4745) Acc D Fake: 86.971%
Loss D: 0.762
Loss G: 1.1451 (1.1331) Acc G: 13.083%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.3186 (0.2722) Acc D Real: 81.022%
Loss D Fake: 0.3995 (0.4741) Acc D Fake: 87.035%
Loss D: 0.718
Loss G: 0.8592 (1.1317) Acc G: 13.167%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.2363 (0.2720) Acc D Real: 81.039%
Loss D Fake: 0.7011 (0.4752) Acc D Fake: 86.869%
Loss D: 0.937
Loss G: 0.7799 (1.1300) Acc G: 13.309%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.3136 (0.2722) Acc D Real: 81.040%
Loss D Fake: 0.6528 (0.4761) Acc D Fake: 86.744%
Loss D: 0.966
Loss G: 1.1497 (1.1301) Acc G: 13.243%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.1621 (0.2717) Acc D Real: 81.100%
Loss D Fake: 0.3797 (0.4756) Acc D Fake: 86.809%
Loss D: 0.542
Loss G: 1.1841 (1.1304) Acc G: 13.178%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.3498 (0.2721) Acc D Real: 81.063%
Loss D Fake: 0.3701 (0.4751) Acc D Fake: 86.874%
Loss D: 0.720
Loss G: 1.2012 (1.1307) Acc G: 13.114%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.1892 (0.2717) Acc D Real: 81.097%
Loss D Fake: 0.3650 (0.4746) Acc D Fake: 86.937%
Loss D: 0.554
Loss G: 1.2103 (1.1311) Acc G: 13.050%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.4837 (0.2727) Acc D Real: 81.009%
Loss D Fake: 0.3627 (0.4741) Acc D Fake: 87.001%
Loss D: 0.846
Loss G: 1.2123 (1.1315) Acc G: 12.987%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.4121 (0.2734) Acc D Real: 80.945%
Loss D Fake: 0.3629 (0.4735) Acc D Fake: 87.063%
Loss D: 0.775
Loss G: 1.2095 (1.1318) Acc G: 12.925%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.2623 (0.2733) Acc D Real: 80.948%
Loss D Fake: 0.3644 (0.4730) Acc D Fake: 87.125%
Loss D: 0.627
Loss G: 1.2063 (1.1322) Acc G: 12.863%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.4085 (0.2739) Acc D Real: 80.893%
Loss D Fake: 0.3658 (0.4725) Acc D Fake: 87.186%
Loss D: 0.774
Loss G: 1.2015 (1.1325) Acc G: 12.802%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.3754 (0.2744) Acc D Real: 80.848%
Loss D Fake: 0.3682 (0.4720) Acc D Fake: 87.247%
Loss D: 0.744
Loss G: 1.1947 (1.1328) Acc G: 12.741%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.4096 (0.2751) Acc D Real: 80.791%
Loss D Fake: 0.3717 (0.4715) Acc D Fake: 87.307%
Loss D: 0.781
Loss G: 1.1835 (1.1331) Acc G: 12.681%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.3672 (0.2755) Acc D Real: 80.746%
Loss D Fake: 0.3775 (0.4711) Acc D Fake: 87.367%
Loss D: 0.745
Loss G: 1.1662 (1.1332) Acc G: 12.621%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3222 (0.2757) Acc D Real: 80.728%
Loss D Fake: 0.3858 (0.4707) Acc D Fake: 87.426%
Loss D: 0.708
Loss G: 1.1455 (1.1333) Acc G: 12.562%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.2947 (0.2758) Acc D Real: 80.708%
Loss D Fake: 0.3963 (0.4703) Acc D Fake: 87.484%
Loss D: 0.691
Loss G: 1.1128 (1.1332) Acc G: 12.504%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.3540 (0.2762) Acc D Real: 80.681%
Loss D Fake: 0.4903 (0.4704) Acc D Fake: 87.488%
Loss D: 0.844
Loss G: 1.1292 (1.1332) Acc G: 12.446%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2977 (0.2763) Acc D Real: 80.676%
Loss D Fake: 0.3926 (0.4701) Acc D Fake: 87.546%
Loss D: 0.690
Loss G: 1.1458 (1.1332) Acc G: 12.389%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2835 (0.2763) Acc D Real: 80.663%
Loss D Fake: 0.3894 (0.4697) Acc D Fake: 87.603%
Loss D: 0.673
Loss G: 1.1496 (1.1333) Acc G: 12.332%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2583 (0.2762) Acc D Real: 80.662%
Loss D Fake: 0.3884 (0.4693) Acc D Fake: 87.660%
Loss D: 0.647
Loss G: 1.1516 (1.1334) Acc G: 12.275%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.3558 (0.2766) Acc D Real: 80.623%
Loss D Fake: 0.3878 (0.4690) Acc D Fake: 87.716%
Loss D: 0.744
Loss G: 1.1519 (1.1335) Acc G: 12.220%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.4716 (0.2775) Acc D Real: 80.542%
Loss D Fake: 0.3883 (0.4686) Acc D Fake: 87.771%
Loss D: 0.860
Loss G: 1.1486 (1.1335) Acc G: 12.164%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.3950 (0.2780) Acc D Real: 80.501%
Loss D Fake: 0.3902 (0.4682) Acc D Fake: 87.826%
Loss D: 0.785
Loss G: 1.1438 (1.1336) Acc G: 12.110%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.4211 (0.2786) Acc D Real: 80.439%
Loss D Fake: 0.3928 (0.4679) Acc D Fake: 87.881%
Loss D: 0.814
Loss G: 1.1366 (1.1336) Acc G: 12.055%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.4995 (0.2796) Acc D Real: 80.340%
Loss D Fake: 0.3967 (0.4676) Acc D Fake: 87.935%
Loss D: 0.896
Loss G: 1.1262 (1.1336) Acc G: 12.001%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.3321 (0.2799) Acc D Real: 80.314%
Loss D Fake: 0.4016 (0.4673) Acc D Fake: 87.989%
Loss D: 0.734
Loss G: 1.1165 (1.1335) Acc G: 11.948%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.1898 (0.2795) Acc D Real: 80.320%
Loss D Fake: 0.4053 (0.4670) Acc D Fake: 88.002%
Loss D: 0.595
Loss G: 1.1109 (1.1334) Acc G: 11.935%
LR: 2.000e-04
Epoch: 20/20
TRAIN Iteration: [   2/226]
Loss D Real: 0.3607 (0.3048) Acc D Real: 75.156%
Loss D Fake: 0.4084 (0.4077) Acc D Fake: 100.000%
Loss D: 0.769
Loss G: 1.1050 (1.1065) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   3/226]
Loss D Real: 0.2764 (0.2953) Acc D Real: 75.781%
Loss D Fake: 0.4099 (0.4084) Acc D Fake: 100.000%
Loss D: 0.686
Loss G: 1.1022 (1.1051) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   4/226]
Loss D Real: 0.2178 (0.2760) Acc D Real: 77.721%
Loss D Fake: 0.4110 (0.4091) Acc D Fake: 100.000%
Loss D: 0.629
Loss G: 1.1001 (1.1038) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   5/226]
Loss D Real: 0.3113 (0.2830) Acc D Real: 77.198%
Loss D Fake: 0.4123 (0.4097) Acc D Fake: 100.000%
Loss D: 0.724
Loss G: 1.0964 (1.1023) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   6/226]
Loss D Real: 0.4312 (0.3077) Acc D Real: 75.061%
Loss D Fake: 0.4157 (0.4107) Acc D Fake: 100.000%
Loss D: 0.847
Loss G: 1.0833 (1.0992) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   7/226]
Loss D Real: 0.3348 (0.3116) Acc D Real: 74.874%
Loss D Fake: 0.4241 (0.4126) Acc D Fake: 100.000%
Loss D: 0.759
Loss G: 1.0638 (1.0941) Acc G: 0.000%
LR: 2.000e-04
TRAIN Iteration: [   8/226]
Loss D Real: 0.2209 (0.3003) Acc D Real: 76.302%
Loss D Fake: 0.4368 (0.4157) Acc D Fake: 100.000%
Loss D: 0.658
Loss G: 0.9072 (1.0707) Acc G: 1.667%
LR: 2.000e-04
TRAIN Iteration: [   9/226]
Loss D Real: 0.2845 (0.2985) Acc D Real: 76.395%
Loss D Fake: 0.7391 (0.4516) Acc D Fake: 95.556%
Loss D: 1.024
Loss G: 1.0262 (1.0658) Acc G: 1.481%
LR: 2.000e-04
TRAIN Iteration: [  10/226]
Loss D Real: 0.1990 (0.2885) Acc D Real: 77.490%
Loss D Fake: 0.4291 (0.4494) Acc D Fake: 96.000%
Loss D: 0.628
Loss G: 1.0791 (1.0671) Acc G: 1.333%
LR: 2.000e-04
TRAIN Iteration: [  11/226]
Loss D Real: 0.2630 (0.2862) Acc D Real: 77.798%
Loss D Fake: 0.4166 (0.4464) Acc D Fake: 96.364%
Loss D: 0.680
Loss G: 1.0978 (1.0699) Acc G: 1.212%
LR: 2.000e-04
TRAIN Iteration: [  12/226]
Loss D Real: 0.2623 (0.2842) Acc D Real: 78.095%
Loss D Fake: 0.4094 (0.4433) Acc D Fake: 96.667%
Loss D: 0.672
Loss G: 1.1107 (1.0733) Acc G: 1.111%
LR: 2.000e-04
TRAIN Iteration: [  13/226]
Loss D Real: 0.2232 (0.2795) Acc D Real: 78.454%
Loss D Fake: 0.4044 (0.4403) Acc D Fake: 96.923%
Loss D: 0.628
Loss G: 1.1192 (1.0768) Acc G: 1.026%
LR: 2.000e-04
TRAIN Iteration: [  14/226]
Loss D Real: 0.3183 (0.2823) Acc D Real: 78.289%
Loss D Fake: 0.4018 (0.4376) Acc D Fake: 97.143%
Loss D: 0.720
Loss G: 1.1213 (1.0800) Acc G: 0.952%
LR: 2.000e-04
TRAIN Iteration: [  15/226]
Loss D Real: 0.3345 (0.2858) Acc D Real: 78.094%
Loss D Fake: 0.4018 (0.4352) Acc D Fake: 97.333%
Loss D: 0.736
Loss G: 1.1202 (1.0827) Acc G: 0.889%
LR: 2.000e-04
TRAIN Iteration: [  16/226]
Loss D Real: 0.3366 (0.2890) Acc D Real: 77.878%
Loss D Fake: 0.4027 (0.4331) Acc D Fake: 97.500%
Loss D: 0.739
Loss G: 1.1176 (1.0849) Acc G: 0.833%
LR: 2.000e-04
TRAIN Iteration: [  17/226]
Loss D Real: 0.3071 (0.2900) Acc D Real: 77.825%
Loss D Fake: 0.4047 (0.4315) Acc D Fake: 97.647%
Loss D: 0.712
Loss G: 1.1103 (1.0864) Acc G: 0.784%
LR: 2.000e-04
TRAIN Iteration: [  18/226]
Loss D Real: 0.2294 (0.2867) Acc D Real: 78.328%
Loss D Fake: 0.4093 (0.4302) Acc D Fake: 97.778%
Loss D: 0.639
Loss G: 1.0981 (1.0870) Acc G: 0.741%
LR: 2.000e-04
TRAIN Iteration: [  19/226]
Loss D Real: 0.3806 (0.2916) Acc D Real: 77.906%
Loss D Fake: 0.4166 (0.4295) Acc D Fake: 97.895%
Loss D: 0.797
Loss G: 1.0788 (1.0866) Acc G: 0.702%
LR: 2.000e-04
TRAIN Iteration: [  20/226]
Loss D Real: 0.2026 (0.2871) Acc D Real: 78.289%
Loss D Fake: 0.4301 (0.4295) Acc D Fake: 98.000%
Loss D: 0.633
Loss G: 1.0650 (1.0855) Acc G: 0.667%
LR: 2.000e-04
TRAIN Iteration: [  21/226]
Loss D Real: 0.2592 (0.2858) Acc D Real: 78.465%
Loss D Fake: 0.4280 (0.4295) Acc D Fake: 98.095%
Loss D: 0.687
Loss G: 1.0819 (1.0853) Acc G: 0.635%
LR: 2.000e-04
TRAIN Iteration: [  22/226]
Loss D Real: 0.1738 (0.2807) Acc D Real: 78.911%
Loss D Fake: 0.4154 (0.4288) Acc D Fake: 98.182%
Loss D: 0.589
Loss G: 1.1015 (1.0861) Acc G: 0.606%
LR: 2.000e-04
TRAIN Iteration: [  23/226]
Loss D Real: 0.2513 (0.2794) Acc D Real: 79.065%
Loss D Fake: 0.4097 (0.4280) Acc D Fake: 98.261%
Loss D: 0.661
Loss G: 1.1018 (1.0868) Acc G: 0.580%
LR: 2.000e-04
TRAIN Iteration: [  24/226]
Loss D Real: 0.2576 (0.2785) Acc D Real: 79.204%
Loss D Fake: 0.4120 (0.4273) Acc D Fake: 98.333%
Loss D: 0.670
Loss G: 1.0986 (1.0872) Acc G: 0.556%
LR: 2.000e-04
TRAIN Iteration: [  25/226]
Loss D Real: 0.2593 (0.2778) Acc D Real: 79.329%
Loss D Fake: 0.4130 (0.4268) Acc D Fake: 98.400%
Loss D: 0.672
Loss G: 1.1000 (1.0878) Acc G: 0.533%
LR: 2.000e-04
TRAIN Iteration: [  26/226]
Loss D Real: 0.2324 (0.2760) Acc D Real: 79.475%
Loss D Fake: 0.4110 (0.4262) Acc D Fake: 98.462%
Loss D: 0.643
Loss G: 1.1066 (1.0885) Acc G: 0.513%
LR: 2.000e-04
TRAIN Iteration: [  27/226]
Loss D Real: 0.2863 (0.2764) Acc D Real: 79.452%
Loss D Fake: 0.4072 (0.4254) Acc D Fake: 98.519%
Loss D: 0.693
Loss G: 1.1146 (1.0894) Acc G: 0.494%
LR: 2.000e-04
TRAIN Iteration: [  28/226]
Loss D Real: 0.3814 (0.2802) Acc D Real: 79.142%
Loss D Fake: 0.4041 (0.4247) Acc D Fake: 98.571%
Loss D: 0.785
Loss G: 1.1186 (1.0905) Acc G: 0.476%
LR: 2.000e-04
TRAIN Iteration: [  29/226]
Loss D Real: 0.3626 (0.2830) Acc D Real: 78.915%
Loss D Fake: 0.4041 (0.4240) Acc D Fake: 98.621%
Loss D: 0.767
Loss G: 1.1139 (1.0913) Acc G: 0.460%
LR: 2.000e-04
TRAIN Iteration: [  30/226]
Loss D Real: 0.3273 (0.2845) Acc D Real: 78.799%
Loss D Fake: 0.4076 (0.4234) Acc D Fake: 98.667%
Loss D: 0.735
Loss G: 1.1081 (1.0919) Acc G: 0.444%
LR: 2.000e-04
TRAIN Iteration: [  31/226]
Loss D Real: 0.2458 (0.2832) Acc D Real: 78.935%
Loss D Fake: 0.4105 (0.4230) Acc D Fake: 98.710%
Loss D: 0.656
Loss G: 1.1029 (1.0922) Acc G: 0.430%
LR: 2.000e-04
TRAIN Iteration: [  32/226]
Loss D Real: 0.1044 (0.2776) Acc D Real: 79.429%
Loss D Fake: 0.4105 (0.4226) Acc D Fake: 98.750%
Loss D: 0.515
Loss G: 1.1129 (1.0929) Acc G: 0.417%
LR: 2.000e-04
TRAIN Iteration: [  33/226]
Loss D Real: 0.2681 (0.2774) Acc D Real: 79.471%
Loss D Fake: 0.4042 (0.4221) Acc D Fake: 98.788%
Loss D: 0.672
Loss G: 1.1208 (1.0937) Acc G: 0.404%
LR: 2.000e-04
TRAIN Iteration: [  34/226]
Loss D Real: 0.2889 (0.2777) Acc D Real: 79.478%
Loss D Fake: 0.4013 (0.4214) Acc D Fake: 98.824%
Loss D: 0.690
Loss G: 1.1271 (1.0947) Acc G: 0.392%
LR: 2.000e-04
TRAIN Iteration: [  35/226]
Loss D Real: 0.2147 (0.2759) Acc D Real: 79.638%
Loss D Fake: 0.4010 (0.4209) Acc D Fake: 98.857%
Loss D: 0.616
Loss G: 1.1073 (1.0951) Acc G: 0.381%
LR: 2.000e-04
TRAIN Iteration: [  36/226]
Loss D Real: 0.3095 (0.2768) Acc D Real: 79.547%
Loss D Fake: 0.6491 (0.4272) Acc D Fake: 98.056%
Loss D: 0.959
Loss G: 1.1704 (1.0971) Acc G: 0.370%
LR: 2.000e-04
TRAIN Iteration: [  37/226]
Loss D Real: 0.2801 (0.2769) Acc D Real: 79.593%
Loss D Fake: 0.3701 (0.4257) Acc D Fake: 98.108%
Loss D: 0.650
Loss G: 1.2135 (1.1003) Acc G: 0.360%
LR: 2.000e-04
TRAIN Iteration: [  38/226]
Loss D Real: 0.2686 (0.2767) Acc D Real: 79.604%
Loss D Fake: 0.3573 (0.4239) Acc D Fake: 98.158%
Loss D: 0.626
Loss G: 1.2387 (1.1039) Acc G: 0.351%
LR: 2.000e-04
TRAIN Iteration: [  39/226]
Loss D Real: 0.4285 (0.2806) Acc D Real: 79.307%
Loss D Fake: 0.3497 (0.4220) Acc D Fake: 98.205%
Loss D: 0.778
Loss G: 1.2536 (1.1078) Acc G: 0.342%
LR: 2.000e-04
TRAIN Iteration: [  40/226]
Loss D Real: 0.3090 (0.2813) Acc D Real: 79.272%
Loss D Fake: 0.3453 (0.4200) Acc D Fake: 98.250%
Loss D: 0.654
Loss G: 1.2627 (1.1116) Acc G: 0.333%
LR: 2.000e-04
TRAIN Iteration: [  41/226]
Loss D Real: 0.4323 (0.2850) Acc D Real: 78.980%
Loss D Fake: 0.3428 (0.4182) Acc D Fake: 98.293%
Loss D: 0.775
Loss G: 1.2669 (1.1154) Acc G: 0.325%
LR: 2.000e-04
TRAIN Iteration: [  42/226]
Loss D Real: 0.3148 (0.2857) Acc D Real: 78.920%
Loss D Fake: 0.3418 (0.4163) Acc D Fake: 98.333%
Loss D: 0.657
Loss G: 1.2687 (1.1191) Acc G: 0.317%
LR: 2.000e-04
TRAIN Iteration: [  43/226]
Loss D Real: 0.2432 (0.2847) Acc D Real: 79.000%
Loss D Fake: 0.3411 (0.4146) Acc D Fake: 98.372%
Loss D: 0.584
Loss G: 1.2710 (1.1226) Acc G: 0.310%
LR: 2.000e-04
TRAIN Iteration: [  44/226]
Loss D Real: 0.4303 (0.2880) Acc D Real: 78.746%
Loss D Fake: 0.3405 (0.4129) Acc D Fake: 98.409%
Loss D: 0.771
Loss G: 1.2710 (1.1260) Acc G: 0.303%
LR: 2.000e-04
TRAIN Iteration: [  45/226]
Loss D Real: 0.5413 (0.2936) Acc D Real: 78.301%
Loss D Fake: 0.3411 (0.4113) Acc D Fake: 98.444%
Loss D: 0.882
Loss G: 1.2669 (1.1291) Acc G: 0.296%
LR: 2.000e-04
TRAIN Iteration: [  46/226]
Loss D Real: 0.3978 (0.2959) Acc D Real: 78.124%
Loss D Fake: 0.3432 (0.4098) Acc D Fake: 98.478%
Loss D: 0.741
Loss G: 1.2607 (1.1320) Acc G: 0.290%
LR: 2.000e-04
TRAIN Iteration: [  47/226]
Loss D Real: 0.4698 (0.2996) Acc D Real: 77.826%
Loss D Fake: 0.3458 (0.4085) Acc D Fake: 98.511%
Loss D: 0.816
Loss G: 1.2527 (1.1345) Acc G: 0.284%
LR: 2.000e-04
TRAIN Iteration: [  48/226]
Loss D Real: 0.3498 (0.3007) Acc D Real: 77.744%
Loss D Fake: 0.3489 (0.4072) Acc D Fake: 98.542%
Loss D: 0.699
Loss G: 1.2446 (1.1368) Acc G: 0.278%
LR: 2.000e-04
TRAIN Iteration: [  49/226]
Loss D Real: 0.3226 (0.3011) Acc D Real: 77.710%
Loss D Fake: 0.3517 (0.4061) Acc D Fake: 98.571%
Loss D: 0.674
Loss G: 1.2383 (1.1389) Acc G: 0.272%
LR: 2.000e-04
TRAIN Iteration: [  50/226]
Loss D Real: 0.3150 (0.3014) Acc D Real: 77.678%
Loss D Fake: 0.3538 (0.4050) Acc D Fake: 98.600%
Loss D: 0.669
Loss G: 1.2333 (1.1408) Acc G: 0.267%
LR: 2.000e-04
TRAIN Iteration: [  51/226]
Loss D Real: 0.2715 (0.3008) Acc D Real: 77.726%
Loss D Fake: 0.3555 (0.4041) Acc D Fake: 98.627%
Loss D: 0.627
Loss G: 1.2301 (1.1425) Acc G: 0.261%
LR: 2.000e-04
TRAIN Iteration: [  52/226]
Loss D Real: 0.4189 (0.3031) Acc D Real: 77.533%
Loss D Fake: 0.3568 (0.4032) Acc D Fake: 98.654%
Loss D: 0.776
Loss G: 1.2258 (1.1441) Acc G: 0.256%
LR: 2.000e-04
TRAIN Iteration: [  53/226]
Loss D Real: 0.2328 (0.3017) Acc D Real: 77.635%
Loss D Fake: 0.3583 (0.4023) Acc D Fake: 98.679%
Loss D: 0.591
Loss G: 1.2232 (1.1456) Acc G: 0.252%
LR: 2.000e-04
TRAIN Iteration: [  54/226]
Loss D Real: 0.3821 (0.3032) Acc D Real: 77.504%
Loss D Fake: 0.3592 (0.4015) Acc D Fake: 98.704%
Loss D: 0.741
Loss G: 1.2207 (1.1470) Acc G: 0.247%
LR: 2.000e-04
TRAIN Iteration: [  55/226]
Loss D Real: 0.4852 (0.3065) Acc D Real: 77.216%
Loss D Fake: 0.3606 (0.4008) Acc D Fake: 98.727%
Loss D: 0.846
Loss G: 1.2158 (1.1483) Acc G: 0.242%
LR: 2.000e-04
TRAIN Iteration: [  56/226]
Loss D Real: 0.5619 (0.3111) Acc D Real: 76.821%
Loss D Fake: 0.3632 (0.4001) Acc D Fake: 98.750%
Loss D: 0.925
Loss G: 1.2072 (1.1493) Acc G: 0.238%
LR: 2.000e-04
TRAIN Iteration: [  57/226]
Loss D Real: 0.3413 (0.3116) Acc D Real: 76.763%
Loss D Fake: 0.3668 (0.3995) Acc D Fake: 98.772%
Loss D: 0.708
Loss G: 1.1992 (1.1502) Acc G: 0.234%
LR: 2.000e-04
TRAIN Iteration: [  58/226]
Loss D Real: 0.1951 (0.3096) Acc D Real: 76.919%
Loss D Fake: 0.3694 (0.3990) Acc D Fake: 98.793%
Loss D: 0.564
Loss G: 1.1954 (1.1510) Acc G: 0.230%
LR: 2.000e-04
TRAIN Iteration: [  59/226]
Loss D Real: 0.4062 (0.3113) Acc D Real: 76.764%
Loss D Fake: 0.3706 (0.3985) Acc D Fake: 98.814%
Loss D: 0.777
Loss G: 1.1917 (1.1517) Acc G: 0.226%
LR: 2.000e-04
TRAIN Iteration: [  60/226]
Loss D Real: 0.3718 (0.3123) Acc D Real: 76.662%
Loss D Fake: 0.3723 (0.3981) Acc D Fake: 98.833%
Loss D: 0.744
Loss G: 1.1876 (1.1523) Acc G: 0.222%
LR: 2.000e-04
TRAIN Iteration: [  61/226]
Loss D Real: 0.4170 (0.3140) Acc D Real: 76.505%
Loss D Fake: 0.3742 (0.3977) Acc D Fake: 98.852%
Loss D: 0.791
Loss G: 1.1824 (1.1528) Acc G: 0.219%
LR: 2.000e-04
TRAIN Iteration: [  62/226]
Loss D Real: 0.4415 (0.3160) Acc D Real: 76.310%
Loss D Fake: 0.3767 (0.3974) Acc D Fake: 98.871%
Loss D: 0.818
Loss G: 1.1756 (1.1531) Acc G: 0.215%
LR: 2.000e-04
TRAIN Iteration: [  63/226]
Loss D Real: 0.3397 (0.3164) Acc D Real: 76.271%
Loss D Fake: 0.3796 (0.3971) Acc D Fake: 98.889%
Loss D: 0.719
Loss G: 1.1697 (1.1534) Acc G: 0.212%
LR: 2.000e-04
TRAIN Iteration: [  64/226]
Loss D Real: 0.4017 (0.3177) Acc D Real: 76.134%
Loss D Fake: 0.3820 (0.3968) Acc D Fake: 98.906%
Loss D: 0.784
Loss G: 1.1636 (1.1536) Acc G: 0.208%
LR: 2.000e-04
TRAIN Iteration: [  65/226]
Loss D Real: 0.3416 (0.3181) Acc D Real: 76.089%
Loss D Fake: 0.3847 (0.3967) Acc D Fake: 98.923%
Loss D: 0.726
Loss G: 1.1580 (1.1536) Acc G: 0.205%
LR: 2.000e-04
TRAIN Iteration: [  66/226]
Loss D Real: 0.3321 (0.3183) Acc D Real: 76.046%
Loss D Fake: 0.3868 (0.3965) Acc D Fake: 98.939%
Loss D: 0.719
Loss G: 1.1540 (1.1536) Acc G: 0.202%
LR: 2.000e-04
TRAIN Iteration: [  67/226]
Loss D Real: 0.2941 (0.3180) Acc D Real: 76.051%
Loss D Fake: 0.3882 (0.3964) Acc D Fake: 98.955%
Loss D: 0.682
Loss G: 1.1514 (1.1536) Acc G: 0.199%
LR: 2.000e-04
TRAIN Iteration: [  68/226]
Loss D Real: 0.3587 (0.3186) Acc D Real: 75.970%
Loss D Fake: 0.3892 (0.3963) Acc D Fake: 98.971%
Loss D: 0.748
Loss G: 1.1493 (1.1535) Acc G: 0.196%
LR: 2.000e-04
TRAIN Iteration: [  69/226]
Loss D Real: 0.6613 (0.3235) Acc D Real: 75.501%
Loss D Fake: 0.3908 (0.3962) Acc D Fake: 98.986%
Loss D: 1.052
Loss G: 1.1423 (1.1534) Acc G: 0.193%
LR: 2.000e-04
TRAIN Iteration: [  70/226]
Loss D Real: 0.2574 (0.3226) Acc D Real: 75.556%
Loss D Fake: 0.3944 (0.3962) Acc D Fake: 99.000%
Loss D: 0.652
Loss G: 1.1355 (1.1531) Acc G: 0.190%
LR: 2.000e-04
TRAIN Iteration: [  71/226]
Loss D Real: 0.2984 (0.3222) Acc D Real: 75.569%
Loss D Fake: 0.3970 (0.3962) Acc D Fake: 99.014%
Loss D: 0.695
Loss G: 1.1304 (1.1528) Acc G: 0.188%
LR: 2.000e-04
TRAIN Iteration: [  72/226]
Loss D Real: 0.3297 (0.3223) Acc D Real: 75.536%
Loss D Fake: 0.3991 (0.3962) Acc D Fake: 99.028%
Loss D: 0.729
Loss G: 1.1263 (1.1524) Acc G: 0.185%
LR: 2.000e-04
TRAIN Iteration: [  73/226]
Loss D Real: 0.3963 (0.3234) Acc D Real: 75.420%
Loss D Fake: 0.4011 (0.3963) Acc D Fake: 99.041%
Loss D: 0.797
Loss G: 1.1214 (1.1520) Acc G: 0.183%
LR: 2.000e-04
TRAIN Iteration: [  74/226]
Loss D Real: 0.3598 (0.3239) Acc D Real: 75.358%
Loss D Fake: 0.4035 (0.3964) Acc D Fake: 99.054%
Loss D: 0.763
Loss G: 1.1162 (1.1515) Acc G: 0.180%
LR: 2.000e-04
TRAIN Iteration: [  75/226]
Loss D Real: 0.3281 (0.3239) Acc D Real: 75.328%
Loss D Fake: 0.4058 (0.3965) Acc D Fake: 99.067%
Loss D: 0.734
Loss G: 1.1112 (1.1510) Acc G: 0.178%
LR: 2.000e-04
TRAIN Iteration: [  76/226]
Loss D Real: 0.4848 (0.3260) Acc D Real: 75.097%
Loss D Fake: 0.4085 (0.3967) Acc D Fake: 99.079%
Loss D: 0.893
Loss G: 1.1044 (1.1504) Acc G: 0.175%
LR: 2.000e-04
TRAIN Iteration: [  77/226]
Loss D Real: 0.2951 (0.3256) Acc D Real: 75.103%
Loss D Fake: 0.4117 (0.3969) Acc D Fake: 99.091%
Loss D: 0.707
Loss G: 1.0989 (1.1497) Acc G: 0.173%
LR: 2.000e-04
TRAIN Iteration: [  78/226]
Loss D Real: 0.4898 (0.3277) Acc D Real: 74.879%
Loss D Fake: 0.4143 (0.3971) Acc D Fake: 99.103%
Loss D: 0.904
Loss G: 1.0923 (1.1490) Acc G: 0.171%
LR: 2.000e-04
TRAIN Iteration: [  79/226]
Loss D Real: 0.2899 (0.3273) Acc D Real: 74.898%
Loss D Fake: 0.4175 (0.3973) Acc D Fake: 99.114%
Loss D: 0.707
Loss G: 1.0866 (1.1482) Acc G: 0.169%
LR: 2.000e-04
TRAIN Iteration: [  80/226]
Loss D Real: 0.3798 (0.3279) Acc D Real: 74.805%
Loss D Fake: 0.4202 (0.3976) Acc D Fake: 99.125%
Loss D: 0.800
Loss G: 1.0805 (1.1473) Acc G: 0.167%
LR: 2.000e-04
TRAIN Iteration: [  81/226]
Loss D Real: 0.2938 (0.3275) Acc D Real: 74.817%
Loss D Fake: 0.4231 (0.3979) Acc D Fake: 99.136%
Loss D: 0.717
Loss G: 1.0761 (1.1465) Acc G: 0.165%
LR: 2.000e-04
TRAIN Iteration: [  82/226]
Loss D Real: 0.3059 (0.3272) Acc D Real: 74.804%
Loss D Fake: 0.4248 (0.3983) Acc D Fake: 99.146%
Loss D: 0.731
Loss G: 1.0735 (1.1456) Acc G: 0.163%
LR: 2.000e-04
TRAIN Iteration: [  83/226]
Loss D Real: 0.3964 (0.3281) Acc D Real: 74.688%
Loss D Fake: 0.4258 (0.3986) Acc D Fake: 99.157%
Loss D: 0.822
Loss G: 1.0718 (1.1447) Acc G: 0.161%
LR: 2.000e-04
TRAIN Iteration: [  84/226]
Loss D Real: 0.3353 (0.3281) Acc D Real: 74.641%
Loss D Fake: 0.4264 (0.3989) Acc D Fake: 99.167%
Loss D: 0.762
Loss G: 1.0710 (1.1438) Acc G: 0.159%
LR: 2.000e-04
TRAIN Iteration: [  85/226]
Loss D Real: 0.3683 (0.3286) Acc D Real: 74.557%
Loss D Fake: 0.4270 (0.3993) Acc D Fake: 99.176%
Loss D: 0.795
Loss G: 1.0691 (1.1429) Acc G: 0.157%
LR: 2.000e-04
TRAIN Iteration: [  86/226]
Loss D Real: 0.1871 (0.3270) Acc D Real: 74.682%
Loss D Fake: 0.4277 (0.3996) Acc D Fake: 99.186%
Loss D: 0.615
Loss G: 1.0688 (1.1421) Acc G: 0.155%
LR: 2.000e-04
TRAIN Iteration: [  87/226]
Loss D Real: 0.3062 (0.3267) Acc D Real: 74.665%
Loss D Fake: 0.4277 (0.3999) Acc D Fake: 99.195%
Loss D: 0.734
Loss G: 1.0686 (1.1412) Acc G: 0.153%
LR: 2.000e-04
TRAIN Iteration: [  88/226]
Loss D Real: 0.3755 (0.3273) Acc D Real: 74.581%
Loss D Fake: 0.4281 (0.4002) Acc D Fake: 99.205%
Loss D: 0.804
Loss G: 1.0671 (1.1404) Acc G: 0.152%
LR: 2.000e-04
TRAIN Iteration: [  89/226]
Loss D Real: 0.3497 (0.3275) Acc D Real: 74.538%
Loss D Fake: 0.4290 (0.4006) Acc D Fake: 99.213%
Loss D: 0.779
Loss G: 1.0649 (1.1395) Acc G: 0.150%
LR: 2.000e-04
TRAIN Iteration: [  90/226]
Loss D Real: 0.4879 (0.3293) Acc D Real: 74.330%
Loss D Fake: 0.4306 (0.4009) Acc D Fake: 99.222%
Loss D: 0.918
Loss G: 1.0604 (1.1386) Acc G: 0.148%
LR: 2.000e-04
TRAIN Iteration: [  91/226]
Loss D Real: 0.3745 (0.3298) Acc D Real: 74.253%
Loss D Fake: 0.4334 (0.4013) Acc D Fake: 99.231%
Loss D: 0.808
Loss G: 1.0543 (1.1377) Acc G: 0.147%
LR: 2.000e-04
TRAIN Iteration: [  92/226]
Loss D Real: 0.4509 (0.3311) Acc D Real: 74.087%
Loss D Fake: 0.4372 (0.4016) Acc D Fake: 99.239%
Loss D: 0.888
Loss G: 1.0454 (1.1367) Acc G: 0.145%
LR: 2.000e-04
TRAIN Iteration: [  93/226]
Loss D Real: 0.3083 (0.3309) Acc D Real: 74.075%
Loss D Fake: 0.4421 (0.4021) Acc D Fake: 99.247%
Loss D: 0.750
Loss G: 1.0374 (1.1356) Acc G: 0.143%
LR: 2.000e-04
TRAIN Iteration: [  94/226]
Loss D Real: 0.3536 (0.3311) Acc D Real: 74.009%
Loss D Fake: 0.4461 (0.4025) Acc D Fake: 99.255%
Loss D: 0.800
Loss G: 1.0294 (1.1345) Acc G: 0.142%
LR: 2.000e-04
TRAIN Iteration: [  95/226]
Loss D Real: 0.2716 (0.3305) Acc D Real: 74.036%
Loss D Fake: 0.4497 (0.4030) Acc D Fake: 99.263%
Loss D: 0.721
Loss G: 1.0263 (1.1334) Acc G: 0.140%
LR: 2.000e-04
TRAIN Iteration: [  96/226]
Loss D Real: 0.2641 (0.3298) Acc D Real: 74.065%
Loss D Fake: 0.4498 (0.4035) Acc D Fake: 99.271%
Loss D: 0.714
Loss G: 1.0280 (1.1323) Acc G: 0.139%
LR: 2.000e-04
TRAIN Iteration: [  97/226]
Loss D Real: 0.2948 (0.3295) Acc D Real: 74.060%
Loss D Fake: 0.4481 (0.4040) Acc D Fake: 99.278%
Loss D: 0.743
Loss G: 1.0314 (1.1312) Acc G: 0.137%
LR: 2.000e-04
TRAIN Iteration: [  98/226]
Loss D Real: 0.2998 (0.3291) Acc D Real: 74.055%
Loss D Fake: 0.4465 (0.4044) Acc D Fake: 99.286%
Loss D: 0.746
Loss G: 1.0332 (1.1302) Acc G: 0.136%
LR: 2.000e-04
TRAIN Iteration: [  99/226]
Loss D Real: 0.2464 (0.3283) Acc D Real: 74.104%
Loss D Fake: 0.4457 (0.4048) Acc D Fake: 99.293%
Loss D: 0.692
Loss G: 1.0350 (1.1293) Acc G: 0.135%
LR: 2.000e-04
TRAIN Iteration: [ 100/226]
Loss D Real: 0.3022 (0.3281) Acc D Real: 74.100%
Loss D Fake: 0.4447 (0.4052) Acc D Fake: 99.300%
Loss D: 0.747
Loss G: 1.0363 (1.1283) Acc G: 0.133%
LR: 2.000e-04
TRAIN Iteration: [ 101/226]
Loss D Real: 0.3936 (0.3287) Acc D Real: 74.006%
Loss D Fake: 0.4445 (0.4056) Acc D Fake: 99.307%
Loss D: 0.838
Loss G: 1.0359 (1.1274) Acc G: 0.132%
LR: 2.000e-04
TRAIN Iteration: [ 102/226]
Loss D Real: 0.4152 (0.3295) Acc D Real: 73.882%
Loss D Fake: 0.4453 (0.4060) Acc D Fake: 99.314%
Loss D: 0.860
Loss G: 1.0333 (1.1265) Acc G: 0.131%
LR: 2.000e-04
TRAIN Iteration: [ 103/226]
Loss D Real: 0.4481 (0.3307) Acc D Real: 73.732%
Loss D Fake: 0.4472 (0.4064) Acc D Fake: 99.320%
Loss D: 0.895
Loss G: 1.0287 (1.1256) Acc G: 0.129%
LR: 2.000e-04
TRAIN Iteration: [ 104/226]
Loss D Real: 0.2854 (0.3303) Acc D Real: 73.740%
Loss D Fake: 0.4498 (0.4068) Acc D Fake: 99.327%
Loss D: 0.735
Loss G: 1.0246 (1.1246) Acc G: 0.128%
LR: 2.000e-04
TRAIN Iteration: [ 105/226]
Loss D Real: 0.3012 (0.3300) Acc D Real: 73.736%
Loss D Fake: 0.4518 (0.4073) Acc D Fake: 99.333%
Loss D: 0.753
Loss G: 1.0215 (1.1236) Acc G: 0.127%
LR: 2.000e-04
TRAIN Iteration: [ 106/226]
Loss D Real: 0.2867 (0.3296) Acc D Real: 73.738%
Loss D Fake: 0.4532 (0.4077) Acc D Fake: 99.340%
Loss D: 0.740
Loss G: 1.0195 (1.1226) Acc G: 0.126%
LR: 2.000e-04
TRAIN Iteration: [ 107/226]
Loss D Real: 0.2122 (0.3285) Acc D Real: 73.822%
Loss D Fake: 0.4537 (0.4081) Acc D Fake: 99.346%
Loss D: 0.666
Loss G: 1.0199 (1.1217) Acc G: 0.125%
LR: 2.000e-04
TRAIN Iteration: [ 108/226]
Loss D Real: 0.3477 (0.3287) Acc D Real: 73.764%
Loss D Fake: 0.4534 (0.4085) Acc D Fake: 99.352%
Loss D: 0.801
Loss G: 1.0200 (1.1207) Acc G: 0.123%
LR: 2.000e-04
TRAIN Iteration: [ 109/226]
Loss D Real: 0.3360 (0.3287) Acc D Real: 73.732%
Loss D Fake: 0.4536 (0.4090) Acc D Fake: 99.358%
Loss D: 0.790
Loss G: 1.0192 (1.1198) Acc G: 0.122%
LR: 2.000e-04
TRAIN Iteration: [ 110/226]
Loss D Real: 0.3274 (0.3287) Acc D Real: 73.704%
Loss D Fake: 0.4543 (0.4094) Acc D Fake: 99.364%
Loss D: 0.782
Loss G: 1.0174 (1.1189) Acc G: 0.121%
LR: 2.000e-04
TRAIN Iteration: [ 111/226]
Loss D Real: 0.3747 (0.3291) Acc D Real: 73.623%
Loss D Fake: 0.4558 (0.4098) Acc D Fake: 99.369%
Loss D: 0.831
Loss G: 1.0136 (1.1179) Acc G: 0.120%
LR: 2.000e-04
TRAIN Iteration: [ 112/226]
Loss D Real: 0.3479 (0.3293) Acc D Real: 73.569%
Loss D Fake: 0.4585 (0.4102) Acc D Fake: 99.375%
Loss D: 0.806
Loss G: 1.0086 (1.1169) Acc G: 0.119%
LR: 2.000e-04
TRAIN Iteration: [ 113/226]
Loss D Real: 0.4159 (0.3301) Acc D Real: 73.457%
Loss D Fake: 0.4618 (0.4107) Acc D Fake: 99.381%
Loss D: 0.878
Loss G: 1.0027 (1.1159) Acc G: 0.118%
LR: 2.000e-04
TRAIN Iteration: [ 114/226]
Loss D Real: 0.3410 (0.3302) Acc D Real: 73.410%
Loss D Fake: 0.4643 (0.4111) Acc D Fake: 99.386%
Loss D: 0.805
Loss G: 1.0021 (1.1149) Acc G: 0.117%
LR: 2.000e-04
TRAIN Iteration: [ 115/226]
Loss D Real: 0.4341 (0.3311) Acc D Real: 73.281%
Loss D Fake: 0.4636 (0.4116) Acc D Fake: 99.391%
Loss D: 0.898
Loss G: 1.0019 (1.1139) Acc G: 0.116%
LR: 2.000e-04
TRAIN Iteration: [ 116/226]
Loss D Real: 0.1776 (0.3297) Acc D Real: 73.384%
Loss D Fake: 0.4631 (0.4120) Acc D Fake: 99.397%
Loss D: 0.641
Loss G: 1.0044 (1.1130) Acc G: 0.115%
LR: 2.000e-04
TRAIN Iteration: [ 117/226]
Loss D Real: 0.2627 (0.3292) Acc D Real: 73.413%
Loss D Fake: 0.4612 (0.4125) Acc D Fake: 99.402%
Loss D: 0.724
Loss G: 1.0071 (1.1121) Acc G: 0.114%
LR: 2.000e-04
TRAIN Iteration: [ 118/226]
Loss D Real: 0.2842 (0.3288) Acc D Real: 73.417%
Loss D Fake: 0.4600 (0.4129) Acc D Fake: 99.407%
Loss D: 0.744
Loss G: 1.0086 (1.1112) Acc G: 0.113%
LR: 2.000e-04
TRAIN Iteration: [ 119/226]
Loss D Real: 0.4029 (0.3294) Acc D Real: 73.319%
Loss D Fake: 0.4598 (0.4133) Acc D Fake: 99.412%
Loss D: 0.863
Loss G: 1.0075 (1.1103) Acc G: 0.112%
LR: 2.000e-04
TRAIN Iteration: [ 120/226]
Loss D Real: 0.2832 (0.3290) Acc D Real: 73.326%
Loss D Fake: 0.4609 (0.4137) Acc D Fake: 99.417%
Loss D: 0.744
Loss G: 1.0052 (1.1095) Acc G: 0.111%
LR: 2.000e-04
TRAIN Iteration: [ 121/226]
Loss D Real: 0.2022 (0.3280) Acc D Real: 73.400%
Loss D Fake: 0.4621 (0.4141) Acc D Fake: 99.421%
Loss D: 0.664
Loss G: 1.0039 (1.1086) Acc G: 0.110%
LR: 2.000e-04
TRAIN Iteration: [ 122/226]
Loss D Real: 0.4881 (0.3293) Acc D Real: 73.230%
Loss D Fake: 0.4636 (0.4145) Acc D Fake: 99.426%
Loss D: 0.952
Loss G: 0.9979 (1.1077) Acc G: 0.109%
LR: 2.000e-04
TRAIN Iteration: [ 123/226]
Loss D Real: 0.2813 (0.3289) Acc D Real: 73.239%
Loss D Fake: 0.4679 (0.4149) Acc D Fake: 99.431%
Loss D: 0.749
Loss G: 0.9946 (1.1068) Acc G: 0.108%
LR: 2.000e-04
TRAIN Iteration: [ 124/226]
Loss D Real: 0.1662 (0.3276) Acc D Real: 73.343%
Loss D Fake: 0.4667 (0.4153) Acc D Fake: 99.435%
Loss D: 0.633
Loss G: 1.0006 (1.1059) Acc G: 0.108%
LR: 2.000e-04
TRAIN Iteration: [ 125/226]
Loss D Real: 0.3199 (0.3275) Acc D Real: 73.323%
Loss D Fake: 0.4626 (0.4157) Acc D Fake: 99.440%
Loss D: 0.783
Loss G: 1.0058 (1.1051) Acc G: 0.107%
LR: 2.000e-04
TRAIN Iteration: [ 126/226]
Loss D Real: 0.5257 (0.3291) Acc D Real: 73.130%
Loss D Fake: 0.4610 (0.4161) Acc D Fake: 99.444%
Loss D: 0.987
Loss G: 1.0056 (1.1043) Acc G: 0.106%
LR: 2.000e-04
TRAIN Iteration: [ 127/226]
Loss D Real: 0.1948 (0.3280) Acc D Real: 73.211%
Loss D Fake: 0.4615 (0.4164) Acc D Fake: 99.449%
Loss D: 0.656
Loss G: 1.0058 (1.1035) Acc G: 0.105%
LR: 2.000e-04
TRAIN Iteration: [ 128/226]
Loss D Real: 0.2610 (0.3275) Acc D Real: 73.236%
Loss D Fake: 0.4611 (0.4168) Acc D Fake: 99.453%
Loss D: 0.722
Loss G: 1.0065 (1.1028) Acc G: 0.104%
LR: 2.000e-04
TRAIN Iteration: [ 129/226]
Loss D Real: 0.3487 (0.3277) Acc D Real: 73.208%
Loss D Fake: 0.4609 (0.4171) Acc D Fake: 99.457%
Loss D: 0.810
Loss G: 1.0064 (1.1020) Acc G: 0.103%
LR: 2.000e-04
TRAIN Iteration: [ 130/226]
Loss D Real: 0.3765 (0.3281) Acc D Real: 73.139%
Loss D Fake: 0.4615 (0.4174) Acc D Fake: 99.462%
Loss D: 0.838
Loss G: 1.0038 (1.1013) Acc G: 0.103%
LR: 2.000e-04
TRAIN Iteration: [ 131/226]
Loss D Real: 0.3379 (0.3281) Acc D Real: 73.100%
Loss D Fake: 0.4637 (0.4178) Acc D Fake: 99.466%
Loss D: 0.802
Loss G: 0.9995 (1.1005) Acc G: 0.102%
LR: 2.000e-04
TRAIN Iteration: [ 132/226]
Loss D Real: 0.2079 (0.3272) Acc D Real: 73.174%
Loss D Fake: 0.4657 (0.4182) Acc D Fake: 99.470%
Loss D: 0.674
Loss G: 0.9992 (1.0997) Acc G: 0.101%
LR: 2.000e-04
TRAIN Iteration: [ 133/226]
Loss D Real: 0.3501 (0.3274) Acc D Real: 73.129%
Loss D Fake: 0.4650 (0.4185) Acc D Fake: 99.474%
Loss D: 0.815
Loss G: 1.0001 (1.0990) Acc G: 0.100%
LR: 2.000e-04
TRAIN Iteration: [ 134/226]
Loss D Real: 0.2799 (0.3270) Acc D Real: 73.135%
Loss D Fake: 0.4642 (0.4189) Acc D Fake: 99.478%
Loss D: 0.744
Loss G: 1.0023 (1.0983) Acc G: 0.100%
LR: 2.000e-04
Loss D Real: 0.2494 (0.3265) Acc D Real: 73.164%
Loss D Fake: 0.4625 (0.4192) Acc D Fake: 99.481%
Loss D: 0.712
Loss G: 1.0059 (1.0976) Acc G: 0.099%
LR: 2.000e-04
TRAIN Iteration: [ 136/226]
Loss D Real: 0.3409 (0.3266) Acc D Real: 73.127%
Loss D Fake: 0.4606 (0.4195) Acc D Fake: 99.485%
Loss D: 0.801
Loss G: 1.0082 (1.0969) Acc G: 0.098%
LR: 2.000e-04
TRAIN Iteration: [ 137/226]
Loss D Real: 0.1068 (0.3250) Acc D Real: 73.267%
Loss D Fake: 0.4591 (0.4198) Acc D Fake: 99.489%
Loss D: 0.566
Loss G: 1.0124 (1.0963) Acc G: 0.097%
LR: 2.000e-04
TRAIN Iteration: [ 138/226]
Loss D Real: 0.2922 (0.3247) Acc D Real: 73.271%
Loss D Fake: 0.4567 (0.4200) Acc D Fake: 99.493%
Loss D: 0.749
Loss G: 1.0159 (1.0957) Acc G: 0.097%
LR: 2.000e-04
TRAIN Iteration: [ 139/226]
Loss D Real: 0.5018 (0.3260) Acc D Real: 73.119%
Loss D Fake: 0.4554 (0.4203) Acc D Fake: 99.496%
Loss D: 0.957
Loss G: 1.0164 (1.0952) Acc G: 0.096%
LR: 2.000e-04
TRAIN Iteration: [ 140/226]
Loss D Real: 0.3089 (0.3259) Acc D Real: 73.103%
Loss D Fake: 0.4558 (0.4205) Acc D Fake: 99.500%
Loss D: 0.765
Loss G: 1.0152 (1.0946) Acc G: 0.095%
LR: 2.000e-04
TRAIN Iteration: [ 141/226]
Loss D Real: 0.3372 (0.3260) Acc D Real: 73.080%
Loss D Fake: 0.4567 (0.4208) Acc D Fake: 99.504%
Loss D: 0.794
Loss G: 1.0136 (1.0940) Acc G: 0.095%
LR: 2.000e-04
TRAIN Iteration: [ 142/226]
Loss D Real: 0.2481 (0.3254) Acc D Real: 73.112%
Loss D Fake: 0.4574 (0.4211) Acc D Fake: 99.507%
Loss D: 0.705
Loss G: 1.0132 (1.0934) Acc G: 0.094%
LR: 2.000e-04
TRAIN Iteration: [ 143/226]
Loss D Real: 0.2235 (0.3247) Acc D Real: 73.160%
Loss D Fake: 0.4571 (0.4213) Acc D Fake: 99.510%
Loss D: 0.681
Loss G: 1.0144 (1.0929) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 144/226]
Loss D Real: 0.1960 (0.3238) Acc D Real: 73.228%
Loss D Fake: 0.4561 (0.4216) Acc D Fake: 99.514%
Loss D: 0.652
Loss G: 1.0169 (1.0924) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 145/226]
Loss D Real: 0.2913 (0.3236) Acc D Real: 73.235%
Loss D Fake: 0.4548 (0.4218) Acc D Fake: 99.517%
Loss D: 0.746
Loss G: 1.0186 (1.0919) Acc G: 0.092%
LR: 2.000e-04
TRAIN Iteration: [ 146/226]
Loss D Real: 0.2583 (0.3231) Acc D Real: 73.258%
Loss D Fake: 0.4540 (0.4220) Acc D Fake: 99.521%
Loss D: 0.712
Loss G: 1.0207 (1.0914) Acc G: 0.091%
LR: 2.000e-04
TRAIN Iteration: [ 147/226]
Loss D Real: 0.2297 (0.3225) Acc D Real: 73.299%
Loss D Fake: 0.4525 (0.4222) Acc D Fake: 99.524%
Loss D: 0.682
Loss G: 1.0244 (1.0909) Acc G: 0.091%
LR: 2.000e-04
TRAIN Iteration: [ 148/226]
Loss D Real: 0.2847 (0.3222) Acc D Real: 73.304%
Loss D Fake: 0.4502 (0.4224) Acc D Fake: 99.527%
Loss D: 0.735
Loss G: 1.0286 (1.0905) Acc G: 0.090%
LR: 2.000e-04
TRAIN Iteration: [ 149/226]
Loss D Real: 0.3206 (0.3222) Acc D Real: 73.291%
Loss D Fake: 0.4482 (0.4226) Acc D Fake: 99.530%
Loss D: 0.769
Loss G: 1.0316 (1.0901) Acc G: 0.089%
LR: 2.000e-04
TRAIN Iteration: [ 150/226]
Loss D Real: 0.3456 (0.3224) Acc D Real: 73.253%
Loss D Fake: 0.4471 (0.4227) Acc D Fake: 99.533%
Loss D: 0.793
Loss G: 1.0325 (1.0897) Acc G: 0.089%
LR: 2.000e-04
TRAIN Iteration: [ 151/226]
Loss D Real: 0.2991 (0.3222) Acc D Real: 73.251%
Loss D Fake: 0.4470 (0.4229) Acc D Fake: 99.536%
Loss D: 0.746
Loss G: 1.0330 (1.0893) Acc G: 0.088%
LR: 2.000e-04
TRAIN Iteration: [ 152/226]
Loss D Real: 0.2663 (0.3219) Acc D Real: 73.271%
Loss D Fake: 0.4467 (0.4231) Acc D Fake: 99.539%
Loss D: 0.713
Loss G: 1.0337 (1.0890) Acc G: 0.088%
LR: 2.000e-04
TRAIN Iteration: [ 153/226]
Loss D Real: 0.1715 (0.3209) Acc D Real: 73.355%
Loss D Fake: 0.4459 (0.4232) Acc D Fake: 99.542%
Loss D: 0.617
Loss G: 1.0365 (1.0886) Acc G: 0.087%
LR: 2.000e-04
TRAIN Iteration: [ 154/226]
Loss D Real: 0.2946 (0.3207) Acc D Real: 73.359%
Loss D Fake: 0.4441 (0.4233) Acc D Fake: 99.545%
Loss D: 0.739
Loss G: 1.0403 (1.0883) Acc G: 0.087%
LR: 2.000e-04
TRAIN Iteration: [ 155/226]
Loss D Real: 0.2885 (0.3205) Acc D Real: 73.362%
Loss D Fake: 0.4423 (0.4235) Acc D Fake: 99.548%
Loss D: 0.731
Loss G: 1.0428 (1.0880) Acc G: 0.086%
LR: 2.000e-04
TRAIN Iteration: [ 156/226]
Loss D Real: 0.3633 (0.3208) Acc D Real: 73.321%
Loss D Fake: 0.4414 (0.4236) Acc D Fake: 99.551%
Loss D: 0.805
Loss G: 1.0439 (1.0877) Acc G: 0.085%
LR: 2.000e-04
TRAIN Iteration: [ 157/226]
Loss D Real: 0.4182 (0.3214) Acc D Real: 73.244%
Loss D Fake: 0.4414 (0.4237) Acc D Fake: 99.554%
Loss D: 0.860
Loss G: 1.0422 (1.0874) Acc G: 0.085%
LR: 2.000e-04
TRAIN Iteration: [ 158/226]
Loss D Real: 0.3224 (0.3214) Acc D Real: 73.226%
Loss D Fake: 0.4429 (0.4238) Acc D Fake: 99.557%
Loss D: 0.765
Loss G: 1.0392 (1.0871) Acc G: 0.084%
LR: 2.000e-04
TRAIN Iteration: [ 159/226]
Loss D Real: 0.2304 (0.3208) Acc D Real: 73.267%
Loss D Fake: 0.4447 (0.4239) Acc D Fake: 99.560%
Loss D: 0.675
Loss G: 1.0367 (1.0868) Acc G: 0.084%
LR: 2.000e-04
TRAIN Iteration: [ 160/226]
Loss D Real: 0.4326 (0.3215) Acc D Real: 73.179%
Loss D Fake: 0.4462 (0.4241) Acc D Fake: 99.562%
Loss D: 0.879
Loss G: 1.0342 (1.0865) Acc G: 0.083%
LR: 2.000e-04
TRAIN Iteration: [ 161/226]
Loss D Real: 0.2386 (0.3210) Acc D Real: 73.221%
Loss D Fake: 0.4461 (0.4242) Acc D Fake: 99.565%
Loss D: 0.685
Loss G: 1.0375 (1.0862) Acc G: 0.083%
LR: 2.000e-04
TRAIN Iteration: [ 162/226]
Loss D Real: 0.4383 (0.3217) Acc D Real: 73.138%
Loss D Fake: 0.4440 (0.4243) Acc D Fake: 99.568%
Loss D: 0.882
Loss G: 1.0391 (1.0859) Acc G: 0.082%
LR: 2.000e-04
TRAIN Iteration: [ 163/226]
Loss D Real: 0.1184 (0.3205) Acc D Real: 73.252%
Loss D Fake: 0.4434 (0.4245) Acc D Fake: 99.571%
Loss D: 0.562
Loss G: 1.0412 (1.0856) Acc G: 0.082%
LR: 2.000e-04
TRAIN Iteration: [ 164/226]
Loss D Real: 0.2290 (0.3199) Acc D Real: 73.294%
Loss D Fake: 0.4420 (0.4246) Acc D Fake: 99.573%
Loss D: 0.671
Loss G: 1.0436 (1.0854) Acc G: 0.081%
LR: 2.000e-04
TRAIN Iteration: [ 165/226]
Loss D Real: 0.2922 (0.3198) Acc D Real: 73.299%
Loss D Fake: 0.4409 (0.4247) Acc D Fake: 99.576%
Loss D: 0.733
Loss G: 1.0453 (1.0851) Acc G: 0.081%
LR: 2.000e-04
TRAIN Iteration: [ 166/226]
Loss D Real: 0.2531 (0.3194) Acc D Real: 73.324%
Loss D Fake: 0.4403 (0.4248) Acc D Fake: 99.578%
Loss D: 0.693
Loss G: 1.0458 (1.0849) Acc G: 0.080%
LR: 2.000e-04
TRAIN Iteration: [ 167/226]
Loss D Real: 0.3200 (0.3194) Acc D Real: 73.310%
Loss D Fake: 0.4407 (0.4249) Acc D Fake: 99.581%
Loss D: 0.761
Loss G: 1.0433 (1.0846) Acc G: 0.080%
LR: 2.000e-04
TRAIN Iteration: [ 168/226]
Loss D Real: 0.3593 (0.3196) Acc D Real: 73.275%
Loss D Fake: 0.4439 (0.4250) Acc D Fake: 99.583%
Loss D: 0.803
Loss G: 1.0338 (1.0843) Acc G: 0.079%
LR: 2.000e-04
TRAIN Iteration: [ 169/226]
Loss D Real: 0.2682 (0.3193) Acc D Real: 73.295%
Loss D Fake: 0.4514 (0.4251) Acc D Fake: 99.586%
Loss D: 0.720
Loss G: 1.0348 (1.0840) Acc G: 0.079%
LR: 2.000e-04
TRAIN Iteration: [ 170/226]
Loss D Real: 0.4015 (0.3198) Acc D Real: 73.227%
Loss D Fake: 0.4444 (0.4252) Acc D Fake: 99.588%
Loss D: 0.846
Loss G: 1.0408 (1.0838) Acc G: 0.078%
LR: 2.000e-04
TRAIN Iteration: [ 171/226]
Loss D Real: 0.2601 (0.3194) Acc D Real: 73.248%
Loss D Fake: 0.4422 (0.4253) Acc D Fake: 99.591%
Loss D: 0.702
Loss G: 1.0437 (1.0836) Acc G: 0.078%
LR: 2.000e-04
TRAIN Iteration: [ 172/226]
Loss D Real: 0.3704 (0.3197) Acc D Real: 73.203%
Loss D Fake: 0.4413 (0.4254) Acc D Fake: 99.593%
Loss D: 0.812
Loss G: 1.0439 (1.0833) Acc G: 0.078%
LR: 2.000e-04
TRAIN Iteration: [ 173/226]
Loss D Real: 0.2915 (0.3196) Acc D Real: 73.206%
Loss D Fake: 0.4416 (0.4255) Acc D Fake: 99.595%
Loss D: 0.733
Loss G: 1.0434 (1.0831) Acc G: 0.077%
LR: 2.000e-04
TRAIN Iteration: [ 174/226]
Loss D Real: 0.2874 (0.3194) Acc D Real: 73.206%
Loss D Fake: 0.4418 (0.4256) Acc D Fake: 99.598%
Loss D: 0.729
Loss G: 1.0427 (1.0829) Acc G: 0.077%
LR: 2.000e-04
TRAIN Iteration: [ 175/226]
Loss D Real: 0.3476 (0.3195) Acc D Real: 73.173%
Loss D Fake: 0.4424 (0.4257) Acc D Fake: 99.600%
Loss D: 0.790
Loss G: 1.0413 (1.0826) Acc G: 0.076%
LR: 2.000e-04
TRAIN Iteration: [ 176/226]
Loss D Real: 0.3452 (0.3197) Acc D Real: 73.143%
Loss D Fake: 0.4434 (0.4258) Acc D Fake: 99.602%
Loss D: 0.789
Loss G: 1.0386 (1.0824) Acc G: 0.076%
LR: 2.000e-04
TRAIN Iteration: [ 177/226]
Loss D Real: 0.2102 (0.3191) Acc D Real: 73.197%
Loss D Fake: 0.4450 (0.4259) Acc D Fake: 99.605%
Loss D: 0.655
Loss G: 1.0359 (1.0821) Acc G: 0.075%
LR: 2.000e-04
TRAIN Iteration: [ 178/226]
Loss D Real: 0.2314 (0.3186) Acc D Real: 73.235%
Loss D Fake: 0.4463 (0.4260) Acc D Fake: 99.607%
Loss D: 0.678
Loss G: 1.0350 (1.0818) Acc G: 0.075%
LR: 2.000e-04
TRAIN Iteration: [ 179/226]
Loss D Real: 0.3182 (0.3186) Acc D Real: 73.218%
Loss D Fake: 0.4459 (0.4261) Acc D Fake: 99.609%
Loss D: 0.764
Loss G: 1.0378 (1.0816) Acc G: 0.074%
LR: 2.000e-04
TRAIN Iteration: [ 180/226]
Loss D Real: 0.2542 (0.3182) Acc D Real: 73.242%
Loss D Fake: 0.4437 (0.4262) Acc D Fake: 99.611%
Loss D: 0.698
Loss G: 1.0417 (1.0814) Acc G: 0.074%
LR: 2.000e-04
TRAIN Iteration: [ 181/226]
Loss D Real: 0.1881 (0.3175) Acc D Real: 73.299%
Loss D Fake: 0.4416 (0.4263) Acc D Fake: 99.613%
Loss D: 0.630
Loss G: 1.0461 (1.0812) Acc G: 0.074%
LR: 2.000e-04
TRAIN Iteration: [ 182/226]
Loss D Real: 0.2876 (0.3173) Acc D Real: 73.301%
Loss D Fake: 0.4396 (0.4264) Acc D Fake: 99.615%
Loss D: 0.727
Loss G: 1.0484 (1.0810) Acc G: 0.073%
LR: 2.000e-04
TRAIN Iteration: [ 183/226]
Loss D Real: 0.2474 (0.3170) Acc D Real: 73.333%
Loss D Fake: 0.4388 (0.4265) Acc D Fake: 99.617%
Loss D: 0.686
Loss G: 1.0501 (1.0808) Acc G: 0.073%
LR: 2.000e-04
TRAIN Iteration: [ 184/226]
Loss D Real: 0.2506 (0.3166) Acc D Real: 73.354%
Loss D Fake: 0.4385 (0.4265) Acc D Fake: 99.620%
Loss D: 0.689
Loss G: 1.0482 (1.0807) Acc G: 0.072%
LR: 2.000e-04
TRAIN Iteration: [ 185/226]
Loss D Real: 0.2263 (0.3161) Acc D Real: 73.389%
Loss D Fake: 0.4409 (0.4266) Acc D Fake: 99.622%
Loss D: 0.667
Loss G: 1.0486 (1.0805) Acc G: 0.072%
LR: 2.000e-04
TRAIN Iteration: [ 186/226]
Loss D Real: 0.2804 (0.3159) Acc D Real: 73.394%
Loss D Fake: 0.4377 (0.4267) Acc D Fake: 99.624%
Loss D: 0.718
Loss G: 1.0567 (1.0804) Acc G: 0.072%
LR: 2.000e-04
TRAIN Iteration: [ 187/226]
Loss D Real: 0.3594 (0.3161) Acc D Real: 73.364%
Loss D Fake: 0.4338 (0.4267) Acc D Fake: 99.626%
Loss D: 0.793
Loss G: 1.0604 (1.0803) Acc G: 0.071%
LR: 2.000e-04
TRAIN Iteration: [ 188/226]
Loss D Real: 0.3487 (0.3163) Acc D Real: 73.334%
Loss D Fake: 0.4329 (0.4267) Acc D Fake: 99.628%
Loss D: 0.782
Loss G: 1.0613 (1.0802) Acc G: 0.071%
LR: 2.000e-04
TRAIN Iteration: [ 189/226]
Loss D Real: 0.2190 (0.3158) Acc D Real: 73.378%
Loss D Fake: 0.4324 (0.4268) Acc D Fake: 99.630%
Loss D: 0.651
Loss G: 1.0632 (1.0801) Acc G: 0.071%
LR: 2.000e-04
TRAIN Iteration: [ 190/226]
Loss D Real: 0.4049 (0.3163) Acc D Real: 73.319%
Loss D Fake: 0.4321 (0.4268) Acc D Fake: 99.632%
Loss D: 0.837
Loss G: 1.0600 (1.0800) Acc G: 0.070%
LR: 2.000e-04
TRAIN Iteration: [ 191/226]
Loss D Real: 0.2931 (0.3162) Acc D Real: 73.323%
Loss D Fake: 0.4351 (0.4268) Acc D Fake: 99.634%
Loss D: 0.728
Loss G: 1.0542 (1.0798) Acc G: 0.070%
LR: 2.000e-04
TRAIN Iteration: [ 192/226]
Loss D Real: 0.2984 (0.3161) Acc D Real: 73.327%
Loss D Fake: 0.4377 (0.4269) Acc D Fake: 99.635%
Loss D: 0.736
Loss G: 1.0537 (1.0797) Acc G: 0.069%
LR: 2.000e-04
TRAIN Iteration: [ 193/226]
Loss D Real: 0.3047 (0.3160) Acc D Real: 73.324%
Loss D Fake: 0.4360 (0.4269) Acc D Fake: 99.637%
Loss D: 0.741
Loss G: 1.0569 (1.0796) Acc G: 0.069%
LR: 2.000e-04
TRAIN Iteration: [ 194/226]
Loss D Real: 0.3273 (0.3161) Acc D Real: 73.308%
Loss D Fake: 0.4346 (0.4270) Acc D Fake: 99.639%
Loss D: 0.762
Loss G: 1.0583 (1.0795) Acc G: 0.069%
LR: 2.000e-04
TRAIN Iteration: [ 195/226]
Loss D Real: 0.2464 (0.3157) Acc D Real: 73.330%
Loss D Fake: 0.4342 (0.4270) Acc D Fake: 99.641%
Loss D: 0.681
Loss G: 1.0588 (1.0794) Acc G: 0.068%
LR: 2.000e-04
TRAIN Iteration: [ 196/226]
Loss D Real: 0.2542 (0.3154) Acc D Real: 73.350%
Loss D Fake: 0.4340 (0.4271) Acc D Fake: 99.643%
Loss D: 0.688
Loss G: 1.0592 (1.0792) Acc G: 0.068%
LR: 2.000e-04
TRAIN Iteration: [ 197/226]
Loss D Real: 0.3397 (0.3155) Acc D Real: 73.322%
Loss D Fake: 0.4339 (0.4271) Acc D Fake: 99.645%
Loss D: 0.774
Loss G: 1.0586 (1.0791) Acc G: 0.068%
LR: 2.000e-04
TRAIN Iteration: [ 198/226]
Loss D Real: 0.3167 (0.3155) Acc D Real: 73.309%
Loss D Fake: 0.4345 (0.4271) Acc D Fake: 99.646%
Loss D: 0.751
Loss G: 1.0571 (1.0790) Acc G: 0.067%
LR: 2.000e-04
TRAIN Iteration: [ 199/226]
Loss D Real: 0.3690 (0.3158) Acc D Real: 73.267%
Loss D Fake: 0.4357 (0.4272) Acc D Fake: 99.648%
Loss D: 0.805
Loss G: 1.0536 (1.0789) Acc G: 0.067%
LR: 2.000e-04
TRAIN Iteration: [ 200/226]
Loss D Real: 0.3073 (0.3157) Acc D Real: 73.257%
Loss D Fake: 0.4380 (0.4272) Acc D Fake: 99.650%
Loss D: 0.745
Loss G: 1.0490 (1.0788) Acc G: 0.067%
LR: 2.000e-04
TRAIN Iteration: [ 201/226]
Loss D Real: 0.2521 (0.3154) Acc D Real: 73.280%
Loss D Fake: 0.4407 (0.4273) Acc D Fake: 99.652%
Loss D: 0.693
Loss G: 1.0431 (1.0786) Acc G: 0.066%
LR: 2.000e-04
TRAIN Iteration: [ 202/226]
Loss D Real: 0.4614 (0.3162) Acc D Real: 73.188%
Loss D Fake: 0.4458 (0.4274) Acc D Fake: 99.653%
Loss D: 0.907
Loss G: 1.0309 (1.0783) Acc G: 0.066%
LR: 2.000e-04
TRAIN Iteration: [ 203/226]
Loss D Real: 0.2696 (0.3159) Acc D Real: 73.191%
Loss D Fake: 0.4536 (0.4275) Acc D Fake: 99.655%
Loss D: 0.723
Loss G: 1.0323 (1.0781) Acc G: 0.066%
LR: 2.000e-04
TRAIN Iteration: [ 204/226]
Loss D Real: 0.2874 (0.3158) Acc D Real: 73.192%
Loss D Fake: 0.4468 (0.4276) Acc D Fake: 99.657%
Loss D: 0.734
Loss G: 1.0352 (1.0779) Acc G: 0.065%
LR: 2.000e-04
TRAIN Iteration: [ 205/226]
Loss D Real: 0.2184 (0.3153) Acc D Real: 73.226%
Loss D Fake: 0.4466 (0.4277) Acc D Fake: 99.659%
Loss D: 0.665
Loss G: 1.0349 (1.0777) Acc G: 0.065%
LR: 2.000e-04
TRAIN Iteration: [ 206/226]
Loss D Real: 0.3784 (0.3156) Acc D Real: 73.182%
Loss D Fake: 0.4472 (0.4278) Acc D Fake: 99.660%
Loss D: 0.826
Loss G: 1.0327 (1.0775) Acc G: 0.065%
LR: 2.000e-04
TRAIN Iteration: [ 207/226]
Loss D Real: 0.2701 (0.3154) Acc D Real: 73.193%
Loss D Fake: 0.4484 (0.4279) Acc D Fake: 99.662%
Loss D: 0.718
Loss G: 1.0311 (1.0773) Acc G: 0.064%
LR: 2.000e-04
TRAIN Iteration: [ 208/226]
Loss D Real: 0.2189 (0.3149) Acc D Real: 73.235%
Loss D Fake: 0.4490 (0.4280) Acc D Fake: 99.663%
Loss D: 0.668
Loss G: 1.0306 (1.0770) Acc G: 0.064%
LR: 2.000e-04
TRAIN Iteration: [ 209/226]
Loss D Real: 0.2108 (0.3144) Acc D Real: 73.278%
Loss D Fake: 0.4489 (0.4281) Acc D Fake: 99.665%
Loss D: 0.660
Loss G: 1.0320 (1.0768) Acc G: 0.064%
LR: 2.000e-04
TRAIN Iteration: [ 210/226]
Loss D Real: 0.2952 (0.3143) Acc D Real: 73.274%
Loss D Fake: 0.4484 (0.4282) Acc D Fake: 99.659%
Loss D: 0.744
Loss G: 1.0320 (1.0766) Acc G: 0.067%
LR: 2.000e-04
TRAIN Iteration: [ 211/226]
Loss D Real: 0.2489 (0.3140) Acc D Real: 73.289%
Loss D Fake: 0.4490 (0.4283) Acc D Fake: 99.652%
Loss D: 0.698
Loss G: 1.0320 (1.0764) Acc G: 0.070%
LR: 2.000e-04
TRAIN Iteration: [ 212/226]
Loss D Real: 0.3280 (0.3141) Acc D Real: 73.261%
Loss D Fake: 0.4490 (0.4284) Acc D Fake: 99.646%
Loss D: 0.777
Loss G: 1.0332 (1.0762) Acc G: 0.078%
LR: 2.000e-04
TRAIN Iteration: [ 213/226]
Loss D Real: 0.2278 (0.3137) Acc D Real: 73.296%
Loss D Fake: 0.4483 (0.4285) Acc D Fake: 99.640%
Loss D: 0.676
Loss G: 1.0336 (1.0760) Acc G: 0.085%
LR: 2.000e-04
TRAIN Iteration: [ 214/226]
Loss D Real: 0.3013 (0.3136) Acc D Real: 73.288%
Loss D Fake: 0.4476 (0.4286) Acc D Fake: 99.634%
Loss D: 0.749
Loss G: 1.0370 (1.0758) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 215/226]
Loss D Real: 0.1460 (0.3129) Acc D Real: 73.355%
Loss D Fake: 0.4454 (0.4287) Acc D Fake: 99.628%
Loss D: 0.591
Loss G: 1.0394 (1.0756) Acc G: 0.097%
LR: 2.000e-04
TRAIN Iteration: [ 216/226]
Loss D Real: 0.2696 (0.3127) Acc D Real: 73.359%
Loss D Fake: 0.4447 (0.4287) Acc D Fake: 99.622%
Loss D: 0.714
Loss G: 1.0396 (1.0755) Acc G: 0.096%
LR: 2.000e-04
TRAIN Iteration: [ 217/226]
Loss D Real: 0.2192 (0.3122) Acc D Real: 73.390%
Loss D Fake: 0.4454 (0.4288) Acc D Fake: 99.624%
Loss D: 0.665
Loss G: 1.0367 (1.0753) Acc G: 0.096%
LR: 2.000e-04
TRAIN Iteration: [ 218/226]
Loss D Real: 0.2572 (0.3120) Acc D Real: 73.406%
Loss D Fake: 0.4475 (0.4289) Acc D Fake: 99.626%
Loss D: 0.705
Loss G: 1.0322 (1.0751) Acc G: 0.096%
LR: 2.000e-04
TRAIN Iteration: [ 219/226]
Loss D Real: 0.2523 (0.3117) Acc D Real: 73.428%
Loss D Fake: 0.4501 (0.4290) Acc D Fake: 99.627%
Loss D: 0.702
Loss G: 1.0281 (1.0749) Acc G: 0.095%
LR: 2.000e-04
TRAIN Iteration: [ 220/226]
Loss D Real: 0.2249 (0.3113) Acc D Real: 73.466%
Loss D Fake: 0.4515 (0.4291) Acc D Fake: 99.629%
Loss D: 0.676
Loss G: 1.0293 (1.0747) Acc G: 0.095%
LR: 2.000e-04
TRAIN Iteration: [ 221/226]
Loss D Real: 0.2597 (0.3111) Acc D Real: 73.475%
Loss D Fake: 0.4491 (0.4292) Acc D Fake: 99.631%
Loss D: 0.709
Loss G: 1.0371 (1.0745) Acc G: 0.094%
LR: 2.000e-04
TRAIN Iteration: [ 222/226]
Loss D Real: 0.2996 (0.3110) Acc D Real: 73.476%
Loss D Fake: 0.4450 (0.4293) Acc D Fake: 99.632%
Loss D: 0.745
Loss G: 1.0419 (1.0744) Acc G: 0.094%
LR: 2.000e-04
TRAIN Iteration: [ 223/226]
Loss D Real: 0.2634 (0.3108) Acc D Real: 73.490%
Loss D Fake: 0.4429 (0.4293) Acc D Fake: 99.634%
Loss D: 0.706
Loss G: 1.0468 (1.0742) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 224/226]
Loss D Real: 0.2373 (0.3105) Acc D Real: 73.514%
Loss D Fake: 0.4414 (0.4294) Acc D Fake: 99.636%
Loss D: 0.679
Loss G: 1.0452 (1.0741) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 225/226]
Loss D Real: 0.2631 (0.3103) Acc D Real: 73.536%
Loss D Fake: 0.4456 (0.4294) Acc D Fake: 99.637%
Loss D: 0.709
Loss G: 1.0301 (1.0739) Acc G: 0.093%
LR: 2.000e-04
TRAIN Iteration: [ 226/226]
Loss D Real: 0.0508 (0.3091) Acc D Real: 73.563%
Loss D Fake: 0.4554 (0.4296) Acc D Fake: 99.638%
Loss D: 0.506
Loss G: 1.0457 (1.0738) Acc G: 0.092%
LR: 2.000e-04
Best Metric: At 2 Epoch Gen 1.035 Dis
MODEL TRAINING COMPLETED.
 BEST RESULT SAVED
 Batch: 1/121
121
Batch [1/121]: Anomaly Score: 494.884 label: 0.0
 Batch: 2/121
Batch [2/121]: Anomaly Score: 555.952 label: 0.0
 Batch: 3/121
Batch [3/121]: Anomaly Score: 528.206 label: 0.0
 Batch: 4/121
Batch [4/121]: Anomaly Score: 482.159 label: 0.0
 Batch: 5/121
Batch [5/121]: Anomaly Score: 530.188 label: 0.0
 Batch: 6/121
Batch [6/121]: Anomaly Score: 432.995 label: 0.0
 Batch: 7/121
Batch [7/121]: Anomaly Score: 449.532 label: 0.0
 Batch: 8/121
Batch [8/121]: Anomaly Score: 533.661 label: 0.0
 Batch: 9/121
Batch [9/121]: Anomaly Score: 491.360 label: 0.0
 Batch: 10/121
Batch [10/121]: Anomaly Score: 401.191 label: 0.0
 Batch: 11/121
Batch [11/121]: Anomaly Score: 442.560 label: 0.0
 Batch: 12/121
Batch [12/121]: Anomaly Score: 307.250 label: 0.0
 Batch: 13/121
Batch [13/121]: Anomaly Score: 523.051 label: 0.0
 Batch: 14/121
Batch [14/121]: Anomaly Score: 463.632 label: 0.0
 Batch: 15/121
Batch [15/121]: Anomaly Score: 514.645 label: 0.0
 Batch: 16/121
Batch [16/121]: Anomaly Score: 547.532 label: 0.0
 Batch: 17/121
Batch [17/121]: Anomaly Score: 505.176 label: 0.0
 Batch: 18/121
Batch [18/121]: Anomaly Score: 494.729 label: 0.0
 Batch: 19/121
Batch [19/121]: Anomaly Score: 536.249 label: 0.0
 Batch: 20/121
Batch [20/121]: Anomaly Score: 513.245 label: 0.0
 Batch: 21/121
Batch [21/121]: Anomaly Score: 561.380 label: 0.0
 Batch: 22/121
Batch [22/121]: Anomaly Score: 526.281 label: 0.0
 Batch: 23/121
Batch [23/121]: Anomaly Score: 541.963 label: 0.0
 Batch: 24/121
Batch [24/121]: Anomaly Score: 509.156 label: 0.0
 Batch: 25/121
Batch [25/121]: Anomaly Score: 478.568 label: 0.0
 Batch: 26/121
Batch [26/121]: Anomaly Score: 484.538 label: 0.0
 Batch: 27/121
Batch [27/121]: Anomaly Score: 425.827 label: 0.0
 Batch: 28/121
Batch [28/121]: Anomaly Score: 525.287 label: 0.0
 Batch: 29/121
Batch [29/121]: Anomaly Score: 499.947 label: 0.0
 Batch: 30/121
Batch [30/121]: Anomaly Score: 448.132 label: 0.0
 Batch: 31/121
Batch [31/121]: Anomaly Score: 330.229 label: 0.0
 Batch: 32/121
Batch [32/121]: Anomaly Score: 308.200 label: 0.0
 Batch: 33/121
Batch [33/121]: Anomaly Score: 329.719 label: 0.0
 Batch: 34/121
Batch [34/121]: Anomaly Score: 414.727 label: 0.0
 Batch: 35/121
Batch [35/121]: Anomaly Score: 251.332 label: 0.0
 Batch: 36/121
Batch [36/121]: Anomaly Score: 454.235 label: 0.0
 Batch: 37/121
Batch [37/121]: Anomaly Score: 421.298 label: 0.0
 Batch: 38/121
Batch [38/121]: Anomaly Score: 443.637 label: 0.0
 Batch: 39/121
Batch [39/121]: Anomaly Score: 318.165 label: 0.0
 Batch: 40/121
Batch [40/121]: Anomaly Score: 455.642 label: 0.0
 Batch: 41/121
Batch [41/121]: Anomaly Score: 388.874 label: 0.0
 Batch: 42/121
Batch [42/121]: Anomaly Score: 296.626 label: 0.0
 Batch: 43/121
Batch [43/121]: Anomaly Score: 313.542 label: 0.0
 Batch: 44/121
Batch [44/121]: Anomaly Score: 291.422 label: 0.0
 Batch: 45/121
Batch [45/121]: Anomaly Score: 325.217 label: 0.0
 Batch: 46/121
Batch [46/121]: Anomaly Score: 419.879 label: 0.0
 Batch: 47/121
Batch [47/121]: Anomaly Score: 306.683 label: 0.0
 Batch: 48/121
Batch [48/121]: Anomaly Score: 452.744 label: 0.0
 Batch: 49/121
Batch [49/121]: Anomaly Score: 453.138 label: 0.0
 Batch: 50/121
Batch [50/121]: Anomaly Score: 22.248 label: 0.0
 Batch: 51/121
Batch [51/121]: Anomaly Score: 298.693 label: 0.0
 Batch: 52/121
Batch [52/121]: Anomaly Score: 18.405 label: 0.0
 Batch: 53/121
Batch [53/121]: Anomaly Score: 330.998 label: 0.0
 Batch: 54/121
Batch [54/121]: Anomaly Score: 329.798 label: 0.0
 Batch: 55/121
Batch [55/121]: Anomaly Score: 276.983 label: 0.0
 Batch: 56/121
Batch [56/121]: Anomaly Score: 303.370 label: 0.0
 Batch: 57/121
Batch [57/121]: Anomaly Score: 323.731 label: 0.0
 Batch: 58/121
Batch [58/121]: Anomaly Score: 417.114 label: 0.0
 Batch: 59/121
Batch [59/121]: Anomaly Score: 314.047 label: 0.0
 Batch: 60/121
Batch [60/121]: Anomaly Score: 327.726 label: 1.0
 Batch: 61/121
Batch [61/121]: Anomaly Score: 317.624 label: 1.0
 Batch: 62/121
Batch [62/121]: Anomaly Score: 353.308 label: 1.0
 Batch: 63/121
Batch [63/121]: Anomaly Score: 398.043 label: 1.0
 Batch: 64/121
Batch [64/121]: Anomaly Score: 332.994 label: 1.0
 Batch: 65/121
Batch [65/121]: Anomaly Score: 310.543 label: 1.0
 Batch: 66/121
Batch [66/121]: Anomaly Score: 316.615 label: 1.0
 Batch: 67/121
Batch [67/121]: Anomaly Score: 329.425 label: 0.0
 Batch: 68/121
Batch [68/121]: Anomaly Score: 417.265 label: 0.0
 Batch: 69/121
Batch [69/121]: Anomaly Score: 276.549 label: 0.0
 Batch: 70/121
Batch [70/121]: Anomaly Score: 29.642 label: 0.0
 Batch: 71/121
Batch [71/121]: Anomaly Score: 343.709 label: 0.0
 Batch: 72/121
Batch [72/121]: Anomaly Score: 323.553 label: 0.0
 Batch: 73/121
Batch [73/121]: Anomaly Score: 241.083 label: 0.0
 Batch: 74/121
Batch [74/121]: Anomaly Score: 416.259 label: 0.0
 Batch: 75/121
Batch [75/121]: Anomaly Score: 198.693 label: 0.0
 Batch: 76/121
Batch [76/121]: Anomaly Score: 416.539 label: 0.0
 Batch: 77/121
Batch [77/121]: Anomaly Score: 454.413 label: 0.0
 Batch: 78/121
Batch [78/121]: Anomaly Score: 491.813 label: 0.0
 Batch: 79/121
Batch [79/121]: Anomaly Score: 403.503 label: 0.0
 Batch: 80/121
Batch [80/121]: Anomaly Score: 465.682 label: 0.0
 Batch: 81/121
Batch [81/121]: Anomaly Score: 425.459 label: 0.0
 Batch: 82/121
Batch [82/121]: Anomaly Score: 458.459 label: 0.0
 Batch: 83/121
Batch [83/121]: Anomaly Score: 422.689 label: 0.0
 Batch: 84/121
Batch [84/121]: Anomaly Score: 467.501 label: 0.0
 Batch: 85/121
Batch [85/121]: Anomaly Score: 539.567 label: 0.0
 Batch: 86/121
Batch [86/121]: Anomaly Score: 509.117 label: 0.0
 Batch: 87/121
Batch [87/121]: Anomaly Score: 447.865 label: 0.0
 Batch: 88/121
Batch [88/121]: Anomaly Score: 484.809 label: 0.0
 Batch: 89/121
Batch [89/121]: Anomaly Score: 384.960 label: 0.0
 Batch: 90/121
Batch [90/121]: Anomaly Score: 506.259 label: 0.0
 Batch: 91/121
Batch [91/121]: Anomaly Score: 526.638 label: 0.0
 Batch: 92/121
Batch [92/121]: Anomaly Score: 559.858 label: 0.0
 Batch: 93/121
Batch [93/121]: Anomaly Score: 565.029 label: 0.0
 Batch: 94/121
Batch [94/121]: Anomaly Score: 488.652 label: 0.0
 Batch: 95/121
Batch [95/121]: Anomaly Score: 529.387 label: 0.0
 Batch: 96/121
Batch [96/121]: Anomaly Score: 551.660 label: 0.0
 Batch: 97/121
Batch [97/121]: Anomaly Score: 524.752 label: 0.0
 Batch: 98/121
Batch [98/121]: Anomaly Score: 536.081 label: 0.0
 Batch: 99/121
Batch [99/121]: Anomaly Score: 535.730 label: 0.0
 Batch: 100/121
Batch [100/121]: Anomaly Score: 527.910 label: 1.0
 Batch: 101/121
Batch [101/121]: Anomaly Score: 558.316 label: 1.0
 Batch: 102/121
Batch [102/121]: Anomaly Score: 533.271 label: 1.0
 Batch: 103/121
Batch [103/121]: Anomaly Score: 568.990 label: 1.0
 Batch: 104/121
Batch [104/121]: Anomaly Score: 586.278 label: 1.0
 Batch: 105/121
Batch [105/121]: Anomaly Score: 542.381 label: 1.0
 Batch: 106/121
Batch [106/121]: Anomaly Score: 566.457 label: 1.0
 Batch: 107/121
Batch [107/121]: Anomaly Score: 567.564 label: 1.0
 Batch: 108/121
Batch [108/121]: Anomaly Score: 522.567 label: 0.0
 Batch: 109/121
Batch [109/121]: Anomaly Score: 554.493 label: 0.0
 Batch: 110/121
Batch [110/121]: Anomaly Score: 562.437 label: 0.0
 Batch: 111/121
Batch [111/121]: Anomaly Score: 557.726 label: 0.0
 Batch: 112/121
Batch [112/121]: Anomaly Score: 583.230 label: 0.0
 Batch: 113/121
Batch [113/121]: Anomaly Score: 559.707 label: 0.0
 Batch: 114/121
Batch [114/121]: Anomaly Score: 521.686 label: 0.0
 Batch: 115/121
Batch [115/121]: Anomaly Score: 571.510 label: 0.0
 Batch: 116/121
Batch [116/121]: Anomaly Score: 527.773 label: 0.0
 Batch: 117/121
Batch [117/121]: Anomaly Score: 541.820 label: 0.0
 Batch: 118/121
Batch [118/121]: Anomaly Score: 583.628 label: 0.0
 Batch: 119/121
Batch [119/121]: Anomaly Score: 527.652 label: 0.0
 Batch: 120/121
Batch [120/121]: Anomaly Score: 547.657 label: 0.0
 Batch: 121/121
Batch [121/121]: Anomaly Score: 551.434 label: 0.0